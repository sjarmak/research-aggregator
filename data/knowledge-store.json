{
  "papers": [
    {
      "id": "38058601",
      "bibcode": "2025arXiv251114426O",
      "title": "MiAD: Mirage Atom Diffusion for De Novo Crystal Generation",
      "authors": [
        "Okhotin, Andrey",
        "Nakhodnov, Maksim",
        "Kazeev, Nikita",
        "E Ustyuzhanin, Andrey",
        "Vetrov, Dmitry"
      ],
      "year": "2025",
      "abstract": "In recent years, diffusion-based models have demonstrated exceptional performance in searching for simultaneously stable, unique, and novel (S.U.N.) crystalline materials. However, most of these models don't have the ability to change the number of atoms in the crystal during the generation process, which limits the variability of model sampling trajectories. In this paper, we demonstrate the severity of this restriction and introduce a simple yet powerful technique, mirage infusion, which enables diffusion models to change the state of the atoms that make up the crystal from existent to non-existent (mirage) and vice versa. We show that this technique improves model quality by up to $\\times2.5$ compared to the same model without this modification. The resulting model, Mirage Atom Diffusion (MiAD), is an equivariant joint diffusion model for de novo crystal generation that is capable of altering the number of atoms during the generation process. MiAD achieves an $8.2\\%$ S.U.N. rate on the MP-20 dataset, which substantially exceeds existing state-of-the-art approaches. The source code can be found at \\href{https://github.com/andrey-okhotin/miad.git}{\\texttt{github.com/andrey-okhotin/miad}}.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251114426O/abstract",
      "keywords": [
        "Machine Learning",
        "Materials Science",
        "Artificial Intelligence",
        "Computational Physics"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.135Z",
      "publishedAt": "2025-11-23T15:01:32.135Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 11
    },
    {
      "id": "38059646",
      "bibcode": "2025arXiv251115408Z",
      "title": "NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework",
      "authors": [
        "Zhou, Shanlin",
        "Wang, Xinpeng",
        "Lian, Jianxun",
        "Liu, Zhenghao",
        "Lakshmanan, Laks V. S.",
        "Yi, Xiaoyuan",
        "Hao, Yongtao"
      ],
      "year": "2025",
      "abstract": "Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251115408Z/abstract",
      "keywords": [
        "Computation and Language",
        "Artificial Intelligence",
        "Information Retrieval",
        "Multiagent Systems",
        "Neural and Evolutionary Computing"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.136Z",
      "publishedAt": "2025-11-23T15:01:32.135Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "agents",
        "observability"
      ],
      "score": 12.499999989666005
    },
    {
      "id": "37621120",
      "bibcode": "2025arXiv251113057P",
      "title": "Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact",
      "authors": [
        "Pati, Satyanarayan"
      ],
      "year": "2025",
      "abstract": "Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the \"performance loss\" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective \"sweet spot,\" achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251113057P/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.136Z",
      "publishedAt": "2025-11-23T15:01:32.136Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability"
      ],
      "score": 11
    },
    {
      "id": "37498307",
      "bibcode": "2025arXiv251100444C",
      "title": "LIR: The First Workshop on Late Interaction and Multi Vector Retrieval @ ECIR 2026",
      "authors": [
        "Clavié, Benjamin",
        "Li, Xianming",
        "Chaffin, Antoine",
        "Khattab, Omar",
        "Aarsen, Tom",
        "Faysse, Manuel",
        "Li, Jing"
      ],
      "year": "2025",
      "abstract": "Late interaction retrieval methods, pioneered by ColBERT, have emerged as a powerful alternative to single-vector neural IR. By leveraging fine-grained, token-level representations, they have been demonstrated to deliver strong generalisation and robustness, particularly in out-of-domain settings. They have recently been shown to be particularly well-suited for novel use cases, such as reasoning-based or cross-modality retrieval. At the same time, these models pose significant challenges of efficiency, usability, and integrations into fully fledged systems; as well as the natural difficulties encountered while researching novel application domains. Recent years have seen rapid advances across many of these areas, but research efforts remain fragmented across communities and frequently exclude practitioners. The purpose of this workshop is to create an environment where all aspects of late interaction can be discussed, with a focus on early research explorations, real-world outcomes, and negative or puzzling results to be freely shared and discussed. The aim of LIR is to provide a highly-interactive environment for researchers from various backgrounds and practitioners to freely discuss their experience, fostering further collaboration.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251100444C/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.136Z",
      "publishedAt": "2025-11-23T15:01:32.136Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 12
    },
    {
      "id": "37500669",
      "bibcode": "2025arXiv251102805Y",
      "title": "MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning",
      "authors": [
        "Yuan, Qianhao",
        "Lou, Jie",
        "Li, Zichao",
        "Chen, Jiawei",
        "Lu, Yaojie",
        "Lin, Hongyu",
        "Sun, Le",
        "Zhang, Debing",
        "Han, Xianpei"
      ],
      "year": "2025",
      "abstract": "Typical search agents concatenate the entire interaction history into the LLM context, preserving information integrity but producing long, noisy contexts, resulting in high computation and memory costs. In contrast, using only the current turn avoids this overhead but discards essential information. This trade-off limits the scalability of search agents. To address this challenge, we propose MemSearcher, an agent workflow that iteratively maintains a compact memory and combines the current turn with it. At each turn, MemSearcher fuses the user's question with the memory to generate reasoning traces, perform search actions, and update memory to retain only information essential for solving the task. This design stabilizes context length across multi-turn interactions, improving efficiency without sacrificing accuracy. To optimize this workflow, we introduce multi-context GRPO, an end-to-end RL framework that jointly optimize reasoning, search strategies, and memory management of MemSearcher Agents. Specifically, multi-context GRPO samples groups of trajectories under different contexts and propagates trajectory-level advantages across all conversations within them. Trained on the same dataset as Search-R1, MemSearcher achieves significant improvements over strong baselines on seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on Qwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher even outperforms 7B-based baselines, demonstrating that striking a balance between information integrity and efficiency yields both higher accuracy and lower computational overhead. The code and models will be publicly available at https://github.com/icip-cas/MemSearcher",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251102805Y/abstract",
      "keywords": [
        "Computation and Language",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.136Z",
      "publishedAt": "2025-11-23T15:01:32.136Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents"
      ],
      "score": 15
    },
    {
      "id": "38074576",
      "bibcode": "2025arXiv251115778T",
      "title": "Balancing Natural Language Processing Accuracy and Normalisation in Extracting Medical Insights",
      "authors": [
        "Tworek, Paulina",
        "Bargieł, Miłosz",
        "Khan, Yousef",
        "Pełech-Pilichowski, Tomasz",
        "Mikołajczyk, Marek",
        "Lewandowski, Roman",
        "Sousa, Jose"
      ],
      "year": "2025",
      "abstract": "Extracting structured medical insights from unstructured clinical text using Natural Language Processing (NLP) remains an open challenge in healthcare, particularly in non-English contexts where resources are scarce. This study presents a comparative analysis of NLP low-compute rule-based methods and Large Language Models (LLMs) for information extraction from electronic health records (EHR) obtained from the Voivodeship Rehabilitation Hospital for Children in Ameryka, Poland. We evaluate both approaches by extracting patient demographics, clinical findings, and prescribed medications while examining the effects of lack of text normalisation and translation-induced information loss. Results demonstrate that rule-based methods provide higher accuracy in information retrieval tasks, particularly for age and sex extraction. However, LLMs offer greater adaptability and scalability, excelling in drug name recognition. The effectiveness of the LLMs was compared with texts originally in Polish and those translated into English, assessing the impact of translation. These findings highlight the trade-offs between accuracy, normalisation, and computational cost when deploying NLP in healthcare settings. We argue for hybrid approaches that combine the precision of rule-based systems with the adaptability of LLMs, offering a practical path toward more reliable and resource-efficient clinical NLP in real-world hospitals.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251115778T/abstract",
      "keywords": [
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.136Z",
      "publishedAt": "2025-11-23T15:01:32.136Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 12
    },
    {
      "id": "37608894",
      "bibcode": "2025arXiv251105549W",
      "title": "AGRAG: Advanced Graph-based Retrieval-Augmented Generation for LLMs",
      "authors": [
        "Wang, Yubo",
        "Li, Haoyang",
        "Teng, Fei",
        "Chen, Lei"
      ],
      "year": "2025",
      "abstract": "Graph-based retrieval-augmented generation (Graph-based RAG) has demonstrated significant potential in enhancing Large Language Models (LLMs) with structured knowledge. However, existing methods face three critical challenges: Inaccurate Graph Construction, caused by LLM hallucination; Poor Reasoning Ability, caused by failing to generate explicit reasons telling LLM why certain chunks were selected; and Inadequate Answering, which only partially answers the query due to the inadequate LLM reasoning, making their performance lag behind NaiveRAG on certain tasks. To address these issues, we propose AGRAG, an advanced graph-based retrieval-augmented generation framework. When constructing the graph, AGRAG substitutes the widely used LLM entity extraction method with a statistics-based method, avoiding hallucination and error propagation. When retrieval, AGRAG formulates the graph reasoning procedure as the Minimum Cost Maximum Influence (MCMI) subgraph generation problem, where we try to include more nodes with high influence score, but with less involving edge cost, to make the generated reasoning paths more comprehensive. We prove this problem to be NP-hard, and propose a greedy algorithm to solve it. The MCMI subgraph generated can serve as explicit reasoning paths to tell LLM why certain chunks were retrieved, thereby making the LLM better focus on the query-related part contents of the chunks, reducing the impact of noise, and improving AGRAG's reasoning ability. Furthermore, compared with the simple tree-structured reasoning paths, our MCMI subgraph can allow more complex graph structures, such as cycles, and improve the comprehensiveness of the generated reasoning paths.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251105549W/abstract",
      "keywords": [
        "Machine Learning",
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.136Z",
      "publishedAt": "2025-11-23T15:01:32.136Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 9
    },
    {
      "id": "38058442",
      "bibcode": "2025arXiv251114256L",
      "title": "PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models",
      "authors": [
        "Liu, Yu",
        "Lin, Xixun",
        "Shang, Yanmin",
        "Li, Yangxi",
        "Wang, Shi",
        "Cao, Yanan"
      ],
      "year": "2025",
      "abstract": "Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a \"Retrieve-Prioritize-Reason\" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251114256L/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.136Z",
      "publishedAt": "2025-11-23T15:01:32.136Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 10
    },
    {
      "id": "37611597",
      "bibcode": "2025arXiv251108181C",
      "title": "MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System",
      "authors": [
        "Cho, Seung Hwan",
        "Yang, Yujin",
        "Baeck, Danik",
        "Kim, Minjoo",
        "Kim, Young-Min",
        "Lee, Heejung",
        "Park, Sangjin"
      ],
      "year": "2025",
      "abstract": "Recommender systems (RS) are currently being studied to mitigate limitations during cold-start conditions by leveraging modality information or introducing Agent concepts based on the exceptional reasoning capabilities of Large Language Models (LLMs). Meanwhile, food and beverage recommender systems have traditionally used knowledge graph and ontology concepts due to the domain's unique data attributes and relationship characteristics. On this background, we propose MARC, a multimodal and multi-task cocktail recommender system based on Agentic Retrieval-Augmented Generation (RAG) utilizing graph database under cold-start conditions. The proposed system generates high-quality, contextually appropriate answers through two core processes: a task recognition router and a reflection process. The graph database was constructed by processing cocktail data from Kaggle, and its effectiveness was evaluated using 200 manually crafted questions. The evaluation used both LLM-as-a-judge and human evaluation to demonstrate that answers generated via the graph database outperformed those from a simple vector database in terms of quality. The code is available at https://github.com/diddbwls/cocktail_rec_agentrag",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251108181C/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.136Z",
      "publishedAt": "2025-11-23T15:01:32.136Z",
      "libraryId": "ads-ingest",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "retrieval",
        "agents"
      ],
      "score": 13
    },
    {
      "id": "37616541",
      "bibcode": "2025arXiv251110277B",
      "title": "Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer Hardware",
      "authors": [
        "Braas, Martin",
        "Esterle, Lukas"
      ],
      "year": "2025",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet their applicability to dialogue systems in computer games remains limited. This limitation arises from their substantial hardware requirements, latency constraints, and the necessity to maintain clearly defined knowledge boundaries within a game setting. In this paper, we propose a modular NPC dialogue system that leverages Small Language Models (SLMs), fine-tuned to encode specific NPC personas and integrated with runtime-swappable memory modules. These memory modules preserve character-specific conversational context and world knowledge, enabling expressive interactions and long-term memory without retraining or model reloading during gameplay. We comprehensively evaluate our system using three open-source SLMs: DistilGPT-2, TinyLlama-1.1B-Chat, and Mistral-7B-Instruct, trained on synthetic persona-aligned data and benchmarked on consumer-grade hardware. While our approach is motivated by applications in gaming, its modular design and persona-driven memory architecture hold significant potential for broader adoption in domains requiring expressive, scalable, and memory-rich conversational agents, such as virtual assistants, customer support bots, or interactive educational systems.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251110277B/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.136Z",
      "publishedAt": "2025-11-23T15:01:32.136Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents"
      ],
      "score": 15
    },
    {
      "id": "37621321",
      "bibcode": "2025arXiv251113271C",
      "title": "Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming",
      "authors": [
        "Chen, Rufeng",
        "Jiang, Shuaishuai",
        "Shen, Jiyun",
        "Moon, AJung",
        "Wei, Lili"
      ],
      "year": "2025",
      "abstract": "The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251113271C/abstract",
      "keywords": [
        "Software Engineering",
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.136Z",
      "publishedAt": "2025-11-23T15:01:32.136Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review"
      ],
      "score": 9.5
    },
    {
      "id": "37499863",
      "bibcode": "2025arXiv251102002J",
      "title": "InteracSPARQL: An Interactive System for SPARQL Query Refinement Using Natural Language Explanations",
      "authors": [
        "Jian, Xiangru",
        "Dong, Zhengyuan",
        "Tamer Özsu, M."
      ],
      "year": "2025",
      "abstract": "In recent years, querying semantic web data using SPARQL has remained challenging, especially for non-expert users, due to the language's complex syntax and the prerequisite of understanding intricate data structures. To address these challenges, we propose InteracSPARQL, an interactive SPARQL query generation and refinement system that leverages natural language explanations (NLEs) to enhance user comprehension and facilitate iterative query refinement. InteracSPARQL integrates LLMs with a rule-based approach to first produce structured explanations directly from SPARQL abstract syntax trees (ASTs), followed by LLM-based linguistic refinements. Users can interactively refine queries through direct feedback or LLM-driven self-refinement, enabling the correction of ambiguous or incorrect query components in real time. We evaluate InteracSPARQL on standard benchmarks, demonstrating significant improvements in query accuracy, explanation clarity, and overall user satisfaction compared to baseline approaches. Our experiments further highlight the effectiveness of combining rule-based methods with LLM-driven refinements to create more accessible and robust SPARQL interfaces.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251102002J/abstract",
      "keywords": [
        "Databases",
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.136Z",
      "publishedAt": "2025-11-23T15:01:32.136Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 9
    },
    {
      "id": "37500928",
      "bibcode": "2025arXiv251103051Z",
      "title": "No-Human in the Loop: Agentic Evaluation at Scale for Recommendation",
      "authors": [
        "Zhang, Tao",
        "Yao, Kehui",
        "Ma, Luyi",
        "Chen, Jiao",
        "Yousefi Maragheh, Reza",
        "Zhao, Kai",
        "Xu, Jianpeng",
        "Korpeoglu, Evren",
        "Kumar, Sushant",
        "Achan, Kannan"
      ],
      "year": "2025",
      "abstract": "Evaluating large language models (LLMs) as judges is increasingly critical for building scalable and trustworthy evaluation pipelines. We present ScalingEval, a large-scale benchmarking study that systematically compares 36 LLMs, including GPT, Gemini, Claude, and Llama, across multiple product categories using a consensus-driven evaluation protocol. Our multi-agent framework aggregates pattern audits and issue codes into ground-truth labels via scalable majority voting, enabling reproducible comparison of LLM evaluators without human annotation. Applied to large-scale complementary-item recommendation, the benchmark reports four key findings: (i) Anthropic Claude 3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers the best overall performance across categories; (iii) GPT-4o provides the most favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among open-source models. Category-level analysis shows strong consensus in structured domains (Electronics, Sports) but persistent disagreement in lifestyle categories (Clothing, Food). These results establish ScalingEval as a reproducible benchmark and evaluation protocol for LLMs as judges, with actionable guidance on scaling, reliability, and model family tradeoffs.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251103051Z/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.136Z",
      "publishedAt": "2025-11-23T15:01:32.136Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "governance"
      ],
      "score": 14.5
    },
    {
      "id": "37610552",
      "bibcode": "2025arXiv251107205P",
      "title": "Twenty-Five Years of MIR Research: Achievements, Practices, Evaluations, and Future Challenges",
      "authors": [
        "Peeters, Geoffroy",
        "Rafii, Zafar",
        "Fuentes, Magdalena",
        "Duan, Zhiyao",
        "Benetos, Emmanouil",
        "Nam, Juhan",
        "Mitsufuji, Yuki"
      ],
      "year": "2025",
      "abstract": "In this paper, we trace the evolution of Music Information Retrieval (MIR) over the past 25 years. While MIR gathers all kinds of research related to music informatics, a large part of it focuses on signal processing techniques for music data, fostering a close relationship with the IEEE Audio and Acoustic Signal Processing Technical Commitee. In this paper, we reflect the main research achievements of MIR along the three EDICS related to music analysis, processing and generation. We then review a set of successful practices that fuel the rapid development of MIR research. One practice is the annual research benchmark, the Music Information Retrieval Evaluation eXchange, where participants compete on a set of research tasks. Another practice is the pursuit of reproducible and open research. The active engagement with industry research and products is another key factor for achieving large societal impacts and motivating younger generations of students to join the field. Last but not the least, the commitment to diversity, equity and inclusion ensures MIR to be a vibrant and open community where various ideas, methodologies, and career pathways collide. We finish by providing future challenges MIR will have to face.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251107205P/abstract",
      "keywords": [
        "Sound",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.137Z",
      "publishedAt": "2025-11-23T15:01:32.136Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 9.999999991732803
    },
    {
      "id": "37571015",
      "bibcode": "2025arXiv251103878P",
      "title": "KnowThyself: An Agentic Assistant for LLM Interpretability",
      "authors": [
        "Prasai, Suraj",
        "Du, Mengnan",
        "Zhang, Ying",
        "Yang, Fan"
      ],
      "year": "2025",
      "abstract": "We develop KnowThyself, an agentic assistant that advances large language model (LLM) interpretability. Existing tools provide useful insights but remain fragmented and code-intensive. KnowThyself consolidates these capabilities into a chat-based interface, where users can upload models, pose natural language questions, and obtain interactive visualizations with guided explanations. At its core, an orchestrator LLM first reformulates user queries, an agent router further directs them to specialized modules, and the outputs are finally contextualized into coherent explanations. This design lowers technical barriers and provides an extensible platform for LLM inspection. By embedding the whole process into a conversational workflow, KnowThyself offers a robust foundation for accessible LLM interpretability.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251103878P/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Information Retrieval",
        "Machine Learning",
        "Multiagent Systems",
        "I.2.7; I.2.0"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.137Z",
      "publishedAt": "2025-11-23T15:01:32.137Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 11
    },
    {
      "id": "37609525",
      "bibcode": "2025arXiv251106179W",
      "title": "MemoriesDB: A Temporal-Semantic-Relational Database for Long-Term Agent Memory / Modeling Experience as a Graph of Temporal-Semantic Surfaces",
      "authors": [
        "Ward, Joel"
      ],
      "year": "2025",
      "abstract": "We introduce MemoriesDB, a unified data architecture designed to avoid decoherence across time, meaning, and relation in long-term computational memory. Each memory is a time-semantic-relational entity-a structure that simultaneously encodes when an event occurred, what it means, and how it connects to other events. Built initially atop PostgreSQL with pgvector extensions, MemoriesDB combines the properties of a time-series datastore, a vector database, and a graph system within a single append-only schema. Each memory is represented as a vertex uniquely labeled by its microsecond timestamp and accompanied by low- and high-dimensional normalized embeddings that capture semantic context. Directed edges between memories form labeled relations with per-edge metadata, enabling multiple contextual links between the same vertices. Together these constructs form a time-indexed stack of temporal-semantic surfaces, where edges project as directional arrows in a 1+1-dimensional similarity field, tracing the evolution of meaning through time while maintaining cross-temporal coherence. This formulation supports efficient time-bounded retrieval, hybrid semantic search, and lightweight structural reasoning in a single query path. A working prototype demonstrates scalable recall and contextual reinforcement using standard relational infrastructure, and we discuss extensions toward a columnar backend, distributed clustering, and emergent topic modeling.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251106179W/abstract",
      "keywords": [
        "Databases",
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.137Z",
      "publishedAt": "2025-11-23T15:01:32.137Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "observability"
      ],
      "score": 15
    },
    {
      "id": "37499138",
      "bibcode": "2025arXiv251101268K",
      "title": "Rescuing the Unpoisoned: Efficient Defense against Knowledge Corruption Attacks on RAG Systems",
      "authors": [
        "Kim, Minseok",
        "Lee, Hankook",
        "Koo, Hyungjoon"
      ],
      "year": "2025",
      "abstract": "Large language models (LLMs) are reshaping numerous facets of our daily lives, leading widespread adoption as web-based services. Despite their versatility, LLMs face notable challenges, such as generating hallucinated content and lacking access to up-to-date information. Lately, to address such limitations, Retrieval-Augmented Generation (RAG) has emerged as a promising direction by generating responses grounded in external knowledge sources. A typical RAG system consists of i) a retriever that probes a group of relevant passages from a knowledge base and ii) a generator that formulates a response based on the retrieved content. However, as with other AI systems, recent studies demonstrate the vulnerability of RAG, such as knowledge corruption attacks by injecting misleading information. In response, several defense strategies have been proposed, including having LLMs inspect the retrieved passages individually or fine-tuning robust retrievers. While effective, such approaches often come with substantial computational costs. In this work, we introduce RAGDefender, a resource-efficient defense mechanism against knowledge corruption (i.e., by data poisoning) attacks in practical RAG deployments. RAGDefender operates during the post-retrieval phase, leveraging lightweight machine learning techniques to detect and filter out adversarial content without requiring additional model training or inference. Our empirical evaluations show that RAGDefender consistently outperforms existing state-of-the-art defenses across multiple models and adversarial scenarios: e.g., RAGDefender reduces the attack success rate (ASR) against the Gemini model from 0.89 to as low as 0.02, compared to 0.69 for RobustRAG and 0.24 for Discern-and-Answer when adversarial passages outnumber legitimate ones by a factor of four (4x).",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251101268K/abstract",
      "keywords": [
        "Cryptography and Security",
        "Artificial Intelligence",
        "Information Retrieval",
        "D.4.6; K.6.5"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.137Z",
      "publishedAt": "2025-11-23T15:01:32.137Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 14
    },
    {
      "id": "38058125",
      "bibcode": "2025arXiv251113942W",
      "title": "CORGI: Efficient Pattern Matching With Quadratic Guarantees",
      "authors": [
        "Weitekamp, Daniel"
      ],
      "year": "2025",
      "abstract": "Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $β$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251113942W/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Data Structures and Algorithms",
        "Information Retrieval",
        "I.5.1; I.2.1; H.3.3"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.137Z",
      "publishedAt": "2025-11-23T15:01:32.137Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "score": 13.5
    },
    {
      "id": "38074785",
      "bibcode": "2025arXiv251116005D",
      "title": "InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution",
      "authors": [
        "Dong, Qingao",
        "Wang, Mengfei",
        "Zhang, Hengzhi",
        "Li, Zhichao",
        "Yuan, Yuan",
        "Li, Mu",
        "Gao, Xiang",
        "Sun, Hailong",
        "Hu, Chunming",
        "Lv, Weifeng"
      ],
      "year": "2025",
      "abstract": "Large language model (LLM) agents have recently shown strong performance on repository-level issue resolution, but existing systems are almost exclusively designed for Python and rely heavily on lexical retrieval and shallow code navigation. These approaches transfer poorly to C++ projects, where overloaded identifiers, nested namespaces, template instantiations, and deep control-flow structures make context retrieval and fault localization substantially more difficult. As a result, state-of-the-art Python-oriented agents show a drastic performance drop on the C++ subset of MultiSWE-bench. We introduce INFCODE-C++, the first C++-aware autonomous system for end-to-end issue resolution. The system combines two complementary retrieval mechanisms -- semantic code-intent retrieval and deterministic AST-structured querying -- to construct accurate, language-aware context for repair.These components enable precise localization and robust patch synthesis in large, statically typed C++ repositories. Evaluated on the \\texttt{MultiSWE-bench-CPP} benchmark, INFCODE-C++ achieves a resolution rate of 25.58\\%, outperforming the strongest prior agent by 10.85 percentage points and more than doubling the performance of MSWE-agent. Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution. INFCODE-C++ highlights the need for language-aware reasoning in multi-language software agents and establishes a foundation for future research on scalable, LLM-driven repair for complex, statically typed ecosystems.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251116005D/abstract",
      "keywords": [
        "Software Engineering",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.137Z",
      "publishedAt": "2025-11-23T15:01:32.137Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 16
    },
    {
      "id": "37620304",
      "bibcode": "2025arXiv251112254Z",
      "title": "Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation",
      "authors": [
        "Zhou, Yuxiang",
        "Li, Jichang",
        "Zhang, Yanhao",
        "Lu, Haonan",
        "Li, Guanbin"
      ],
      "year": "2025",
      "abstract": "Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112254Z/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.137Z",
      "publishedAt": "2025-11-23T15:01:32.137Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 16
    },
    {
      "id": "37501020",
      "bibcode": "2025arXiv251103153O",
      "title": "RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring",
      "authors": [
        "Oueslati, Khouloud",
        "Lamothe, Maxime",
        "Khomh, Foutse"
      ],
      "year": "2025",
      "abstract": "Large Language Models (LLMs) have substantially influenced various software engineering tasks. Indeed, in the case of software refactoring, traditional LLMs have shown the ability to reduce development time and enhance code quality. However, these LLMs often rely on static, detailed instructions for specific tasks. In contrast, LLM-based agents can dynamically adapt to evolving contexts and autonomously make decisions by interacting with software tools and executing workflows. In this paper, we explore the potential of LLM-based agents in supporting refactoring activities. Specifically, we introduce RefAgent, a multi-agent LLM-based framework for end-to-end software refactoring. RefAgent consists of specialized agents responsible for planning, executing, testing, and iteratively refining refactorings using self-reflection and tool-calling capabilities. We evaluate RefAgent on eight open-source Java projects, comparing its effectiveness against a single-agent approach, a search-based refactoring tool, and historical developer refactorings. Our assessment focuses on: (1) the impact of generated refactorings on software quality, (2) the ability to identify refactoring opportunities, and (3) the contribution of each LLM agent through an ablation study. Our results show that RefAgent achieves a median unit test pass rate of 90%, reduces code smells by a median of 52.5%, and improves key quality attributes (e.g., reusability) by a median of 8.6%. Additionally, it closely aligns with developer refactorings and the search-based tool in identifying refactoring opportunities, attaining a median F1-score of 79.15% and 72.7%, respectively. Compared to single-agent approaches, RefAgent improves the median unit test pass rate by 64.7% and the median compilation success rate by 40.1%. These findings highlight the promise of multi-agent architectures in advancing automated software refactoring.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251103153O/abstract",
      "keywords": [
        "Software Engineering",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.137Z",
      "publishedAt": "2025-11-23T15:01:32.137Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "testing"
      ],
      "score": 16
    },
    {
      "id": "37620438",
      "bibcode": "2025arXiv251112379S",
      "title": "Quantum Optimization Algorithms",
      "authors": [
        "Stein, Jonas",
        "Zorn, Maximilian",
        "Sünkel, Leo",
        "Gabor, Thomas"
      ],
      "year": "2025",
      "abstract": "Quantum optimization allows for up to exponential quantum speedups for specific, possibly industrially relevant problems. As the key algorithm in this field, we motivate and discuss the Quantum Approximate Optimization Algorithm (QAOA), which can be understood as a slightly generalized version of Quantum Annealing for gate-based quantum computers. We delve into the quantum circuit implementation of the QAOA, including Hamiltonian simulation techniques for higher-order Ising models, and discuss parameter training using the parameter shift rule. An example implementation with Pennylane source code demonstrates practical application for the Maximum Cut problem. Further, we show how constraints can be incorporated into the QAOA using Grover mixers, allowing to restrict the search space to strictly valid solutions for specific problems. Finally, we outline the Variational Quantum Eigensolver (VQE) as a generalization of the QAOA, highlighting its potential in the NISQ era and addressing challenges such as barren plateaus and ansatz design.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112379S/abstract",
      "keywords": [
        "Quantum Physics",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.137Z",
      "publishedAt": "2025-11-23T15:01:32.137Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review"
      ],
      "score": 9.5
    },
    {
      "id": "38075060",
      "bibcode": "2025arXiv251116278S",
      "title": "\"To Survive, I Must Defect\": Jailbreaking LLMs via the Game-Theory Scenarios",
      "authors": [
        "Sun, Zhen",
        "Zhang, Zongmin",
        "Liang, Deqi",
        "Sun, Han",
        "Liu, Yule",
        "Shen, Yun",
        "Gao, Xiangshan",
        "Yang, Yilong",
        "Liu, Shuai",
        "Yue, Yutao",
        "He, Xinlei"
      ],
      "year": "2025",
      "abstract": "As LLMs become more common, non-expert users can pose risks, prompting extensive research into jailbreak attacks. However, most existing black-box jailbreak attacks rely on hand-crafted heuristics or narrow search spaces, which limit scalability. Compared with prior attacks, we propose Game-Theory Attack (GTA), an scalable black-box jailbreak framework. Concretely, we formalize the attacker's interaction against safety-aligned LLMs as a finite-horizon, early-stoppable sequential stochastic game, and reparameterize the LLM's randomized outputs via quantal response. Building on this, we introduce a behavioral conjecture \"template-over-safety flip\": by reshaping the LLM's effective objective through game-theoretic scenarios, the originally safety preference may become maximizing scenario payoffs within the template, which weakens safety constraints in specific contexts. We validate this mechanism with classical game such as the disclosure variant of the Prisoner's Dilemma, and we further introduce an Attacker Agent that adaptively escalates pressure to increase the ASR. Experiments across multiple protocols and datasets show that GTA achieves over 95% ASR on LLMs such as Deepseek-R1, while maintaining efficiency. Ablations over components, decoding, multilingual settings, and the Agent's core model confirm effectiveness and generalization. Moreover, scenario scaling studies further establish scalability. GTA also attains high ASR on other game-theoretic scenarios, and one-shot LLM-generated variants that keep the model mechanism fixed while varying background achieve comparable ASR. Paired with a Harmful-Words Detection Agent that performs word-level insertions, GTA maintains high ASR while lowering detection under prompt-guard models. Beyond benchmarks, GTA jailbreaks real-world LLM applications and reports a longitudinal safety monitoring of popular HuggingFace LLMs.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251116278S/abstract",
      "keywords": [
        "Cryptography and Security",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.137Z",
      "publishedAt": "2025-11-23T15:01:32.137Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "agents",
        "observability"
      ],
      "score": 10.5
    },
    {
      "id": "38074902",
      "bibcode": "2025arXiv251116108C",
      "title": "SkyRL-Agent: Efficient RL Training for Multi-turn LLM Agent",
      "authors": [
        "Cao, Shiyi",
        "Li, Dacheng",
        "Zhao, Fangzhou",
        "Yuan, Shuo",
        "Hegde, Sumanth R.",
        "Chen, Connor",
        "Ruan, Charlie",
        "Griggs, Tyler",
        "Liu, Shu",
        "Tang, Eric",
        "Liaw, Richard",
        "Moritz, Philipp",
        "Zaharia, Matei",
        "Gonzalez, Joseph E.",
        "Stoica, Ion"
      ],
      "year": "2025",
      "abstract": "We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation. It provides efficient asynchronous dispatching, lightweight tool integration, and flexible backend interoperability, enabling seamless use with existing RL frameworks such as SkyRL-train, VeRL, and Tinker. Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning. We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. Together, these optimizations enable SA-SWE-32B to reach 39.4% Pass@1 on SWE-Bench Verified with more than 2x cost reduction compared to prior models reaching similar performance. Despite being trained solely on SWE tasks, SA-SWE-32B generalizes effectively to other agentic tasks, including Terminal-Bench, BrowseComp-Plus, and WebArena. We further demonstrate SkyRL-Agent's extensibility through case studies on deep research, computer use, and memory agents, each trained using a different training backend.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251116108C/abstract",
      "keywords": [
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.137Z",
      "publishedAt": "2025-11-23T15:01:32.137Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 16
    },
    {
      "id": "37611367",
      "bibcode": "2025arXiv251107943X",
      "title": "Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction",
      "authors": [
        "Xu, Jun",
        "Du, Xinkai",
        "Ao, Yu",
        "Zhao, Peilong",
        "Li, Yang",
        "Zhong, Ling",
        "Yuan, Lin",
        "Bo, Zhongpu",
        "Wang, Xiaorui",
        "Sun, Mengshu",
        "Gui, Zhengke",
        "Zhang, Dalong",
        "Wang, Zhaoyang",
        "Wang, Qiwei",
        "Hou, Yangyang",
        "Yin, Zhiying",
        "Wang, Haofen",
        "Chen, Huajun",
        "Liang, Lei",
        "Zhou, Jun"
      ],
      "year": "2025",
      "abstract": "Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251107943X/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Computation and Language"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.137Z",
      "publishedAt": "2025-11-23T15:01:32.137Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 11
    },
    {
      "id": "37499717",
      "bibcode": "2025arXiv251101857E",
      "title": "Trove: A Flexible Toolkit for Dense Retrieval",
      "authors": [
        "Esfandiarpoor, Reza",
        "Zuo, Max",
        "Bach, Stephen H."
      ],
      "year": "2025",
      "abstract": "We introduce Trove, an easy-to-use open-source retrieval toolkit that simplifies research experiments without sacrificing flexibility or speed. For the first time, we introduce efficient data management features that load and process (filter, select, transform, and combine) retrieval datasets on the fly, with just a few lines of code. This gives users the flexibility to easily experiment with different dataset configurations without the need to compute and store multiple copies of large datasets. Trove is highly customizable: in addition to many built-in options, it allows users to freely modify existing components or replace them entirely with user-defined objects. It also provides a low-code and unified pipeline for evaluation and hard negative mining, which supports multi-node execution without any code changes. Trove's data management features reduce memory consumption by a factor of 2.6. Moreover, Trove's easy-to-use inference pipeline incurs no overhead, and inference times decrease linearly with the number of available nodes. Most importantly, we demonstrate how Trove simplifies retrieval experiments and allows for arbitrary customizations, thus facilitating exploratory research.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251101857E/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.137Z",
      "publishedAt": "2025-11-23T15:01:32.137Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 11.999999990079365
    },
    {
      "id": "37609482",
      "bibcode": "2025arXiv251106142T",
      "title": "MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning",
      "authors": [
        "Tang, Sizhe",
        "Chen, Jiayu",
        "Lan, Tian"
      ],
      "year": "2025",
      "abstract": "Monte Carlo Tree Search (MCTS), which leverages Upper Confidence Bound for Trees (UCTs) to balance exploration and exploitation through randomized sampling, is instrumental to solving complex planning problems. However, for multi-agent planning, MCTS is confronted with a large combinatorial action space that often grows exponentially with the number of agents. As a result, the branching factor of MCTS during tree expansion also increases exponentially, making it very difficult to efficiently explore and exploit during tree search. To this end, we propose MALinZero, a new approach to leverage low-dimensional representational structures on joint-action returns and enable efficient MCTS in complex multi-agent planning. Our solution can be viewed as projecting the joint-action returns into the low-dimensional space representable using a contextual linear bandit problem formulation. We solve the contextual linear bandit problem with convex and $μ$-smooth loss functions -- in order to place more importance on better joint actions and mitigate potential representational limitations -- and derive a linear Upper Confidence Bound applied to trees (LinUCT) to enable novel multi-agent exploration and exploitation in the low-dimensional space. We analyze the regret of MALinZero for low-dimensional reward functions and propose an $(1-\\tfrac1e)$-approximation algorithm for the joint action selection by maximizing a sub-modular objective. MALinZero demonstrates state-of-the-art performance on multi-agent benchmarks such as matrix games, SMAC, and SMACv2, outperforming both model-based and model-free multi-agent reinforcement learning baselines with faster learning speed and better performance.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251106142T/abstract",
      "keywords": [
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.138Z",
      "publishedAt": "2025-11-23T15:01:32.138Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 16
    },
    {
      "id": "37610152",
      "bibcode": "2025arXiv251106803Z",
      "title": "Learning to Fast Unrank in Collaborative Filtering Recommendation",
      "authors": [
        "Zhao, Junpeng",
        "Li, Lin",
        "Li, Ming",
        "Bhuiyan, Amran",
        "Huang, Jimmy"
      ],
      "year": "2025",
      "abstract": "Modern data-driven recommendation systems risk memorizing sensitive user behavioral patterns, raising privacy concerns. Existing recommendation unlearning methods, while capable of removing target data influence, suffer from inefficient unlearning speed and degraded performance, failing to meet real-time unlearning demands. Considering the ranking-oriented nature of recommendation systems, we present unranking, the process of reducing the ranking positions of target items while ensuring the formal guarantees of recommendation unlearning. To achieve efficient unranking, we propose Learning to Fast Unrank in Collaborative Filtering Recommendation (L2UnRank), which operates through three key stages: (a) identifying the influenced scope via interaction-based p-hop propagation, (b) computing structural and semantic influences for entities within this scope, and (c) performing efficient, ranking-aware parameter updates guided by influence information. Extensive experiments across multiple datasets and backbone models demonstrate L2UnRank's model-agnostic nature, achieving state-of-the-art unranking effectiveness and maintaining recommendation quality comparable to retraining, while also delivering a 50x speedup over existing methods. Codes are available at https://github.com/Juniper42/L2UnRank.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251106803Z/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence",
        "Machine Learning"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.138Z",
      "publishedAt": "2025-11-23T15:01:32.138Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "ide"
      ],
      "score": 9.5
    },
    {
      "id": "37612531",
      "bibcode": "2025arXiv251109109W",
      "title": "Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning",
      "authors": [
        "Wei, Wenda",
        "Liu, Yu-An",
        "Zhang, Ruqing",
        "Guo, Jiafeng",
        "Su, Lixin",
        "Wang, Shuaiqiang",
        "Yin, Dawei",
        "de Rijke, Maarten",
        "Cheng, Xueqi"
      ],
      "year": "2025",
      "abstract": "Retrieval-augmented generation (RAG) has proven to be effective in mitigating hallucinations in large language models, yet its effectiveness remains limited in complex, multi-step reasoning scenarios. Recent efforts have incorporated search-based interactions into RAG, enabling iterative reasoning with real-time retrieval. Most approaches rely on outcome-based supervision, offering no explicit guidance for intermediate steps. This often leads to reward hacking and degraded response quality. We propose Bi-RAR, a novel retrieval-augmented reasoning framework that evaluates each intermediate step jointly in both forward and backward directions. To assess the information completeness of each step, we introduce a bidirectional information distance grounded in Kolmogorov complexity, approximated via language model generation probabilities. This quantification measures both how far the current reasoning is from the answer and how well it addresses the question. To optimize reasoning under these bidirectional signals, we adopt a multi-objective reinforcement learning framework with a cascading reward structure that emphasizes early trajectory alignment. Empirical results on seven question answering benchmarks demonstrate that Bi-RAR surpasses previous methods and enables efficient interaction and reasoning with the search engine during training and inference.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251109109W/abstract",
      "keywords": [
        "Computation and Language",
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.138Z",
      "publishedAt": "2025-11-23T15:01:32.138Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 11
    },
    {
      "id": "37619702",
      "bibcode": "2025arXiv251111653S",
      "title": "GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning",
      "authors": [
        "Sun, Duolin",
        "Long, Meixiu",
        "Yang, Dan",
        "Jiao, Yihan",
        "Tan, Zhehao",
        "Feng, Jie",
        "Wang, Junjie",
        "Shen, Yue",
        "Wei, Peng",
        "Wang, Jian",
        "Gu, Jinjie"
      ],
      "year": "2025",
      "abstract": "Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251111653S/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence",
        "Machine Learning"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.138Z",
      "publishedAt": "2025-11-23T15:01:32.138Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "observability"
      ],
      "score": 12
    },
    {
      "id": "37618950",
      "bibcode": "2025arXiv251111257Y",
      "title": "AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery",
      "authors": [
        "Yin, Yuqi",
        "Fu, Yibo",
        "Wang, Siyuan",
        "Sun, Peng",
        "Wang, Hongyu",
        "Wang, Xiaohui",
        "Zheng, Lei",
        "Li, Zhiyong",
        "Liu, Zhirong",
        "Wang, Jianji",
        "Sun, Zhaoxi"
      ],
      "year": "2025",
      "abstract": "The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251111257Y/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Computational Engineering",
        "Finance",
        "and Science",
        "Machine Learning"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.138Z",
      "publishedAt": "2025-11-23T15:01:32.138Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "agents"
      ],
      "score": 10
    },
    {
      "id": "37571616",
      "bibcode": "2025arXiv251104473C",
      "title": "Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs",
      "authors": [
        "Cattaneo, Alberto",
        "Luschi, Carlo",
        "Justus, Daniel"
      ],
      "year": "2025",
      "abstract": "Retrieval of information from graph-structured knowledge bases represents a promising direction for improving the factuality of LLMs. While various solutions have been proposed, a comparison of methods is difficult due to the lack of challenging QA datasets with ground-truth targets for graph retrieval. We present SynthKGQA, a framework for generating high-quality synthetic Knowledge Graph Question Answering datasets from any Knowledge Graph, providing the full set of ground-truth facts in the KG to reason over each question. We show how, in addition to enabling more informative benchmarking of KG retrievers, the data produced with SynthKGQA also allows us to train better models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset designed to test zero-shot generalization abilities of KG retrievers with respect to unseen graph structures and relation types, and benchmark popular solutions for KG-augmented LLMs on it.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251104473C/abstract",
      "keywords": [
        "Machine Learning",
        "Artificial Intelligence",
        "Computation and Language",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.138Z",
      "publishedAt": "2025-11-23T15:01:32.138Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 9
    },
    {
      "id": "37610601",
      "bibcode": "2025arXiv251107262J",
      "title": "AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery in Scientific Machine Learning",
      "authors": [
        "Jiang, Qile",
        "Karniadakis, George"
      ],
      "year": "2025",
      "abstract": "Scientific Machine Learning (SciML) integrates data-driven inference with physical modeling to solve complex problems in science and engineering. However, the design of SciML architectures, loss formulations, and training strategies remains an expert-driven research process, requiring extensive experimentation and problem-specific insights. Here we introduce AgenticSciML, a collaborative multi-agent system in which over 10 specialized AI agents collaborate to propose, critique, and refine SciML solutions through structured reasoning and iterative evolution. The framework integrates structured debate, retrieval-augmented method memory, and ensemble-guided evolutionary search, enabling the agents to generate and assess new hypotheses about architectures and optimization procedures. Across physics-informed learning and operator learning tasks, the framework discovers solution methods that outperform single-agent and human-designed baselines by up to four orders of magnitude in error reduction. The agents produce novel strategies -- including adaptive mixture-of-expert architectures, decomposition-based PINNs, and physics-informed operator learning models -- that do not appear explicitly in the curated knowledge base. These results show that collaborative reasoning among AI agents can yield emergent methodological innovation, suggesting a path toward scalable, transparent, and autonomous discovery in scientific computing.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251107262J/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Computational Engineering",
        "Finance",
        "and Science",
        "Machine Learning"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.138Z",
      "publishedAt": "2025-11-23T15:01:32.138Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 16
    },
    {
      "id": "37612428",
      "bibcode": "2025arXiv251109005C",
      "title": "AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines",
      "authors": [
        "Chauhan, Alvin"
      ],
      "year": "2025",
      "abstract": "Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of the search space in a gradual, incremental, and sequential (GIS) manner. Stated succinctly, high-quality reasoning is a controlled, incremental search. To test this framework, we investigate the efficacy of recursive refinement (RR)--an iterative process of self-criticism, adversarial stress-testing, and integrating critical feedback--as a practical method for implementing GIS search. We designed an experiment comparing a simple, linear pipeline against a complex, explicitly structured pipeline leveraging a recursive refinement layer. The multi-agent models were constructed to reflect the historical personas of three US Founding Fathers (Hamilton, Jefferson, and Madison) using RAG-powered corpora and were prompted to generate responses to three contemporary political issues. Model performance was evaluated using a two-tiered approach: a quantitative score from an LLM arbiter agent and qualitative human judgment. Our results revealed that the complex model consistently outperformed the simple model across all nine test cases with an average arbiter-outputted score of 88.3 versus 71.7. The complex model's arguments were superior in analytical depth, structural nuance, and strategic framing. We conclude that recursive refinement is a robust architectural feature for enhancing LLM reasoning via GIS search.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251109005C/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Computation and Language",
        "Multiagent Systems",
        "I.2.11"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.138Z",
      "publishedAt": "2025-11-23T15:01:32.138Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "testing"
      ],
      "score": 11.5
    },
    {
      "id": "37618963",
      "bibcode": "2025arXiv251111265R",
      "title": "SQuaD: The Software Quality Dataset",
      "authors": [
        "Robredo, Mikel",
        "Esposito, Matteo",
        "Taibi, Davide",
        "Peñaloza, Rafael",
        "Lenarduzzi, Valentina"
      ],
      "year": "2025",
      "abstract": "Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251111265R/abstract",
      "keywords": [
        "Software Engineering",
        "Artificial Intelligence",
        "Computation and Language",
        "Cryptography and Security",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.138Z",
      "publishedAt": "2025-11-23T15:01:32.138Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "ide",
        "observability"
      ],
      "score": 12.5
    },
    {
      "id": "38059284",
      "bibcode": "2025arXiv251115061C",
      "title": "Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering",
      "authors": [
        "Chen, Haodong",
        "Zuccon, Guido",
        "Leelanupab, Teerapong"
      ],
      "year": "2025",
      "abstract": "Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization. In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution. OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251115061C/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Information Retrieval",
        "Machine Learning",
        "H.3.3"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.138Z",
      "publishedAt": "2025-11-23T15:01:32.138Z",
      "libraryId": "ads-ingest",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 17
    },
    {
      "id": "37616720",
      "bibcode": "2025arXiv251110465X",
      "title": "Beyond Elicitation: Provision-based Prompt Optimization for Knowledge-Intensive Tasks",
      "authors": [
        "Xu, Yunzhe",
        "Zhang, Zhuosheng",
        "Liu, Zhe"
      ],
      "year": "2025",
      "abstract": "While prompt optimization has emerged as a critical technique for enhancing language model performance, existing approaches primarily focus on elicitation-based strategies that search for optimal prompts to activate models' capabilities. These methods exhibit fundamental limitations when addressing knowledge-intensive tasks, as they operate within fixed parametric boundaries rather than providing the factual knowledge, terminology precision, and reasoning patterns required in specialized domains. To address these limitations, we propose Knowledge-Provision-based Prompt Optimization (KPPO), a framework that reformulates prompt optimization as systematic knowledge integration rather than potential elicitation. KPPO introduces three key innovations: 1) a knowledge gap filling mechanism for knowledge gap identification and targeted remediation; 2) a batch-wise candidate evaluation approach that considers both performance improvement and distributional stability; 3) an adaptive knowledge pruning strategy that balances performance and token efficiency, reducing up to 29% token usage. Extensive evaluation on 15 knowledge-intensive benchmarks from various domains demonstrates KPPO's superiority over elicitation-based methods, with an average performance improvement of ~6% over the strongest baseline while achieving comparable or lower token consumption. Code at: https://github.com/xyz9911/KPPO.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251110465X/abstract",
      "keywords": [
        "Computation and Language",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.138Z",
      "publishedAt": "2025-11-23T15:01:32.138Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 13
    },
    {
      "id": "38058185",
      "bibcode": "2025arXiv251113998Q",
      "title": "LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering",
      "authors": [
        "Qiu, Jielin",
        "Liu, Zuxin",
        "Liu, Zhiwei",
        "Murthy, Rithesh",
        "Zhang, Jianguo",
        "Chen, Haolin",
        "Wang, Shiyu",
        "Zhu, Ming",
        "Yang, Liangwei",
        "Tan, Juntao",
        "Ram, Roshan",
        "Prabhakar, Akshara",
        "Awalgaonkar, Tulika",
        "Chen, Zixiang",
        "Cen, Zhepeng",
        "Qian, Cheng",
        "Heinecke, Shelby",
        "Yao, Weiran",
        "Savarese, Silvio",
        "Xiong, Caiming",
        "Wang, Huan"
      ],
      "year": "2025",
      "abstract": "As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \\textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251113998Q/abstract",
      "keywords": [
        "Software Engineering",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.138Z",
      "publishedAt": "2025-11-23T15:01:32.138Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "observability"
      ],
      "score": 15.5
    },
    {
      "id": "38058281",
      "bibcode": "2025arXiv251114096L",
      "title": "NeuroPath: Neurobiology-Inspired Path Tracking and Reflection for Semantically Coherent Retrieval",
      "authors": [
        "Li, Junchen",
        "Wang, Rongzheng",
        "Huang, Yihong",
        "Chen, Qizhi",
        "Zhang, Jiasheng",
        "Liang, Shuang"
      ],
      "year": "2025",
      "abstract": "Retrieval-augmented generation (RAG) greatly enhances large language models (LLMs) performance in knowledge-intensive tasks. However, naive RAG methods struggle with multi-hop question answering due to their limited capacity to capture complex dependencies across documents. Recent studies employ graph-based RAG to capture document connections. However, these approaches often result in a loss of semantic coherence and introduce irrelevant noise during node matching and subgraph construction. To address these limitations, we propose NeuroPath, an LLM-driven semantic path tracking RAG framework inspired by the path navigational planning of place cells in neurobiology. It consists of two steps: Dynamic Path Tracking and Post-retrieval Completion. Dynamic Path Tracking performs goal-directed semantic path tracking and pruning over the constructed knowledge graph (KG), improving noise reduction and semantic coherence. Post-retrieval Completion further reinforces these benefits by conducting second-stage retrieval using intermediate reasoning and the original query to refine the query goal and complete missing information in the reasoning path. NeuroPath surpasses current state-of-the-art baselines on three multi-hop QA datasets, achieving average improvements of 16.3% on recall@2 and 13.5% on recall@5 over advanced graph-based RAG methods. Moreover, compared to existing iter-based RAG methods, NeuroPath achieves higher accuracy and reduces token consumption by 22.8%. Finally, we demonstrate the robustness of NeuroPath across four smaller LLMs (Llama3.1, GLM4, Mistral0.3, and Gemma3), and further validate its scalability across tasks of varying complexity. Code is available at https://github.com/KennyCaty/NeuroPath.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251114096L/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.138Z",
      "publishedAt": "2025-11-23T15:01:32.138Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 11
    },
    {
      "id": "37500072",
      "bibcode": "2025arXiv251102200W",
      "title": "Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration",
      "authors": [
        "Wang, Jingbo",
        "Zhao, Sendong",
        "Wang, Haochun",
        "Fan, Yuzheng",
        "Zhang, Lizhe",
        "Liu, Yan",
        "Liu, Ting"
      ],
      "year": "2025",
      "abstract": "The emergence of multi-agent systems powered by large language models (LLMs) has unlocked new frontiers in complex task-solving, enabling diverse agents to integrate unique expertise, collaborate flexibly, and address challenges unattainable for individual models. However, the full potential of such systems is hindered by rigid agent scheduling and inefficient coordination strategies that fail to adapt to evolving task requirements. In this paper, we propose STRMAC, a state-aware routing framework designed for efficient collaboration in multi-agent systems. Our method separately encodes interaction history and agent knowledge to power the router, which adaptively selects the most suitable single agent at each step for efficient and effective collaboration. Furthermore, we introduce a self-evolving data generation approach that accelerates the collection of high-quality execution paths for efficient system training. Experiments on challenging collaborative reasoning benchmarks demonstrate that our method achieves state-of-the-art performance, achieving up to 23.8% improvement over baselines and reducing data collection overhead by up to 90.1% compared to exhaustive search.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251102200W/abstract",
      "keywords": [
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.138Z",
      "publishedAt": "2025-11-23T15:01:32.138Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "score": 13.499999988839285
    },
    {
      "id": "37501201",
      "bibcode": "2025arXiv251103330W",
      "title": "Discourse-Aware Scientific Paper Recommendation via QA-Style Summarization and Multi-Level Contrastive Learning",
      "authors": [
        "Wang, Shenghua",
        "Yin, Zhen"
      ],
      "year": "2025",
      "abstract": "The rapid growth of open-access (OA) publications has intensified the challenge of identifying relevant scientific papers. Due to privacy constraints and limited access to user interaction data, recent efforts have shifted toward content-based recommendation, which relies solely on textual information. However, existing models typically treat papers as unstructured text, neglecting their discourse organization and thereby limiting semantic completeness and interpretability. To address these limitations, we propose OMRC-MR, a hierarchical framework that integrates QA-style OMRC (Objective, Method, Result, Conclusion) summarization, multi-level contrastive learning, and structure-aware re-ranking for scholarly recommendation. The QA-style summarization module converts raw papers into structured and discourse-consistent representations, while multi-level contrastive objectives align semantic representations across metadata, section, and document levels. The final re-ranking stage further refines retrieval precision through contextual similarity calibration. Experiments on DBLP, S2ORC, and the newly constructed Sci-OMRC dataset demonstrate that OMRC-MR consistently surpasses state-of-the-art baselines, achieving up to 7.2% and 3.8% improvements in Precision@10 and Recall@10, respectively. Additional evaluations confirm that QA-style summarization produces more coherent and factually complete representations. Overall, OMRC-MR provides a unified and interpretable content-based paradigm for scientific paper recommendation, advancing trustworthy and privacy-aware scholarly information retrieval.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251103330W/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.139Z",
      "publishedAt": "2025-11-23T15:01:32.139Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 9
    },
    {
      "id": "37573252",
      "bibcode": "2025arXiv251104939N",
      "title": "Search Is Not Retrieval: Decoupling Semantic Matching from Contextual Assembly in RAG",
      "authors": [
        "Nainwani, Harshit",
        "Baban, Hediyeh"
      ],
      "year": "2025",
      "abstract": "Retrieval systems are essential to contemporary AI pipelines, although most confuse two separate processes: finding relevant information and giving enough context for reasoning. We introduce the Search-Is-Not-Retrieve (SINR) framework, a dual-layer architecture that distinguishes between fine-grained search representations and coarse-grained retrieval contexts. SINR enhances the composability, scalability, and context fidelity of retrieval systems by directly connecting small, semantically accurate search chunks to larger, contextually complete retrieve chunks, all without incurring extra processing costs. This design changes retrieval from a passive step to an active one, making the system architecture more like how people process information. We discuss the SINR framework's conceptual foundation, formal structure, implementation issues, and qualitative outcomes. This provides a practical foundation for the next generation of AI systems that use retrieval.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251104939N/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.139Z",
      "publishedAt": "2025-11-23T15:01:32.139Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 9
    },
    {
      "id": "37620972",
      "bibcode": "2025arXiv251112920H",
      "title": "Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy",
      "authors": [
        "Hu, Desheng",
        "Baumann, Joachim",
        "Urman, Aleksandra",
        "Lichtenegger, Elsa",
        "Forsberg, Robin",
        "Hannak, Aniko",
        "Wilson, Christo"
      ],
      "year": "2025",
      "abstract": "Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112920H/abstract",
      "keywords": [
        "Computation and Language",
        "Artificial Intelligence",
        "Computers and Society",
        "Human-Computer Interaction",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.139Z",
      "publishedAt": "2025-11-23T15:01:32.139Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "ide",
        "governance"
      ],
      "score": 12.5
    },
    {
      "id": "37620508",
      "bibcode": "2025arXiv251112449N",
      "title": "MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding",
      "authors": [
        "Nie, Zhanheng",
        "Fu, Chenghan",
        "Zhang, Daoze",
        "Wu, Junxian",
        "Guan, Wanxian",
        "Wang, Pengjie",
        "Xu, Jian",
        "Zheng, Bo"
      ],
      "year": "2025",
      "abstract": "The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112449N/abstract",
      "keywords": [
        "Computer Vision and Pattern Recognition",
        "Artificial Intelligence",
        "Information Retrieval",
        "Machine Learning"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.139Z",
      "publishedAt": "2025-11-23T15:01:32.139Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 12
    },
    {
      "id": "37500686",
      "bibcode": "2025arXiv251102824M",
      "title": "Kosmos: An AI Scientist for Autonomous Discovery",
      "authors": [
        "Mitchener, Ludovico",
        "Yiu, Angela",
        "Chang, Benjamin",
        "Bourdenx, Mathieu",
        "Nadolski, Tyler",
        "Sulovari, Arvis",
        "Landsness, Eric C.",
        "Barabasi, Daniel L.",
        "Narayanan, Siddharth",
        "Evans, Nicky",
        "Reddy, Shriya",
        "Foiani, Martha",
        "Kamal, Aizad",
        "Shriver, Leah P.",
        "Cao, Fang",
        "Wassie, Asmamaw T.",
        "Laurent, Jon M.",
        "Melville-Green, Edwin",
        "Caldas, Mayk",
        "Bou, Albert",
        "Roberts, Kaleigh F.",
        "Zagorac, Sladjana",
        "Orr, Timothy C.",
        "Orr, Miranda E.",
        "Zwezdaryk, Kevin J.",
        "Ghareeb, Ali E.",
        "McCoy, Laurie",
        "Gomes, Bruna",
        "Ashley, Euan A.",
        "Duff, Karen E.",
        "Buonassisi, Tonio",
        "Rainforth, Tom",
        "Bateman, Randall J.",
        "Skarlinski, Michael",
        "Rodriques, Samuel G.",
        "Hinks, Michaela M.",
        "White, Andrew D."
      ],
      "year": "2025",
      "abstract": "Data-driven scientific discovery requires iterative cycles of literature search, hypothesis generation, and data analysis. Substantial progress has been made towards AI agents that can automate scientific research, but all such agents remain limited in the number of actions they can take before losing coherence, thus limiting the depth of their findings. Here we present Kosmos, an AI scientist that automates data-driven discovery. Given an open-ended objective and a dataset, Kosmos runs for up to 12 hours performing cycles of parallel data analysis, literature search, and hypothesis generation before synthesizing discoveries into scientific reports. Unlike prior systems, Kosmos uses a structured world model to share information between a data analysis agent and a literature search agent. The world model enables Kosmos to coherently pursue the specified objective over 200 agent rollouts, collectively executing an average of 42,000 lines of code and reading 1,500 papers per run. Kosmos cites all statements in its reports with code or primary literature, ensuring its reasoning is traceable. Independent scientists found 79.4% of statements in Kosmos reports to be accurate, and collaborators reported that a single 20-cycle Kosmos run performed the equivalent of 6 months of their own research time on average. Furthermore, collaborators reported that the number of valuable scientific findings generated scales linearly with Kosmos cycles (tested up to 20 cycles). We highlight seven discoveries made by Kosmos that span metabolomics, materials science, neuroscience, and statistical genetics. Three discoveries independently reproduce findings from preprinted or unpublished manuscripts that were not accessed by Kosmos at runtime, while four make novel contributions to the scientific literature.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251102824M/abstract",
      "keywords": [
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.139Z",
      "publishedAt": "2025-11-23T15:01:32.139Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents"
      ],
      "score": 15
    },
    {
      "id": "37571382",
      "bibcode": "2025arXiv251104237L",
      "title": "Denoised Recommendation Model with Collaborative Signal Decoupling",
      "authors": [
        "Li, Zefeng",
        "Yang, Ning"
      ],
      "year": "2025",
      "abstract": "Although the collaborative filtering (CF) algorithm has achieved remarkable performance in recommendation systems, it suffers from suboptimal recommendation performance due to noise in the user-item interaction matrix. Numerous noise-removal studies have improved recommendation models, but most existing approaches conduct denoising on a single graph. This may cause attenuation of collaborative signals: removing edges between two nodes can interrupt paths between other nodes, weakening path-dependent collaborative information. To address these limitations, this study proposes a novel GNN-based CF model called DRCSD for denoising unstable interactions. DRCSD includes two core modules: a collaborative signal decoupling module (decomposes signals into distinct orders by structural characteristics) and an order-wise denoising module (performs targeted denoising on each order). Additionally, the information aggregation mechanism of traditional GNN-based CF models is modified to avoid cross-order signal interference until the final pooling operation. Extensive experiments on three public real-world datasets show that DRCSD has superior robustness against unstable interactions and achieves statistically significant performance improvements in recommendation accuracy metrics compared to state-of-the-art baseline models.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251104237L/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.139Z",
      "publishedAt": "2025-11-23T15:01:32.139Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "observability"
      ],
      "score": 7.5
    },
    {
      "id": "37571138",
      "bibcode": "2025arXiv251103985Y",
      "title": "ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering",
      "authors": [
        "Yuan, Zhuowen",
        "Liu, Tao",
        "Yang, Yang",
        "Wang, Yang",
        "Qi, Feng",
        "Rangadurai, Kaushik",
        "Li, Bo",
        "Yang, Shuang"
      ],
      "year": "2025",
      "abstract": "Recent LLM-based agents have demonstrated strong capabilities in automated ML engineering. However, they heavily rely on repeated full training runs to evaluate candidate solutions, resulting in significant computational overhead, limited scalability to large search spaces, and slow iteration cycles. To address these challenges, we introduce ArchPilot, a multi-agent system that integrates architecture generation, proxy-based evaluation, and adaptive search into a unified framework. ArchPilot consists of three specialized agents: an orchestration agent that coordinates the search process using a Monte Carlo Tree Search (MCTS)-inspired novel algorithm with a restart mechanism and manages memory of previous candidates; a generation agent that iteratively generates, improves, and debugs candidate architectures; and an evaluation agent that executes proxy training runs, generates and optimizes proxy functions, and aggregates the proxy scores into a fidelity-aware performance metric. This multi-agent collaboration allows ArchPilot to prioritize high-potential candidates with minimal reliance on expensive full training runs, facilitating efficient ML engineering under limited budgets. Experiments on MLE-Bench demonstrate that ArchPilot outperforms SOTA baselines such as AIDE and ML-Master, validating the effectiveness of our multi-agent system.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251103985Y/abstract",
      "keywords": [
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.139Z",
      "publishedAt": "2025-11-23T15:01:32.139Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "score": 14.5
    },
    {
      "id": "37621466",
      "bibcode": "2025arXiv251113418B",
      "title": "Exploring Multi-Table Retrieval Through Iterative Search",
      "authors": [
        "Boutaleb, Allaa",
        "Amann, Bernd",
        "Angarita, Rafael",
        "Naacke, Hubert"
      ],
      "year": "2025",
      "abstract": "Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251113418B/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence",
        "Computation and Language",
        "Databases",
        "Machine Learning"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.139Z",
      "publishedAt": "2025-11-23T15:01:32.139Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 9
    },
    {
      "id": "37621215",
      "bibcode": "2025arXiv251113166S",
      "title": "Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users",
      "authors": [
        "Shen, Zhaoxin",
        "Wu, Dan"
      ],
      "year": "2025",
      "abstract": "To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251113166S/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence",
        "Human-Computer Interaction",
        "68T09 (Primary) 68M11",
        "68W27 (Secondary)",
        "H.3.3"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.139Z",
      "publishedAt": "2025-11-23T15:01:32.139Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 8
    },
    {
      "id": "37621022",
      "bibcode": "2025arXiv251112971C",
      "title": "Esim: EVM Bytecode Similarity Detection Based on Stable-Semantic Graph",
      "authors": [
        "Chen, Zhuo",
        "Ji, Gaoqiang",
        "He, Yiling",
        "Wu, Lei",
        "Zhou, Yajin"
      ],
      "year": "2025",
      "abstract": "Decentralized finance (DeFi) is experiencing rapid expansion. However, prevalent code reuse and limited open-source contributions have introduced significant challenges to the blockchain ecosystem, including plagiarism and the propagation of vulnerable code. Consequently, an effective and accurate similarity detection method for EVM bytecode is urgently needed to identify similar contracts. Traditional binary similarity detection methods are typically based on instruction stream or control flow graph (CFG), which have limitations on EVM bytecode due to specific features like low-level EVM bytecode and heavily-reused basic blocks. Moreover, the highly-diverse Solidity Compiler (Solc) versions further complicate accurate similarity detection. Motivated by these challenges, we propose a novel EVM bytecode representation called Stable-Semantic Graph (SSG), which captures relationships between 'stable instructions' (special instructions identified by our study). Moreover, we implement a prototype, Esim, which embeds SSG into matrices for similarity detection using a heterogeneous graph neural network. Esim demonstrates high accuracy in SSG construction, achieving F1-scores of 100% for control flow and 95.16% for data flow, and its similarity detection performance reaches 96.3% AUC, surpassing traditional approaches. Our large-scale study, analyzing 2,675,573 smart contracts on six EVM-compatible chains over a one-year period, also demonstrates that Esim outperforms the SOTA tool Etherscan in vulnerability search.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112971C/abstract",
      "keywords": [
        "Cryptography and Security",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T15:01:32.139Z",
      "publishedAt": "2025-11-23T15:01:32.139Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "ide"
      ],
      "score": 12.5
    }
  ],
  "articles": [
    {
      "id": "4d6c02b365aac92d5988754e55d4e3e2",
      "title": "Qodo Named a Visionary in the 2025 Gartner® Magic Quadrant™ for AI Code Assistants",
      "url": "https://www.qodo.ai/blog/qodo-named-a-visionary-in-the-2025-gartner-magic-quadrant-for-ai-code-assistants/",
      "content": "We are proud to share that Qodo has been named a Visionary in the 2025 Gartner Magic Quadrant for AI Code Assistants. In our view, this recognition reflects what…",
      "summary": "We are proud to share that Qodo has been named a Visionary in the 2025 Gartner Magic Quadrant for AI Code Assistants. In our view, this recognition reflects what…",
      "publishedAt": "2025-11-23T17:37:39.463Z",
      "source": "rss",
      "feedName": "Qodo",
      "sourceType": "competitor_blog",
      "company": "Qodo",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.463Z",
      "score": 7
    },
    {
      "id": "fa0c3f81f7318840b49ed794960adc2e",
      "title": "Sandboxing agents at the kernel level",
      "url": "https://www.greptile.com/blog/sandboxing-agents-at-the-kernel-level",
      "publishedAt": "2025-11-23T17:37:39.557Z",
      "source": "rss",
      "feedName": "Greptile",
      "sourceType": "competitor_blog",
      "company": "Greptile",
      "contentType": "feature_update",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.557Z",
      "score": 11
    },
    {
      "id": "7c3f5684a5fd7cc074e70b759738057f",
      "title": "Series A and Greptile v3",
      "url": "https://www.greptile.com/blog/series-a",
      "publishedAt": "2025-11-23T17:37:39.557Z",
      "source": "rss",
      "feedName": "Greptile",
      "sourceType": "competitor_blog",
      "company": "Greptile",
      "contentType": "funding_mna",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.557Z",
      "score": 6
    },
    {
      "id": "7dae11ba79a9943e85f306fe93dda24e",
      "title": "Greptile's work culture",
      "url": "https://www.greptile.com/blog/work-culture",
      "publishedAt": "2025-11-23T17:37:39.557Z",
      "source": "rss",
      "feedName": "Greptile",
      "sourceType": "competitor_blog",
      "company": "Greptile",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.557Z",
      "score": 4
    },
    {
      "id": "a36e916befb223deb61be1571dbf0a6f",
      "title": "Software Needs An Independent Auditor",
      "url": "https://www.greptile.com/blog/auditor",
      "publishedAt": "2025-11-23T17:37:39.557Z",
      "source": "rss",
      "feedName": "Greptile",
      "sourceType": "competitor_blog",
      "company": "Greptile",
      "contentType": "general",
      "tags": [
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.557Z",
      "score": 6
    },
    {
      "id": "454079e66b8b39011f5af8c18d740b18",
      "title": "What Developers Need to Know About AI Code Reviews",
      "url": "https://www.greptile.com/blog/ai-code-review",
      "publishedAt": "2025-11-23T17:37:39.557Z",
      "source": "rss",
      "feedName": "Greptile",
      "sourceType": "competitor_blog",
      "company": "Greptile",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.557Z",
      "score": 7
    },
    {
      "id": "05b2fa94a152ec1961919bd0194044eb",
      "title": "Greptile's Biggest Update Yet",
      "url": "https://www.greptile.com/blog/greptile-update",
      "publishedAt": "2025-11-23T17:37:39.557Z",
      "source": "rss",
      "feedName": "Greptile",
      "sourceType": "competitor_blog",
      "company": "Greptile",
      "contentType": "feature_update",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.557Z",
      "score": 6
    },
    {
      "id": "386c84908edf5af91ee7c03a5cc1f349",
      "title": "Do Larger PRs Get Merged Faster?",
      "url": "https://www.greptile.com/blog/do-larger-prs-get-merged-faster",
      "publishedAt": "2025-11-23T17:37:39.557Z",
      "source": "rss",
      "feedName": "Greptile",
      "sourceType": "competitor_blog",
      "company": "Greptile",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.557Z",
      "score": 7
    },
    {
      "id": "8957526d2e2f83f1e172ba5370c00280",
      "title": "AI Code Review: Should the Author Be The Reviewer?",
      "url": "https://www.greptile.com/blog/ai-code-reviews-conflict",
      "publishedAt": "2025-11-23T17:37:39.557Z",
      "source": "rss",
      "feedName": "Greptile",
      "sourceType": "competitor_blog",
      "company": "Greptile",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.557Z",
      "score": 7
    },
    {
      "id": "92f7a7d66fc2b2de72a5393dfd2b42db",
      "title": "Codebases are uniquely hard to search semantically",
      "url": "https://www.greptile.com/blog/semantic-codebase-search",
      "publishedAt": "2025-11-23T17:37:39.557Z",
      "source": "rss",
      "feedName": "Greptile",
      "sourceType": "competitor_blog",
      "company": "Greptile",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.557Z",
      "score": 4
    },
    {
      "id": "874b7cc94d178d37c69af0bc579921d6",
      "title": "Is Two Reviewers the New Standard?",
      "url": "https://www.greptile.com/blog/two-reviewers",
      "publishedAt": "2025-11-23T17:37:39.557Z",
      "source": "rss",
      "feedName": "Greptile",
      "sourceType": "competitor_blog",
      "company": "Greptile",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.557Z",
      "score": 4
    },
    {
      "id": "78ce540679092d064a551fd2778d50ab",
      "title": "The Content-ification of Software",
      "url": "https://www.greptile.com/blog/contentification-of-software",
      "publishedAt": "2025-11-23T17:37:39.557Z",
      "source": "rss",
      "feedName": "Greptile",
      "sourceType": "competitor_blog",
      "company": "Greptile",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.557Z",
      "score": 4
    },
    {
      "id": "66bbf293ff2069e3a5fe1640b5c32b95",
      "title": "How to Make LLMs Shut Up",
      "url": "https://www.greptile.com/blog/make-llms-shut-up",
      "publishedAt": "2025-11-23T17:37:39.557Z",
      "source": "rss",
      "feedName": "Greptile",
      "sourceType": "competitor_blog",
      "company": "Greptile",
      "contentType": "thought_leadership",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.557Z",
      "score": 3.5
    },
    {
      "id": "fb2c80b4b4f93ba5902f1e7c330d9a88",
      "title": "Splitting engineering teams into defense and offense",
      "url": "https://www.greptile.com/blog/how-we-engineer",
      "publishedAt": "2025-11-23T17:37:39.557Z",
      "source": "rss",
      "feedName": "Greptile",
      "sourceType": "competitor_blog",
      "company": "Greptile",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.557Z",
      "score": 4
    },
    {
      "id": "78fd0923fc9e4b23488fe559b0957561",
      "title": "Introducing Greptile 2.0",
      "url": "https://www.greptile.com/blog/greptile-2",
      "publishedAt": "2025-11-23T17:37:39.557Z",
      "source": "rss",
      "feedName": "Greptile",
      "sourceType": "competitor_blog",
      "company": "Greptile",
      "contentType": "product_launch",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.557Z",
      "score": 7
    },
    {
      "id": "082eb48d310c7ede042bb09586bb5a39",
      "title": "Announcing our $4.1M seed round",
      "url": "https://www.greptile.com/blog/seed",
      "publishedAt": "2025-11-23T17:37:39.557Z",
      "source": "rss",
      "feedName": "Greptile",
      "sourceType": "competitor_blog",
      "company": "Greptile",
      "contentType": "product_launch",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.557Z",
      "score": 10
    },
    {
      "id": "5980d07d609850bf74a0d367a96dceff",
      "title": "Star Your Favorite Deep Search Threads",
      "url": "https://webflow.sourcegraph.com/blog/star-your-favorite-deep-search-threads",
      "content": "star-your-favorite-deep-search-threads",
      "summary": "star-your-favorite-deep-search-threads",
      "publishedAt": "2025-11-14T00:13:11.000Z",
      "source": "rss",
      "feedName": "Sourcegraph",
      "sourceType": "competitor_blog",
      "company": "Sourcegraph",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.613Z",
      "score": 1.9969636441793017
    },
    {
      "id": "7a58461dddf4e81a93e61151f76b1b01",
      "title": "Deep Search goes GA, now with role-based permissions",
      "url": "https://webflow.sourcegraph.com/blog/deep-search-goes-ga-now-with-role-based-permissions",
      "content": "deep-search-goes-ga-now-with-role-based-permissions",
      "summary": "deep-search-goes-ga-now-with-role-based-permissions",
      "publishedAt": "2025-10-21T19:50:18.000Z",
      "source": "rss",
      "feedName": "Sourcegraph",
      "sourceType": "competitor_blog",
      "company": "Sourcegraph",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.613Z",
      "score": 0.38126174625309106
    },
    {
      "id": "bba10982ecbf35d9c90c4c0e63a89e0b",
      "title": "Auggie supports ACP–now available in Zed, Neovim, and Emacs",
      "url": "https://www.augmentcode.com/blog/auggie-acp-zed-neovim-emacs",
      "content": "Auggie supports ACP–now available in Zed, Neovim, and Emacs",
      "summary": "Auggie supports ACP–now available in Zed, Neovim, and Emacs",
      "publishedAt": "2025-11-06T05:00:00.000Z",
      "source": "rss",
      "feedName": "Augment Code",
      "sourceType": "competitor_blog",
      "company": "Augment Code",
      "contentType": "product_launch",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.650Z",
      "score": 2.0017905442417776
    },
    {
      "id": "86f390f261260f74857c986089ef2ff9",
      "title": "Our new credit-based plans are now live",
      "url": "https://www.augmentcode.com/blog/our-new-credit-based-plans-are-now-live",
      "content": "Our new credit-based plans are now live",
      "summary": "Our new credit-based plans are now live",
      "publishedAt": "2025-10-21T04:00:00.000Z",
      "source": "rss",
      "feedName": "Augment Code",
      "sourceType": "competitor_blog",
      "company": "Augment Code",
      "contentType": "pricing_business",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.650Z",
      "score": 0.5455602880157391
    },
    {
      "id": "f2d40f1dac51d8b18e4bf8f8d94c550b",
      "title": "Developers are choosing older AI models — and the data explain why",
      "url": "https://www.augmentcode.com/blog/developers-are-choosing-older-ai-models-and-16b-tokens-of-data-explain-why",
      "content": "Developers are choosing older AI models — and the data explain why",
      "summary": "Developers are choosing older AI models — and the data explain why",
      "publishedAt": "2025-10-20T04:00:00.000Z",
      "source": "rss",
      "feedName": "Augment Code",
      "sourceType": "competitor_blog",
      "company": "Augment Code",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.650Z",
      "score": 0.33863391883737415
    },
    {
      "id": "443da25532240b0be88b11104d637a3a",
      "title": "Brand Guidelines in the Agent Era",
      "url": "https://www.augmentcode.com/blog/brand-guidelines-in-the-agent-era",
      "content": "Brand Guidelines in the Agent Era",
      "summary": "Brand Guidelines in the Agent Era",
      "publishedAt": "2025-10-10T04:00:00.000Z",
      "source": "rss",
      "feedName": "Augment Code",
      "sourceType": "competitor_blog",
      "company": "Augment Code",
      "contentType": "general",
      "tags": [
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.650Z",
      "score": 0.2901069685673647
    },
    {
      "id": "19ae609220c35fbf11a6f6fff8a3e071",
      "title": "Augment Code’s pricing is changing on October 20, 2025",
      "url": "https://www.augmentcode.com/blog/augment-codes-pricing-is-changing",
      "content": "Augment Code’s pricing is changing on October 20, 2025",
      "summary": "Augment Code’s pricing is changing on October 20, 2025",
      "publishedAt": "2025-10-06T04:00:00.000Z",
      "source": "rss",
      "feedName": "Augment Code",
      "sourceType": "competitor_blog",
      "company": "Augment Code",
      "contentType": "pricing_business",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.650Z",
      "score": 0.3425852562648691
    },
    {
      "id": "fb8afddc0a629adca356abafd60320d2",
      "title": "Announcing New MCP Integrations: Vercel, Honeycomb, Render, and Heroku",
      "url": "https://www.augmentcode.com/blog/announcing-new-mcp-integrations",
      "content": "Announcing New MCP Integrations: Vercel, Honeycomb, Render, and Heroku",
      "summary": "Announcing New MCP Integrations: Vercel, Honeycomb, Render, and Heroku",
      "publishedAt": "2025-09-16T04:00:00.000Z",
      "source": "rss",
      "feedName": "Augment Code",
      "sourceType": "competitor_blog",
      "company": "Augment Code",
      "contentType": "product_launch",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.650Z",
      "score": 0.08956463098205156
    },
    {
      "id": "ce9e45014ad1798b21ea8eec4dffda95",
      "title": "Augment is now more affordable. Introducing our $20 per month Indie plan",
      "url": "https://www.augmentcode.com/blog/augment-is-now-more-affordable-introducing-our-usd20-per-month-indie-plan",
      "content": "Augment is now more affordable. Introducing our $20 per month Indie plan",
      "summary": "Augment is now more affordable. Introducing our $20 per month Indie plan",
      "publishedAt": "2025-09-12T04:00:00.000Z",
      "source": "rss",
      "feedName": "Augment Code",
      "sourceType": "competitor_blog",
      "company": "Augment Code",
      "contentType": "product_launch",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.650Z",
      "score": 0.03926170875997941
    },
    {
      "id": "240f8d3224cfb032cd8c0b6520587696",
      "title": "How we built Memory Review",
      "url": "https://www.augmentcode.com/blog/how-we-built-memory-review",
      "content": "How we built Memory Review",
      "summary": "How we built Memory Review",
      "publishedAt": "2025-09-08T04:00:00.000Z",
      "source": "rss",
      "feedName": "Augment Code",
      "sourceType": "competitor_blog",
      "company": "Augment Code",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.650Z",
      "score": 0.016859590068834043
    },
    {
      "id": "aefea79e4811a3716e35cd155520b0da",
      "title": "Using the Auggie CLI for automated code review",
      "url": "https://www.augmentcode.com/blog/using-the-auggie-cli-for-automated-code-review",
      "content": "Using the Auggie CLI for automated code review",
      "summary": "Using the Auggie CLI for automated code review",
      "publishedAt": "2025-09-04T04:00:00.000Z",
      "source": "rss",
      "feedName": "Augment Code",
      "sourceType": "competitor_blog",
      "company": "Augment Code",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.650Z",
      "score": 0.02217179843775117
    },
    {
      "id": "30427eb55ee48d8320b6b0d3d7a7218e",
      "title": "Automating customer feedback and support with AI Agents",
      "url": "https://www.augmentcode.com/blog/automating-customer-feedback-and-support-with-ai-agents",
      "content": "Automating customer feedback and support with AI Agents",
      "summary": "Automating customer feedback and support with AI Agents",
      "publishedAt": "2025-09-03T04:00:00.000Z",
      "source": "rss",
      "feedName": "Augment Code",
      "sourceType": "competitor_blog",
      "company": "Augment Code",
      "contentType": "feature_update",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.650Z",
      "score": 0.03243952844705559
    },
    {
      "id": "3c5c243af215186b25807fba50e87981",
      "title": "Gemini 3 Pro Is Now Available in JetBrains IDEs",
      "url": "https://blog.jetbrains.com/ai/2025/11/gemini-3-pro-is-now-available-in-jetbrains-ides/",
      "content": "The latest AI model from Google, Gemini 3 Pro, is now live in JetBrains IDEs. From day one, it powers AI Chat and Junie, our coding agent, giving you smarter reasoning, stronger instruction following, and seamless integration into your workflow. What’s new Gemini 3 Pro shows clear improvements that matter in day-to-day development: Overall, it’s [&#8230;]",
      "summary": "The latest AI model from Google, Gemini 3 Pro, is now live in JetBrains IDEs. From day one, it powers AI Chat and Junie, our coding agent, giving you smarter reasoning, stronger instruction following, and seamless integration into your workflow. What’s new Gemini 3 Pro shows clear improvements that matter in day-to-day development: Overall, it’s […]",
      "publishedAt": "2025-11-18T16:12:47.000Z",
      "author": "Anna Maltseva",
      "source": "rss",
      "feedName": "JetBrains AI",
      "sourceType": "competitor_blog",
      "company": "JetBrains",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.670Z",
      "score": 11.147727384460888
    },
    {
      "id": "453738f82cb85a590f7152c2b6c4d9c2",
      "title": "Why Diffusion Models Could Change Developer Workflows in 2026",
      "url": "https://blog.jetbrains.com/ai/2025/11/why-diffusion-models-could-change-developer-workflows-in-2026/",
      "content": "Developers spend much of their time editing, refactoring, and debugging rather than producing entirely new code. Code creation tends to involve non-sequential back-and-forth refinement rather than typing out a complete function in one uninterrupted sequence. You might sketch a part, adjust parameters, skip ahead, then revisit earlier sections to refine them.&#160; Diffusion models, and in [&#8230;]",
      "summary": "Developers spend much of their time editing, refactoring, and debugging rather than producing entirely new code. Code creation tends to involve non-sequential back-and-forth refinement rather than typing out a complete function in one uninterrupted sequence. You might sketch a part, adjust parameters, skip ahead, then revisit earlier sections to refine them.  Diffusion models, and in […]",
      "publishedAt": "2025-11-17T07:23:13.000Z",
      "author": "Damian Bogunowicz",
      "source": "rss",
      "feedName": "JetBrains AI",
      "sourceType": "competitor_blog",
      "company": "JetBrains",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.670Z",
      "score": 4.423186317404147
    },
    {
      "id": "15c9c9ca5fb5f3877e5c744e92cf6ce2",
      "title": "Building AI Agents in Kotlin – Part 1: A Minimal Coding Agent ",
      "url": "https://blog.jetbrains.com/ai/2025/11/building-ai-agents-in-kotlin-part-1-a-minimal-coding-agent/",
      "content": "Building agents is weird. You&#8217;re not writing code that does things. You&#8217;re writing code that gives an LLM the ability to do things, and the LLM decides what to do. What is an agent? An agent is an LLM that calls your functions in a loop until it decides the task is complete. That shift [&#8230;]",
      "summary": "Building agents is weird. You’re not writing code that does things. You’re writing code that gives an LLM the ability to do things, and the LLM decides what to do. What is an agent? An agent is an LLM that calls your functions in a loop until it decides the task is complete. That shift […]",
      "publishedAt": "2025-11-11T08:35:38.000Z",
      "author": "Fatimazahra El Akkary",
      "source": "rss",
      "feedName": "JetBrains AI",
      "sourceType": "competitor_blog",
      "company": "JetBrains",
      "contentType": "feature_update",
      "tags": [
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.670Z",
      "score": 4.957380575148046
    },
    {
      "id": "acc725dd6921fc2d7d6a747220daa70d",
      "title": "Bring Your Own Key (BYOK) Is Coming Soon to JetBrains AI",
      "url": "https://blog.jetbrains.com/ai/2025/11/bring-your-own-key-byok-is-coming-soon-to-jetbrains-ai/",
      "content": "At JetBrains, we&#8217;ve always believed in giving developers the best possible tools to enhance productivity and creativity. Recently, you&#8217;ve shared important feedback about AI usage limits, transparency, and the freedom to choose which AI providers you use. We heard you loud and clear. That’s why we are working to introduce bring-your-own-key (BYOK) support for JetBrains [&#8230;]",
      "summary": "At JetBrains, we’ve always believed in giving developers the best possible tools to enhance productivity and creativity. Recently, you’ve shared important feedback about AI usage limits, transparency, and the freedom to choose which AI providers you use. We heard you loud and clear. That’s why we are working to introduce bring-your-own-key (BYOK) support for JetBrains […]",
      "publishedAt": "2025-11-03T15:15:02.000Z",
      "author": "Denis Shiryaev",
      "source": "rss",
      "feedName": "JetBrains AI",
      "sourceType": "competitor_blog",
      "company": "JetBrains",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.670Z",
      "score": 2.3796153836185536
    },
    {
      "id": "1ea6b6793bd5f724632636a5e539f2b7",
      "title": "Why Trust Leads and Speed Follows in Agentic Design",
      "url": "https://blog.jetbrains.com/ai/2025/11/why-trust-leads-and-speed-follows-in-agentic-design/",
      "content": "Every developer recognises the trade-off. You can take the shortcut that delivers today but breaks tomorrow, or choose the slower approach that earns lasting confidence. This dilemma has become even more poignant with the widespread integration of AI agents into real workflows. Speed feels tempting, but speed without trust usually wastes more time than it [&#8230;]",
      "summary": "Every developer recognises the trade-off. You can take the shortcut that delivers today but breaks tomorrow, or choose the slower approach that earns lasting confidence. This dilemma has become even more poignant with the widespread integration of AI agents into real workflows. Speed feels tempting, but speed without trust usually wastes more time than it […]",
      "publishedAt": "2025-11-03T08:28:44.000Z",
      "author": "Conrad Schwellnus",
      "source": "rss",
      "feedName": "JetBrains AI",
      "sourceType": "competitor_blog",
      "company": "JetBrains",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.670Z",
      "score": 3.4982058030938616
    },
    {
      "id": "9f325e8a3d5698d3307e447c5a1152b7",
      "title": "Introducing Matter: The AI Development Companion for Product Teams",
      "url": "https://blog.jetbrains.com/matter/2025/10/introducing-matter-the-ai-development-companion-for-product-teams/",
      "content": "At JetBrains, we&#8217;ve spent 25 years building tools for developers. Over the past two years, we&#8217;ve seen that AI isn&#8217;t just changing how developers write code – it&#8217;s beginning to reshape the entire process of product development. Not only is AI making developers more productive, helping with everything from code completion to testing and deployment, [&#8230;]",
      "summary": "At JetBrains, we’ve spent 25 years building tools for developers. Over the past two years, we’ve seen that AI isn’t just changing how developers write code – it’s beginning to reshape the entire process of product development. Not only is AI making developers more productive, helping with everything from code completion to testing and deployment, […]",
      "publishedAt": "2025-10-22T15:27:31.000Z",
      "author": "Elena Berendeeva",
      "source": "rss",
      "feedName": "JetBrains AI",
      "sourceType": "competitor_blog",
      "company": "JetBrains",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "ide",
        "testing"
      ],
      "ingestedAt": "2025-11-23T17:37:39.670Z",
      "score": 1.2630870677398482
    },
    {
      "id": "1ab7771ea588fab42df1ff924d35683b",
      "title": "Koog 0.5.0 Is Out: Smarter Tools, Persistent Agents, and Simplified Strategy Design",
      "url": "https://blog.jetbrains.com/ai/2025/10/koog-0-5-0-is-out-smarter-tools-persistent-agents-and-simplified-strategy-design/",
      "content": "We recently released Koog 0.5.0, introducing full Agent2Agent (A2A) protocol support, which makes it easier than ever to build systems of interconnected AI agents in Kotlin. But A2A is just the beginning. Koog 0.5.0 brings a host of improvements that make agents more persistent, tools smarter, and strategy design more intuitive. Let’s dive into the [&#8230;]",
      "summary": "We recently released Koog 0.5.0, introducing full Agent2Agent (A2A) protocol support, which makes it easier than ever to build systems of interconnected AI agents in Kotlin. But A2A is just the beginning. Koog 0.5.0 brings a host of improvements that make agents more persistent, tools smarter, and strategy design more intuitive. Let’s dive into the […]",
      "publishedAt": "2025-10-17T07:19:48.000Z",
      "author": "Daniela Bentrup",
      "source": "rss",
      "feedName": "JetBrains AI",
      "sourceType": "competitor_blog",
      "company": "JetBrains",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.670Z",
      "score": 1.0351487983590304
    },
    {
      "id": "27bfb7470dcae2e978544dc3f1ec2d70",
      "title": "JetBrains × Zed: Open Interoperability for AI Coding Agents in Your IDE",
      "url": "https://blog.jetbrains.com/ai/2025/10/jetbrains-zed-open-interoperability-for-ai-coding-agents-in-your-ide/",
      "content": "At JetBrains, our mission is simple: help developers achieve engineering excellence without distractions or vendor lock-in. Today, we’re excited to share that we’re collaborating with Zed on the Agent Client Protocol (ACP) – an open protocol that lets AI coding agents work inside editors, including JetBrains IDEs, if both are ACP-compatible. Why this collaboration matters [&#8230;]",
      "summary": "At JetBrains, our mission is simple: help developers achieve engineering excellence without distractions or vendor lock-in. Today, we’re excited to share that we’re collaborating with Zed on the Agent Client Protocol (ACP) – an open protocol that lets AI coding agents work inside editors, including JetBrains IDEs, if both are ACP-compatible. Why this collaboration matters […]",
      "publishedAt": "2025-10-06T15:57:18.000Z",
      "author": "Denis Shiryaev",
      "source": "rss",
      "feedName": "JetBrains AI",
      "sourceType": "competitor_blog",
      "company": "JetBrains",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.670Z",
      "score": 0.4840827286336772
    },
    {
      "id": "f6f8ded377d19d5b775d359ada47ddf3",
      "title": "Koog × A2A: Building Connected AI Agents in Kotlin",
      "url": "https://blog.jetbrains.com/ai/2025/10/koog-a2a-building-connected-ai-agents-in-kotlin/",
      "content": "If you’ve ever tried building a system of multiple AI agents, you’ve probably run into the problem. It starts simple enough: You’ve got one agent writing blog posts, another proofreading them, and maybe a third suggesting or generating images. Individually, they’re effective. But getting them to work together? That&#8217;s where things might start falling apart. [&#8230;]",
      "summary": "If you’ve ever tried building a system of multiple AI agents, you’ve probably run into the problem. It starts simple enough: You’ve got one agent writing blog posts, another proofreading them, and maybe a third suggesting or generating images. Individually, they’re effective. But getting them to work together? That’s where things might start falling apart. […]",
      "publishedAt": "2025-10-02T14:48:45.000Z",
      "author": "Andrey Bragin",
      "source": "rss",
      "feedName": "JetBrains AI",
      "sourceType": "competitor_blog",
      "company": "JetBrains",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.670Z",
      "score": 0.3383728399172041
    },
    {
      "id": "ef59b28511a289b02380ab378a6dc388",
      "title": "Introducing Claude Agent in JetBrains IDEs",
      "url": "https://blog.jetbrains.com/ai/2025/09/introducing-claude-agent-in-jetbrains-ides/",
      "content": "TL;DR: The new Claude Agent is now seamlessly integrated into JetBrains IDEs via the AI chat and included in the JetBrains AI subscription. No extra plugins, no extra subscriptions – just advanced AI coding assistance built on Anthropic’s Agent SDK, which powers Claude Code, right inside your IDE. The new agent leverages Anthropic models, including [&#8230;]",
      "summary": "TL;DR: The new Claude Agent is now seamlessly integrated into JetBrains IDEs via the AI chat and included in the JetBrains AI subscription. No extra plugins, no extra subscriptions – just advanced AI coding assistance built on Anthropic’s Agent SDK, which powers Claude Code, right inside your IDE. The new agent leverages Anthropic models, including […]",
      "publishedAt": "2025-09-29T17:04:16.000Z",
      "author": "Ivan Moiseev",
      "source": "rss",
      "feedName": "JetBrains AI",
      "sourceType": "competitor_blog",
      "company": "JetBrains",
      "contentType": "product_launch",
      "tags": [
        "retrieval",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.670Z",
      "score": 0.22585075670379112
    },
    {
      "id": "a8b24dc6ec7be044f804e5624f4d39f3",
      "title": "Continuing Positive Impact: This Year’s AI4SE Interns in their New Roles at JetBrains",
      "url": "https://blog.jetbrains.com/research/2025/09/ai4se-interns-employees-part-2/",
      "content": "In our last post, we introduced the talented intern cohort from our AI for Software Engineering (AI4SE) research partnership with Delft University of Technology (TU Delft). In this post, we&#8217;re excited to spotlight the next chapter in their journey: their evolution from interns to full-time members of our tech team. As interns, they didn&#8217;t just [&#8230;]",
      "summary": "In our last post, we introduced the talented intern cohort from our AI for Software Engineering (AI4SE) research partnership with Delft University of Technology (TU Delft). In this post, we’re excited to spotlight the next chapter in their journey: their evolution from interns to full-time members of our tech team. As interns, they didn’t just […]",
      "publishedAt": "2025-09-25T14:37:14.000Z",
      "author": "Katie Fraser",
      "source": "rss",
      "feedName": "JetBrains AI",
      "sourceType": "competitor_blog",
      "company": "JetBrains",
      "contentType": "general",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.670Z",
      "score": 0.07325581891175709
    },
    {
      "id": "c6901fb883338f81f607bd5b421c8d36",
      "title": "FAQ: New AI Quotas",
      "url": "https://blog.jetbrains.com/ai/2025/09/faq-new-ai-quota/",
      "content": "We hear you. And you’re right – the new quotas are a reduction from what you experienced. When we launched, we had little usage data for agentic experiences and no top‑up feature existed, so we set quotas much higher than the real cost of AI usage (tokens from providers) as a temporary measure. That felt [&#8230;]",
      "summary": "We hear you. And you’re right – the new quotas are a reduction from what you experienced. When we launched, we had little usage data for agentic experiences and no top‑up feature existed, so we set quotas much higher than the real cost of AI usage (tokens from providers) as a temporary measure. That felt […]",
      "publishedAt": "2025-09-16T15:01:26.000Z",
      "author": "Ilya Petrov",
      "source": "rss",
      "feedName": "JetBrains AI",
      "sourceType": "competitor_blog",
      "company": "JetBrains",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.670Z",
      "score": 0.12340254652262735
    },
    {
      "id": "5e59a8e3a87a68eb8723077bd2fe1e05",
      "title": "OpenAI and Foxconn collaborate to strengthen U.S. manufacturing across the AI supply chain",
      "url": "https://openai.com/index/openai-and-foxconn-collaborate",
      "content": "OpenAI and Foxconn are collaborating to design and manufacture next-generation AI infrastructure hardware in the U.S. The partnership will develop multiple generations of data-center systems, strengthen U.S. supply chains, and build key components domestically to accelerate advanced AI infrastructure.",
      "summary": "OpenAI and Foxconn are collaborating to design and manufacture next-generation AI infrastructure hardware in the U.S. The partnership will develop multiple generations of data-center systems, strengthen U.S. supply chains, and build key components domestically to accelerate advanced AI infrastructure.",
      "publishedAt": "2025-11-20T14:50:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "security_incident",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 4.802598670135846
    },
    {
      "id": "72be2ab1ebb821913376387da739f8d4",
      "title": "Helping 1,000 small businesses build with AI",
      "url": "https://openai.com/index/small-business-ai-jam",
      "content": "OpenAI is partnering with DoorDash, SCORE, and local organizations to help 1,000 small businesses build with AI. The Small Business AI Jam gives Main Street business owners hands-on tools and training to compete and grow.",
      "summary": "OpenAI is partnering with DoorDash, SCORE, and local organizations to help 1,000 small businesses build with AI. The Small Business AI Jam gives Main Street business owners hands-on tools and training to compete and grow.",
      "publishedAt": "2025-11-20T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 2.338992539362838
    },
    {
      "id": "0c40a000ff645391fcc87c00e55a58bd",
      "title": "Early experiments in accelerating science with GPT-5",
      "url": "https://openai.com/index/accelerating-science-gpt-5",
      "content": "OpenAI introduces the first research cases showing how GPT-5 accelerates scientific progress across math, physics, biology, and computer science. Explore how AI and researchers collaborate to generate proofs, uncover new insights, and reshape the pace of discovery.",
      "summary": "OpenAI introduces the first research cases showing how GPT-5 accelerates scientific progress across math, physics, biology, and computer science. Explore how AI and researchers collaborate to generate proofs, uncover new insights, and reshape the pace of discovery.",
      "publishedAt": "2025-11-20T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 4.595191063256645
    },
    {
      "id": "0ebef0cfe225255149ed76d29443910d",
      "title": "Strengthening our safety ecosystem with external testing",
      "url": "https://openai.com/index/strengthening-safety-with-external-testing",
      "content": "OpenAI works with independent experts to evaluate frontier AI systems. Third-party testing strengthens safety, validates safeguards, and increases transparency in how we assess model capabilities and risks.",
      "summary": "OpenAI works with independent experts to evaluate frontier AI systems. Third-party testing strengthens safety, validates safeguards, and increases transparency in how we assess model capabilities and risks.",
      "publishedAt": "2025-11-19T12:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "testing"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 3.3254798432803208
    },
    {
      "id": "2aba7a56124890447b62f7caad623b9d",
      "title": "How evals drive the next chapter in AI for businesses",
      "url": "https://openai.com/index/evals-drive-next-chapter-of-ai",
      "content": "Learn how evals help businesses define, measure, and improve AI performance—reducing risk, boosting productivity, and driving strategic advantage.",
      "summary": "Learn how evals help businesses define, measure, and improve AI performance—reducing risk, boosting productivity, and driving strategic advantage.",
      "publishedAt": "2025-11-19T11:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 4.42079639374572
    },
    {
      "id": "94c1a542d3710b59d4d9aec07e4b9c31",
      "title": "OpenAI and Target team up on new AI-powered experiences",
      "url": "https://openai.com/index/target-partnership",
      "content": "OpenAI and Target are partnering to bring a new Target app to ChatGPT, offering personalized shopping and faster checkout. Target will also expand its use of ChatGPT Enterprise to boost productivity and guest experiences.",
      "summary": "OpenAI and Target are partnering to bring a new Target app to ChatGPT, offering personalized shopping and faster checkout. Target will also expand its use of ChatGPT Enterprise to boost productivity and guest experiences.",
      "publishedAt": "2025-11-19T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 7.2591629846871175
    },
    {
      "id": "2cebf18ae568df8657f35714cc29b1bc",
      "title": "GPT-5.1-Codex-Max System Card",
      "url": "https://openai.com/index/gpt-5-1-codex-max-system-card",
      "content": "This system card outlines the comprehensive safety measures implemented for GPT‑5.1-CodexMax. It details both model-level mitigations, such as specialized safety training for harmful tasks and prompt injections, and product-level mitigations like agent sandboxing and configurable network access.",
      "summary": "This system card outlines the comprehensive safety measures implemented for GPT‑5.1-CodexMax. It details both model-level mitigations, such as specialized safety training for harmful tasks and prompt injections, and product-level mitigations like agent sandboxing and configurable network access.",
      "publishedAt": "2025-11-19T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 7.13068560771136
    },
    {
      "id": "e6949d4d910f746de520e1d46478a327",
      "title": "Teacher Access Terms",
      "url": "https://openai.com/policies/education-terms",
      "content": "Teacher Access Terms outline how verified educators may use ChatGPT for Teachers, covering eligibility, account management, and data privacy requirements.",
      "summary": "Teacher Access Terms outline how verified educators may use ChatGPT for Teachers, covering eligibility, account management, and data privacy requirements.",
      "publishedAt": "2025-11-19T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 4.278411364626816
    },
    {
      "id": "17045c091cb6e6fcb5f6a7f519b1245e",
      "title": "How Scania is accelerating work with AI across its global workforce",
      "url": "https://openai.com/index/scania",
      "content": "Description: Global manufacturer Scania is scaling AI with ChatGPT Enterprise. With team-based onboarding and strong guardrails, AI is boosting productivity, quality, and innovation.",
      "summary": "Description: Global manufacturer Scania is scaling AI with ChatGPT Enterprise. With team-based onboarding and strong guardrails, AI is boosting productivity, quality, and innovation.",
      "publishedAt": "2025-11-19T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 7.13068560771136
    },
    {
      "id": "10c90d7cc7819d97e798a3759b2b0abf",
      "title": "A free version of ChatGPT built for teachers",
      "url": "https://openai.com/index/chatgpt-for-teachers",
      "content": "ChatGPT for Teachers is a secure workspace with education‑grade privacy and admin controls. Free for verified U.S. K–12 educators through June 2027.",
      "summary": "ChatGPT for Teachers is a secure workspace with education‑grade privacy and admin controls. Free for verified U.S. K–12 educators through June 2027.",
      "publishedAt": "2025-11-19T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 4.278411364626816
    },
    {
      "id": "8a6a49d607bcd5fa4da85f6781aec3aa",
      "title": "Building more with GPT-5.1-Codex-Max",
      "url": "https://openai.com/index/gpt-5-1-codex-max",
      "content": "Introducing GPT-5.1-Codex-Max, a faster, more intelligent agentic coding model for Codex. The model is designed for long-running, project-scale work with enhanced reasoning and token efficiency.",
      "summary": "Introducing GPT-5.1-Codex-Max, a faster, more intelligent agentic coding model for Codex. The model is designed for long-running, project-scale work with enhanced reasoning and token efficiency.",
      "publishedAt": "2025-11-19T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 7.843754168482496
    },
    {
      "id": "b7f6337287f50eb44829e8f73a37876f",
      "title": "Intuit and OpenAI join forces on new AI-powered experiences",
      "url": "https://openai.com/index/intuit-partnership",
      "content": "OpenAI and Intuit have entered a $100M+ multi-year partnership to launch Intuit app experiences in ChatGPT and expand Intuit’s use of OpenAI’s frontier models to power personalized financial tools.",
      "summary": "OpenAI and Intuit have entered a $100M+ multi-year partnership to launch Intuit app experiences in ChatGPT and expand Intuit’s use of OpenAI’s frontier models to power personalized financial tools.",
      "publishedAt": "2025-11-18T05:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 6.064785975179486
    },
    {
      "id": "0ee69ec84e3309ca88e2a257f13122c4",
      "title": "OpenAI named Emerging Leader in Generative AI",
      "url": "https://openai.com/index/gartner-2025-emerging-leader",
      "content": "OpenAI has been named an Emerging Leader in Gartner’s 2025 Innovation Guide for Generative AI Model Providers. The recognition reflects our enterprise momentum, with over 1 million companies building with ChatGPT.",
      "summary": "OpenAI has been named an Emerging Leader in Gartner’s 2025 Innovation Guide for Generative AI Model Providers. The recognition reflects our enterprise momentum, with over 1 million companies building with ChatGPT.",
      "publishedAt": "2025-11-17T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 7.0049870399753775
    },
    {
      "id": "41722a5599ac5dfac4a04db42448e49f",
      "title": "Introducing OpenAI for Ireland",
      "url": "https://openai.com/index/openai-for-ireland",
      "content": "OpenAI launches OpenAI for Ireland, partnering with the Irish Government, Dogpatch Labs and Patch to help SMEs, founders and young builders use AI to innovate, boost productivity and build the next generation of Irish tech startups.",
      "summary": "OpenAI launches OpenAI for Ireland, partnering with the Irish Government, Dogpatch Labs and Patch to help SMEs, founders and young builders use AI to innovate, boost productivity and build the next generation of Irish tech startups.",
      "publishedAt": "2025-11-14T04:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 6.058673304865515
    },
    {
      "id": "965e60a14502559303595f93f6b0ee74",
      "title": "Understanding neural networks through sparse circuits",
      "url": "https://openai.com/index/understanding-neural-networks-through-sparse-circuits",
      "content": "OpenAI is exploring mechanistic interpretability to understand how neural networks reason. Our new sparse model approach could make AI systems more transparent and support safer, more reliable behavior.",
      "summary": "OpenAI is exploring mechanistic interpretability to understand how neural networks reason. Our new sparse model approach could make AI systems more transparent and support safer, more reliable behavior.",
      "publishedAt": "2025-11-13T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 2.871321108451722
    },
    {
      "id": "c78557dbdad9b4f12ac173ddd735e477",
      "title": "Introducing GPT-5.1 for developers",
      "url": "https://openai.com/index/gpt-5-1-for-developers",
      "content": "GPT-5.1 is now available in the API, bringing faster adaptive reasoning, extended prompt caching, improved coding performance, and new apply_patch and shell tools.",
      "summary": "GPT-5.1 is now available in the API, bringing faster adaptive reasoning, extended prompt caching, improved coding performance, and new apply_patch and shell tools.",
      "publishedAt": "2025-11-13T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 4.1806864006539755
    },
    {
      "id": "a7818ab7bde93e745d5e7e936314b982",
      "title": "How Philips is scaling AI literacy across 70,000 employees",
      "url": "https://openai.com/index/philips",
      "content": "Philips is scaling AI literacy with ChatGPT Enterprise, training 70,000 employees to use AI responsibly and improve healthcare outcomes worldwide.",
      "summary": "Philips is scaling AI literacy with ChatGPT Enterprise, training 70,000 employees to use AI responsibly and improve healthcare outcomes worldwide.",
      "publishedAt": "2025-11-13T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 5.109727823021525
    },
    {
      "id": "402d1c676b28ff91394671aa38a74c10",
      "title": "Introducing group chats in ChatGPT",
      "url": "https://openai.com/index/group-chats-in-chatgpt",
      "content": "We’re piloting group chats in ChatGPT to make collaboration simple. Bring others—and ChatGPT—into one shared conversation to plan, brainstorm, and create together.",
      "summary": "We’re piloting group chats in ChatGPT to make collaboration simple. Bring others—and ChatGPT—into one shared conversation to plan, brainstorm, and create together.",
      "publishedAt": "2025-11-13T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 2.78712426710265
    },
    {
      "id": "39cff7c19932507f265b3e9ed2a3a7c3",
      "title": "Neuro drives national retail wins with ChatGPT Business",
      "url": "https://openai.com/index/neurogum",
      "content": "Neuro uses ChatGPT Business to scale nationwide with fewer than seventy employees. From drafting contracts to uncovering insights in customer data, the team saves time, cuts costs, and turns ideas into growth.",
      "summary": "Neuro uses ChatGPT Business to scale nationwide with fewer than seventy employees. From drafting contracts to uncovering insights in customer data, the team saves time, cuts costs, and turns ideas into growth.",
      "publishedAt": "2025-11-12T11:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 1.787565702102548
    },
    {
      "id": "b377a85ca19b7ac5211137128691f9b5",
      "title": "Fighting the New York Times’ invasion of user privacy",
      "url": "https://openai.com/index/fighting-nyt-user-privacy-invasion",
      "content": "OpenAI is fighting the New York Times’ demand for 20 million private ChatGPT conversations and accelerating new security and privacy protections to protect your data.",
      "summary": "OpenAI is fighting the New York Times’ demand for 20 million private ChatGPT conversations and accelerating new security and privacy protections to protect your data.",
      "publishedAt": "2025-11-12T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "security_incident",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 4.843195405470187
    },
    {
      "id": "36ade2f70cc53016a3448fc3a473cc35",
      "title": "GPT-5.1 Instant and GPT-5.1 Thinking System Card Addendum",
      "url": "https://openai.com/index/gpt-5-system-card-addendum-gpt-5-1",
      "content": "This GPT-5 system card addendum provides updated safety metrics for GPT-5.1 Instant and Thinking, including new evaluations for mental health and emotional reliance.",
      "summary": "This GPT-5 system card addendum provides updated safety metrics for GPT-5.1 Instant and Thinking, including new evaluations for mental health and emotional reliance.",
      "publishedAt": "2025-11-12T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "ide",
        "observability"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 4.324979445848552
    },
    {
      "id": "9cbe22667b832a7e186116d4a4eebc4c",
      "title": "GPT-5.1: A smarter, more conversational ChatGPT",
      "url": "https://openai.com/index/gpt-5-1",
      "content": "We’re upgrading the GPT-5 series with warmer, more capable models and new ways to customize ChatGPT’s tone and style. GPT-5.1 starts rolling out today to paid users.",
      "summary": "We’re upgrading the GPT-5 series with warmer, more capable models and new ways to customize ChatGPT’s tone and style. GPT-5.1 starts rolling out today to paid users.",
      "publishedAt": "2025-11-12T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 1.2974938337545654
    },
    {
      "id": "963896ca7550df724537c1d3a3f6fb07",
      "title": "Free ChatGPT for transitioning U.S. servicemembers and veterans",
      "url": "https://openai.com/index/chatgpt-for-veterans",
      "content": "OpenAI is offering U.S. servicemembers and veterans within 12 months of retirement or separation a free year of ChatGPT Plus to support their transition to civilian life. The tools can help with resumes, interviews, education, and planning for what’s next.",
      "summary": "OpenAI is offering U.S. servicemembers and veterans within 12 months of retirement or separation a free year of ChatGPT Plus to support their transition to civilian life. The tools can help with resumes, interviews, education, and planning for what’s next.",
      "publishedAt": "2025-11-10T02:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 1.131483746849546
    },
    {
      "id": "fe9a5f2a83a904e45e648632b6863145",
      "title": "Understanding prompt injections: a frontier security challenge ",
      "url": "https://openai.com/index/prompt-injections",
      "content": "Prompt injections are a frontier security challenge for AI systems. Learn how these attacks work and how OpenAI is advancing research, training models, and building safeguards for users.",
      "summary": "Prompt injections are a frontier security challenge for AI systems. Learn how these attacks work and how OpenAI is advancing research, training models, and building safeguards for users.",
      "publishedAt": "2025-11-07T11:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "security_incident",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 3.444576279483573
    },
    {
      "id": "fbb44934033164d0cf9a7af53ff519bc",
      "title": "Notion’s rebuild for agentic AI: How GPT‑5 helped unlock autonomous workflows",
      "url": "https://openai.com/index/notion",
      "content": "Discover how Notion rebuilt its AI architecture with GPT-5 to create autonomous agents that reason, act, and adapt across workflows. Learn how this shift unlocked smarter, faster, and more flexible productivity in Notion 3.0.",
      "summary": "Discover how Notion rebuilt its AI architecture with GPT-5 to create autonomous agents that reason, act, and adapt across workflows. Learn how this shift unlocked smarter, faster, and more flexible productivity in Notion 3.0.",
      "publishedAt": "2025-11-07T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 4.052729886310766
    },
    {
      "id": "79d8d3cdcb4b7b140ef6ae2a79cbfb99",
      "title": "From Pilot to Practice: How BBVA Is Scaling AI Across the Organization",
      "url": "https://openai.com/index/bbva-2025",
      "content": "BBVA is reimagining how employees work with ChatGPT Enterprise, embedding AI into everyday operations. The bank has saved hours per week per employee, created 20,000+ Custom GPTs, and achieved up to 80% efficiency gains.",
      "summary": "BBVA is reimagining how employees work with ChatGPT Enterprise, embedding AI into everyday operations. The bank has saved hours per week per employee, created 20,000+ Custom GPTs, and achieved up to 80% efficiency gains.",
      "publishedAt": "2025-11-06T09:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 3.3329963735964085
    },
    {
      "id": "18d486d98d8e15ab9bd6e59a4cf8fb68",
      "title": "Introducing the Teen Safety Blueprint",
      "url": "https://openai.com/index/introducing-the-teen-safety-blueprint",
      "content": "Discover OpenAI’s Teen Safety Blueprint—a roadmap for building AI responsibly with safeguards, age-appropriate design, and collaboration to protect and empower young people online.",
      "summary": "Discover OpenAI’s Teen Safety Blueprint—a roadmap for building AI responsibly with safeguards, age-appropriate design, and collaboration to protect and empower young people online.",
      "publishedAt": "2025-11-06T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 2.5357144806402907
    },
    {
      "id": "98b1ba064888fdf56902e113065c3f82",
      "title": "AI progress and recommendations",
      "url": "https://openai.com/index/ai-progress-and-recommendations",
      "content": "AI is advancing fast. We have the chance to shape its progress—toward discovery, safety, and a better future for everyone.",
      "summary": "AI is advancing fast. We have the chance to shape its progress—toward discovery, safety, and a better future for everyone.",
      "publishedAt": "2025-11-06T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 1.6904763204268605
    },
    {
      "id": "b0c322f8b9c6c755a6cdaa6ded1b80bb",
      "title": "How CRED is tapping AI to deliver premium customer experiences",
      "url": "https://openai.com/index/cred-swamy-seetharaman",
      "content": "CRED is transforming premium customer experiences in India with OpenAI. Using GPT-powered tools, the company is improving support accuracy, reducing response times, and boosting customer satisfaction. ",
      "summary": "CRED is transforming premium customer experiences in India with OpenAI. Using GPT-powered tools, the company is improving support accuracy, reducing response times, and boosting customer satisfaction.",
      "publishedAt": "2025-11-05T21:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 1.6779450487400678
    },
    {
      "id": "903883be1da5291d9c594b8e86c84af9",
      "title": "How Chime is redefining marketing through AI",
      "url": "https://openai.com/index/chime-vineet-mehra",
      "content": "Vineet Mehra, Chief Marketing Officer at Chime, shares how AI is reshaping marketing into an agent-driven discipline. He explains why CMOs who champion AI literacy and thoughtful adoption will lead in the new era of growth.",
      "summary": "Vineet Mehra, Chief Marketing Officer at Chime, shares how AI is reshaping marketing into an agent-driven discipline. He explains why CMOs who champion AI literacy and thoughtful adoption will lead in the new era of growth.",
      "publishedAt": "2025-11-05T15:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.752Z",
      "score": 1.3714973013408662
    },
    {
      "id": "0116832adda7180dde75dfc8f43bf0e9",
      "title": "1 million business customers putting AI to work",
      "url": "https://openai.com/index/1-million-businesses-putting-ai-to-work",
      "content": "More than 1 million business customers around the world now use OpenAI. Across healthcare, life sciences, financial services, and more, ChatGPT and our APIs are driving a new era of intelligent, AI-powered work.",
      "summary": "More than 1 million business customers around the world now use OpenAI. Across healthcare, life sciences, financial services, and more, ChatGPT and our APIs are driving a new era of intelligent, AI-powered work.",
      "publishedAt": "2025-11-05T05:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.7987682184861835
    },
    {
      "id": "cfa7130eb87b757a8f3079190eb6c6b6",
      "title": "Brazil’s AI moment is here",
      "url": "https://openai.com/global-affairs/brazil-ai-moment-is-here",
      "content": "Brazil is now one of the most engaged countries in the world when it comes to AI. From classrooms to farms and small businesses, Brazilians are using OpenAI products to learn, create, and drive innovation.",
      "summary": "Brazil is now one of the most engaged countries in the world when it comes to AI. From classrooms to farms and small businesses, Brazilians are using OpenAI products to learn, create, and drive innovation.",
      "publishedAt": "2025-11-04T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 1.5097062518567672
    },
    {
      "id": "5f20663e3457bcdb3265601be63b85dd",
      "title": "Introducing IndQA",
      "url": "https://openai.com/index/introducing-indqa",
      "content": "OpenAI introduces IndQA, a new benchmark for evaluating AI systems in Indian languages. Built with domain experts, IndQA tests cultural understanding and reasoning across 12 languages and 10 knowledge areas.",
      "summary": "OpenAI introduces IndQA, a new benchmark for evaluating AI systems in Indian languages. Built with domain experts, IndQA tests cultural understanding and reasoning across 12 languages and 10 knowledge areas.",
      "publishedAt": "2025-11-03T22:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 1.458909014769815
    },
    {
      "id": "e68f6330a84831492f791da1bba7c027",
      "title": "AWS and OpenAI announce multi-year strategic partnership",
      "url": "https://openai.com/index/aws-and-openai-partnership",
      "content": "OpenAI and AWS have entered a multi-year, $38 billion partnership to scale advanced AI workloads. AWS will provide world-class infrastructure and compute capacity to power OpenAI’s next generation of models.",
      "summary": "OpenAI and AWS have entered a multi-year, $38 billion partnership to scale advanced AI workloads. AWS will provide world-class infrastructure and compute capacity to power OpenAI’s next generation of models.",
      "publishedAt": "2025-11-03T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 1.6204962722104201
    },
    {
      "id": "505e58f8ab61940d5c691b82e6f50f7e",
      "title": "Expanding Stargate to Michigan",
      "url": "https://openai.com/index/expanding-stargate-to-michigan",
      "content": "OpenAI is expanding Stargate to Michigan with a new one-gigawatt campus that strengthens America’s AI infrastructure. The project will create jobs, drive investment, and support economic growth across the Midwest.",
      "summary": "OpenAI is expanding Stargate to Michigan with a new one-gigawatt campus that strengthens America’s AI infrastructure. The project will create jobs, drive investment, and support economic growth across the Midwest.",
      "publishedAt": "2025-10-30T13:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 1.0673606364167436
    },
    {
      "id": "c4440d30b857fbf5c0a2e557dd7047c2",
      "title": "Introducing Aardvark: OpenAI’s agentic security researcher",
      "url": "https://openai.com/index/introducing-aardvark",
      "content": "OpenAI introduces Aardvark, an AI-powered security researcher that autonomously finds, validates, and helps fix software vulnerabilities at scale. The system is in private beta—sign up to join early testing.",
      "summary": "OpenAI introduces Aardvark, an AI-powered security researcher that autonomously finds, validates, and helps fix software vulnerabilities at scale. The system is in private beta—sign up to join early testing.",
      "publishedAt": "2025-10-30T11:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents",
        "testing"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 3.0900579406181325
    },
    {
      "id": "da48b434eb0d31b33c96b13edca512c6",
      "title": "How we built OWL, the new architecture behind our ChatGPT-based browser, Atlas",
      "url": "https://openai.com/index/building-chatgpt-atlas",
      "content": "A deep dive into OWL, the new architecture powering ChatGPT Atlas—decoupling Chromium, enabling fast startup, rich UI, and agentic browsing with ChatGPT.",
      "summary": "A deep dive into OWL, the new architecture powering ChatGPT Atlas—decoupling Chromium, enabling fast startup, rich UI, and agentic browsing with ChatGPT.",
      "publishedAt": "2025-10-30T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "thought_leadership",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.7689942877570737
    },
    {
      "id": "38a263f439fa2281877e4af4ff39e33c",
      "title": "Technical Report: Performance and baseline evaluations of gpt-oss-safeguard-120b and gpt-oss-safeguard-20b",
      "url": "https://openai.com/index/gpt-oss-safeguard-technical-report",
      "content": "gpt-oss-safeguard-120b and gpt-oss-safeguard-20b are two open-weight reasoning models post-trained from the gpt-oss models and trained to reason from a provided policy in order to label content under that policy. In this report, we describe gpt-oss-safeguard’s capabilities and provide our baseline safety evaluations on the gpt-oss-safeguard models, using the underlying gpt-oss models as a baseline. For more information about the development and architecture of the underlying gpt-oss models, see the original gpt-oss model model card⁠.",
      "summary": "gpt-oss-safeguard-120b and gpt-oss-safeguard-20b are two open-weight reasoning models post-trained from the gpt-oss models and trained to reason from a provided policy in order to label content under that policy. In this report, we describe gpt-oss-safeguard’s capabilities and provide our baseline safety evaluations on the gpt-oss-safeguard models, using the underlying gpt-oss models as a baseline. For more information about the development and architecture of the underlying gpt-oss models, see the original gpt-oss model model card⁠.",
      "publishedAt": "2025-10-29T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 1.7501781223315065
    },
    {
      "id": "d210a60ba3c9ac11de685cfaad8c759f",
      "title": "Introducing gpt-oss-safeguard",
      "url": "https://openai.com/index/introducing-gpt-oss-safeguard",
      "content": "OpenAI introduces gpt-oss-safeguard—open-weight reasoning models for safety classification that let developers apply and iterate on custom policies.",
      "summary": "OpenAI introduces gpt-oss-safeguard—open-weight reasoning models for safety classification that let developers apply and iterate on custom policies.",
      "publishedAt": "2025-10-29T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.9546426121808217
    },
    {
      "id": "d0aeb835ebcc55244a78cab0dea6c915",
      "title": "Knowledge preservation powered by ChatGPT",
      "url": "https://openai.com/index/dai-nippon-printing",
      "content": "Dai Nippon Printing (DNP) rolled out ChatGPT Enterprise across ten core departments to drive companywide adoption. Within three months, it achieved 95% faster patent research, 10x processing volume, 100% weekly active usage, 87% automation, and 70% knowledge reuse.",
      "summary": "Dai Nippon Printing (DNP) rolled out ChatGPT Enterprise across ten core departments to drive companywide adoption. Within three months, it achieved 95% faster patent research, 10x processing volume, 100% weekly active usage, 87% automation, and 70% knowledge reuse.",
      "publishedAt": "2025-10-28T17:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 1.7140932671672453
    },
    {
      "id": "c9a5421993a12907c5ef4f4b2a1bbb29",
      "title": "Doppel’s AI defense system stops attacks before they spread",
      "url": "https://openai.com/index/doppel",
      "content": "Discover how Doppel uses OpenAI’s GPT-5 and reinforcement fine-tuning (RFT) to stop deepfake and impersonation attacks before they spread, cutting analyst workloads by 80% and reducing threat response from hours to minutes.",
      "summary": "Discover how Doppel uses OpenAI’s GPT-5 and reinforcement fine-tuning (RFT) to stop deepfake and impersonation attacks before they spread, cutting analyst workloads by 80% and reducing threat response from hours to minutes.",
      "publishedAt": "2025-10-28T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.9156831289109721
    },
    {
      "id": "e38d1cab2aaf508a8f8c92e705c84a1e",
      "title": "The next chapter of the Microsoft–OpenAI partnership",
      "url": "https://openai.com/index/next-chapter-of-microsoft-openai-partnership",
      "content": "Microsoft and OpenAI sign a new agreement that strengthens its long-term partnership, expands innovation, and ensures responsible AI progress.",
      "summary": "Microsoft and OpenAI sign a new agreement that strengthens its long-term partnership, expands innovation, and ensures responsible AI progress.",
      "publishedAt": "2025-10-28T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.9048467694011528
    },
    {
      "id": "3b1dccf662884f705b8f3ee8f71556ad",
      "title": "Built to benefit everyone",
      "url": "https://openai.com/index/built-to-benefit-everyone",
      "content": "OpenAI’s recapitalization strengthens mission-focused governance, expanding resources to ensure AI benefits everyone while advancing innovation responsibly.",
      "summary": "OpenAI’s recapitalization strengthens mission-focused governance, expanding resources to ensure AI benefits everyone while advancing innovation responsibly.",
      "publishedAt": "2025-10-28T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 1.0556545643013449
    },
    {
      "id": "0b114fe032efff2c9e404ee3519459ad",
      "title": "Seizing the AI opportunity",
      "url": "https://openai.com/global-affairs/seizing-the-ai-opportunity",
      "content": "Meeting the demands of the Intelligence Age will require strategic investment in energy and infrastructure. OpenAI’s submission to the White House details how expanding capacity and workforce readiness can sustain U.S. leadership in AI and economic growth.",
      "summary": "Meeting the demands of the Intelligence Age will require strategic investment in energy and infrastructure. OpenAI’s submission to the White House details how expanding capacity and workforce readiness can sustain U.S. leadership in AI and economic growth.",
      "publishedAt": "2025-10-27T12:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.4288241828139878
    },
    {
      "id": "0b4d41f3f0256100b027dc00b4f17856",
      "title": "Strengthening ChatGPT’s responses in sensitive conversations",
      "url": "https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations",
      "content": "OpenAI collaborated with 170+ mental health experts to improve ChatGPT’s ability to recognize distress, respond empathetically, and guide users toward real-world support—reducing unsafe responses by up to 80%. Learn how we’re making ChatGPT safer and more supportive in sensitive moments.",
      "summary": "OpenAI collaborated with 170+ mental health experts to improve ChatGPT’s ability to recognize distress, respond empathetically, and guide users toward real-world support—reducing unsafe responses by up to 80%. Learn how we’re making ChatGPT safer and more supportive in sensitive moments.",
      "publishedAt": "2025-10-27T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.9946515592205811
    },
    {
      "id": "645cb42d567084e0db8ca964b86b17cb",
      "title": "Addendum to GPT-5 System Card: Sensitive conversations",
      "url": "https://openai.com/index/gpt-5-system-card-sensitive-conversations",
      "content": "This system card details GPT-5’s improvements in handling sensitive conversations, including new benchmarks for emotional reliance, mental health, and jailbreak resistance.",
      "summary": "This system card details GPT-5’s improvements in handling sensitive conversations, including new benchmarks for emotional reliance, mental health, and jailbreak resistance.",
      "publishedAt": "2025-10-27T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.9946515592205811
    },
    {
      "id": "530334a2b1f084930d8c41f1f42dbc97",
      "title": "A law and tax firm redefines efficiency with ChatGPT Business",
      "url": "https://openai.com/index/steuerrecht",
      "content": "Learn how Steuerrecht.com uses ChatGPT Business to streamline legal workflows, automate tax research, and scale client service—helping law firms boost productivity and stay competitive.",
      "summary": "Learn how Steuerrecht.com uses ChatGPT Business to streamline legal workflows, automate tax research, and scale client service—helping law firms boost productivity and stay competitive.",
      "publishedAt": "2025-10-27T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.8275585826593379
    },
    {
      "id": "56c92596194fcf972d95186fa117d291",
      "title": "OpenAI acquires Software Applications Incorporated, maker of Sky",
      "url": "https://openai.com/index/openai-acquires-software-applications-incorporated",
      "content": "OpenAI has acquired Software Applications Incorporated, maker of Sky—a natural language interface for Mac that brings AI directly into your desktop experience. Together, we’re integrating Sky’s deep macOS capabilities into ChatGPT to make AI more intuitive, contextual, and action-oriented.",
      "summary": "OpenAI has acquired Software Applications Incorporated, maker of Sky—a natural language interface for Mac that brings AI directly into your desktop experience. Together, we’re integrating Sky’s deep macOS capabilities into ChatGPT to make AI more intuitive, contextual, and action-oriented.",
      "publishedAt": "2025-10-23T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "funding_mna",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.533898615197282
    },
    {
      "id": "64b3f66ab547fb2f8b4bad62612426e1",
      "title": "Consensus accelerates research with GPT-5 and Responses API",
      "url": "https://openai.com/index/consensus",
      "content": "Consensus uses GPT-5 and OpenAI’s Responses API to power a multi-agent research assistant that reads, analyzes, and synthesizes evidence in minutes—helping over 8 million researchers accelerate scientific discovery.",
      "summary": "Consensus uses GPT-5 and OpenAI’s Responses API to power a multi-agent research assistant that reads, analyzes, and synthesizes evidence in minutes—helping over 8 million researchers accelerate scientific discovery.",
      "publishedAt": "2025-10-23T09:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.6387743921266998
    },
    {
      "id": "98db91455deebf842653cbcf27fddc0c",
      "title": "AI in South Korea—OpenAI’s Economic Blueprint",
      "url": "https://openai.com/index/south-korea-economic-blueprint",
      "content": "OpenAI's Korea Economic Blueprint outlines how South Korea can scale trusted AI through sovereign capabilities and strategic partnerships to drive growth.",
      "summary": "OpenAI's Korea Economic Blueprint outlines how South Korea can scale trusted AI through sovereign capabilities and strategic partnerships to drive growth.",
      "publishedAt": "2025-10-23T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.6218914835580595
    },
    {
      "id": "6cab6185e60b3c00f2df28ddd700a237",
      "title": "Work smarter with your company knowledge in ChatGPT",
      "url": "https://openai.com/index/introducing-company-knowledge",
      "content": "Company knowledge brings context from your apps into ChatGPT for answers specific to your business, with clear citations, security, privacy, and admin controls. Available now for Business, Enterprise, and Edu users.",
      "summary": "Company knowledge brings context from your apps into ChatGPT for answers specific to your business, with clear citations, security, privacy, and admin controls. Available now for Business, Enterprise, and Edu users.",
      "publishedAt": "2025-10-23T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 1.243782967116119
    },
    {
      "id": "9e3769b2689a02b5c992bdc0aeeeff10",
      "title": "The next chapter for UK sovereign AI",
      "url": "https://openai.com/index/the-next-chapter-for-uk-sovereign-ai",
      "content": "OpenAI expands its UK partnership with a new Ministry of Justice agreement, bringing ChatGPT to civil servants. It also introduces UK data residency for ChatGPT Enterprise, ChatGPT Edu, and the API Platform to support trusted and secure AI adoption.",
      "summary": "OpenAI expands its UK partnership with a new Ministry of Justice agreement, bringing ChatGPT to civil servants. It also introduces UK data residency for ChatGPT Enterprise, ChatGPT Edu, and the API Platform to support trusted and secure AI adoption.",
      "publishedAt": "2025-10-22T16:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 1.1133089474184414
    },
    {
      "id": "be4be33746705b0d0950d2be47f72e6c",
      "title": "AI in Japan—OpenAI’s Japan Economic Blueprint",
      "url": "https://openai.com/index/japan-economic-blueprint",
      "content": "OpenAI’s Japan Economic Blueprint outlines how Japan can harness AI to boost innovation, strengthen competitiveness, and enable sustainable, inclusive growth.",
      "summary": "OpenAI’s Japan Economic Blueprint outlines how Japan can harness AI to boost innovation, strengthen competitiveness, and enable sustainable, inclusive growth.",
      "publishedAt": "2025-10-22T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.5790200133558256
    },
    {
      "id": "e4d1b0c932ea2468037dbe669b213a6d",
      "title": "Continue your ChatGPT experience beyond WhatsApp",
      "url": "https://openai.com/index/chatgpt-whatsapp-transition",
      "content": "ChatGPT will no longer be available on WhatsApp after January 15, 2026. Learn how to link your ChatGPT account and continue your conversations across devices.",
      "summary": "ChatGPT will no longer be available on WhatsApp after January 15, 2026. Learn how to link your ChatGPT account and continue your conversations across devices.",
      "publishedAt": "2025-10-21T17:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "thought_leadership",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.23628411821453008
    },
    {
      "id": "b9ca7a0cbf8b850f689c19fbc99e7ea3",
      "title": "Introducing ChatGPT Atlas, the browser with ChatGPT built in",
      "url": "https://openai.com/index/introducing-chatgpt-atlas",
      "content": "ChatGPT Atlas, the browser with ChatGPT built it. Get instant answers, summaries, and smart web help—right from any page. With privacy settings you can control. Available now for MacOS.",
      "summary": "ChatGPT Atlas, the browser with ChatGPT built it. Get instant answers, summaries, and smart web help—right from any page. With privacy settings you can control. Available now for MacOS.",
      "publishedAt": "2025-10-21T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.808655974709003
    },
    {
      "id": "28547405a2a03a9d9f75af7980ce49d4",
      "title": "Plex Coffee delivers fast service and personal connections with ChatGPT Business",
      "url": "https://openai.com/index/plex-coffee",
      "content": "Learn how Plex Coffee uses ChatGPT Business to centralize knowledge, train staff faster, and preserve personal connections while expanding.",
      "summary": "Learn how Plex Coffee uses ChatGPT Business to centralize knowledge, train staff faster, and preserve personal connections while expanding.",
      "publishedAt": "2025-10-15T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.3511933906875267
    },
    {
      "id": "1add205a7b950a6c34f0aff1705b621a",
      "title": "Expert Council on Well-Being and AI",
      "url": "https://openai.com/index/expert-council-on-well-being-and-ai",
      "content": "OpenAI’s new Expert Council on Well-Being and AI brings together leading psychologists, clinicians, and researchers to guide how ChatGPT supports emotional health, especially for teens. Learn how their insights are shaping safer, more caring AI experiences.",
      "summary": "OpenAI’s new Expert Council on Well-Being and AI brings together leading psychologists, clinicians, and researchers to guide how ChatGPT supports emotional health, especially for teens. Learn how their insights are shaping safer, more caring AI experiences.",
      "publishedAt": "2025-10-14T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.22457399850259083
    },
    {
      "id": "1dc90d6c45eeb820572ee35457dd3973",
      "title": "Argentina’s AI opportunity",
      "url": "https://openai.com/global-affairs/argentinas-ai-opportunity",
      "content": "OpenAI and Sur Energy are exploring Argentina’s first Stargate project—an AI and clean energy collaboration that could make Argentina a Latin American leader in artificial intelligence, sustainable infrastructure, and digital innovation.",
      "summary": "OpenAI and Sur Energy are exploring Argentina’s first Stargate project—an AI and clean energy collaboration that could make Argentina a Latin American leader in artificial intelligence, sustainable infrastructure, and digital innovation.",
      "publishedAt": "2025-10-14T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.33287452387308103
    },
    {
      "id": "bb222ce6ef6f18b3cbd95657b37d61e2",
      "title": "OpenAI and Broadcom announce strategic collaboration to deploy 10 gigawatts of OpenAI-designed AI accelerators",
      "url": "https://openai.com/index/openai-and-broadcom-announce-strategic-collaboration",
      "content": "OpenAI and Broadcom announce a multi-year partnership to deploy 10 gigawatts of OpenAI-designed AI accelerators, co-developing next-generation systems and Ethernet solutions to power scalable, energy-efficient AI infrastructure by 2029.",
      "summary": "OpenAI and Broadcom announce a multi-year partnership to deploy 10 gigawatts of OpenAI-designed AI accelerators, co-developing next-generation systems and Ethernet solutions to power scalable, energy-efficient AI infrastructure by 2029.",
      "publishedAt": "2025-10-13T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.15496353974496202
    },
    {
      "id": "cc033ceceb1f0dd641d673a39364802a",
      "title": "HYGH powers next-gen digital ads with ChatGPT Business",
      "url": "https://openai.com/index/hygh",
      "content": "HYGH speeds up software development and campaign delivery with ChatGPT Business, cutting turnaround times, scaling output, and driving revenue growth.",
      "summary": "HYGH speeds up software development and campaign delivery with ChatGPT Business, cutting turnaround times, scaling output, and driving revenue growth.",
      "publishedAt": "2025-10-10T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.12286018538585863
    },
    {
      "id": "e82e5aa8257125e1a056f2c6f042b7ed",
      "title": "Defining and evaluating political bias in LLMs",
      "url": "https://openai.com/index/defining-and-evaluating-political-bias-in-llms",
      "content": "Learn how OpenAI evaluates political bias in ChatGPT through new real-world testing methods that improve objectivity and reduce bias.",
      "summary": "Learn how OpenAI evaluates political bias in ChatGPT through new real-world testing methods that improve objectivity and reduce bias.",
      "publishedAt": "2025-10-09T13:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review",
        "testing"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.29725776002637694
    },
    {
      "id": "c4c17fc22989318d842838e2d6084c2c",
      "title": "Growing impact and scale with ChatGPT",
      "url": "https://openai.com/index/hibob",
      "content": "Discover how HiBob uses ChatGPT Enterprise and custom GPTs to scale AI adoption, boost revenue, streamline HR workflows, and deliver AI-powered features in the Bob platform.",
      "summary": "Discover how HiBob uses ChatGPT Enterprise and custom GPTs to scale AI adoption, boost revenue, streamline HR workflows, and deliver AI-powered features in the Bob platform.",
      "publishedAt": "2025-10-08T08:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.36357012335351435
    },
    {
      "id": "04d6a1d695c66b58622e52c69cb7b03f",
      "title": "Disrupting malicious uses of AI: October 2025",
      "url": "https://openai.com/global-affairs/disrupting-malicious-uses-of-ai-october-2025",
      "content": "Discover how OpenAI is detecting and disrupting malicious uses of AI in our October 2025 report. Learn how we’re countering misuse, enforcing policies, and protecting users from real-world harms.",
      "summary": "Discover how OpenAI is detecting and disrupting malicious uses of AI in our October 2025 report. Learn how we’re countering misuse, enforcing policies, and protecting users from real-world harms.",
      "publishedAt": "2025-10-07T03:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.20010396217276957
    },
    {
      "id": "cfa376d35cdc706ff41e08fab86264a2",
      "title": "Codex is now generally available",
      "url": "https://openai.com/index/codex-now-generally-available",
      "content": "OpenAI Codex is now generally available with powerful new features for developers: a Slack integration, Codex SDK, and admin tools like usage dashboards and workspace management—making Codex easier to use and manage at scale.",
      "summary": "OpenAI Codex is now generally available with powerful new features for developers: a Slack integration, Codex SDK, and admin tools like usage dashboards and workspace management—making Codex easier to use and manage at scale.",
      "publishedAt": "2025-10-06T10:50:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.753Z",
      "score": 0.28605584989260696
    },
    {
      "id": "827fa6105627442a73e477a0c4b0877e",
      "title": "Introducing apps in ChatGPT and the new Apps SDK",
      "url": "https://openai.com/index/introducing-apps-in-chatgpt",
      "content": "We’re introducing a new generation of apps you can chat with, right inside ChatGPT. Developers can start building them today with the new Apps SDK, available in preview.",
      "summary": "We’re introducing a new generation of apps you can chat with, right inside ChatGPT. Developers can start building them today with the new Apps SDK, available in preview.",
      "publishedAt": "2025-10-06T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.3170525164532773
    },
    {
      "id": "5a187a71faa5ec4d12cac0311ea99cf3",
      "title": "AMD and OpenAI announce strategic partnership to deploy 6 gigawatts of AMD GPUs",
      "url": "https://openai.com/index/openai-amd-strategic-partnership",
      "content": "AMD and OpenAI have announced a multi-year partnership to deploy 6 gigawatts of AMD Instinct GPUs, beginning with 1 gigawatt in 2026, to power OpenAI’s next-generation AI infrastructure and accelerate global AI innovation.",
      "summary": "AMD and OpenAI have announced a multi-year partnership to deploy 6 gigawatts of AMD Instinct GPUs, beginning with 1 gigawatt in 2026, to power OpenAI’s next-generation AI infrastructure and accelerate global AI innovation.",
      "publishedAt": "2025-10-06T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.0939901379152132
    },
    {
      "id": "85ca9d16f1edb3b31b657898c77ac088",
      "title": "Introducing AgentKit, new Evals, and RFT for agents",
      "url": "https://openai.com/index/introducing-agentkit",
      "content": "Today, we’re releasing  new tools to help developers go from prototype to production faster: AgentKit, expanded evals capabilities, and reinforcement fine-tuning for agents.",
      "summary": "Today, we’re releasing  new tools to help developers go from prototype to production faster: AgentKit, expanded evals capabilities, and reinforcement fine-tuning for agents.",
      "publishedAt": "2025-10-06T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.4308576508327682
    },
    {
      "id": "1477c89fc230ebc2a6a524720fca682d",
      "title": "Accelerating AI adoption in Europe",
      "url": "https://openai.com/global-affairs/accelerating-ai-uptake-in-europe",
      "content": "OpenAI and Allied for Startups release the Hacktivate AI report with 20 actionable policy ideas to accelerate AI adoption in Europe, boost competitiveness, and empower innovators.",
      "summary": "OpenAI and Allied for Startups release the Hacktivate AI report with 20 actionable policy ideas to accelerate AI adoption in Europe, boost competitiveness, and empower innovators.",
      "publishedAt": "2025-10-06T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "ide",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.24620437190443897
    },
    {
      "id": "dbc72c125358e94388b0d3c5f49c854c",
      "title": "With GPT-5, Wrtn builds lifestyle AI for millions in Korea",
      "url": "https://openai.com/index/wrtn",
      "content": "Wrtn scaled AI apps to 6.5M users in Korea with GPT-5, creating ‘Lifestyle AI’ that blends productivity, creativity, and learning—now expanding across East Asia.",
      "summary": "Wrtn scaled AI apps to 6.5M users in Korea with GPT-5, creating ‘Lifestyle AI’ that blends productivity, creativity, and learning—now expanding across East Asia.",
      "publishedAt": "2025-10-02T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.14295466009620988
    },
    {
      "id": "20d626c31e241a549f0fca70bdc6f587",
      "title": "OpenAI announces strategic collaboration with Japan’s Digital Agency",
      "url": "https://openai.com/global-affairs/strategic-collaboration-with-japan-digital-agency",
      "content": "OpenAI and Japan’s Digital Agency partner to advance generative AI in public services, support international AI governance, and promote safe, trustworthy AI adoption worldwide.",
      "summary": "OpenAI and Japan’s Digital Agency partner to advance generative AI in public services, support international AI governance, and promote safe, trustworthy AI adoption worldwide.",
      "publishedAt": "2025-10-02T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.254398368045317
    },
    {
      "id": "e484c6c3bfd6bfbe7d8dd664728de4dc",
      "title": "Samsung and SK join OpenAI’s Stargate initiative to advance global AI infrastructure",
      "url": "https://openai.com/index/samsung-and-sk-join-stargate",
      "content": "Samsung and SK join OpenAI’s Stargate initiative to expand global AI infrastructure, scaling advanced memory chip production and building next-gen data centers in Korea.",
      "summary": "Samsung and SK join OpenAI’s Stargate initiative to expand global AI infrastructure, scaling advanced memory chip production and building next-gen data centers in Korea.",
      "publishedAt": "2025-10-01T03:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.1303555364182915
    },
    {
      "id": "4d9450ab9ccf696eb09be15d4a74ed55",
      "title": "The Sora feed philosophy",
      "url": "https://openai.com/index/sora-feed-philosophy",
      "content": "Discover the Sora feed philosophy—built to spark creativity, foster connections, and keep experiences safe with personalized recommendations, parental controls, and strong guardrails.",
      "summary": "Discover the Sora feed philosophy—built to spark creativity, foster connections, and keep experiences safe with personalized recommendations, parental controls, and strong guardrails.",
      "publishedAt": "2025-09-30T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.06196211775185176
    },
    {
      "id": "2fac130322d848b9b924cf9b4ddbf67a",
      "title": "Sora 2 is here ",
      "url": "https://openai.com/index/sora-2",
      "content": "Our latest video generation model is more physically accurate, realistic, and controllable than prior systems. It also features synchronized dialogue and sound effects. Create with it in the new Sora app.",
      "summary": "Our latest video generation model is more physically accurate, realistic, and controllable than prior systems. It also features synchronized dialogue and sound effects. Create with it in the new Sora app.",
      "publishedAt": "2025-09-30T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.14033875099427154
    },
    {
      "id": "f69a340d1942c483799966425ef86bdf",
      "title": "Launching Sora responsibly",
      "url": "https://openai.com/index/launching-sora-responsibly",
      "content": "To address the novel safety challenges posed by a state-of-the-art video model as well as a new social creation platform, we’ve built Sora 2 and the Sora app with safety at the foundation. Our approach is anchored in concrete protections.",
      "summary": "To address the novel safety challenges posed by a state-of-the-art video model as well as a new social creation platform, we’ve built Sora 2 and the Sora app with safety at the foundation. Our approach is anchored in concrete protections.",
      "publishedAt": "2025-09-30T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.2606291089893614
    },
    {
      "id": "a6fde770d64655cd6c8349474e7d5696",
      "title": "Sora 2 System Card",
      "url": "https://openai.com/index/sora-2-system-card",
      "content": "Sora 2 is our new state of the art video and audio generation model. Building on the foundation of Sora, this new model introduces capabilities that have been difficult for prior video models to achieve– such as more accurate physics, sharper realism, synchronized audio, enhanced steerability, and an expanded stylistic range.",
      "summary": "Sora 2 is our new state of the art video and audio generation model. Building on the foundation of Sora, this new model introduces capabilities that have been difficult for prior video models to achieve– such as more accurate physics, sharper realism, synchronized audio, enhanced steerability, and an expanded stylistic range.",
      "publishedAt": "2025-09-30T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.14033875099427154
    },
    {
      "id": "5dddb19ef1f395f47468164a91736c7e",
      "title": "Converting inbound leads into customers at OpenAI",
      "url": "https://openai.com/index/openai-inbound-sales-assistant",
      "content": "Learn how OpenAI used AI to deliver personalized answers at scale, converting inbound leads into customers.",
      "summary": "Learn how OpenAI used AI to deliver personalized answers at scale, converting inbound leads into customers.",
      "publishedAt": "2025-09-29T13:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.058294706377949174
    },
    {
      "id": "f08b360665f3ef27edf5f2e4a308247a",
      "title": "Turning contracts into searchable data at OpenAI",
      "url": "https://openai.com/index/openai-contract-data-agent",
      "content": "OpenAI built a system to extract contract data quickly, cutting turnaround times and making it easier for teams to access the details they need.",
      "summary": "OpenAI built a system to extract contract data quickly, cutting turnaround times and making it easier for teams to access the details they need.",
      "publishedAt": "2025-09-29T13:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.058294706377949174
    },
    {
      "id": "bb50d1f5dcd988ee2be4db2bd185a5f0",
      "title": "Improving support with every interaction at OpenAI",
      "url": "https://openai.com/index/openai-support-model",
      "content": "Learn how OpenAI uses AI to enhance support, cutting response times, improving quality, and scaling to meet hypergrowth.",
      "summary": "Learn how OpenAI uses AI to enhance support, cutting response times, improving quality, and scaling to meet hypergrowth.",
      "publishedAt": "2025-09-29T13:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.11658941275589835
    },
    {
      "id": "f971e17cab1cf7230ce6366613bf6f73",
      "title": "Empowering teams to unlock insights faster at OpenAI",
      "url": "https://openai.com/index/openai-research-assistant",
      "content": "OpenAI’s research assistant helps teams analyze millions of support tickets, surface insights faster, and scale curiosity across the company.",
      "summary": "OpenAI’s research assistant helps teams analyze millions of support tickets, surface insights faster, and scale curiosity across the company.",
      "publishedAt": "2025-09-29T13:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.058294706377949174
    },
    {
      "id": "8b0251310f43d0b052900ad6d0eebb3d",
      "title": "Driving sales productivity and customer success at OpenAI",
      "url": "https://openai.com/index/openai-gtm-assistant",
      "content": "Learn how OpenAI boosts sales productivity by automating prep, centralizing knowledge, and scaling top-selling practices.",
      "summary": "Learn how OpenAI boosts sales productivity by automating prep, centralizing knowledge, and scaling top-selling practices.",
      "publishedAt": "2025-09-29T13:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.11658941275589835
    },
    {
      "id": "975901811b7cd7780e3112a4d4b312a0",
      "title": "Building OpenAI with OpenAI",
      "url": "https://openai.com/index/building-openai-with-openai",
      "content": "At OpenAI, we rely on our own technology to help streamline work, scale expertise, and drive outcomes. In our new series, OpenAI on OpenAI, we share lessons to help other organizations do the same.",
      "summary": "At OpenAI, we rely on our own technology to help streamline work, scale expertise, and drive outcomes. In our new series, OpenAI on OpenAI, we share lessons to help other organizations do the same.",
      "publishedAt": "2025-09-29T13:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.058294706377949174
    },
    {
      "id": "094b6d8bc0c0e95589a0abdb802878e7",
      "title": "Introducing parental controls",
      "url": "https://openai.com/index/introducing-parental-controls",
      "content": "We’re rolling out parental controls and a new parent resource page to help families guide how ChatGPT works in their homes. ",
      "summary": "We’re rolling out parental controls and a new parent resource page to help families guide how ChatGPT works in their homes.",
      "publishedAt": "2025-09-29T03:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.13183605590294634
    },
    {
      "id": "ccfe69df641ddbdd0816ab0322aff250",
      "title": "Combating online child sexual exploitation & abuse",
      "url": "https://openai.com/index/combating-online-child-sexual-exploitation-abuse",
      "content": "Discover how OpenAI combats online child sexual exploitation and abuse with strict usage policies, advanced detection tools, and industry collaboration to block, report, and prevent AI misuse.",
      "summary": "Discover how OpenAI combats online child sexual exploitation and abuse with strict usage policies, advanced detection tools, and industry collaboration to block, report, and prevent AI misuse.",
      "publishedAt": "2025-09-29T03:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "security_incident",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.1695035004466453
    },
    {
      "id": "a08a05bcf099abba82d52afaff908801",
      "title": "Buy it in ChatGPT: Instant Checkout and the Agentic Commerce Protocol",
      "url": "https://openai.com/index/buy-it-in-chatgpt",
      "content": "We’re taking first steps toward agentic commerce in ChatGPT with new ways for people, AI agents, and businesses to shop together.",
      "summary": "We’re taking first steps toward agentic commerce in ChatGPT with new ways for people, AI agents, and businesses to shop together.",
      "publishedAt": "2025-09-29T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.2426620626874176
    },
    {
      "id": "eb382ae71c88bf997db66713d5c18306",
      "title": "Partnering with AARP to help keep older adults safe online",
      "url": "https://openai.com/index/aarp-partnership-older-adults-online-safety",
      "content": "OpenAI and AARP are partnering to help older adults stay safe online with new AI training, scam-spotting tools, and nationwide programs through OpenAI Academy and OATS’s Senior Planet initiative.",
      "summary": "OpenAI and AARP are partnering to help older adults stay safe online with new AI training, scam-spotting tools, and nationwide programs through OpenAI Academy and OATS’s Senior Planet initiative.",
      "publishedAt": "2025-09-26T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.10736153889300086
    },
    {
      "id": "51e0b2cb3881a875c1accfb152c1d346",
      "title": "More ways to work with your team and tools in ChatGPT",
      "url": "https://openai.com/index/more-ways-to-work-with-your-team",
      "content": "ChatGPT business plans now support shared projects, smarter connectors, and enhanced compliance features to help teams work faster and more securely.",
      "summary": "ChatGPT business plans now support shared projects, smarter connectors, and enhanced compliance features to help teams work faster and more securely.",
      "publishedAt": "2025-09-25T11:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.17392964680711664
    },
    {
      "id": "8e2e6c05adfeada02b2aaaf9cd48dd73",
      "title": "Measuring the performance of our models on real-world tasks",
      "url": "https://openai.com/index/gdpval",
      "content": "OpenAI introduces GDPval, a new evaluation that measures model performance on real-world economically valuable tasks across 44 occupations.",
      "summary": "OpenAI introduces GDPval, a new evaluation that measures model performance on real-world economically valuable tasks across 44 occupations.",
      "publishedAt": "2025-09-25T09:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.04322435660564659
    },
    {
      "id": "ce60d03934966df51726d812522c5569",
      "title": "Introducing ChatGPT Pulse",
      "url": "https://openai.com/index/introducing-chatgpt-pulse",
      "content": "Today we're releasing a preview of ChatGPT Pulse to Pro users on mobile. Pulse is a new experience where ChatGPT proactively does research to deliver personalized updates based on your chats, feedback, and connected apps like your calendar. ",
      "summary": "Today we're releasing a preview of ChatGPT Pulse to Pro users on mobile. Pulse is a new experience where ChatGPT proactively does research to deliver personalized updates based on your chats, feedback, and connected apps like your calendar.",
      "publishedAt": "2025-09-25T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.126245790000281
    },
    {
      "id": "25e1e85d3fb851ce95a300d3ce82405e",
      "title": "Transforming the manufacturing industry with ChatGPT",
      "url": "https://openai.com/index/eneos-materials",
      "content": "By deploying ChatGPT Enterprise, ENEOS Materials transformed operations with faster research, safer plant design, and streamlined HR processes. Over 80% of employees report major workflow improvements, strengthening competitiveness in manufacturing.",
      "summary": "By deploying ChatGPT Enterprise, ENEOS Materials transformed operations with faster research, safer plant design, and streamlined HR processes. Over 80% of employees report major workflow improvements, strengthening competitiveness in manufacturing.",
      "publishedAt": "2025-09-24T17:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.137380974660401
    },
    {
      "id": "b9772dbe916eeb921363e414d1015162",
      "title": "SAP and OpenAI partner to launch sovereign ‘OpenAI for Germany’",
      "url": "https://openai.com/global-affairs/openai-for-germany",
      "content": "SAP and OpenAI launch OpenAI for Germany, a 2026 partnership to bring secure, sovereign AI to Germany’s public sector, enabling safe, efficient public services.",
      "summary": "SAP and OpenAI launch OpenAI for Germany, a 2026 partnership to bring secure, sovereign AI to Germany’s public sector, enabling safe, efficient public services.",
      "publishedAt": "2025-09-24T04:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.11895043711758008
    },
    {
      "id": "cfd16316f767c456c01780da65181fad",
      "title": "OpenAI, Oracle, and SoftBank expand Stargate with five new AI datacenter sites",
      "url": "https://openai.com/index/five-new-stargate-sites",
      "content": "OpenAI, Oracle, and SoftBank announce five new Stargate AI datacenter sites, accelerating a $500B, 10-gigawatt U.S. infrastructure buildout to power next-generation AI and create tens of thousands of jobs.",
      "summary": "OpenAI, Oracle, and SoftBank announce five new Stargate AI datacenter sites, accelerating a $500B, 10-gigawatt U.S. infrastructure buildout to power next-generation AI and create tens of thousands of jobs.",
      "publishedAt": "2025-09-23T14:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.038032001733902095
    },
    {
      "id": "5d8fe756e5c59b6ba3c4ddac18750f2e",
      "title": "CNA is transforming its newsroom with AI ",
      "url": "https://openai.com/index/cna-walter-fernandez",
      "content": "In this Executive Function series from OpenAI, discover how CNA is transforming its newsroom with AI. Editor-in-Chief Walter Fernandez shares insights on AI adoption, culture, and the future of journalism.",
      "summary": "In this Executive Function series from OpenAI, discover how CNA is transforming its newsroom with AI. Editor-in-Chief Walter Fernandez shares insights on AI adoption, culture, and the future of journalism.",
      "publishedAt": "2025-09-22T17:17:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "thought_leadership",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.04171754943380831
    },
    {
      "id": "98abe74afed5312cf32e09c01561c8ad",
      "title": "American-made innovation",
      "url": "https://openai.com/global-affairs/american-made-innovation",
      "content": "American-made innovation",
      "summary": "American-made innovation",
      "publishedAt": "2025-09-22T11:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.0351476903810783
    },
    {
      "id": "9b042a61ba87a8a943bb08bb61cd6292",
      "title": "Creating a safe, observable AI infrastructure for 1 million classrooms",
      "url": "https://openai.com/index/schoolai",
      "content": "Discover how SchoolAI, built on OpenAI’s GPT-4.1, image generation, and TTS, powers safe, teacher-guided AI tools for 1 million classrooms worldwide—boosting engagement, oversight, and personalized learning.",
      "summary": "Discover how SchoolAI, built on OpenAI’s GPT-4.1, image generation, and TTS, powers safe, teacher-guided AI tools for 1 million classrooms worldwide—boosting engagement, oversight, and personalized learning.",
      "publishedAt": "2025-09-22T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.046654841029932496
    },
    {
      "id": "4b2a7a48f7c5d49808f505194f48653c",
      "title": "OpenAI and NVIDIA announce strategic partnership to deploy 10 gigawatts of NVIDIA systems",
      "url": "https://openai.com/index/openai-nvidia-systems-partnership",
      "content": "OpenAI and NVIDIA announce a strategic partnership to deploy 10 gigawatts of AI datacenters powered by NVIDIA systems, with the first phase launching in 2026.",
      "summary": "OpenAI and NVIDIA announce a strategic partnership to deploy 10 gigawatts of AI datacenters powered by NVIDIA systems, with the first phase launching in 2026.",
      "publishedAt": "2025-09-22T08:45:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.10458359182915097
    },
    {
      "id": "c92c326e7977c38c715e4093d91f4e55",
      "title": "Outbound coordinated vulnerability disclosure policy",
      "url": "https://openai.com/policies/outbound-coordinated-disclosure-policy",
      "content": "Outbound coordinated vulnerability disclosure policy",
      "summary": "Outbound coordinated vulnerability disclosure policy",
      "publishedAt": "2025-09-22T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "security_incident",
      "tags": [
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.13586029012525638
    },
    {
      "id": "7c673bb5582cb600bdb5d37ee67f10c2",
      "title": "Detecting and reducing scheming in AI models",
      "url": "https://openai.com/index/detecting-and-reducing-scheming-in-ai-models",
      "content": "Apollo Research and OpenAI developed evaluations for hidden misalignment (“scheming”) and found behaviors consistent with scheming in controlled tests across frontier models. The team shared concrete examples and stress tests of an early method to reduce scheming. ",
      "summary": "Apollo Research and OpenAI developed evaluations for hidden misalignment (“scheming”) and found behaviors consistent with scheming in controlled tests across frontier models. The team shared concrete examples and stress tests of an early method to reduce scheming.",
      "publishedAt": "2025-09-17T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.023764428480114886
    },
    {
      "id": "e90c5244240c96a3e507e5163d66592a",
      "title": "Introducing Stargate UK",
      "url": "https://openai.com/index/introducing-stargate-uk",
      "content": "OpenAI, NVIDIA, and Nscale launch Stargate UK, a sovereign AI infrastructure partnership delivering up to 50,000 GPUs and the UK’s largest supercomputer to power national AI innovation, public services, and economic growth.",
      "summary": "OpenAI, NVIDIA, and Nscale launch Stargate UK, a sovereign AI infrastructure partnership delivering up to 50,000 GPUs and the UK’s largest supercomputer to power national AI innovation, public services, and economic growth.",
      "publishedAt": "2025-09-16T14:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.06930578221178546
    },
    {
      "id": "7e49bfef600761c4c581a839ef9acd1e",
      "title": "Teen safety, freedom, and privacy",
      "url": "https://openai.com/index/teen-safety-freedom-and-privacy",
      "content": "Explore OpenAI’s approach to balancing teen safety, freedom, and privacy in AI use.",
      "summary": "Explore OpenAI’s approach to balancing teen safety, freedom, and privacy in AI use.",
      "publishedAt": "2025-09-16T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.045049667933372575
    },
    {
      "id": "d42b318d4ecd54b84becb6ac86c8d257",
      "title": "Building towards age prediction",
      "url": "https://openai.com/index/building-towards-age-prediction",
      "content": "Learn how OpenAI is building age prediction and parental controls in ChatGPT to create safer, age-appropriate experiences for teens while supporting families with new tools.",
      "summary": "Learn how OpenAI is building age prediction and parental controls in ChatGPT to create safer, age-appropriate experiences for teens while supporting families with new tools.",
      "publishedAt": "2025-09-16T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.045049667933372575
    },
    {
      "id": "a7a0c8c9ee2c2652fa75f4e745beed59",
      "title": "Introducing upgrades to Codex",
      "url": "https://openai.com/index/introducing-upgrades-to-codex",
      "content": "Codex just got faster, more reliable, and better at real-time collaboration and tackling tasks independently anywhere you develop—whether via the terminal, IDE, web, or even your phone.",
      "summary": "Codex just got faster, more reliable, and better at real-time collaboration and tackling tasks independently anywhere you develop—whether via the terminal, IDE, web, or even your phone.",
      "publishedAt": "2025-09-15T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.04952078514017774
    },
    {
      "id": "9bf2135904c60a85a9b6620e76029750",
      "title": "How people are using ChatGPT",
      "url": "https://openai.com/index/how-people-are-using-chatgpt",
      "content": "New research from the largest study of ChatGPT use shows how the tool creates economic value through both personal and professional use. Adoption is broadening beyond early users, closing gaps and making AI a part of everyday life.",
      "summary": "New research from the largest study of ChatGPT use shows how the tool creates economic value through both personal and professional use. Adoption is broadening beyond early users, closing gaps and making AI a part of everyday life.",
      "publishedAt": "2025-09-15T03:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.041571235347276816
    },
    {
      "id": "7d1e13edca9f0e939d70af04915921b7",
      "title": "Addendum to GPT-5 system card: GPT-5-Codex",
      "url": "https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex",
      "content": "This addendum to the GPT-5 system card shares a new model: GPT-5-Codex, a version of GPT-5 further optimized for agentic coding in Codex. GPT-5-Codex adjusts its thinking effort more dynamically based on task complexity, responding quickly to simple conversational queries or small tasks, while independently working for longer on more complex tasks.",
      "summary": "This addendum to the GPT-5 system card shares a new model: GPT-5-Codex, a version of GPT-5 further optimized for agentic coding in Codex. GPT-5-Codex adjusts its thinking effort more dynamically based on task complexity, responding quickly to simple conversational queries or small tasks, while independently working for longer on more complex tasks.",
      "publishedAt": "2025-09-15T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.034334763082675634
    },
    {
      "id": "1524b7a996a733768616f5c9a020c8c8",
      "title": "Working with US CAISI and UK AISI to build more secure AI systems",
      "url": "https://openai.com/index/us-caisi-uk-aisi-ai-update",
      "content": "OpenAI shares progress on the partnership with the US CAISI and UK AISI to strengthen AI safety and security. The collaboration is setting new standards for responsible frontier AI deployment through joint red-teaming, biosecurity safeguards, and agentic system testing.",
      "summary": "OpenAI shares progress on the partnership with the US CAISI and UK AISI to strengthen AI safety and security. The collaboration is setting new standards for responsible frontier AI deployment through joint red-teaming, biosecurity safeguards, and agentic system testing.",
      "publishedAt": "2025-09-12T12:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "agents",
        "testing"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.08328743107225506
    },
    {
      "id": "d3941dee22b6e8fd53bdf86ab78ebea9",
      "title": "Announcing OpenAI Grove",
      "url": "https://openai.com/index/openai-grove",
      "content": "Applications are now open for OpenAI Grove, a 5-week founder program designed for individuals at any stage, from pre-idea to product. Participants receive $50K in API credits, early access to AI tools, and hands-on mentorship from the OpenAI team.",
      "summary": "Applications are now open for OpenAI Grove, a 5-week founder program designed for individuals at any stage, from pre-idea to product. Participants receive $50K in API credits, early access to AI tools, and hands-on mentorship from the OpenAI team.",
      "publishedAt": "2025-09-12T07:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.08488676989891146
    },
    {
      "id": "046838869b448dd196dce13cb4eb0d42",
      "title": "A joint statement from OpenAI and Microsoft",
      "url": "https://openai.com/index/joint-statement-from-openai-and-microsoft",
      "content": "OpenAI and Microsoft sign a new MOU, reinforcing their partnership and shared commitment to AI safety and innovation.",
      "summary": "OpenAI and Microsoft sign a new MOU, reinforcing their partnership and shared commitment to AI safety and innovation.",
      "publishedAt": "2025-09-11T14:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.01613974880260672
    },
    {
      "id": "4f06a3965c9b93e0f55e8d2a88a14d8e",
      "title": "Statement on OpenAI’s Nonprofit and PBC",
      "url": "https://openai.com/index/statement-on-openai-nonprofit-and-pbc",
      "content": "OpenAI reaffirms its nonprofit leadership with a new structure granting equity in its PBC, enabling over $100B in resources to advance safe, beneficial AI for humanity.",
      "summary": "OpenAI reaffirms its nonprofit leadership with a new structure granting equity in its PBC, enabling over $100B in resources to advance safe, beneficial AI for humanity.",
      "publishedAt": "2025-09-11T14:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.03227949760521344
    },
    {
      "id": "55eec3c25bc99547b97bcef342d7dd55",
      "title": "Shipping smarter agents with every new model",
      "url": "https://openai.com/index/safetykit",
      "content": "Discover how SafetyKit leverages OpenAI GPT-5 to enhance content moderation, enforce compliance, and outpace legacy safety systems with greater accuracy .",
      "summary": "Discover how SafetyKit leverages OpenAI GPT-5 to enhance content moderation, enforce compliance, and outpace legacy safety systems with greater accuracy .",
      "publishedAt": "2025-09-09T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "feature_update",
      "tags": [
        "retrieval",
        "agents",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.754Z",
      "score": 0.0714323558283839
    },
    {
      "id": "a9357f717f7361d40f0a5905f3b249d1",
      "title": "A People-First AI Fund: $50M to support nonprofits",
      "url": "https://openai.com/index/people-first-ai-fund",
      "content": "Applications are now open for OpenAI’s People-First AI Fund, a $50M initiative supporting U.S. nonprofits advancing education, community innovation, and economic opportunity. Apply by October 8, 2025, for unrestricted grants that help communities shape AI for the public good.",
      "summary": "Applications are now open for OpenAI’s People-First AI Fund, a $50M initiative supporting U.S. nonprofits advancing education, community innovation, and economic opportunity. Apply by October 8, 2025, for unrestricted grants that help communities shape AI for the public good.",
      "publishedAt": "2025-09-08T14:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.755Z",
      "score": 0.026053355360046923
    },
    {
      "id": "fd79768a741eb7c71583266838e7f6bb",
      "title": "Why language models hallucinate",
      "url": "https://openai.com/index/why-language-models-hallucinate",
      "content": "OpenAI’s new research explains why language models hallucinate. The findings show how improved evaluations can enhance AI reliability, honesty, and safety.",
      "summary": "OpenAI’s new research explains why language models hallucinate. The findings show how improved evaluations can enhance AI reliability, honesty, and safety.",
      "publishedAt": "2025-09-05T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.755Z",
      "score": 0.020779274845752156
    },
    {
      "id": "551e662c8b623388488699db9222981d",
      "title": "GPT-5 bio bug bounty call",
      "url": "https://openai.com/gpt-5-bio-bug-bounty",
      "content": "OpenAI invites researchers to its Bio Bug Bounty. Test GPT-5’s safety with a universal jailbreak prompt and win up to $25,000.",
      "summary": "OpenAI invites researchers to its Bio Bug Bounty. Test GPT-5’s safety with a universal jailbreak prompt and win up to $25,000.",
      "publishedAt": "2025-09-05T08:45:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.755Z",
      "score": 0.02070211461209245
    },
    {
      "id": "967ad6c60a3b2cd3baf60c4d50419a91",
      "title": "OpenAI and Greek Government launch ‘OpenAI for Greece’",
      "url": "https://openai.com/global-affairs/openai-for-greece",
      "content": "OpenAI and the Greek Government have launched “OpenAI for Greece” to bring ChatGPT Edu into secondary schools and support responsible AI learning. This partnership aims to boost AI literacy, fuel local start-ups, and drive national economic growth.",
      "summary": "OpenAI and the Greek Government have launched “OpenAI for Greece” to bring ChatGPT Edu into secondary schools and support responsible AI learning. This partnership aims to boost AI literacy, fuel local start-ups, and drive national economic growth.",
      "publishedAt": "2025-09-05T08:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.755Z",
      "score": 0.03098393410534252
    },
    {
      "id": "a66b5ce615173d7b25ef8b4d4bebb469",
      "title": "Expanding economic opportunity with AI",
      "url": "https://openai.com/index/expanding-economic-opportunity-with-ai",
      "content": "OpenAI is launching a Jobs Platform and new Certifications to connect workers with jobs, training, and certifications. Learn how we’re expanding economic opportunity and making AI skills more accessible.",
      "summary": "OpenAI is launching a Jobs Platform and new Certifications to connect workers with jobs, training, and certifications. Learn how we’re expanding economic opportunity and making AI skills more accessible.",
      "publishedAt": "2025-09-04T11:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.755Z",
      "score": 0.029150058239426284
    },
    {
      "id": "3dc331a0e3074238c370a97b3f70a150",
      "title": "Vijaye Raji to become CTO of Applications with acquisition of Statsig",
      "url": "https://openai.com/index/vijaye-raji-to-become-cto-of-applications-with-acquisition-of-statsig",
      "content": "Vijaye Raji will step into a new role as CTO of Applications, reporting to CEO of Applications, Fidji Simo, following the acquisition of Statsig.",
      "summary": "Vijaye Raji will step into a new role as CTO of Applications, reporting to CEO of Applications, Fidji Simo, following the acquisition of Statsig.",
      "publishedAt": "2025-09-02T11:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "funding_mna",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.755Z",
      "score": 0.014017758747263765
    },
    {
      "id": "1568f222ff7a1cd3bde45a9a84a4fe0b",
      "title": "Building more helpful ChatGPT experiences for everyone",
      "url": "https://openai.com/index/building-more-helpful-chatgpt-experiences-for-everyone",
      "content": "We’re partnering with experts, strengthening protections for teens with parental controls, and routing sensitive conversations to reasoning models in ChatGPT.",
      "summary": "We’re partnering with experts, strengthening protections for teens with parental controls, and routing sensitive conversations to reasoning models in ChatGPT.",
      "publishedAt": "2025-09-02T04:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.755Z",
      "score": 0.016474491767124327
    },
    {
      "id": "20e7ffe2a0312e60d0f5e06ea458b516",
      "title": "Introducing gpt-realtime and Realtime API updates",
      "url": "https://openai.com/index/introducing-gpt-realtime",
      "content": "We’re releasing a more advanced speech-to-speech model and new API capabilities including MCP server support, image input, and SIP phone calling support.",
      "summary": "We’re releasing a more advanced speech-to-speech model and new API capabilities including MCP server support, image input, and SIP phone calling support.",
      "publishedAt": "2025-08-28T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.755Z",
      "score": 0.021513127290556426
    },
    {
      "id": "b203b36d85228904428c06875fbf7088",
      "title": "Supporting nonprofit and community innovation",
      "url": "https://openai.com/index/supporting-nonprofit-and-community-innovation",
      "content": "OpenAI launches a $50M People-First AI Fund to help U.S. nonprofits scale impact with AI. Applications open Sept 8–Oct 8, 2025 for grants in education, healthcare, research, and more.",
      "summary": "OpenAI launches a $50M People-First AI Fund to help U.S. nonprofits scale impact with AI. Applications open Sept 8–Oct 8, 2025 for grants in education, healthcare, research, and more.",
      "publishedAt": "2025-08-28T05:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.755Z",
      "score": 0.02312221271921461
    },
    {
      "id": "9fa6c6e036dfa556ed306bc643069ee0",
      "title": "Collective alignment: public input on our Model Spec",
      "url": "https://openai.com/index/collective-alignment-aug-2025-updates",
      "content": "OpenAI surveyed over 1,000 people worldwide on how AI should behave and compared their views to our Model Spec. Learn how collective alignment is shaping AI defaults to better reflect diverse human values and perspectives.",
      "summary": "OpenAI surveyed over 1,000 people worldwide on how AI should behave and compared their views to our Model Spec. Learn how collective alignment is shaping AI defaults to better reflect diverse human values and perspectives.",
      "publishedAt": "2025-08-27T13:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.755Z",
      "score": 0.007348986472261012
    },
    {
      "id": "36bc196e70e92d79a1d8bf2690ab1169",
      "title": "OpenAI and Anthropic share findings from a joint safety evaluation",
      "url": "https://openai.com/index/openai-anthropic-safety-evaluation",
      "content": "OpenAI and Anthropic share findings from a first-of-its-kind joint safety evaluation, testing each other’s models for misalignment, instruction following, hallucinations, jailbreaking, and more—highlighting progress, challenges, and the value of cross-lab collaboration.",
      "summary": "OpenAI and Anthropic share findings from a first-of-its-kind joint safety evaluation, testing each other’s models for misalignment, instruction following, hallucinations, jailbreaking, and more—highlighting progress, challenges, and the value of cross-lab collaboration.",
      "publishedAt": "2025-08-27T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review",
        "testing"
      ],
      "ingestedAt": "2025-11-23T17:37:39.755Z",
      "score": 0.013656867337685404
    },
    {
      "id": "cbf71272abd2ae5723d9bbf20dd37500",
      "title": "Helping people when they need it most",
      "url": "https://openai.com/index/helping-people-when-they-need-it-most",
      "content": "How we think about safety for users experiencing mental or emotional distress, the limits of today’s systems, and the work underway to refine them.",
      "summary": "How we think about safety for users experiencing mental or emotional distress, the limits of today’s systems, and the work underway to refine them.",
      "publishedAt": "2025-08-26T04:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.755Z",
      "score": 0.004996142179972133
    },
    {
      "id": "c6c36d19bd8a791c8f3ef1931c1b1c82",
      "title": "GitLab 18.6: From configuration to control",
      "url": "https://about.gitlab.com/blog/gitlab-18-6-from-configuration-to-control/",
      "content": "<p><em>Editor’s note: After this blog was originally published, the default Security Manager role was withdrawn from the release. It will be included in a future update. The content below has been updated for accuracy.</em></p>\n<p>With <a href=\"https://about.gitlab.com/releases/2025/11/20/gitlab-18-6-released/\">GitLab 18.6</a>, we’re continuing to advance how AI integrates into everyday software development with enhancements that give teams greater choice and control. GitLab 18.6 will help plan, build, and secure software more intelligently across the entire software lifecycle.\nTeams now have greater flexibility to select the right models for their workflows, extend AI into secure and self-managed environments, and strengthen visibility and governance across every stage of development.</p>\n<h2>AI that adapts to you</h2>\n<p>With 18.6, GitLab’s AI becomes more adaptable to real-world workflows. GitLab Duo Agents now plan with greater context, work seamlessly across IDEs and self-managed instances, and offer new open-source model options — helping teams accelerate delivery without compromising compliance or control.</p>\n<p><strong>GitLab Duo Planner and Security Analyst agent enhancements</strong></p>\n<p>In 18.6, <a href=\"https://about.gitlab.com/blog/ace-your-planning-without-the-context-switching/\">GitLab Duo Planner</a> and <a href=\"https://docs.gitlab.com/user/duo_agent_platform/agents/foundational_agents/security_analyst_agent/\">GitLab Duo Security Analyst</a> are now available by default in the Agentic Chat dropdown — no configuration or setup required. Both agents can be used immediately across projects and groups, giving teams built-in assistance for planning, issue refinement, and security analysis.</p>\n<p>GitLab Duo Planner agent now works at the group level with awareness of the epic being viewed and supports milestone and iteration workflows. Security Analyst agent provides automated vulnerability review, context interpretation, and guided remediation suggestions. Both agents are also available to self-managed customers.</p>\n<p>For a full list of what these agents can do, see the <a href=\"https://docs.gitlab.com/user/duo_agent_platform/agents/foundational_agents/\">documentation</a>.</p>\n<p><strong>gpt-oss-120b model support for GitLab Duo Agent Platform</strong></p>\n<p>GitLab Duo Self-Hosted customers can now deploy the <a href=\"https://platform.openai.com/docs/models/gpt-oss-120b\"><strong>gpt-oss-120b</strong></a> model within the GitLab Duo Agent Platform — a high-performance, fully open-source model optimized for agentic workflows. This addition enables teams to execute complex tasks and reasoning-driven processes while maintaining control over model transparency and infrastructure. For organizations that require open, auditable models to address compliance or data sovereignty requirements, gpt-oss-120b provides a reliable alternative to proprietary models without sacrificing performance.</p>\n<p>For more information on supported models, please see our <a href=\"https://docs.gitlab.com/administration/gitlab_duo_self_hosted/supported_models_and_hardware_requirements/#supported-models\">documentation</a>.</p>\n<p><strong>End-user model selection for cloud-connected self-managed instances (GA)</strong></p>\n<p>Cloud-connected self-managed end users can now choose which AI model powers their GitLab Duo Agentic Chat experience directly from the GitLab UI. This gives administrators and end users more control over how conversations perform and how costs and governance requirements are managed.</p>\n<p>No matter the deployment environment — on-premises, private cloud, or public cloud —  teams can select regionally compliant or in-house models to help satisfy data residency needs and compare model quality for speed or accuracy. This flexibility ensures that every organization can tailor Agentic Chat to its operational priorities.</p>\n<p>For full details on how to select a model in Agentic Chat, see the model selection section of the GitLab <a href=\"https://docs.gitlab.com/user/gitlab_duo_chat/agentic_chat/#select-a-model\">documentation</a>.</p>\n<p><strong>Web IDE support for air-gapped deployments</strong></p>\n<p>Air-gapped or tightly controlled environments — such as public sector organizations, defense agencies, and regulated enterprises — can now run the Web IDE with full functionality even without internet access. By allowing administrators to configure their own Web IDE extension host domain, GitLab enables markdown preview, code editing, and GitLab Duo Chat capabilities in isolated or offline systems. This makes it possible for development teams in secure or restricted networks to benefit from modern IDE workflows without sacrificing security and compliance.</p>\n<p><strong>Modern interface now default for self-managed instances</strong></p>\n<p>Self-managed GitLab instances now default to the modern interface in 18.6, bringing the same streamlined experience already available on GitLab.com to on-premises deployments. The updated layout improves navigation consistency and makes core workflows more intuitive across the platform. Administrators maintain full flexibility with opt-out controls via feature flag or user-level toggling if needed. This update ensures self-managed customers benefit from GitLab's latest interface improvements while maintaining the control and customization options enterprise environments require.</p>\n<h2>Platform security with awareness and authority</h2>\n<p>GitLab 18.6 strengthens platform security with deeper context and clearer control, helping security teams focus on the risks that matter most while maintaining governance across every project.</p>\n<p><strong>Security attributes and context filtering</strong></p>\n<p>Security teams can now apply custom business context labels to projects and groups, transforming raw scan results into prioritized, risk-based insights. Instead of viewing vulnerabilities in isolation, teams can tag projects by business unit, application type, or criticality — then filter and sort security data by impact. This allows organizations to focus remediation on the areas of greatest business risk, helping to accelerate time to resolution for the issues that matter most.</p>\n<h2>AI that adapts to your workflow</h2>\n<p>This release represents more than new capabilities — it's about how GitLab Duo Agent Platform is becoming an embedded part of everyday software development workflows. Watch a walkthrough video that shows how a member of your software development team can start on a new project using GitLab Duo Agent Platform:</p>\n<p>&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/1138657697?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;18.6 Demo (TO BE UPDATED)&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;</p>\n<h2>Get started today</h2>\n<p>GitLab Premium and Ultimate users can start using these capabilities today on <a href=\"https://GitLab.com\">GitLab.com</a> and self-managed environments, with availability for GitLab Dedicated customers planned for next month.</p>\n<p>New to GitLab? <a href=\"https://about.gitlab.com/free-trial/devsecops/\">Start your free trial</a> and see why the future of development is AI-powered, secure, and orchestrated through the world’s most comprehensive DevSecOps platform.</p>\n<p><em><strong>Note:</strong> GitLab Duo Agent Platform is currently in beta. Platform capabilities that are in beta are available as part of the GitLab Beta program. They are free to use during the beta period, and when generally available, they are planned to be made available with a paid add-on option for GitLab Duo Agent Platform.</em></p>\n<h3>Stay up to date with GitLab</h3>\n<p>To make sure you’re getting the latest features, security updates, and performance improvements, we recommend keeping your GitLab instance up to date. The following resources can help you plan and complete your upgrade:</p>\n<ul>\n<li>\n<p><a href=\"https://gitlab-com.gitlab.io/support/toolbox/upgrade-path/\">Upgrade Path Tool</a> — enter your current version and see the exact upgrade steps for your instance</p>\n</li>\n<li>\n<p><a href=\"https://docs.gitlab.com/update/upgrade_paths/\">Upgrade Documentation</a> — detailed guides for each supported version, including requirements, step-by-step instructions, and best practices</p>\n</li>\n</ul>\n<p>By upgrading regularly, you’ll ensure your team benefits from the newest GitLab capabilities and remains secure and supported.</p>\n<p>For organizations that want a hands-off approach, consider <a href=\"https://content.gitlab.com/viewer/d1fe944dddb06394e6187f0028f010ad#1\">GitLab’s Managed Maintenance service</a>. With Managed Maintenance, your team stays focused on innovation while GitLab experts keep your Self-Managed instance reliably upgraded, secure, and ready to lead in DevSecOps. Ask your account manager for more information.</p>\n<p><em>This blog post contains &quot;forward‑looking statements&quot; within the meaning of Section 27A of the Securities Act of 1933, as amended, and Section 21E of the Securities Exchange Act of 1934. Although we believe that the expectations reflected in these statements are reasonable, they are subject to known and unknown risks, uncertainties, assumptions and other factors that may cause actual results or outcomes to differ materially. Further information on these risks and other factors is included under the caption &quot;Risk Factors&quot; in our filings with the SEC. We do not undertake any obligation to update or revise these statements after the date of this blog post, except as required by law.</em></p>\n",
      "summary": "Editor’s note: After this blog was originally published, the default Security Manager role was withdrawn from the release. It will be included in a future update. The content below has been updated for accuracy.\nWith GitLab 18.6, we’re continuing to advance how AI integrates into everyday software development with enhancements that give teams greater choice and control. GitLab 18.6 will help plan, build, and secure software more intelligently across the entire software lifecycle.\nTeams now have greater flexibility to select the right models for their workflows, extend AI into secure and self-managed environments, and strengthen visibility and governance across every stage of development.\nAI that adapts to you\nWith 18.6, GitLab’s AI becomes more adaptable to real-world workflows. GitLab Duo Agents now plan with greater context, work seamlessly across IDEs and self-managed instances, and offer new open-source model options — helping teams accelerate delivery without compromising compliance or control.\nGitLab Duo Planner and Security Analyst agent enhancements\nIn 18.6, GitLab Duo Planner and GitLab Duo Security Analyst are now available by default in the Agentic Chat dropdown — no configuration or setup required. Both agents can be used immediately across projects and groups, giving teams built-in assistance for planning, issue refinement, and security analysis.\nGitLab Duo Planner agent now works at the group level with awareness of the epic being viewed and supports milestone and iteration workflows. Security Analyst agent provides automated vulnerability review, context interpretation, and guided remediation suggestions. Both agents are also available to self-managed customers.\nFor a full list of what these agents can do, see the documentation.\ngpt-oss-120b model support for GitLab Duo Agent Platform\nGitLab Duo Self-Hosted customers can now deploy the gpt-oss-120b model within the GitLab Duo Agent Platform — a high-performance, fully open-source model optimized for agentic workflows. This addition enables teams to execute complex tasks and reasoning-driven processes while maintaining control over model transparency and infrastructure. For organizations that require open, auditable models to address compliance or data sovereignty requirements, gpt-oss-120b provides a reliable alternative to proprietary models without sacrificing performance.\nFor more information on supported models, please see our documentation.\nEnd-user model selection for cloud-connected self-managed instances (GA)\nCloud-connected self-managed end users can now choose which AI model powers their GitLab Duo Agentic Chat experience directly from the GitLab UI. This gives administrators and end users more control over how conversations perform and how costs and governance requirements are managed.\nNo matter the deployment environment — on-premises, private cloud, or public cloud —  teams can select regionally compliant or in-house models to help satisfy data residency needs and compare model quality for speed or accuracy. This flexibility ensures that every organization can tailor Agentic Chat to its operational priorities.\nFor full details on how to select a model in Agentic Chat, see the model selection section of the GitLab documentation.\nWeb IDE support for air-gapped deployments\nAir-gapped or tightly controlled environments — such as public sector organizations, defense agencies, and regulated enterprises — can now run the Web IDE with full functionality even without internet access. By allowing administrators to configure their own Web IDE extension host domain, GitLab enables markdown preview, code editing, and GitLab Duo Chat capabilities in isolated or offline systems. This makes it possible for development teams in secure or restricted networks to benefit from modern IDE workflows without sacrificing security and compliance.\nModern interface now default for self-managed instances\nSelf-managed GitLab instances now default to the modern interface in 18.6, bringing the same streamlined experience already available on GitLab.com to on-premises deployments. The updated layout improves navigation consistency and makes core workflows more intuitive across the platform. Administrators maintain full flexibility with opt-out controls via feature flag or user-level toggling if needed. This update ensures self-managed customers benefit from GitLab's latest interface improvements while maintaining the control and customization options enterprise environments require.\nPlatform security with awareness and authority\nGitLab 18.6 strengthens platform security with deeper context and clearer control, helping security teams focus on the risks that matter most while maintaining governance across every project.\nSecurity attributes and context filtering\nSecurity teams can now apply custom business context labels to projects and groups, transforming raw scan results into prioritized, risk-based insights. Instead of viewing vulnerabilities in isolation, teams can tag projects by business unit, application type, or criticality — then filter and sort security data by impact. This allows organizations to focus remediation on the areas of greatest business risk, helping to accelerate time to resolution for the issues that matter most.\nAI that adapts to your workflow\nThis release represents more than new capabilities — it's about how GitLab Duo Agent Platform is becoming an embedded part of everyday software development workflows. Watch a walkthrough video that shows how a member of your software development team can start on a new project using GitLab Duo Agent Platform:\n<div style=\"padding:56.25% 0 0 0;position:relative;\"><iframe src=\"https://player.vimeo.com/video/1138657697?badge=0&autopause=0&player_id=0&app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"18.6 Demo (TO BE UPDATED)\"></iframe></div><script src=\"https://player.vimeo.com/api/player.js\"></script>\nGet started today\nGitLab Premium and Ultimate users can start using these capabilities today on GitLab.com and self-managed environments, with availability for GitLab Dedicated customers planned for next month.\nNew to GitLab? Start your free trial and see why the future of development is AI-powered, secure, and orchestrated through the world’s most comprehensive DevSecOps platform.\nNote: GitLab Duo Agent Platform is currently in beta. Platform capabilities that are in beta are available as part of the GitLab Beta program. They are free to use during the beta period, and when generally available, they are planned to be made available with a paid add-on option for GitLab Duo Agent Platform.\nStay up to date with GitLab\nTo make sure you’re getting the latest features, security updates, and performance improvements, we recommend keeping your GitLab instance up to date. The following resources can help you plan and complete your upgrade:\nUpgrade Path Tool — enter your current version and see the exact upgrade steps for your instance\nUpgrade Documentation — detailed guides for each supported version, including requirements, step-by-step instructions, and best practices\nBy upgrading regularly, you’ll ensure your team benefits from the newest GitLab capabilities and remains secure and supported.\nFor organizations that want a hands-off approach, consider GitLab’s Managed Maintenance service. With Managed Maintenance, your team stays focused on innovation while GitLab experts keep your Self-Managed instance reliably upgraded, secure, and ready to lead in DevSecOps. Ask your account manager for more information.\nThis blog post contains \"forward‑looking statements\" within the meaning of Section 27A of the Securities Act of 1933, as amended, and Section 21E of the Securities Exchange Act of 1934. Although we believe that the expectations reflected in these statements are reasonable, they are subject to known and unknown risks, uncertainties, assumptions and other factors that may cause actual results or outcomes to differ materially. Further information on these risks and other factors is included under the caption \"Risk Factors\" in our filings with the SEC. We do not undertake any obligation to update or revise these statements after the date of this blog post, except as required by law.",
      "publishedAt": "2025-11-20T00:00:00.000Z",
      "author": "Bill Staples",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "documentation",
        "agents",
        "ide",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.764Z",
      "score": 19.1466292402893
    },
    {
      "id": "e1a987b4ca19fee90af038f7a93e15f0",
      "title": "GitLab engineer: How I improved my onboarding experience with AI",
      "url": "https://about.gitlab.com/blog/gitlab-engineer-how-i-improved-my-onboarding-experience-with-ai/",
      "content": "<p>Starting a new job is exciting, and overwhelming. New teammates, new tools, and, in GitLab’s case, a lot of documentation. Six weeks ago, I joined GitLab’s Growth team as a fullstack engineer. Anyone who has gone through <a href=\"https://about.gitlab.com/the-source/platform/how-to-accelerate-developer-onboarding-and-why-it-matters/\">onboarding at GitLab</a> knows it’s transparent, extensive, and thorough.</p>\n<p>GitLab's onboarding process includes a lot of docs, videos, and trainings that will bring you up to speed. Also, in line with GitLab's values, my team encouraged me to start contributing right away. I quickly realized that onboarding here is both diligent and intense. Luckily, I had a secret helper: <a href=\"https://about.gitlab.com/gitlab-duo/\">GitLab Duo</a>.</p>\n<h2>My main use cases</h2>\n<p>I’ve found GitLab Duo's AI assistance, available throughout the software development lifecycle, useful in three key areas: exploration, reviewing, and debugging. With GitLab Duo, I was able to get my first tiny MR deployed to production in the first week and actively contribute to <a href=\"https://about.gitlab.com/releases/2025/10/16/gitlab-18-5-released/#pick-up-where-you-left-off-on-the-new-personal-homepage\">the personal homepage</a> in GitLab 18.5 in the weeks after.</p>\n<h3>Exploration</h3>\n<p>Early in onboarding, I often remembered reading something but couldn’t recall where. GitLab has a <a href=\"https://handbook.gitlab.com/\">public-facing handbook</a>, an internal handbook, and <a href=\"https://docs.gitlab.com/\">GitLab Docs</a>. It can be difficult to search across all of them efficiently.</p>\n<p>GitLab Duo simplifies this task: I can describe what I’m looking for in natural language via <a href=\"https://docs.gitlab.com/user/gitlab_duo_chat/\">GitLab Duo Chat</a> and search across all resources at once.</p>\n<p>Example prompt:</p>\n<blockquote>\n<p>I remember reading about how RSpec tests are done at GitLab. Can you find relevant documentation across the Handbook, the internal handbook and the GitLab Docs?</p>\n</blockquote>\n<p>Before starting work on an issue, I use GitLab Duo to identify edge cases and hidden dependencies. GitLab Duo will relate the requirements of the issue against the whole GitLab codebase, assess similar features, and prepare all the findings. Based on its output I am able to refine the issue with my product manager and designer, and make sure my implementation covers all edge cases or define future iterations.</p>\n<p>Example prompt:</p>\n<blockquote>\n<p>Analyze this issue in the context of its epic and identify:</p>\n<ul>\n<li>Implementation questions to ask PM/design before coding</li>\n<li>Edge cases not covered in requirements</li>\n<li>Cross-feature dependencies that might be affected</li>\n<li>Missing acceptance criteria</li>\n</ul>\n</blockquote>\n<p>I also check that my planned solution follows GitLab best practices and common patterns.</p>\n<p>Example prompt:</p>\n<blockquote>\n<p>I want to implement XZY behavior — how is this usually done at GitLab, and what other options do I have?</p>\n</blockquote>\n<h3>Reviewing</h3>\n<p>I always <a href=\"https://docs.gitlab.com/user/project/merge_requests/duo_in_merge_requests/#have-gitlab-duo-review-your-code\">let GitLab Duo review my merge requests</a> before assigning human reviewers. It often catches small mistakes, suggests improvements, and highlights edge cases I missed. This shortens the review cycle and helps my teammates focus on more complex and bigger-picture feedback.</p>\n<p>Since I’m still new to GitLab’s codebase and coding practices, some review comments are hard to interpret. In those cases, GitLab Duo helps me understand what a reviewer means and how it relates to my code.</p>\n<p>Example prompt:</p>\n<blockquote>\n<p>I don’t understand the comment on this MR about following the user instead of testing component internals, what does it mean and how does it relate to my implementation?</p>\n</blockquote>\n<h3>Debugging</h3>\n<p>Sometimes pipeline tests on my merge requests failed unexpectedly. If I can’t tell whether my changes are the cause, GitLab Duo helps me investigate and fix the failures. Using <a href=\"https://docs.gitlab.com/user/gitlab_duo_chat/agentic_chat/\">GitLab Duo Agentic Chat</a>, Duo can apply changes to debug the failing job.</p>\n<p>Example prompt:</p>\n<blockquote>\n<p>The pipeline job “rspec system pg16 12/32” is failing, but I don’t know whether that relates to my changes. Can you check, if my changes are causing the pipeline failure and, if so, guide me through the steps of fixing it.</p>\n</blockquote>\n<h2>How Duo aligns with GitLab’s values</h2>\n<p>Using GitLab Duo doesn’t just help me, it also supports <a href=\"https://handbook.gitlab.com/handbook/values/\">GitLab’s CREDIT values</a>:</p>\n<ul>\n<li>\n<p><strong>Collaboration:</strong> I ask teammates fewer basic questions. And when I do ask questions, they’re more thoughtful and informed. This respects their time.</p>\n</li>\n<li>\n<p><strong>Results for customers:</strong> By identifying edge cases early and improving code quality, GitLab Duo helps me deliver better outcomes for customers.</p>\n</li>\n<li>\n<p><strong>Efficiency:</strong> Streamlined preparation, faster reviews, and improved debugging make me more efficient.</p>\n</li>\n<li>\n<p><strong>Diversity, inclusion &amp; belonging:</strong> AI guidance might mitigate misunderstandings and different barriers to entry based on differing backgrounds and abilities.</p>\n</li>\n<li>\n<p><strong>Iteration:</strong> The ability to try ideas faster and identify potential improvements, enables faster iteration.</p>\n</li>\n<li>\n<p><strong>Transparency:</strong> GitLab Duo makes the already transparent documentation at GitLab more accessible.</p>\n</li>\n</ul>\n<h2>Staying cautious with AI</h2>\n<p>It never has been as easy and difficult to be competent as in the days of AI. It can be a powerful tool, but AI does get things wrong. Therefore, I avoid <a href=\"https://link.springer.com/article/10.1007/s00146-025-02422-7\">automation bias</a> by always validating AI's outputs. If I don’t understand the output, I don’t use it.\nI’m also cautious of over-reliance. Studies suggest that heavy AI use can lead to <a href=\"https://www.mdpi.com/2075-4698/15/1/6\">cognitive offloading</a> and worse outcomes in the long run. One study shows that users of AI <a href=\"https://arxiv.org/abs/2404.19699?utm_source=chatgpt.com\">perform worse in exams</a>. To avoid negatively affecting my skills, I use AI as a discussion partner rather than just implementing the code it generates.</p>\n<h2>Summary</h2>\n<p>Onboarding is always a stressful time, but using GitLab Duo made mine smoother and less overwhelming. I learned more about GitLab’s codebase, culture, and best practices than I could have managed on my own.</p>\n<blockquote>\n<p>Want to make GitLab Duo part of your onboarding experience? Sign up for <a href=\"https://about.gitlab.com/gitlab-duo/\">a free trial</a> today.</p>\n</blockquote>\n<h2>Resources</h2>\n<ul>\n<li><a href=\"https://docs.gitlab.com/user/get_started/getting_started_gitlab_duo/\">Getting started with GitLab Duo</a></li>\n<li><a href=\"https://about.gitlab.com/blog/get-started-with-gitlab-duo-agentic-chat-in-the-web-ui/\">Get started with GitLab Duo Agentic Chat in the web UI</a></li>\n<li><a href=\"https://about.gitlab.com/blog/10-best-practices-for-using-ai-powered-gitlab-duo-chat/\">10 best practices for using AI-powered GitLab Duo Chat</a></li>\n</ul>\n",
      "summary": "Starting a new job is exciting, and overwhelming. New teammates, new tools, and, in GitLab’s case, a lot of documentation. Six weeks ago, I joined GitLab’s Growth team as a fullstack engineer. Anyone who has gone through onboarding at GitLab knows it’s transparent, extensive, and thorough.\nGitLab's onboarding process includes a lot of docs, videos, and trainings that will bring you up to speed. Also, in line with GitLab's values, my team encouraged me to start contributing right away. I quickly realized that onboarding here is both diligent and intense. Luckily, I had a secret helper: GitLab Duo.\nMy main use cases\nI’ve found GitLab Duo's AI assistance, available throughout the software development lifecycle, useful in three key areas: exploration, reviewing, and debugging. With GitLab Duo, I was able to get my first tiny MR deployed to production in the first week and actively contribute to the personal homepage in GitLab 18.5 in the weeks after.\nExploration\nEarly in onboarding, I often remembered reading something but couldn’t recall where. GitLab has a public-facing handbook, an internal handbook, and GitLab Docs. It can be difficult to search across all of them efficiently.\nGitLab Duo simplifies this task: I can describe what I’m looking for in natural language via GitLab Duo Chat and search across all resources at once.\nExample prompt:\nI remember reading about how RSpec tests are done at GitLab. Can you find relevant documentation across the Handbook, the internal handbook and the GitLab Docs?\nBefore starting work on an issue, I use GitLab Duo to identify edge cases and hidden dependencies. GitLab Duo will relate the requirements of the issue against the whole GitLab codebase, assess similar features, and prepare all the findings. Based on its output I am able to refine the issue with my product manager and designer, and make sure my implementation covers all edge cases or define future iterations.\nExample prompt:\nAnalyze this issue in the context of its epic and identify:\nImplementation questions to ask PM/design before coding\nEdge cases not covered in requirements\nCross-feature dependencies that might be affected\nMissing acceptance criteria\nI also check that my planned solution follows GitLab best practices and common patterns.\nExample prompt:\nI want to implement XZY behavior — how is this usually done at GitLab, and what other options do I have?\nReviewing\nI always let GitLab Duo review my merge requests before assigning human reviewers. It often catches small mistakes, suggests improvements, and highlights edge cases I missed. This shortens the review cycle and helps my teammates focus on more complex and bigger-picture feedback.\nSince I’m still new to GitLab’s codebase and coding practices, some review comments are hard to interpret. In those cases, GitLab Duo helps me understand what a reviewer means and how it relates to my code.\nExample prompt:\nI don’t understand the comment on this MR about following the user instead of testing component internals, what does it mean and how does it relate to my implementation?\nDebugging\nSometimes pipeline tests on my merge requests failed unexpectedly. If I can’t tell whether my changes are the cause, GitLab Duo helps me investigate and fix the failures. Using GitLab Duo Agentic Chat, Duo can apply changes to debug the failing job.\nExample prompt:\nThe pipeline job “rspec system pg16 12/32” is failing, but I don’t know whether that relates to my changes. Can you check, if my changes are causing the pipeline failure and, if so, guide me through the steps of fixing it.\nHow Duo aligns with GitLab’s values\nUsing GitLab Duo doesn’t just help me, it also supports GitLab’s CREDIT values:\nCollaboration: I ask teammates fewer basic questions. And when I do ask questions, they’re more thoughtful and informed. This respects their time.\nResults for customers: By identifying edge cases early and improving code quality, GitLab Duo helps me deliver better outcomes for customers.\nEfficiency: Streamlined preparation, faster reviews, and improved debugging make me more efficient.\nDiversity, inclusion & belonging: AI guidance might mitigate misunderstandings and different barriers to entry based on differing backgrounds and abilities.\nIteration: The ability to try ideas faster and identify potential improvements, enables faster iteration.\nTransparency: GitLab Duo makes the already transparent documentation at GitLab more accessible.\nStaying cautious with AI\nIt never has been as easy and difficult to be competent as in the days of AI. It can be a powerful tool, but AI does get things wrong. Therefore, I avoid automation bias by always validating AI's outputs. If I don’t understand the output, I don’t use it.\nI’m also cautious of over-reliance. Studies suggest that heavy AI use can lead to cognitive offloading and worse outcomes in the long run. One study shows that users of AI perform worse in exams. To avoid negatively affecting my skills, I use AI as a discussion partner rather than just implementing the code it generates.\nSummary\nOnboarding is always a stressful time, but using GitLab Duo made mine smoother and less overwhelming. I learned more about GitLab’s codebase, culture, and best practices than I could have managed on my own.\nWant to make GitLab Duo part of your onboarding experience? Sign up for a free trial today.\nResources\nGetting started with GitLab Duo\nGet started with GitLab Duo Agentic Chat in the web UI\n10 best practices for using AI-powered GitLab Duo Chat",
      "publishedAt": "2025-11-17T00:00:00.000Z",
      "author": "Konstantin Greif",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "thought_leadership",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "testing"
      ],
      "ingestedAt": "2025-11-23T17:37:39.765Z",
      "score": 8.344935490892775
    },
    {
      "id": "b33941af2e8b543ac60d0bed12fca5d3",
      "title": "Achieve CMMC Level 2 with GitLab Dedicated for Government",
      "url": "https://about.gitlab.com/blog/achieve-cmmc-level-2-fast-with-gitlab-dedicated-for-government/",
      "content": "<p>For Defense Industrial Base (DIB) companies, the U.S. Department of Defense's release of the Cybersecurity Maturity Model Certification (CMMC) <a href=\"https://www.federalregister.gov/documents/2025/09/10/2025-17359/defense-federal-acquisition-regulation-supplement-assessing-contractor-implementation-of_\">Final Rule</a> and new guidance on “FedRAMP equivalency” has dramatically increased the cost of compliance and fundamentally changed the way in which they drive their risk management programs. Gone is the era of “self-attestation” of security programs; DIB companies are required to strictly apply NIST 800-171 to their environments that handle Controlled Unclassified Information (CUI), and have their security controls audited by a Third-Party Assessment Organization (3PAO) every three years.</p>\n<p>DIB companies are engineering focused, not compliance driven, and formal audits get pricey quickly. These changes add significant complications for companies focused on supporting the warfighter. The good news? <a href=\"https://about.gitlab.com/press/releases/2025-05-19-gitlab-announces-gitlab-achieves-fedramp-moderate-authorization/\">GitLab Dedicated for Government's FedRAMP Moderate Authorization</a> means DIB companies can directly use GitLab Dedicated for Government with no additional audits or authorizations, which reduces the impact and cost of compliance.</p>\n<h2>The foundational rule: FedRAMP Moderate Equivalency</h2>\n<p>The protection of Controlled Unclassified Information (CUI) within the DIB is driven by a foundational legal and contractual mandate: the Defense Federal Acquisition Regulation Supplement (DFARS) <a href=\"https://www.acquisition.gov/dfars/252.204-7012-safeguarding-covered-defense-information-and-cyber-incident-reporting.\">Clause 252.204-7012</a>. This clause specifically states that if a contractor uses an external cloud service provider to &quot;store, process, or transmit any covered defense information,&quot; that provider must meet security requirements &quot;equivalent to those established by the Government for the FedRAMP Moderate baseline.&quot;</p>\n<p>The DOD's January 2, 2024, memorandum, &quot;<a href=\"https://dodcio.defense.gov/Portals/0/Documents/Library/FEDRAMP-EquivalencyCloudServiceProviders.pdf\">Federal Risk and Authorization Management Program (FedRAMP) Moderate Equivalency for Cloud Service Provider's (CSPs) Cloud Service Offerings</a>&quot; defines “FedRAMP Moderate Equivalency,” and also directly specifies that FedRAMP Moderate Cloud Service Offerings (CSOs) can be used without any additional assessment, such as individual CMMC assessment, to meet equivalency requirements:</p>\n<p>“This memorandum does not apply to CSOs that are FedRAMP Moderate Authorized under the existing FedRAMP process. <strong>FedRAMP Moderate Authorized CSOs identified in the FedRAMP Marketplace</strong> provide the required security to store, process or transmit CDI in accordance with Defense Federal Acquisition Regulations Supplement (DFARS) Clause 252.204-7012, &quot;Safeguarding Covered Defense Information and Cyber Incident Reporting&quot; and <strong>can be leveraged without further assessment to meet the equivalency requirements</strong>.”</p>\n<h2>The GitLab platform: A proven path to compliance</h2>\n<p>GitLab's GovCloud Offering, GitLab Dedicated for Government, <a href=\"https://marketplace.fedramp.gov/products/FR2411959145\">has achieved FedRAMP Moderate Authorization</a>. This means that DIB companies can leverage GitLab Dedicated for Government as their DevSecOps platform immediately and without any additional audits or compliance checks. DIB companies leveraging GitLab Dedicated for Government inherit all of our security controls and our Body of Evidence, shifting the risk and cost of compliance away from themselves and allowing them to focus on their mission.</p>\n<h2>The Shared Responsibility Matrix: Your role as a DIB contractor</h2>\n<p>While a FedRAMP-authorized solution significantly reduces your compliance burden, compliance is a joint effort. You are responsible for the security controls that fall under your purview. This is where the Shared Responsibility Matrix (SRM), also called the Customer Responsibility Matrix (CRM), comes in.</p>\n<p>When you adopt GitLab Dedicated for Government, you will receive a comprehensive SRM that clearly delineates which security controls are managed by GitLab and which are your responsibility as the customer. Your CMMC C3PAO will use this document to ensure you have implemented the necessary controls on your end. By leveraging GitLab's FedRAMP-authorized platform, you can confidently address your CMMC Level 2 compliance requirements, focusing on your mission while trusting that GitLab has you covered.</p>\n<blockquote>\n<p>To learn more about GitLab Dedicated for Government, visit our <a href=\"https://about.gitlab.com/solutions/public-sector/\">GitLab for Public Sector</a> page. Interested in a demo? Contact Sales for more information at <a href=\"mailto:sales-pubsec@gitlab.com\">sales-pubsec@gitlab.com</a>.</p>\n</blockquote>\n<h2>References</h2>\n<ul>\n<li><a href=\"https://www.federalregister.gov/documents/2025/09/10/2025-17359/defense-federal-acquisition-regulation-supplement-assessing-contractor-implementation-of\">CMMC “Final Rule” DFARS Supplement</a></li>\n<li><a href=\"https://dodcio.defense.gov/Portals/0/Documents/Library/FEDRAMP-EquivalencyCloudServiceProviders.pdf\">DOD-CIO “FedRAMP Moderate Equivalency” Memo</a></li>\n<li><a href=\"https://marketplace.fedramp.gov/products/FR2411959145\">GitLab Dedicated for Government FedRAMP Marketplace Listing</a></li>\n</ul>\n",
      "summary": "For Defense Industrial Base (DIB) companies, the U.S. Department of Defense's release of the Cybersecurity Maturity Model Certification (CMMC) Final Rule and new guidance on “FedRAMP equivalency” has dramatically increased the cost of compliance and fundamentally changed the way in which they drive their risk management programs. Gone is the era of “self-attestation” of security programs; DIB companies are required to strictly apply NIST 800-171 to their environments that handle Controlled Unclassified Information (CUI), and have their security controls audited by a Third-Party Assessment Organization (3PAO) every three years.\nDIB companies are engineering focused, not compliance driven, and formal audits get pricey quickly. These changes add significant complications for companies focused on supporting the warfighter. The good news? GitLab Dedicated for Government's FedRAMP Moderate Authorization means DIB companies can directly use GitLab Dedicated for Government with no additional audits or authorizations, which reduces the impact and cost of compliance.\nThe foundational rule: FedRAMP Moderate Equivalency\nThe protection of Controlled Unclassified Information (CUI) within the DIB is driven by a foundational legal and contractual mandate: the Defense Federal Acquisition Regulation Supplement (DFARS) Clause 252.204-7012. This clause specifically states that if a contractor uses an external cloud service provider to \"store, process, or transmit any covered defense information,\" that provider must meet security requirements \"equivalent to those established by the Government for the FedRAMP Moderate baseline.\"\nThe DOD's January 2, 2024, memorandum, \"Federal Risk and Authorization Management Program (FedRAMP) Moderate Equivalency for Cloud Service Provider's (CSPs) Cloud Service Offerings\" defines “FedRAMP Moderate Equivalency,” and also directly specifies that FedRAMP Moderate Cloud Service Offerings (CSOs) can be used without any additional assessment, such as individual CMMC assessment, to meet equivalency requirements:\n“This memorandum does not apply to CSOs that are FedRAMP Moderate Authorized under the existing FedRAMP process. FedRAMP Moderate Authorized CSOs identified in the FedRAMP Marketplace provide the required security to store, process or transmit CDI in accordance with Defense Federal Acquisition Regulations Supplement (DFARS) Clause 252.204-7012, \"Safeguarding Covered Defense Information and Cyber Incident Reporting\" and can be leveraged without further assessment to meet the equivalency requirements.”\nThe GitLab platform: A proven path to compliance\nGitLab's GovCloud Offering, GitLab Dedicated for Government, has achieved FedRAMP Moderate Authorization. This means that DIB companies can leverage GitLab Dedicated for Government as their DevSecOps platform immediately and without any additional audits or compliance checks. DIB companies leveraging GitLab Dedicated for Government inherit all of our security controls and our Body of Evidence, shifting the risk and cost of compliance away from themselves and allowing them to focus on their mission.\nThe Shared Responsibility Matrix: Your role as a DIB contractor\nWhile a FedRAMP-authorized solution significantly reduces your compliance burden, compliance is a joint effort. You are responsible for the security controls that fall under your purview. This is where the Shared Responsibility Matrix (SRM), also called the Customer Responsibility Matrix (CRM), comes in.\nWhen you adopt GitLab Dedicated for Government, you will receive a comprehensive SRM that clearly delineates which security controls are managed by GitLab and which are your responsibility as the customer. Your CMMC C3PAO will use this document to ensure you have implemented the necessary controls on your end. By leveraging GitLab's FedRAMP-authorized platform, you can confidently address your CMMC Level 2 compliance requirements, focusing on your mission while trusting that GitLab has you covered.\nTo learn more about GitLab Dedicated for Government, visit our GitLab for Public Sector page. Interested in a demo? Contact Sales for more information at sales-pubsec@gitlab.com.\nReferences\nCMMC “Final Rule” DFARS Supplement\nDOD-CIO “FedRAMP Moderate Equivalency” Memo\nGitLab Dedicated for Government FedRAMP Marketplace Listing",
      "publishedAt": "2025-11-12T00:00:00.000Z",
      "author": "Drew Wilmoth",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.765Z",
      "score": 7.568713948891311
    },
    {
      "id": "fb31a67a2b8db1c5c327d58bb2b8e97e",
      "title": "Secure AI agent deployment to GKE",
      "url": "https://about.gitlab.com/blog/secure-ai-agent-deployment-to-gke/",
      "content": "<p>Building <a href=\"https://about.gitlab.com/gitlab-duo/agent-platform/\">AI agents</a> is</p>\n<p>exciting, but deploying them securely to production shouldn't be</p>\n<p>complicated. In this tutorial, you will learn how GitLab's <a href=\"https://cloud.google.com/blog/topics/partners/understand-the-google-cloud-gitlab-integration\">native Google Cloud integration</a> makes it straightforward to deploy AI agents to Google Kubernetes Engine (GKE) — with built-in scanning and zero service account keys.</p>\n<h2>Why choose GKE to deploy your AI agents?</h2>\n<p>GKE provides enterprise-grade orchestration that connects seamlessly with GitLab CI/CD pipelines through OIDC authentication. Your development team can deploy AI agents while maintaining complete visibility, compliance, and control over your cloud infrastructure. This guide uses Google's Agent Development Kit (<a href=\"https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/\">ADK</a>) to build the app, so you can expect increased seamlessness as this is deployed using GitLab.</p>\n<p>Three key advantages to this approach:</p>\n<p><strong>Full infrastructure control</strong> - Your data, your rules, your environment. You maintain complete control over where your AI agents run and how they're configured.</p>\n<p><strong>Native GitLab integration</strong> - No complex workarounds. Your existing pipelines work right out of the box thanks to GitLab's native integration with Google Cloud.</p>\n<p><strong>Production-grade scaling</strong> - GKE automatically handles the heavy lifting of scaling and internal orchestration as your AI workloads grow.</p>\n<p>The key point is that GitLab with GKE provides the enterprise reliability your AI deployments demand without sacrificing the developer experience your teams expect.</p>\n<h2>Prerequisites</h2>\n<p>Before you start, make sure you have these APIs enabled:</p>\n<ul>\n<li>\n<p>GKE API</p>\n</li>\n<li>\n<p>Artifact Registry API</p>\n</li>\n<li>\n<p>Vertex AI API</p>\n</li>\n</ul>\n<p>Also make sure you have:</p>\n<ul>\n<li>\n<p>GitLab project created</p>\n</li>\n<li>\n<p>GKE cluster provisioned</p>\n</li>\n<li>\n<p>Artifact Registry repository created</p>\n</li>\n</ul>\n<h2>The deployment process</h2>\n<h3>1. Set up IAM and permissions on GitLab</h3>\n<p>Navigate to your GitLab integrations to configure Google Cloud authentication (IAM).</p>\n<p>Go to <strong>Settings &gt; Integrations</strong> and configure the Google Cloud integration. If you're using a group-level integration, notice that default settings are already inherited by projects. This means you configure once at the group level, and all projects benefit and inherit this setting.</p>\n<p>To set this up from scratch, provide:</p>\n<ul>\n<li>\n<p>Project ID</p>\n</li>\n<li>\n<p>Project Number</p>\n</li>\n<li>\n<p>Workload Identity Pool ID</p>\n</li>\n<li>\n<p>Provider ID</p>\n</li>\n</ul>\n<p>Once configured, GitLab provides a script to run in Google Cloud Console, via Cloud Shell. The outcome of running this script is a Workload Identity Federation pool with the necessary service principal to enable the proper access.</p>\n<h3>2. Configure Artifact Registry integration</h3>\n<p>Still in GitLab's integration settings, configure Artifact Management:</p>\n<ol>\n<li>\n<p>Click <strong>Artifact Management</strong>.</p>\n</li>\n<li>\n<p>Select <strong>Google Artifact Registry</strong>.</p>\n</li>\n<li>\n<p>Provide:</p>\n<ul>\n<li>Project ID</li>\n<li>Repository Name (created beforehand)</li>\n<li>Repository Location</li>\n</ul>\n</li>\n</ol>\n<p>GitLab provides another script to run in Google Cloud Console.</p>\n<p><strong>Important:</strong> Before proceeding, add these extra roles to the Workload Identity Federation pool:</p>\n<ul>\n<li>\n<p>Service Account User</p>\n</li>\n<li>\n<p>Kubernetes Developer</p>\n</li>\n<li>\n<p>Kubernetes Cluster Viewer</p>\n</li>\n</ul>\n<p>These permissions allow GitLab to deploy to GKE in subsequent steps.</p>\n<h3>3. Create the CI/CD pipeline</h3>\n<p>Now for the key part — creating the CI/CD pipeline for deployment.</p>\n<p>Head to <strong>Build &gt; Pipeline Editor</strong> and define your pipeline with four stages:</p>\n<ul>\n<li>\n<p><strong>Build</strong> - Docker creates the container image.</p>\n</li>\n<li>\n<p><strong>Test</strong> - GitLab Auto DevOps provides built-in security scans to ensure there are no vulnerabilities.</p>\n</li>\n<li>\n<p><strong>Upload</strong> - Uses GitLab's built-in CI/CD component to push to Google Artifact Registry.</p>\n</li>\n<li>\n<p><strong>Deploy</strong> - Uses Kubernetes configuration to deploy to GKE.</p>\n</li>\n</ul>\n<p>Here's the complete <code>.gitlab-ci.yml</code>:</p>\n<pre><code class=\"language-yaml\">\n\ndefault:\n  tags: [ saas-linux-2xlarge-amd64 ]\n\nstages:\n  - build\n  - test\n  - upload\n  - deploy\n\nvariables:\n  GITLAB_IMAGE: $CI_REGISTRY_IMAGE/main:$CI_COMMIT_SHORT_SHA\n  AR_IMAGE: $GOOGLE_ARTIFACT_REGISTRY_REPOSITORY_LOCATION-docker.pkg.dev/$GOOGLE_ARTIFACT_REGISTRY_PROJECT_ID/$GOOGLE_ARTIFACT_REGISTRY_REPOSITORY_NAME/main:$CI_COMMIT_SHORT_SHA\n  GCP_PROJECT_ID: &quot;your-project-id&quot;\n  GKE_CLUSTER: &quot;your-cluster&quot;\n  GKE_REGION: &quot;us-central1&quot;\n  KSA_NAME: &quot;ai-agent-ksa&quot;\n\nbuild:\n  image: docker:24.0.5\n  stage: build\n  services:\n    - docker:24.0.5-dind\n  before_script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n  script:\n    - docker build -t $GITLAB_IMAGE .\n    - docker push $GITLAB_IMAGE\n\ninclude:\n  - template: Jobs/Dependency-Scanning.gitlab-ci.yml\n  - template: Jobs/Container-Scanning.gitlab-ci.yml\n  - template: Jobs/Secret-Detection.gitlab-ci.yml\n  - component: gitlab.com/google-gitlab-components/artifact-registry/upload-artifact-registry@main\n    inputs:\n      stage: upload\n      source: $GITLAB_IMAGE\n      target: $AR_IMAGE\n\ndeploy:\n  stage: deploy\n  image: google/cloud-sdk:slim\n  identity: google_cloud\n  before_script:\n    - apt-get update &amp;&amp; apt-get install -y kubectl google-cloud-sdk-gke-gcloud-auth-plugin\n    - gcloud container clusters get-credentials $GKE_CLUSTER --region $GKE_REGION --project $GCP_PROJECT_ID\n  script:\n    - |\n      kubectl apply -f - &lt;&lt;EOF\n      apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        name: ai-agent\n        namespace: default\n      spec:\n        replicas: 2\n        selector:\n          matchLabels:\n            app: ai-agent\n        template:\n          metadata:\n            labels:\n              app: ai-agent\n          spec:\n            serviceAccountName: $KSA_NAME\n            containers:\n            - name: ai-agent\n              image: $AR_IMAGE\n              ports:\n              - containerPort: 8080\n              resources:\n                requests: {cpu: 500m, memory: 1Gi}\n                limits: {cpu: 2000m, memory: 4Gi}\n              livenessProbe:\n                httpGet: {path: /health, port: 8080}\n                initialDelaySeconds: 60\n              readinessProbe:\n                httpGet: {path: /health, port: 8080}\n                initialDelaySeconds: 30\n      ---\n      apiVersion: v1\n      kind: Service\n      metadata:\n        name: ai-agent-service\n        namespace: default\n      spec:\n        type: LoadBalancer\n        ports:\n        - port: 80\n          targetPort: 8080\n        selector:\n          app: ai-agent\n      ---\n      apiVersion: autoscaling/v2\n      kind: HorizontalPodAutoscaler\n      metadata:\n        name: ai-agent-hpa\n        namespace: default\n      spec:\n        scaleTargetRef:\n          apiVersion: apps/v1\n          kind: Deployment\n          name: ai-agent\n        minReplicas: 2\n        maxReplicas: 10\n        metrics:\n        - type: Resource\n          resource:\n            name: cpu\n            target: {type: Utilization, averageUtilization: 70}\n      EOF\n      \n      kubectl rollout status deployment/ai-agent -n default --timeout=5m\n      EXTERNAL_IP=$(kubectl get service ai-agent-service -n default -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n      echo &quot;Deployed at: http://$EXTERNAL_IP&quot;\n  only:\n    - main\n</code></pre>\n<h4>The critical configuration for GKE</h4>\n<p>What makes this work — and why we need this extra configuration for GKE— is that we must have a Kubernetes Service Account in the cluster that can work with Vertex AI. We need that service account to be permitted to access the AI capabilities of Google Cloud.</p>\n<p>Without this, we can deploy the application, but the AI agent won't work. We need to create a Kubernetes Service Account that can access Vertex AI.</p>\n<p>Run this one-time setup:</p>\n<pre><code class=\"language-bash\">\n\n#!/bin/bash\n\n\nPROJECT_ID=&quot;your-project-id&quot;\n\n\nGSA_NAME=&quot;ai-agent-vertex&quot;\n\n\nGSA_EMAIL=&quot;${GSA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com&quot;\n\n\nKSA_NAME=&quot;ai-agent-ksa&quot;\n\n\nCLUSTER_NAME=&quot;your-cluster&quot;\n\n\nREGION=&quot;us-central1&quot;\n\n\n\n# Create GCP Service Account\n\n\ngcloud iam service-accounts create $GSA_NAME \\\n    --display-name=&quot;AI Agent Vertex AI&quot; \\\n    --project=$PROJECT_ID\n\n# Grant Vertex AI permissions\n\n\ngcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member=&quot;serviceAccount:${GSA_EMAIL}&quot; \\\n    --role=&quot;roles/aiplatform.user&quot;\n\n# Get cluster credentials\n\n\ngcloud container clusters get-credentials $CLUSTER_NAME \\\n    --region $REGION --project $PROJECT_ID\n\n# Create Kubernetes Service Account\n\n\nkubectl create serviceaccount $KSA_NAME -n default\n\n\n\n# Link accounts\n\n\nkubectl annotate serviceaccount $KSA_NAME -n default \\\n    iam.gke.io/gcp-service-account=${GSA_EMAIL}\n\ngcloud iam service-accounts add-iam-policy-binding ${GSA_EMAIL} \\\n    --role=roles/iam.workloadIdentityUser \\\n    --member=&quot;serviceAccount:${PROJECT_ID}.svc.id.goog[default/${KSA_NAME}]&quot; \\\n    --project=$PROJECT_ID\n</code></pre>\n<h3>4. Deploy to GKE</h3>\n<p>Once you're done, push this change to the pipeline and you're good to go.</p>\n<p>You can see the pipeline has just deployed. Go to <strong>CI/CD &gt; Pipelines</strong> and you'll see the four stages:</p>\n<ul>\n<li>\n<p>Build</p>\n</li>\n<li>\n<p>Test (with all defined security scans)</p>\n</li>\n<li>\n<p>Upload to Artifact Registry (successful)</p>\n</li>\n<li>\n<p>Deploy to Kubernetes in GKE (success)</p>\n</li>\n</ul>\n<h2>Summary</h2>\n<p>With GitLab and Google Cloud together, you're able to deploy your AI agent to GKE with ease and security. We didn't have to go through a lot of steps — we were able to do that thanks to GitLab's native integration with Google Cloud.</p>\n<p>Watch this demo:</p>\n<p>&lt;!-- blank line --&gt;</p>\n<p>&lt;figure class=&quot;video_container&quot;&gt;\n&lt;iframe src=&quot;https://www.youtube.com/embed/mc2pCL5Qjus?si=QoH02lvz5KH5Ku9O&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt; &lt;/iframe&gt;\n&lt;/figure&gt;</p>\n<p>&lt;!-- blank line --&gt;</p>\n<blockquote>\n<p>Use this tutorial's <a href=\"https://gitlab.com/gitlab-partners-public/google-cloud/demos/gke-ai-agent\">complete code example</a> to get started now. Not a GitLab customer yet? Explore the DevSecOps platform with <a href=\"https://about.gitlab.com/free-trial/\">a free trial</a>. Startups hosted on Google Cloud have a <a href=\"https://about.gitlab.com/solutions/startups/google-cloud/\">special perk to try and use GitLab</a>.</p>\n</blockquote>\n",
      "summary": "Building AI agents is\nexciting, but deploying them securely to production shouldn't be\ncomplicated. In this tutorial, you will learn how GitLab's native Google Cloud integration makes it straightforward to deploy AI agents to Google Kubernetes Engine (GKE) — with built-in scanning and zero service account keys.\nWhy choose GKE to deploy your AI agents?\nGKE provides enterprise-grade orchestration that connects seamlessly with GitLab CI/CD pipelines through OIDC authentication. Your development team can deploy AI agents while maintaining complete visibility, compliance, and control over your cloud infrastructure. This guide uses Google's Agent Development Kit (ADK) to build the app, so you can expect increased seamlessness as this is deployed using GitLab.\nThree key advantages to this approach:\nFull infrastructure control - Your data, your rules, your environment. You maintain complete control over where your AI agents run and how they're configured.\nNative GitLab integration - No complex workarounds. Your existing pipelines work right out of the box thanks to GitLab's native integration with Google Cloud.\nProduction-grade scaling - GKE automatically handles the heavy lifting of scaling and internal orchestration as your AI workloads grow.\nThe key point is that GitLab with GKE provides the enterprise reliability your AI deployments demand without sacrificing the developer experience your teams expect.\nPrerequisites\nBefore you start, make sure you have these APIs enabled:\nGKE API\nArtifact Registry API\nVertex AI API\nAlso make sure you have:\nGitLab project created\nGKE cluster provisioned\nArtifact Registry repository created\nThe deployment process\n1. Set up IAM and permissions on GitLab\nNavigate to your GitLab integrations to configure Google Cloud authentication (IAM).\nGo to Settings > Integrations and configure the Google Cloud integration. If you're using a group-level integration, notice that default settings are already inherited by projects. This means you configure once at the group level, and all projects benefit and inherit this setting.\nTo set this up from scratch, provide:\nProject ID\nProject Number\nWorkload Identity Pool ID\nProvider ID\nOnce configured, GitLab provides a script to run in Google Cloud Console, via Cloud Shell. The outcome of running this script is a Workload Identity Federation pool with the necessary service principal to enable the proper access.\n2. Configure Artifact Registry integration\nStill in GitLab's integration settings, configure Artifact Management:\nClick Artifact Management.\nSelect Google Artifact Registry.\nProvide:\nProject ID\nRepository Name (created beforehand)\nRepository Location\nGitLab provides another script to run in Google Cloud Console.\nImportant: Before proceeding, add these extra roles to the Workload Identity Federation pool:\nService Account User\nKubernetes Developer\nKubernetes Cluster Viewer\nThese permissions allow GitLab to deploy to GKE in subsequent steps.\n3. Create the CI/CD pipeline\nNow for the key part — creating the CI/CD pipeline for deployment.\nHead to Build > Pipeline Editor and define your pipeline with four stages:\nBuild - Docker creates the container image.\nTest - GitLab Auto DevOps provides built-in security scans to ensure there are no vulnerabilities.\nUpload - Uses GitLab's built-in CI/CD component to push to Google Artifact Registry.\nDeploy - Uses Kubernetes configuration to deploy to GKE.\nHere's the complete .gitlab-ci.yml:\n\n\ndefault:\n  tags: [ saas-linux-2xlarge-amd64 ]\n\nstages:\n  - build\n  - test\n  - upload\n  - deploy\n\nvariables:\n  GITLAB_IMAGE: $CI_REGISTRY_IMAGE/main:$CI_COMMIT_SHORT_SHA\n  AR_IMAGE: $GOOGLE_ARTIFACT_REGISTRY_REPOSITORY_LOCATION-docker.pkg.dev/$GOOGLE_ARTIFACT_REGISTRY_PROJECT_ID/$GOOGLE_ARTIFACT_REGISTRY_REPOSITORY_NAME/main:$CI_COMMIT_SHORT_SHA\n  GCP_PROJECT_ID: \"your-project-id\"\n  GKE_CLUSTER: \"your-cluster\"\n  GKE_REGION: \"us-central1\"\n  KSA_NAME: \"ai-agent-ksa\"\n\nbuild:\n  image: docker:24.0.5\n  stage: build\n  services:\n    - docker:24.0.5-dind\n  before_script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n  script:\n    - docker build -t $GITLAB_IMAGE .\n    - docker push $GITLAB_IMAGE\n\ninclude:\n  - template: Jobs/Dependency-Scanning.gitlab-ci.yml\n  - template: Jobs/Container-Scanning.gitlab-ci.yml\n  - template: Jobs/Secret-Detection.gitlab-ci.yml\n  - component: gitlab.com/google-gitlab-components/artifact-registry/upload-artifact-registry@main\n    inputs:\n      stage: upload\n      source: $GITLAB_IMAGE\n      target: $AR_IMAGE\n\ndeploy:\n  stage: deploy\n  image: google/cloud-sdk:slim\n  identity: google_cloud\n  before_script:\n    - apt-get update && apt-get install -y kubectl google-cloud-sdk-gke-gcloud-auth-plugin\n    - gcloud container clusters get-credentials $GKE_CLUSTER --region $GKE_REGION --project $GCP_PROJECT_ID\n  script:\n    - |\n      kubectl apply -f - <<EOF\n      apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        name: ai-agent\n        namespace: default\n      spec:\n        replicas: 2\n        selector:\n          matchLabels:\n            app: ai-agent\n        template:\n          metadata:\n            labels:\n              app: ai-agent\n          spec:\n            serviceAccountName: $KSA_NAME\n            containers:\n            - name: ai-agent\n              image: $AR_IMAGE\n              ports:\n              - containerPort: 8080\n              resources:\n                requests: {cpu: 500m, memory: 1Gi}\n                limits: {cpu: 2000m, memory: 4Gi}\n              livenessProbe:\n                httpGet: {path: /health, port: 8080}\n                initialDelaySeconds: 60\n              readinessProbe:\n                httpGet: {path: /health, port: 8080}\n                initialDelaySeconds: 30\n      ---\n      apiVersion: v1\n      kind: Service\n      metadata:\n        name: ai-agent-service\n        namespace: default\n      spec:\n        type: LoadBalancer\n        ports:\n        - port: 80\n          targetPort: 8080\n        selector:\n          app: ai-agent\n      ---\n      apiVersion: autoscaling/v2\n      kind: HorizontalPodAutoscaler\n      metadata:\n        name: ai-agent-hpa\n        namespace: default\n      spec:\n        scaleTargetRef:\n          apiVersion: apps/v1\n          kind: Deployment\n          name: ai-agent\n        minReplicas: 2\n        maxReplicas: 10\n        metrics:\n        - type: Resource\n          resource:\n            name: cpu\n            target: {type: Utilization, averageUtilization: 70}\n      EOF\n      \n      kubectl rollout status deployment/ai-agent -n default --timeout=5m\n      EXTERNAL_IP=$(kubectl get service ai-agent-service -n default -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n      echo \"Deployed at: http://$EXTERNAL_IP\"\n  only:\n    - main\n\nThe critical configuration for GKE\nWhat makes this work — and why we need this extra configuration for GKE— is that we must have a Kubernetes Service Account in the cluster that can work with Vertex AI. We need that service account to be permitted to access the AI capabilities of Google Cloud.\nWithout this, we can deploy the application, but the AI agent won't work. We need to create a Kubernetes Service Account that can access Vertex AI.\nRun this one-time setup:\n\n\n#!/bin/bash\n\n\nPROJECT_ID=\"your-project-id\"\n\n\nGSA_NAME=\"ai-agent-vertex\"\n\n\nGSA_EMAIL=\"${GSA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\"\n\n\nKSA_NAME=\"ai-agent-ksa\"\n\n\nCLUSTER_NAME=\"your-cluster\"\n\n\nREGION=\"us-central1\"\n\n\n\n# Create GCP Service Account\n\n\ngcloud iam service-accounts create $GSA_NAME \\\n    --display-name=\"AI Agent Vertex AI\" \\\n    --project=$PROJECT_ID\n\n# Grant Vertex AI permissions\n\n\ngcloud projects add-iam-policy-binding $PROJECT_ID \\\n    --member=\"serviceAccount:${GSA_EMAIL}\" \\\n    --role=\"roles/aiplatform.user\"\n\n# Get cluster credentials\n\n\ngcloud container clusters get-credentials $CLUSTER_NAME \\\n    --region $REGION --project $PROJECT_ID\n\n# Create Kubernetes Service Account\n\n\nkubectl create serviceaccount $KSA_NAME -n default\n\n\n\n# Link accounts\n\n\nkubectl annotate serviceaccount $KSA_NAME -n default \\\n    iam.gke.io/gcp-service-account=${GSA_EMAIL}\n\ngcloud iam service-accounts add-iam-policy-binding ${GSA_EMAIL} \\\n    --role=roles/iam.workloadIdentityUser \\\n    --member=\"serviceAccount:${PROJECT_ID}.svc.id.goog[default/${KSA_NAME}]\" \\\n    --project=$PROJECT_ID\n\n4. Deploy to GKE\nOnce you're done, push this change to the pipeline and you're good to go.\nYou can see the pipeline has just deployed. Go to CI/CD > Pipelines and you'll see the four stages:\nBuild\nTest (with all defined security scans)\nUpload to Artifact Registry (successful)\nDeploy to Kubernetes in GKE (success)\nSummary\nWith GitLab and Google Cloud together, you're able to deploy your AI agent to GKE with ease and security. We didn't have to go through a lot of steps — we were able to do that thanks to GitLab's native integration with Google Cloud.\nWatch this demo:\n<!-- blank line -->\n<figure class=\"video_container\">\n<iframe src=\"https://www.youtube.com/embed/mc2pCL5Qjus?si=QoH02lvz5KH5Ku9O\" frameborder=\"0\" allowfullscreen=\"true\"> </iframe>\n</figure>\n<!-- blank line -->\nUse this tutorial's complete code example to get started now. Not a GitLab customer yet? Explore the DevSecOps platform with a free trial. Startups hosted on Google Cloud have a special perk to try and use GitLab.",
      "publishedAt": "2025-11-10T00:00:00.000Z",
      "author": "Regnard Raquedan",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "observability",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.765Z",
      "score": 9.185611192554726
    },
    {
      "id": "0182663030237ca91d898d4b555ce5e6",
      "title": "Migrate from pipeline variables to pipeline inputs for better security",
      "url": "https://about.gitlab.com/blog/migrate-from-pipeline-variables-to-pipeline-inputs-for-better-security/",
      "content": "<p><a href=\"https://docs.gitlab.com/ci/variables/#use-pipeline-variables\">Pipeline\nvariables</a>\nhave long been a convenient way to customize GitLab CI/CD pipelines at\nruntime. However, as CI/CD security best practices have evolved, we've\nrecognized the need for stronger controls around pipeline customization.\nUnrestricted pipeline variables allow any users with pipeline trigger\npermissions to override values without validation or type checking.</p>\n<p>Beyond security considerations, pipeline variables lack proper documentation and explicit declaration, making it difficult to understand what inputs are expected and how they're used throughout your pipeline. This can lead to maintenance challenges and make it harder to establish proper governance over your <a href=\"https://about.gitlab.com/topics/ci-cd/\">CI/CD</a> processes.</p>\n<h2>Enter pipeline inputs</h2>\n<p>Instead of relying on pipeline variables, we strongly recommend using GitLab's <a href=\"https://docs.gitlab.com/ci/inputs/#for-a-pipeline\">pipeline inputs</a> feature. Pipeline inputs provide:</p>\n<ul>\n<li>\n<p><strong>Explicit declaration</strong>: Inputs must be explicitly declared in your <code>.gitlab-ci.yml</code> and are self-documented.</p>\n</li>\n<li>\n<p><strong>Type safety</strong>: Support for different input types (string, boolean, number, array)</p>\n</li>\n<li>\n<p><strong>Built-in validation</strong>: Automatic validation of input values</p>\n</li>\n<li>\n<p><strong>Better security</strong>: No risk of variable injection attacks — only the declared inputs can be passed from the outside</p>\n</li>\n</ul>\n<h3>Basic example</h3>\n<pre><code>\nspec:\n  inputs:\n    deployment_env:\n      description: &quot;Target deployment environment&quot;\n      type: string\n      options: [&quot;staging&quot;, &quot;production&quot;]\n      default: &quot;staging&quot;\n    enable_tests:\n      description: &quot;Run test suite&quot;\n      type: boolean\n      default: true\n\ntest:\n  script:\n    - echo &quot;Running tests&quot;\n  rules:\n    - if: $[[ inputs.enable_tests ]] == true\n\ndeploy:\n  script:\n    - echo &quot;Deploying to $[[ inputs.deployment_env ]]&quot;\n</code></pre>\n<p>Learn more about how CI/CD inputs provide type-safe parameter passing with validation in this <a href=\"https://about.gitlab.com/blog/ci-cd-inputs-secure-and-preferred-method-to-pass-parameters-to-a-pipeline/\">tutorial</a>.</p>\n<h2>Restrict pipeline variables</h2>\n<p>To effectively move to pipeline inputs and away from pipeline variables, you should configure the <a href=\"https://docs.gitlab.com/ci/variables/#restrict-pipeline-variables\">&quot;Minimum role to use pipeline variables&quot;</a> setting. This setting provides fine-grained control over which role can use pipeline variables when triggering pipelines.</p>\n<p><strong>At the project level:</strong> Navigate to your project's <strong>Settings &gt; CI/CD &gt; Variables &gt; Minimum role to use pipeline variables</strong> to configure the setting.</p>\n<p>Available options are:</p>\n<ul>\n<li>\n<p><strong>No one allowed</strong> (<code>no_one_allowed</code>) - Recommended and most secure option. Prevents all variable overrides.</p>\n</li>\n<li>\n<p><strong>Developer</strong> (<code>developer</code>) - Allows developers and above to override variables</p>\n</li>\n<li>\n<p><strong>Maintainer</strong> (<code>maintainer</code>) - Requires maintainer role or higher</p>\n</li>\n<li>\n<p><strong>Owner</strong> (<code>owner</code>) - Only project owners can override variables</p>\n</li>\n</ul>\n<p><strong>At the group level:</strong> Group maintainers can go to <strong>Settings &gt; CI/CD &gt; Variables &gt; Default role to use pipeline variables</strong> to establish secure defaults for all new projects within their group, ensuring consistent security policies across your organization. Here we recommend again to use <strong>No one allowed</strong> as default value — this way, new projects in this group are created with a secure default setting. Note that this still allows project owners to change the setting.</p>\n<p>When pipeline variables are restricted completely (with “No one allowed”), the <a href=\"https://docs.gitlab.com/ci/pipelines/#prefill-variables-in-manual-pipelines\">prefilled variables</a> won’t appear in the &quot;New Pipeline UI&quot; form.</p>\n<h2>How to migrate from pipeline variables</h2>\n<h3>Close the gaps</h3>\n<p>Your group may have projects that have pipeline variables enabled by default despite never having used them when triggering a pipeline. These projects can be migrated to the more secure setting without a risk of interruption. GitLab <a href=\"https://docs.gitlab.com/ci/variables/#enable-pipeline-variable-restriction-for-multiple-projects\">provides migration functionality</a> via group settings:</p>\n<ul>\n<li>\n<p>Go to <strong>Settings &gt; CI/CD &gt; Variables</strong></p>\n</li>\n<li>\n<p>In <strong>Disable pipeline variables in projects that don’t use them,</strong> select <strong>Start migration</strong>.</p>\n</li>\n</ul>\n<p>This migration is a background job that safely disables pipeline variables via project settings for all projects that historically have not used them.</p>\n<h3>Convert pipeline variables to inputs</h3>\n<p>For each identified pipeline variable, create a corresponding pipeline input.</p>\n<p><strong>Before (using pipeline variables)</strong></p>\n<pre><code>\nvariables:\n  DEPLOY_ENV:\n    description: &quot;Deployment environment&quot;\n    value: &quot;staging&quot;\n  ENABLE_CACHE:\n    description: &quot;Enable deployment cache&quot;\n    value: &quot;true&quot;\n  VERSION:\n    description: &quot;Application version&quot;\n    value: &quot;1.0.0&quot;\n\ndeploy:\n  script:\n    - echo &quot;Deploying version $VERSION to $DEPLOY_ENV&quot;\n    - |\n      if [ &quot;$ENABLE_CACHE&quot; = &quot;true&quot; ]; then\n        echo &quot;Cache enabled&quot;\n      fi\n</code></pre>\n<p><strong>After (using pipeline inputs)</strong></p>\n<pre><code>\nspec:\n  inputs:\n    deploy_env:\n      description: &quot;Deployment environment&quot;\n      type: string\n      default: &quot;staging&quot;\n      options: [&quot;dev&quot;, &quot;staging&quot;, &quot;production&quot;]\n\n    enable_cache:\n      description: &quot;Enable deployment cache&quot;\n      type: boolean\n      default: true\n    \n    version:\n      description: &quot;Application version&quot;\n      type: string\n      default: &quot;1.0.0&quot;\n      regex: '^[0-9]+\\.[0-9]+\\.[0-9]+$'\n\ndeploy:\n  script:\n    - echo &quot;Deploying version $[[ inputs.version ]] to $[[ inputs.deploy_env ]]&quot;\n    - |\n      if [ &quot;$[[ inputs.enable_cache ]]&quot; = &quot;true&quot; ]; then\n        echo &quot;Cache enabled&quot;\n      fi\n</code></pre>\n<h3>Migrate trigger jobs</h3>\n<p>If you use trigger jobs with the <code>trigger</code> keyword, ensure they don't define job-level <code>variables</code> or disable inheriting variables from top-level <code>variables</code>, <code>extends</code>, or <code>include</code>, because variables could implicitly be passed downstream as pipeline variables. If pipeline variables are restricted on the downstream project, pipeline creation will fail.</p>\n<p>Consider updating your CI configuration to use pipeline inputs instead of pipeline variables.</p>\n<pre><code>\nvariables:\n  FOO: bar\n\ndeploy-staging:\n  inherit:\n    variables: false # otherwise FOO would be sent downstream as a pipeline variable\n  trigger:\n    project: myorg/deployer\n    inputs:\n      deployment_env: staging\n      enable_tests: true\n</code></pre>\n<h2>Summary</h2>\n<p>Migrating from pipeline variables to pipeline inputs is a security enhancement that protects your CI/CD infrastructure from variable injection while providing better documentation, type safety, and validation. By implementing these restrictions and adopting pipeline inputs, you're not just improving security, you're also making your pipelines more maintainable, self-documenting, and resilient.</p>\n<p>The transition requires some initial effort, but the long-term benefits far outweigh the migration costs. Start by restricting pipeline variables at the group level for new projects, then systematically migrate existing pipelines using the step-by-step approach outlined above.</p>\n<p>Security is not a destination but a journey. Pipeline inputs are one important step in creating a more secure CI/CD environment, complementing other GitLab security features like protected branches, job token allowlists, and container registry protections.</p>\n<blockquote>\n<p>To get started with pipeline inputs, <a href=\"https://about.gitlab.com/free-trial/devsecops/\">sign up for a free trial of GitLab Ultimate today</a>.</p>\n</blockquote>\n",
      "summary": "Pipeline\nvariables\nhave long been a convenient way to customize GitLab CI/CD pipelines at\nruntime. However, as CI/CD security best practices have evolved, we've\nrecognized the need for stronger controls around pipeline customization.\nUnrestricted pipeline variables allow any users with pipeline trigger\npermissions to override values without validation or type checking.\nBeyond security considerations, pipeline variables lack proper documentation and explicit declaration, making it difficult to understand what inputs are expected and how they're used throughout your pipeline. This can lead to maintenance challenges and make it harder to establish proper governance over your CI/CD processes.\nEnter pipeline inputs\nInstead of relying on pipeline variables, we strongly recommend using GitLab's pipeline inputs feature. Pipeline inputs provide:\nExplicit declaration: Inputs must be explicitly declared in your .gitlab-ci.yml and are self-documented.\nType safety: Support for different input types (string, boolean, number, array)\nBuilt-in validation: Automatic validation of input values\nBetter security: No risk of variable injection attacks — only the declared inputs can be passed from the outside\nBasic example\n\nspec:\n  inputs:\n    deployment_env:\n      description: \"Target deployment environment\"\n      type: string\n      options: [\"staging\", \"production\"]\n      default: \"staging\"\n    enable_tests:\n      description: \"Run test suite\"\n      type: boolean\n      default: true\n\ntest:\n  script:\n    - echo \"Running tests\"\n  rules:\n    - if: $[[ inputs.enable_tests ]] == true\n\ndeploy:\n  script:\n    - echo \"Deploying to $[[ inputs.deployment_env ]]\"\n\nLearn more about how CI/CD inputs provide type-safe parameter passing with validation in this tutorial.\nRestrict pipeline variables\nTo effectively move to pipeline inputs and away from pipeline variables, you should configure the \"Minimum role to use pipeline variables\" setting. This setting provides fine-grained control over which role can use pipeline variables when triggering pipelines.\nAt the project level: Navigate to your project's Settings > CI/CD > Variables > Minimum role to use pipeline variables to configure the setting.\nAvailable options are:\nNo one allowed (no_one_allowed) - Recommended and most secure option. Prevents all variable overrides.\nDeveloper (developer) - Allows developers and above to override variables\nMaintainer (maintainer) - Requires maintainer role or higher\nOwner (owner) - Only project owners can override variables\nAt the group level: Group maintainers can go to Settings > CI/CD > Variables > Default role to use pipeline variables to establish secure defaults for all new projects within their group, ensuring consistent security policies across your organization. Here we recommend again to use No one allowed as default value — this way, new projects in this group are created with a secure default setting. Note that this still allows project owners to change the setting.\nWhen pipeline variables are restricted completely (with “No one allowed”), the prefilled variables won’t appear in the \"New Pipeline UI\" form.\nHow to migrate from pipeline variables\nClose the gaps\nYour group may have projects that have pipeline variables enabled by default despite never having used them when triggering a pipeline. These projects can be migrated to the more secure setting without a risk of interruption. GitLab provides migration functionality via group settings:\nGo to Settings > CI/CD > Variables\nIn Disable pipeline variables in projects that don’t use them, select Start migration.\nThis migration is a background job that safely disables pipeline variables via project settings for all projects that historically have not used them.\nConvert pipeline variables to inputs\nFor each identified pipeline variable, create a corresponding pipeline input.\nBefore (using pipeline variables)\n\nvariables:\n  DEPLOY_ENV:\n    description: \"Deployment environment\"\n    value: \"staging\"\n  ENABLE_CACHE:\n    description: \"Enable deployment cache\"\n    value: \"true\"\n  VERSION:\n    description: \"Application version\"\n    value: \"1.0.0\"\n\ndeploy:\n  script:\n    - echo \"Deploying version $VERSION to $DEPLOY_ENV\"\n    - |\n      if [ \"$ENABLE_CACHE\" = \"true\" ]; then\n        echo \"Cache enabled\"\n      fi\n\nAfter (using pipeline inputs)\n\nspec:\n  inputs:\n    deploy_env:\n      description: \"Deployment environment\"\n      type: string\n      default: \"staging\"\n      options: [\"dev\", \"staging\", \"production\"]\n\n    enable_cache:\n      description: \"Enable deployment cache\"\n      type: boolean\n      default: true\n    \n    version:\n      description: \"Application version\"\n      type: string\n      default: \"1.0.0\"\n      regex: '^[0-9]+\\.[0-9]+\\.[0-9]+$'\n\ndeploy:\n  script:\n    - echo \"Deploying version $[[ inputs.version ]] to $[[ inputs.deploy_env ]]\"\n    - |\n      if [ \"$[[ inputs.enable_cache ]]\" = \"true\" ]; then\n        echo \"Cache enabled\"\n      fi\n\nMigrate trigger jobs\nIf you use trigger jobs with the trigger keyword, ensure they don't define job-level variables or disable inheriting variables from top-level variables, extends, or include, because variables could implicitly be passed downstream as pipeline variables. If pipeline variables are restricted on the downstream project, pipeline creation will fail.\nConsider updating your CI configuration to use pipeline inputs instead of pipeline variables.\n\nvariables:\n  FOO: bar\n\ndeploy-staging:\n  inherit:\n    variables: false # otherwise FOO would be sent downstream as a pipeline variable\n  trigger:\n    project: myorg/deployer\n    inputs:\n      deployment_env: staging\n      enable_tests: true\n\nSummary\nMigrating from pipeline variables to pipeline inputs is a security enhancement that protects your CI/CD infrastructure from variable injection while providing better documentation, type safety, and validation. By implementing these restrictions and adopting pipeline inputs, you're not just improving security, you're also making your pipelines more maintainable, self-documenting, and resilient.\nThe transition requires some initial effort, but the long-term benefits far outweigh the migration costs. Start by restricting pipeline variables at the group level for new projects, then systematically migrate existing pipelines using the step-by-step approach outlined above.\nSecurity is not a destination but a journey. Pipeline inputs are one important step in creating a more secure CI/CD environment, complementing other GitLab security features like protected branches, job token allowlists, and container registry protections.\nTo get started with pipeline inputs, sign up for a free trial of GitLab Ultimate today.",
      "publishedAt": "2025-11-04T00:00:00.000Z",
      "author": "Fabio Pitino",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "documentation",
        "ide",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.765Z",
      "score": 4.152070215025209
    },
    {
      "id": "987224a4de1728719ce2b7ab59e361a6",
      "title": "Claude Sonnet 3.7 deprecation notice for GitLab Duo",
      "url": "https://about.gitlab.com/blog/claude-sonnet-3-7-deprecation-notice-for-gitlab-duo/",
      "content": "<p>Anthropic has announced the <a href=\"https://docs.claude.com/en/docs/about-claude/model-deprecations\">deprecation of Claude Sonnet 3.7</a>. To ensure continued service and access to the latest AI capabilities, GitLab will be removing Claude Sonnet 3.7 support from GitLab Duo features in GitLab 18.8 (which is planned for January 15, 2026).</p>\n<h2>Timeline</h2>\n<ul>\n<li><strong>Now:</strong> Claude Sonnet 3.7 is still available in GitLab Duo</li>\n<li><strong>GitLab 18.8:</strong> GitLab will remove Claude Sonnet 3.7 support from GitLab Duo features</li>\n<li><strong>Recommended action:</strong> Migrate to Claude 4.0+ models immediately</li>\n</ul>\n<h2>Additional resources</h2>\n<ul>\n<li><a href=\"https://docs.claude.com/en/docs/about-claude/model-deprecations\">Anthropic Model Deprecations</a></li>\n<li><a href=\"https://docs.gitlab.com/user/gitlab_duo/model_selection/\">GitLab Duo model selection</a></li>\n</ul>\n<p>If you have questions about this change or need assistance with migration, please reach out to <a href=\"https://support.gitlab.com/\">GitLab Support</a>.</p>\n",
      "summary": "Anthropic has announced the deprecation of Claude Sonnet 3.7. To ensure continued service and access to the latest AI capabilities, GitLab will be removing Claude Sonnet 3.7 support from GitLab Duo features in GitLab 18.8 (which is planned for January 15, 2026).\nTimeline\nNow: Claude Sonnet 3.7 is still available in GitLab Duo\nGitLab 18.8: GitLab will remove Claude Sonnet 3.7 support from GitLab Duo features\nRecommended action: Migrate to Claude 4.0+ models immediately\nAdditional resources\nAnthropic Model Deprecations\nGitLab Duo model selection\nIf you have questions about this change or need assistance with migration, please reach out to GitLab Support.",
      "publishedAt": "2025-10-31T00:00:00.000Z",
      "author": "Karishma Kumar",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.765Z",
      "score": 1.1012422891219995
    },
    {
      "id": "9e4661e93751ad1ca497a29d0e37c189",
      "title": "Ace your planning without the context-switching",
      "url": "https://about.gitlab.com/blog/ace-your-planning-without-the-context-switching/",
      "content": "<p>Software development teams face a challenging balancing act: dozens of tasks, limited time, and constant pressure to pick the right thing to work on next.</p>\n<p>The planning overhead of structuring requirements, managing backlogs, tracking delivery, and writing status updates steals hours from strategic thinking.</p>\n<p>The result? Less time for the high-value decisions that actually drive products forward.</p>\n<p>That’s why we developed <a href=\"https://docs.gitlab.com/user/duo_agent_platform/agents/foundational_agents/planner/\">GitLab Duo Planner</a>, an AI agent built on <a href=\"https://about.gitlab.com/gitlab-duo/agent-platform/\">GitLab Duo Agent Platform</a> to support product managers directly within GitLab.</p>\n<p>GitLab Duo Planner isn't another generic AI assistant. GitLab's product and engineering teams, who live these challenges daily like many of our customers, purpose-built GitLab Duo Planner to orchestrate planning workflows and reduce overhead while improving alignment and predictability.</p>\n<h2>Your new planning teammate</h2>\n<p>Today’s planning workflows face three major problems:</p>\n<ol>\n<li>Prone to drift -  Unplanned and orphaned work reduce trust in the plan.</li>\n<li>Disruptive to developers - Constant interruptions for status updates break flow.</li>\n<li>Opaque - Hidden risks surface too late to course-correct.</li>\n</ol>\n<p>Transforming the way teams work, GitLab Duo Planner turns manual overhead like vague ideas into structured requirements in minutes. Surface hidden backlog problems before they derail sprints. Apply RICE and MoSCoW frameworks instantly to make confident prioritization decisions. With awareness of GitLab context across the platform, every interaction with GitLab Duo Planner saves time and improves decision quality. This is possible because of the foundational agent architecture, bringing deep domain expertise and context awareness specific to GitLab.</p>\n<h2>Built for teams</h2>\n<p>GitLab Duo Planner leverages work items (epics, issues, tasks) and understands the nuances of work breakdown structures, dependency analysis, and effort estimation, making it well positioned to improve visibility, alignment, and confidence in delivery.</p>\n<ul>\n<li>\n<p>Platform approach - Unlike point solutions, Duo Planner orchestrates across your entire GitLab platform, from planning through development and testing, driving visibility across teams and workflows.</p>\n</li>\n<li>\n<p>Embedded in the flow - No more context-switching between tools or diving deep into GitLab to retrieve information. Duo Planner enables contributions, collaboration, and transparency from users across the software development lifecycle.</p>\n</li>\n<li>\n<p>Saves time and effort - Use Duo Planner to free your teams from repetitive coordination work, improving delivery predictability, reducing missed commitments while bringing in focus on what actually moves the needle.</p>\n</li>\n</ul>\n<h2>From chaos to clarity</h2>\n<p>GitLab Duo Planner can help at different stages of software planning and delivery while operating within the planning scope, providing a safe, bounded environment with project visibility.</p>\n<p>The agent can help with six flows:</p>\n<ul>\n<li>\n<p>Prioritization - Apply frameworks like RICE, MoSCoW, or WSJF to rank work items intelligently</p>\n</li>\n<li>\n<p>Work breakdown - Decompose initiatives into epics, features, and user stories to structure requirements</p>\n</li>\n<li>\n<p>Dependency analysis - Identify blocked work and understand relationships between items to maintain velocity</p>\n</li>\n<li>\n<p>Planning -  Organize sprints, milestones, or quarterly planning</p>\n</li>\n<li>\n<p>Status reporting -  Generate summaries of project progress, risks, and blockers to track delivery</p>\n</li>\n<li>\n<p>Backlog management -  Identify stale issues, duplicates, or items needing refinement to improve data hygiene</p>\n</li>\n</ul>\n<p>Here is an example how GitLab Duo Planner can check the status of an initiative:</p>\n<p>&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/1131065078?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;GitLab Duo Planner Agent&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;</p>\n<p>&lt;p&gt;&lt;/p&gt;</p>\n<p>Duo Planner is available as a custom agent in the Duo Chat side panel, with the current page context.</p>\n<p>&lt;p&gt;&lt;/p&gt;</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1761323689/ener1mkyj9shg6zvtp4f.png\" alt=\"Duo Planner as a custom agent in the Duo Chat side panel\"></p>\n<p>&lt;p&gt;&lt;/p&gt;</p>\n<p>Let’s ask Duo Planner about the status of an initiative by providing the epic link:</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1761323689/gzv2xudegtjhtesz1oaz.png\" alt=\"Asking Duo Planner about the status of an initiative by providing the epic link\"></p>\n<p>&lt;p&gt;&lt;/p&gt;</p>\n<p>We receive a structured summary with an overview, current status of milestones, in-progress items, dependencies, and blockers, along with actionable recommendations.</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1761323690/guoyqe1b9bstmbjzunez.png\" alt=\"Structured summary\"></p>\n<p>&lt;p&gt;&lt;/p&gt;</p>\n<p>Next, let’s ask for an executive summary to share with stakeholders:\nGitLab Duo Planner eliminates hours of manual analysis and reporting effort, helping to make decisions faster and keep all stakeholders updated.</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1761323689/xs9zxawqrytfu54ejx2b.png\" alt=\"Ask for executive summary\"></p>\n<p>&lt;p&gt;&lt;/p&gt;</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1761323690/bsbpvjaqnymobzg4knhu.png\" alt=\"Output of executive summary\"></p>\n<p>&lt;p&gt;&lt;/p&gt;</p>\n<p>Here are a few more prompts you can try with GitLab Duo Planner:</p>\n<ul>\n<li>“Which of the bugs with a “boards” label should we fix first, considering user impact?”</li>\n<li>“Rank these epics by strategic value for Q1.”</li>\n<li>“Help me prioritize technical debt against new features.”</li>\n<li>“What tasks are needed to implement this user story?”</li>\n<li>“Suggest a phased approach for this project: (insert URL).”</li>\n</ul>\n<h2>What's next</h2>\n<p>GitLab Duo Planner focuses intentionally on product managers and engineering managers working in Agile environments. Why? Because specificity drives performance. By training Duo Planner deeply on GitLab's planning workflows and Agile frameworks, we deliver reliable, actionable insights rather than generic suggestions.</p>\n<p>As we evolve the platform, we envision a family of specialized agents, each optimized for specific workflows while contributing to a unified intelligence layer. Today's planner for software teams is just the beginning of how AI will transform work prioritization across all teams.</p>\n<blockquote>\n<p>If you’re an existing GitLab customer and would like to try GitLab Duo Planner with a prompt of your own, visit our <a href=\"https://docs.gitlab.com/user/duo_agent_platform/agents/foundational_agents/planner/\">documentation</a> where we cover prerequisites, use cases, and more.</p>\n</blockquote>\n",
      "summary": "Software development teams face a challenging balancing act: dozens of tasks, limited time, and constant pressure to pick the right thing to work on next.\nThe planning overhead of structuring requirements, managing backlogs, tracking delivery, and writing status updates steals hours from strategic thinking.\nThe result? Less time for the high-value decisions that actually drive products forward.\nThat’s why we developed GitLab Duo Planner, an AI agent built on GitLab Duo Agent Platform to support product managers directly within GitLab.\nGitLab Duo Planner isn't another generic AI assistant. GitLab's product and engineering teams, who live these challenges daily like many of our customers, purpose-built GitLab Duo Planner to orchestrate planning workflows and reduce overhead while improving alignment and predictability.\nYour new planning teammate\nToday’s planning workflows face three major problems:\nProne to drift -  Unplanned and orphaned work reduce trust in the plan.\nDisruptive to developers - Constant interruptions for status updates break flow.\nOpaque - Hidden risks surface too late to course-correct.\nTransforming the way teams work, GitLab Duo Planner turns manual overhead like vague ideas into structured requirements in minutes. Surface hidden backlog problems before they derail sprints. Apply RICE and MoSCoW frameworks instantly to make confident prioritization decisions. With awareness of GitLab context across the platform, every interaction with GitLab Duo Planner saves time and improves decision quality. This is possible because of the foundational agent architecture, bringing deep domain expertise and context awareness specific to GitLab.\nBuilt for teams\nGitLab Duo Planner leverages work items (epics, issues, tasks) and understands the nuances of work breakdown structures, dependency analysis, and effort estimation, making it well positioned to improve visibility, alignment, and confidence in delivery.\nPlatform approach - Unlike point solutions, Duo Planner orchestrates across your entire GitLab platform, from planning through development and testing, driving visibility across teams and workflows.\nEmbedded in the flow - No more context-switching between tools or diving deep into GitLab to retrieve information. Duo Planner enables contributions, collaboration, and transparency from users across the software development lifecycle.\nSaves time and effort - Use Duo Planner to free your teams from repetitive coordination work, improving delivery predictability, reducing missed commitments while bringing in focus on what actually moves the needle.\nFrom chaos to clarity\nGitLab Duo Planner can help at different stages of software planning and delivery while operating within the planning scope, providing a safe, bounded environment with project visibility.\nThe agent can help with six flows:\nPrioritization - Apply frameworks like RICE, MoSCoW, or WSJF to rank work items intelligently\nWork breakdown - Decompose initiatives into epics, features, and user stories to structure requirements\nDependency analysis - Identify blocked work and understand relationships between items to maintain velocity\nPlanning -  Organize sprints, milestones, or quarterly planning\nStatus reporting -  Generate summaries of project progress, risks, and blockers to track delivery\nBacklog management -  Identify stale issues, duplicates, or items needing refinement to improve data hygiene\nHere is an example how GitLab Duo Planner can check the status of an initiative:\n<div style=\"padding:56.25% 0 0 0;position:relative;\"><iframe src=\"https://player.vimeo.com/video/1131065078?badge=0&autopause=0&player_id=0&app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"GitLab Duo Planner Agent\"></iframe></div><script src=\"https://player.vimeo.com/api/player.js\"></script>\n<p></p>\nDuo Planner is available as a custom agent in the Duo Chat side panel, with the current page context.\n<p></p>\n\n<p></p>\nLet’s ask Duo Planner about the status of an initiative by providing the epic link:\n\n<p></p>\nWe receive a structured summary with an overview, current status of milestones, in-progress items, dependencies, and blockers, along with actionable recommendations.\n\n<p></p>\nNext, let’s ask for an executive summary to share with stakeholders:\nGitLab Duo Planner eliminates hours of manual analysis and reporting effort, helping to make decisions faster and keep all stakeholders updated.\n\n<p></p>\n\n<p></p>\nHere are a few more prompts you can try with GitLab Duo Planner:\n“Which of the bugs with a “boards” label should we fix first, considering user impact?”\n“Rank these epics by strategic value for Q1.”\n“Help me prioritize technical debt against new features.”\n“What tasks are needed to implement this user story?”\n“Suggest a phased approach for this project: (insert URL).”\nWhat's next\nGitLab Duo Planner focuses intentionally on product managers and engineering managers working in Agile environments. Why? Because specificity drives performance. By training Duo Planner deeply on GitLab's planning workflows and Agile frameworks, we deliver reliable, actionable insights rather than generic suggestions.\nAs we evolve the platform, we envision a family of specialized agents, each optimized for specific workflows while contributing to a unified intelligence layer. Today's planner for software teams is just the beginning of how AI will transform work prioritization across all teams.\nIf you’re an existing GitLab customer and would like to try GitLab Duo Planner with a prompt of your own, visit our documentation where we cover prerequisites, use cases, and more.",
      "publishedAt": "2025-10-28T00:00:00.000Z",
      "author": "Aathira Nair",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "testing",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.765Z",
      "score": 3.40719007917883
    },
    {
      "id": "79e47dc9d0d744c93a1573d894be18d8",
      "title": "Modernize Java applications quickly with GitLab Duo with Amazon Q",
      "url": "https://about.gitlab.com/blog/modernize-java-applications-quickly-with-gitlab-duo-with-amazon-q/",
      "content": "<p>Upgrading applications to newer, supported versions of Java has traditionally been a tedious and time-consuming process. Development teams must spend countless hours learning about deprecated APIs, updated libraries, and new language features. In many cases, significant code rewrites are necessary, turning what should be a straightforward upgrade into a multi-week project that diverts resources from building new features.</p>\n<p><a href=\"https://about.gitlab.com/gitlab-duo/duo-amazon-q/\">GitLab Duo with Amazon Q</a> changes this paradigm entirely with AI-powered automation. What once took weeks can now be accomplished in minutes, with full traceability and ready-to-review merge requests that maintain your application's functionality while leveraging modern Java features.</p>\n<h2>How it works: Upgrade your Java application</h2>\n<p>Let's walk through how you can modernize a Java 8 application to Java 17.</p>\n<p><strong>Start with an issue</strong></p>\n<p>First, create an issue in your GitLab project describing your modernization goal. You don't need to specify version details - GitLab Duo with Amazon Q is able to detect that your application is currently built with Java 8 and needs to be upgraded. Simply describe that you want to refactor your code to Java 17 in the issue title and description.</p>\n<p><strong>Trigger the transformation</strong></p>\n<p>Once your issue is created, invoke GitLab Duo with Amazon Q using the <code>/q transform</code> command in a comment on the issue. This simple command sets in motion an automated process that will analyze your entire codebase, create a comprehensive upgrade plan, and generate all necessary code changes.</p>\n<p><strong>Automated analysis and implementation</strong></p>\n<p>Behind the scenes, Amazon Q analyzes your Java 8 codebase to understand your application's structure, dependencies, and implementation patterns. It identifies deprecated features, determines which Java 17 constructs can replace existing code, and creates a merge request with all the necessary updates. The transformation updates not just your source code files — including CLI, GUI, and model classes — but also your build configuration files like <code>pom.xml</code> with Java 17 settings and dependencies.</p>\n<p><strong>Review and verification</strong></p>\n<p>The generated merge request provides a complete view of all changes. You can review how your code has been modernized with Java 17 language features and verify that all tests still pass. The beauty of this approach is that all functionality is preserved and your application works exactly the same way, just with improved, more modern code.</p>\n<h2>Why use GitLab Duo with Amazon Q</h2>\n<p>Leveraging GitLab Duo with Amazon Q for application modernization has a number of advantages for development teams:</p>\n<p><strong>Time reduction</strong>: What traditionally takes weeks of developer effort is reduced to hours or minutes, freeing your team to focus on building new features rather than managing technical debt.</p>\n<p><strong>Minimized risk</strong>: The automated analysis and transformation process reduces the risk of human error that often accompanies manual code migrations. Every change is traceable and reviewable through GitLab's merge request workflow.</p>\n<p><strong>Complete audit trail</strong>: Every transformation is documented through GitLab's version control, providing a clear record of what changed and why, which is essential for compliance and troubleshooting.</p>\n<p><strong>Enterprise-grade security</strong>: The integration leverages GitLab's end-to-end security features and AWS's robust cloud infrastructure, helping to ensure your code and data remain protected throughout the modernization process.</p>\n<p>Are you ready to see GitLab Duo with Amazon Q in action? Watch our complete walkthrough video demonstrating the Java modernization process from start to finish:</p>\n<p>&lt;!-- blank line --&gt;\n&lt;figure class=&quot;video_container&quot;&gt;\n&lt;iframe src=&quot;https://www.youtube.com/embed/qGyzG9wTsEo?si=47JnSb6flOgZAJcR&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt; &lt;/iframe&gt;\n&lt;/figure&gt;\n&lt;!-- blank line --&gt;</p>\n<blockquote>\n<p>To learn more about GitLab Duo with Amazon Q visit our <a href=\"https://about.gitlab.com/gitlab-duo/duo-amazon-q/\">web site</a> or reach out to your GitLab representative.</p>\n</blockquote>\n<h2>Read more</h2>\n<ul>\n<li><a href=\"https://about.gitlab.com/blog/agentic-ai-guides-and-resources/\">Agentic AI guides and resources</a></li>\n<li><a href=\"https://about.gitlab.com/blog/gitlab-duo-with-amazon-q-devsecops-meets-agentic-ai/\">GitLab Duo with Amazon Q: DevSecOps meets agentic AI</a></li>\n<li><a href=\"https://about.gitlab.com/blog/agentic-ai-guides-and-resources/#gitlab-duo-with-amazon-q-tutorials\">More GitLab Duo with Amazon Q tutorials</a></li>\n</ul>\n",
      "summary": "Upgrading applications to newer, supported versions of Java has traditionally been a tedious and time-consuming process. Development teams must spend countless hours learning about deprecated APIs, updated libraries, and new language features. In many cases, significant code rewrites are necessary, turning what should be a straightforward upgrade into a multi-week project that diverts resources from building new features.\nGitLab Duo with Amazon Q changes this paradigm entirely with AI-powered automation. What once took weeks can now be accomplished in minutes, with full traceability and ready-to-review merge requests that maintain your application's functionality while leveraging modern Java features.\nHow it works: Upgrade your Java application\nLet's walk through how you can modernize a Java 8 application to Java 17.\nStart with an issue\nFirst, create an issue in your GitLab project describing your modernization goal. You don't need to specify version details - GitLab Duo with Amazon Q is able to detect that your application is currently built with Java 8 and needs to be upgraded. Simply describe that you want to refactor your code to Java 17 in the issue title and description.\nTrigger the transformation\nOnce your issue is created, invoke GitLab Duo with Amazon Q using the /q transform command in a comment on the issue. This simple command sets in motion an automated process that will analyze your entire codebase, create a comprehensive upgrade plan, and generate all necessary code changes.\nAutomated analysis and implementation\nBehind the scenes, Amazon Q analyzes your Java 8 codebase to understand your application's structure, dependencies, and implementation patterns. It identifies deprecated features, determines which Java 17 constructs can replace existing code, and creates a merge request with all the necessary updates. The transformation updates not just your source code files — including CLI, GUI, and model classes — but also your build configuration files like pom.xml with Java 17 settings and dependencies.\nReview and verification\nThe generated merge request provides a complete view of all changes. You can review how your code has been modernized with Java 17 language features and verify that all tests still pass. The beauty of this approach is that all functionality is preserved and your application works exactly the same way, just with improved, more modern code.\nWhy use GitLab Duo with Amazon Q\nLeveraging GitLab Duo with Amazon Q for application modernization has a number of advantages for development teams:\nTime reduction: What traditionally takes weeks of developer effort is reduced to hours or minutes, freeing your team to focus on building new features rather than managing technical debt.\nMinimized risk: The automated analysis and transformation process reduces the risk of human error that often accompanies manual code migrations. Every change is traceable and reviewable through GitLab's merge request workflow.\nComplete audit trail: Every transformation is documented through GitLab's version control, providing a clear record of what changed and why, which is essential for compliance and troubleshooting.\nEnterprise-grade security: The integration leverages GitLab's end-to-end security features and AWS's robust cloud infrastructure, helping to ensure your code and data remain protected throughout the modernization process.\nAre you ready to see GitLab Duo with Amazon Q in action? Watch our complete walkthrough video demonstrating the Java modernization process from start to finish:\n<!-- blank line -->\n<figure class=\"video_container\">\n<iframe src=\"https://www.youtube.com/embed/qGyzG9wTsEo?si=47JnSb6flOgZAJcR\" frameborder=\"0\" allowfullscreen=\"true\"> </iframe>\n</figure>\n<!-- blank line -->\nTo learn more about GitLab Duo with Amazon Q visit our web site or reach out to your GitLab representative.\nRead more\nAgentic AI guides and resources\nGitLab Duo with Amazon Q: DevSecOps meets agentic AI\nMore GitLab Duo with Amazon Q tutorials",
      "publishedAt": "2025-10-22T00:00:00.000Z",
      "author": "Cesar Saavedra",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.766Z",
      "score": 2.2678283612704946
    },
    {
      "id": "1263110e81f0394b83cf738ebddad317",
      "title": "Delivering faster and smarter scans with Advanced SAST",
      "url": "https://about.gitlab.com/blog/delivering-faster-and-smarter-scans-with-advanced-sast/",
      "content": "<p>Static application security testing (SAST) is critical to building secure software, helping teams identify vulnerabilities in code before they can be exploited. Last year, with GitLab 17.4, we <a href=\"https://about.gitlab.com/blog/gitlab-advanced-sast-is-now-generally-available/\">launched Advanced SAST</a> to deliver higher-quality scan results directly in developer workflows. Since then, Advanced SAST has powered millions of scans across over a hundred thousand codebases, reducing risk and helping customers build more secure applications from the start.</p>\n<p>We’re building on that foundation with a set of performance enhancements designed to improve accuracy and speed, so developers get results they can trust, without losing their flow. <a href=\"https://about.gitlab.com/blog/gitlab-18-5-intelligence-that-moves-software-development-forward/\">New capabilities</a> include better out-of-the-box precision, the ability to add custom detection rules, and a trio of improvements to accelerate scan times through multi-core scanning, algorithmic optimizations, and diff-based scanning. Together, these improvements make <a href=\"https://docs.gitlab.com/user/application_security/sast/gitlab_advanced_sast/\">Advanced SAST</a> smarter and faster, delivering security that’s developer-friendly by design.</p>\n<h2>SAST adoption hinges on both accuracy and speed</h2>\n<p>Most SAST programs rarely fail due to inaccurate vulnerability detection; they fail because developers don’t adopt security tooling. Too often, AppSec solutions like SAST deliver accuracy at the expense of the developer experience, or developer experience at the expense of accuracy. In reality, both are necessary. Without accuracy, developers don’t trust the results; without speed and usability, adoption lags.</p>\n<p>When both come together, security fits naturally into the development process — and that’s the only way security teams successfully drive SAST adoption at scale. This philosophy guides the GitLab roadmap for Advanced SAST.</p>\n<h2>Add custom detection rules for greater accuracy</h2>\n<p>The built-in Advanced SAST rules are informed by our in-house security research team, designed to maximize accuracy out of the box. Until now, you could <a href=\"https://docs.gitlab.com/user/application_security/sast/customize_rulesets/\">disable rules</a> or adjust their name, description, or severity, but you couldn’t add new detection logic. With GitLab 18.5, teams can now define their own custom, pattern-based rules to catch organization-specific issues — like flagging banned function calls — while still using GitLab’s curated ruleset as the baseline. Any violations of custom rules are reported in the same place as built-in GitLab rules, so developers can glean information from a single dashboard.</p>\n<p>Custom rules are effective at catching straightforward issues that matter to your organization, but they don’t influence the taint analysis that Advanced SAST uses to catch injections and similar flaws. Customizations are managed through simple TOML files, just like other SAST ruleset configurations. The result is higher-quality scan results tuned to your context, giving security teams more control and developers clearer, more actionable findings.</p>\n<h2>Faster scans to get developers in the flow</h2>\n<p>Speed matters. If a SAST scan takes too long, developers often switch to another task, so adoption suffers.</p>\n<p>That’s why we’ve invested in several performance-based enhancements to dramatically reduce scan times without compromising on accuracy, including:</p>\n<ul>\n<li><strong>Multi-core scanning</strong>: Leverages multiple CPU cores on GitLab Runners</li>\n<li><strong>Diff-based scanning</strong>: Scans only the changed code in a merge request</li>\n<li><strong>Ongoing optimizations</strong>: Smarter algorithms and engine enhancements</li>\n</ul>\n<p>These improvements build on each other, delivering faster scans with significant impact:</p>\n<ul>\n<li>Multi-core scanning typically reduces scan runtime by up to <strong>50%.</strong></li>\n<li>Diff-based scanning helps the most in large repositories, where less code is modified in each change. It’s specifically designed to give faster feedback in the code review process by delivering faster scans in merge requests. In our testing, many large repositories now take less than <strong>10 minutes to return results in MRs, where previously scans took more than 20 minutes.</strong></li>\n<li>In recent internal testing, algorithmic optimizations <strong>cut scan times by up to 71%</strong> on large open-source codebases, with Apache Lucene (Java) showing the biggest improvement. Other projects, including Django (Python), Kafka, and Zulip, also saw <strong>performance boosts of over 50% in single-core mode</strong>. You can see the results for yourself below.</li>\n</ul>\n<p>For developers, these improvements mean quicker feedback in merge requests, less waiting on security results, and a smoother path to adoption. And with multi-core scanning and diff-based analysis layered on top, the gains will be even greater.</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1760714805/rxl2zzo58j7y0k2ldxeq.png\" alt=\"chart showing Python scan times\">\n&lt;p&gt;&lt;/p&gt;</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1760714805/hz9bsrir6nrqthkjddvi.png\" alt=\"chart showing Java scan times\"></p>\n<p>&lt;p&gt;&lt;/p&gt;</p>\n<blockquote>\n<p>These performance gains reflect GitLab’s broader focus on improving the developer experience across our platform. For example, one of our customers recently transitioned to GitLab’s <a href=\"https://docs.gitlab.com/user/application_security/policies/pipeline_execution_policies/\">Pipeline Execution Policies</a> (PEP) to gain greater control and flexibility over how security scans run within their pipelines. By standardizing templates, adding caching, and optimizing pipeline logic, their teams cut dependency scan runtimes from <strong>15–60 minutes down to just 1–2 minutes per job — saving roughly 100,000 compute minutes every day across 15,000 scans</strong>. It’s a clear example of how more customizable and efficient pipeline execution policies lead to faster feedback loops, higher productivity, and broader adoption.</p>\n</blockquote>\n<p>With these latest enhancements, Advanced SAST gives security and development teams the accuracy, speed, and flexibility they need to keep up with modern software development. By reducing false positives, enabling custom detection, and accelerating scan times, we’re making security an enabler — not a blocker — for developers.</p>\n<p>Like all of <a href=\"https://about.gitlab.com/solutions/application-security-testing/\">GitLab’s application security capabilities</a>, Advanced SAST is built directly into our DevSecOps platform, making security a natural part of how developers build, test, deploy, and secure software.</p>\n<p>The result: faster adoption, fewer bottlenecks, and more secure applications delivered from the start.</p>\n<blockquote>\n<p>Get started with Advanced SAST today! Sign up for a <a href=\"https://about.gitlab.com/free-trial/\">free trial of GitLab Ultimate</a>.</p>\n</blockquote>\n<h2>Learn more</h2>\n<ul>\n<li><a href=\"https://about.gitlab.com/blog/gitlab-advanced-sast-is-now-generally-available/\">GitLab Advanced SAST is now generally available</a></li>\n<li><a href=\"https://about.gitlab.com/blog/comprehensive-guide-to-gitlab-dast/\">A comprehensive guide to GitLab DAST</a></li>\n<li><a href=\"https://about.gitlab.com/solutions/application-security-testing/\">GitLab Security Testing solutions</a></li>\n</ul>\n",
      "summary": "Static application security testing (SAST) is critical to building secure software, helping teams identify vulnerabilities in code before they can be exploited. Last year, with GitLab 17.4, we launched Advanced SAST to deliver higher-quality scan results directly in developer workflows. Since then, Advanced SAST has powered millions of scans across over a hundred thousand codebases, reducing risk and helping customers build more secure applications from the start.\nWe’re building on that foundation with a set of performance enhancements designed to improve accuracy and speed, so developers get results they can trust, without losing their flow. New capabilities include better out-of-the-box precision, the ability to add custom detection rules, and a trio of improvements to accelerate scan times through multi-core scanning, algorithmic optimizations, and diff-based scanning. Together, these improvements make Advanced SAST smarter and faster, delivering security that’s developer-friendly by design.\nSAST adoption hinges on both accuracy and speed\nMost SAST programs rarely fail due to inaccurate vulnerability detection; they fail because developers don’t adopt security tooling. Too often, AppSec solutions like SAST deliver accuracy at the expense of the developer experience, or developer experience at the expense of accuracy. In reality, both are necessary. Without accuracy, developers don’t trust the results; without speed and usability, adoption lags.\nWhen both come together, security fits naturally into the development process — and that’s the only way security teams successfully drive SAST adoption at scale. This philosophy guides the GitLab roadmap for Advanced SAST.\nAdd custom detection rules for greater accuracy\nThe built-in Advanced SAST rules are informed by our in-house security research team, designed to maximize accuracy out of the box. Until now, you could disable rules or adjust their name, description, or severity, but you couldn’t add new detection logic. With GitLab 18.5, teams can now define their own custom, pattern-based rules to catch organization-specific issues — like flagging banned function calls — while still using GitLab’s curated ruleset as the baseline. Any violations of custom rules are reported in the same place as built-in GitLab rules, so developers can glean information from a single dashboard.\nCustom rules are effective at catching straightforward issues that matter to your organization, but they don’t influence the taint analysis that Advanced SAST uses to catch injections and similar flaws. Customizations are managed through simple TOML files, just like other SAST ruleset configurations. The result is higher-quality scan results tuned to your context, giving security teams more control and developers clearer, more actionable findings.\nFaster scans to get developers in the flow\nSpeed matters. If a SAST scan takes too long, developers often switch to another task, so adoption suffers.\nThat’s why we’ve invested in several performance-based enhancements to dramatically reduce scan times without compromising on accuracy, including:\nMulti-core scanning: Leverages multiple CPU cores on GitLab Runners\nDiff-based scanning: Scans only the changed code in a merge request\nOngoing optimizations: Smarter algorithms and engine enhancements\nThese improvements build on each other, delivering faster scans with significant impact:\nMulti-core scanning typically reduces scan runtime by up to 50%.\nDiff-based scanning helps the most in large repositories, where less code is modified in each change. It’s specifically designed to give faster feedback in the code review process by delivering faster scans in merge requests. In our testing, many large repositories now take less than 10 minutes to return results in MRs, where previously scans took more than 20 minutes.\nIn recent internal testing, algorithmic optimizations cut scan times by up to 71% on large open-source codebases, with Apache Lucene (Java) showing the biggest improvement. Other projects, including Django (Python), Kafka, and Zulip, also saw performance boosts of over 50% in single-core mode. You can see the results for yourself below.\nFor developers, these improvements mean quicker feedback in merge requests, less waiting on security results, and a smoother path to adoption. And with multi-core scanning and diff-based analysis layered on top, the gains will be even greater.\n\n<p></p>\n\n<p></p>\nThese performance gains reflect GitLab’s broader focus on improving the developer experience across our platform. For example, one of our customers recently transitioned to GitLab’s Pipeline Execution Policies (PEP) to gain greater control and flexibility over how security scans run within their pipelines. By standardizing templates, adding caching, and optimizing pipeline logic, their teams cut dependency scan runtimes from 15–60 minutes down to just 1–2 minutes per job — saving roughly 100,000 compute minutes every day across 15,000 scans. It’s a clear example of how more customizable and efficient pipeline execution policies lead to faster feedback loops, higher productivity, and broader adoption.\nWith these latest enhancements, Advanced SAST gives security and development teams the accuracy, speed, and flexibility they need to keep up with modern software development. By reducing false positives, enabling custom detection, and accelerating scan times, we’re making security an enabler — not a blocker — for developers.\nLike all of GitLab’s application security capabilities, Advanced SAST is built directly into our DevSecOps platform, making security a natural part of how developers build, test, deploy, and secure software.\nThe result: faster adoption, fewer bottlenecks, and more secure applications delivered from the start.\nGet started with Advanced SAST today! Sign up for a free trial of GitLab Ultimate.\nLearn more\nGitLab Advanced SAST is now generally available\nA comprehensive guide to GitLab DAST\nGitLab Security Testing solutions",
      "publishedAt": "2025-10-21T00:00:00.000Z",
      "author": "Salman Ladha",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "testing"
      ],
      "ingestedAt": "2025-11-23T17:37:39.766Z",
      "score": 1.7970132578179796
    },
    {
      "id": "94bfb212ae6a7718a63e051c4967d7dc",
      "title": "GitLab 18.5: Intelligence that moves software development forward",
      "url": "https://about.gitlab.com/blog/gitlab-18-5-intelligence-that-moves-software-development-forward/",
      "content": "<p>Software development teams are drowning in noise. Thousands of vulnerabilities flood security dashboards, but only a fraction pose real risk. Developers context-switch between planning backlogs, triaging security findings, reviewing code, and responding to CI/CD failures — losing hours to manual work. <a href=\"https://about.gitlab.com/releases/2025/10/16/gitlab-18-5-released/\">GitLab 18.5</a> calms this chaos.</p>\n<p>At the heart of this release is a valuable improvement in overall usability of GitLab and how AI integrates into your user experience. A new panel-based UI makes it easier to see data in context, and allows GitLab Duo Chat to be persistently visible across the platform, wherever it is needed. Purpose-built agents tackle vulnerability triage and backlog management, and popular AI tools integrate with agentic workflows even more seamlessly than before. We’ve also extended our market-leading security capabilities to help you better identify exploitable vulnerabilities versus theoretical ones, distinguish active credentials from expired ones, and scan only changed code to keep developers in flow.</p>\n<h2>What’s new in 18.5</h2>\n<p>18.5 represents our biggest release so far this year — watch our introduction to the release, and read more details below.\n&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/1128975773?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;GitLab_18.5 Release_101925_MP_v2&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;</p>\n<p>&lt;p&gt;&lt;/p&gt;</p>\n<h3>Modern user experience with quick access to GitLab Duo everywhere</h3>\n<p>GitLab 18.5 delivers a modernized user experience with a more intuitive interface driven by a new panel-based layout.</p>\n<p>With panels, key information appears side by side so that you can work contextually, without losing your place. For example, when you click on an issue in the issues list, its details automatically open in a side panel. You can also launch the GitLab Duo panel on the right, bringing Duo wherever you are in GitLab. This lets you ask contextual questions or give instructions, right alongside your work.</p>\n<p>Several usability improvements make navigation easier. The global search box now appears at the top center for improved accessibility. Global navigation elements, including Issues, Merge Requests, To-Dos, and your avatar have moved to the top right. Additionally, the left sidebar is now collapsible and expandable, giving you more control over your workspace.</p>\n<p>Teams using experimental and GitLab Duo beta features will be the first to receive the new interface, followed by all GitLab.com users who will be able to turn this experience on using the toggle located under your user icon. To learn more about this feature, reference our documentation <a href=\"https://docs.gitlab.com/user/interface_redesign/#turn-new-navigation-on-or-off\">here</a>. Please share your feedback or report any issues <a href=\"https://gitlab.com/gitlab-org/gitlab/-/issues/577554\">here</a>, you're helping us shape a better GitLab!</p>\n<h3>Updates to GitLab Duo Agent Platform</h3>\n<p><strong>Security Analyst Agent: Transform manual vulnerability triage into intelligent automation</strong></p>\n<p>GitLab Duo <a href=\"https://docs.gitlab.com/user/duo_agent_platform/agents/foundational_agents/security_analyst_agent/\">Security Analyst Agent</a> automates vulnerability management workflows through AI-powered analysis, helping transform hours of manual triage into intelligent automation. Building on the Vulnerability Management Tools available through GitLab Duo Agentic Chat, Security Analyst Agent orchestrates multiple tools, applying security policies, and creating custom flows for recurring workflows automatically.</p>\n<p>Security teams can access enriched vulnerability data, including CVE details, static reachability analysis, and code flow information, while executing operations like dismissing false positives, confirming threats, adjusting severity levels, and creating linked issues for remediation — all through conversational AI. The agent reduces repetitive clicking through vulnerability dashboards and replaces custom scripts with simple natural language commands.</p>\n<p>For example, when a security scan reveals dozens of vulnerabilities, simply prompt: &quot;Dismiss vulnerabilities with reachable=FALSE and create issues for critical findings.&quot; Security Analyst Agent analyzes reachability data, applies security policies, and completes bulk operations in moments — helping decrease work that would otherwise take hours.</p>\n<p>While individual Vulnerability Management Tools can be accessed directly through Agentic Chat for specific tasks, Security Analyst Agent orchestrates these tools intelligently and automates complex multi-step workflows. Note that Vulnerability Management Tools are available through Agentic Chat on GitLab Self-managed and GitLab.com instances, and Security Analyst Agent is available on GitLab.com only for 18.5, while availability in Self-managed and Dedicated environments will come with our next release.\nWatch this demo:</p>\n<p>&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/1128975984?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;18.5 Security Demo&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;</p>\n<p>&lt;p&gt;&lt;/p&gt;</p>\n<p><strong>GitLab Duo Planner: Turn backlog chaos into strategic clarity</strong></p>\n<p>Managing complex software delivery requires constant context-switching between planning tasks. <a href=\"https://docs.gitlab.com/user/duo_agent_platform/agents/foundational_agents/planner/\">GitLab Duo Planner</a> addresses the real-world planning challenges we see teams face every day. Duo Planner acts as your teammate with awareness of your project context, including how you manage issues, epics, and merge requests. Unlike generic AI assistants, it's purpose-built with deep knowledge of GitLab's planning workflows coupled with Agile and prioritization frameworks to help you balance effort, risk, and strategic alignment.</p>\n<p>GitLab Duo Planner can turn vague ideas into structured planning hierarchies, identify stale backlog items, and draft executive updates. For example, when refining your backlog with hundreds of issues accumulated over months, simply prompt: &quot;Identify stale backlog items and suggest priorities.&quot; Within seconds, you'll receive a structured summary showing issues without recent activity, items missing key details, duplicate work, and recommended priorities based on labels and milestones, complete with actionable recommendations.</p>\n<p>For teams managing complex roadmaps, the Planner aims to eliminate hours of manual analysis and context-switching, helping Product Managers and engineering leads make faster, more informed decisions. As of 18.5, GitLab Duo Planner is currently “read-only,” meaning that it can analyze, plan, and suggest, but cannot yet take direct action to modify anything. Please see our <a href=\"https://docs.gitlab.com/user/duo_agent_platform/agents/foundational_agents/planner/\">documentation</a> for more information.</p>\n<p><strong>Extensible Agent Catalog: Popular AI tools as native GitLab agents</strong></p>\n<p>GitLab 18.5 introduces popular AI agents directly into the <a href=\"https://docs.gitlab.com/user/duo_agent_platform/ai_catalog/\">AI Catalog</a>, making external tools like Claude, OpenAI Codex, Google Gemini CLI, Amazon Q Developer, and OpenCode available as native GitLab agents. Users can now discover, configure, and deploy these agents through the same unified catalog interface used for GitLab's built-in agents, with automatic syncing of foundational agents across organization catalogs.</p>\n<p>This eliminates the complexity of manual agent setup by providing a point-and-click catalog experience while maintaining enterprise-grade security through GitLab's authentication and audit systems. GitLab Duo Enterprise subscriptions now include built-in usage of Claude and Codex within GitLab, allowing you to use your existing GitLab subscription for these tools without requiring separate API keys or additional billing setup. Other agents may still require separate subscriptions and configuration while we finalize our integration plans.</p>\n<p><strong>Self-hosted GitLab Duo Agent Platform (Beta): Address data sovereignty requirements without sacrificing AI power</strong></p>\n<p>GitLab 18.5 moves GitLab Duo Agent Platform's self-hosted capabilities from experimental to beta, enabling organizations to execute AI agents and flows entirely within their own infrastructure — critical for regulated industries and data sovereignty requirements. The beta release includes improved timeout configurations and AI Gateway settings, allowing teams to use AI agents for code reviews, bug fixes, and feature implementations, while providing enterprise-grade security for sensitive code.</p>\n<h2>Smarter, faster security: Prioritize real risks and keep developers in the flow</h2>\n<p>GitLab 18.5 introduces new application security capabilities that help teams focus on exploitable risk, reduce noise, and strengthen software supply chain security. These updates continue our commitment to building security directly into the development process — delivering precision, speed, and insight without disrupting developer flow.</p>\n<p><strong>Static Reachability Analysis</strong></p>\n<p>With over <a href=\"https://www.cvedetails.com/\">37,000 new CVEs</a> issued this year, security teams face an overwhelming volume of vulnerabilities and struggle to understand which ones are truly exploitable. Static Reachability Analysis, now in limited availability, brings library-level precision by helping to identify whether vulnerable code is actually invoked in your application, not just present in dependencies.</p>\n<p>Paired with our <a href=\"https://docs.gitlab.com/user/application_security/vulnerabilities/risk_assessment_data/\">recently released</a> Exploit Prediction Scoring System (EPSS) and Known Exploited Vulnerability (KEV) data, security teams can more effectively accelerate vulnerability triage and prioritize real risks to help strengthen overall supply chain security. In 18.5, we’re adding support for Java, alongside existing support for Python, JavaScript, and TypeScript.</p>\n<p><strong>Secret Validity Checks</strong></p>\n<p>Just as Static Reachability Analysis helps teams prioritize exploitable vulnerabilities from open source dependencies, Secret Validity Checks bring the same insight to exposed secrets — currently available in beta on GitLab.com and GitLab Self-Managed. For GitLab-issued security tokens, instead of manually verifying whether a leaked credential or API key is active, GitLab automatically distinguishes active secrets from expired ones directly in the <a href=\"https://docs.gitlab.com/user/application_security/vulnerability_report/\">Vulnerability Report</a>. This helps enable security and development teams to focus remediation efforts on genuine risks. Support for AWS- and GCP-issued secrets is planned for future releases.</p>\n<p><strong>Custom rules for Advanced SAST</strong></p>\n<p>Advanced SAST runs on rules informed by our in-house security research team, designed to maximize accuracy out of the box. However, some teams required additional flexibility to tune the SAST engine for their specific organization. With Custom Rules for Advanced SAST, AppSec teams can define atomic, pattern-based detection logic to help capture security issues specific to their organization — like flagging banned function calls — while still using GitLab’s curated ruleset as the baseline. Customizations are managed through simple TOML files, just like other SAST ruleset configurations. While these rules will not support taint analysis, they do give organizations greater flexibility in achieving accurate SAST results.</p>\n<p><strong>Advanced SAST C and C++ language support</strong></p>\n<p>We’re expanding our language coverage for Advanced SAST to include C and C++, which are widely used languages in embedded systems software development. To enable scanning, projects must generate a compilation database that captures compiler commands and includes paths used during builds. This works to ensure the scanner can accurately parse and analyze source files, delivering precise, context-aware results that help security teams identify real vulnerabilities in the development process. The implementation requirements for C and C++ require specific configurations, which can be found in our <a href=\"https://docs.gitlab.com/user/application_security/sast/cpp_advanced_sast/\">documentation</a>. Advanced SAST C and C++ support are currently available in beta.</p>\n<p><strong>Diff-based SAST scanning</strong></p>\n<p>Traditional SAST scans re-analyze entire codebases with every commit, slowing pipelines and disrupting developer flow. The developer experience is a critical consideration that can make or break the adoption of application security testing. Diff-based SAST scanning aims to speed up scan times by focusing only on the code changed in a merge request, reducing redundant analysis and surfacing relevant results tied to the developer’s work. By aligning scans with actual code changes, GitLab delivers faster, more focused feedback that helps keep developers in flow while maintaining strong security coverage.</p>\n<h2>Simplify API configurations</h2>\n<p>API-driven workflows offer power and flexibility, but they can also create unnecessary complexity for tasks that teams need to perform regularly. The new Maven Virtual Registry interface brings a UI layer to these operations.</p>\n<h3>Maven Virtual Registry interface</h3>\n<p>The new web-based interface for managing Maven Virtual Registries turns complex API configurations into visual simplicity, providing a more intuitive experience for package administrators and platform engineers.</p>\n<p>Previously, teams configured and maintained virtual registries only through API calls, which made routine maintenance time-consuming and required specialized platform knowledge. The new interface removes that barrier, helping to make everyday tasks faster and easier.</p>\n<p>With this update, you can now:</p>\n<ul>\n<li>Create virtual registries to simplify dependency configuration</li>\n<li>Create and order upstreams to help improve performance and compliance</li>\n<li>Browse and clear stale cache entries directly in the UI</li>\n</ul>\n<p>This visual experience helps reduce operational overhead and provides development teams with clearer insight into how dependencies are resolved, enabling them to make better decisions about build performance and security policies.</p>\n<p>Watch a demo:</p>\n<p>&lt;!-- blank line --&gt;\n&lt;figure class=&quot;video_container&quot;&gt;\n&lt;iframe src=&quot;https://www.youtube.com/embed/CiOZJPhAvaI?si=cYaoR_OIgqFKbyM2&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt; &lt;/iframe&gt;\n&lt;/figure&gt;\n&lt;!-- blank line --&gt;</p>\n<p>&lt;p&gt;&lt;/p&gt;</p>\n<p>We invite enterprise customers to join the <a href=\"https://gitlab.com/gitlab-org/gitlab/-/issues/543045\">Maven Virtual Registry Beta program</a> and share feedback to help shape the final release.</p>\n<h2>AI that adapts to your workflow</h2>\n<p>This release represents more than new capabilities — it's about choice and control. Watch the walkthrough video here:</p>\n<p>&lt;p&gt;&lt;/p&gt;</p>\n<p>&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/1128992281?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;18.5-tech-demo&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;</p>\n<p>&lt;p&gt;&lt;/p&gt;</p>\n<p>GitLab Premium and Ultimate users can start using these capabilities today on <a href=\"https://GitLab.com\">GitLab.com</a> and self-managed environments, with availability for GitLab Dedicated customers planned for next month.</p>\n<p>GitLab Duo Agent Platform is currently in <strong>beta</strong> — enable beta and experimental features to experience how full-context AI can transform the way your teams build software. New to GitLab? <a href=\"https://about.gitlab.com/free-trial/devsecops/\">Start your free trial</a> and see why the future of development is AI-powered, secure, and orchestrated through the world’s most comprehensive DevSecOps platform.</p>\n<p><em><strong>Note:</strong> Platform capabilities that are in beta are available as part of the GitLab Beta program. They are free to use during the beta period, and when generally available, they will be made available with a paid add-on option for GitLab Duo Agent Platform.</em></p>\n<h3>Stay up to date with GitLab</h3>\n<p>To make sure you’re getting the latest features, security updates, and performance improvements, we recommend keeping your GitLab instance up to date. The following resources can help you plan and complete your upgrade:</p>\n<ul>\n<li><a href=\"https://gitlab-com.gitlab.io/support/toolbox/upgrade-path/\">Upgrade Path Tool</a> – enter your current version and see the exact upgrade steps for your instance</li>\n<li><a href=\"https://docs.gitlab.com/update/upgrade_paths/\">Upgrade Documentation</a> – detailed guides for each supported version, including requirements, step-by-step instructions, and best practices</li>\n</ul>\n<p>By upgrading regularly, you’ll ensure your team benefits from the newest GitLab capabilities and remains secure and supported.</p>\n<p>For organizations that want a hands-off approach, consider <a href=\"https://content.gitlab.com/viewer/d1fe944dddb06394e6187f0028f010ad#1\">GitLab’s Managed Maintenance service</a>. With Managed Maintenance, your team stays focused on innovation while GitLab experts keep your Self-Managed instance reliably upgraded, secure, and ready to lead in DevSecOps. Ask your account manager for more information.</p>\n<p><em>This blog post contains &quot;forward‑looking statements&quot; within the meaning of Section 27A of the Securities Act of 1933, as amended, and Section 21E of the Securities Exchange Act of 1934. Although we believe that the expectations reflected in these statements are reasonable, they are subject to known and unknown risks, uncertainties, assumptions and other factors that may cause actual results or outcomes to differ materially. Further information on these risks and other factors is included under the caption &quot;Risk Factors&quot; in our filings with the SEC. We do not undertake any obligation to update or revise these statements after the date of this blog post, except as required by law.</em></p>\n",
      "summary": "Software development teams are drowning in noise. Thousands of vulnerabilities flood security dashboards, but only a fraction pose real risk. Developers context-switch between planning backlogs, triaging security findings, reviewing code, and responding to CI/CD failures — losing hours to manual work. GitLab 18.5 calms this chaos.\nAt the heart of this release is a valuable improvement in overall usability of GitLab and how AI integrates into your user experience. A new panel-based UI makes it easier to see data in context, and allows GitLab Duo Chat to be persistently visible across the platform, wherever it is needed. Purpose-built agents tackle vulnerability triage and backlog management, and popular AI tools integrate with agentic workflows even more seamlessly than before. We’ve also extended our market-leading security capabilities to help you better identify exploitable vulnerabilities versus theoretical ones, distinguish active credentials from expired ones, and scan only changed code to keep developers in flow.\nWhat’s new in 18.5\n18.5 represents our biggest release so far this year — watch our introduction to the release, and read more details below.\n<div style=\"padding:56.25% 0 0 0;position:relative;\"><iframe src=\"https://player.vimeo.com/video/1128975773?badge=0&autopause=0&player_id=0&app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"GitLab_18.5 Release_101925_MP_v2\"></iframe></div><script src=\"https://player.vimeo.com/api/player.js\"></script>\n<p></p>\nModern user experience with quick access to GitLab Duo everywhere\nGitLab 18.5 delivers a modernized user experience with a more intuitive interface driven by a new panel-based layout.\nWith panels, key information appears side by side so that you can work contextually, without losing your place. For example, when you click on an issue in the issues list, its details automatically open in a side panel. You can also launch the GitLab Duo panel on the right, bringing Duo wherever you are in GitLab. This lets you ask contextual questions or give instructions, right alongside your work.\nSeveral usability improvements make navigation easier. The global search box now appears at the top center for improved accessibility. Global navigation elements, including Issues, Merge Requests, To-Dos, and your avatar have moved to the top right. Additionally, the left sidebar is now collapsible and expandable, giving you more control over your workspace.\nTeams using experimental and GitLab Duo beta features will be the first to receive the new interface, followed by all GitLab.com users who will be able to turn this experience on using the toggle located under your user icon. To learn more about this feature, reference our documentation here. Please share your feedback or report any issues here, you're helping us shape a better GitLab!\nUpdates to GitLab Duo Agent Platform\nSecurity Analyst Agent: Transform manual vulnerability triage into intelligent automation\nGitLab Duo Security Analyst Agent automates vulnerability management workflows through AI-powered analysis, helping transform hours of manual triage into intelligent automation. Building on the Vulnerability Management Tools available through GitLab Duo Agentic Chat, Security Analyst Agent orchestrates multiple tools, applying security policies, and creating custom flows for recurring workflows automatically.\nSecurity teams can access enriched vulnerability data, including CVE details, static reachability analysis, and code flow information, while executing operations like dismissing false positives, confirming threats, adjusting severity levels, and creating linked issues for remediation — all through conversational AI. The agent reduces repetitive clicking through vulnerability dashboards and replaces custom scripts with simple natural language commands.\nFor example, when a security scan reveals dozens of vulnerabilities, simply prompt: \"Dismiss vulnerabilities with reachable=FALSE and create issues for critical findings.\" Security Analyst Agent analyzes reachability data, applies security policies, and completes bulk operations in moments — helping decrease work that would otherwise take hours.\nWhile individual Vulnerability Management Tools can be accessed directly through Agentic Chat for specific tasks, Security Analyst Agent orchestrates these tools intelligently and automates complex multi-step workflows. Note that Vulnerability Management Tools are available through Agentic Chat on GitLab Self-managed and GitLab.com instances, and Security Analyst Agent is available on GitLab.com only for 18.5, while availability in Self-managed and Dedicated environments will come with our next release.\nWatch this demo:\n<div style=\"padding:56.25% 0 0 0;position:relative;\"><iframe src=\"https://player.vimeo.com/video/1128975984?badge=0&autopause=0&player_id=0&app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"18.5 Security Demo\"></iframe></div><script src=\"https://player.vimeo.com/api/player.js\"></script>\n<p></p>\nGitLab Duo Planner: Turn backlog chaos into strategic clarity\nManaging complex software delivery requires constant context-switching between planning tasks. GitLab Duo Planner addresses the real-world planning challenges we see teams face every day. Duo Planner acts as your teammate with awareness of your project context, including how you manage issues, epics, and merge requests. Unlike generic AI assistants, it's purpose-built with deep knowledge of GitLab's planning workflows coupled with Agile and prioritization frameworks to help you balance effort, risk, and strategic alignment.\nGitLab Duo Planner can turn vague ideas into structured planning hierarchies, identify stale backlog items, and draft executive updates. For example, when refining your backlog with hundreds of issues accumulated over months, simply prompt: \"Identify stale backlog items and suggest priorities.\" Within seconds, you'll receive a structured summary showing issues without recent activity, items missing key details, duplicate work, and recommended priorities based on labels and milestones, complete with actionable recommendations.\nFor teams managing complex roadmaps, the Planner aims to eliminate hours of manual analysis and context-switching, helping Product Managers and engineering leads make faster, more informed decisions. As of 18.5, GitLab Duo Planner is currently “read-only,” meaning that it can analyze, plan, and suggest, but cannot yet take direct action to modify anything. Please see our documentation for more information.\nExtensible Agent Catalog: Popular AI tools as native GitLab agents\nGitLab 18.5 introduces popular AI agents directly into the AI Catalog, making external tools like Claude, OpenAI Codex, Google Gemini CLI, Amazon Q Developer, and OpenCode available as native GitLab agents. Users can now discover, configure, and deploy these agents through the same unified catalog interface used for GitLab's built-in agents, with automatic syncing of foundational agents across organization catalogs.\nThis eliminates the complexity of manual agent setup by providing a point-and-click catalog experience while maintaining enterprise-grade security through GitLab's authentication and audit systems. GitLab Duo Enterprise subscriptions now include built-in usage of Claude and Codex within GitLab, allowing you to use your existing GitLab subscription for these tools without requiring separate API keys or additional billing setup. Other agents may still require separate subscriptions and configuration while we finalize our integration plans.\nSelf-hosted GitLab Duo Agent Platform (Beta): Address data sovereignty requirements without sacrificing AI power\nGitLab 18.5 moves GitLab Duo Agent Platform's self-hosted capabilities from experimental to beta, enabling organizations to execute AI agents and flows entirely within their own infrastructure — critical for regulated industries and data sovereignty requirements. The beta release includes improved timeout configurations and AI Gateway settings, allowing teams to use AI agents for code reviews, bug fixes, and feature implementations, while providing enterprise-grade security for sensitive code.\nSmarter, faster security: Prioritize real risks and keep developers in the flow\nGitLab 18.5 introduces new application security capabilities that help teams focus on exploitable risk, reduce noise, and strengthen software supply chain security. These updates continue our commitment to building security directly into the development process — delivering precision, speed, and insight without disrupting developer flow.\nStatic Reachability Analysis\nWith over 37,000 new CVEs issued this year, security teams face an overwhelming volume of vulnerabilities and struggle to understand which ones are truly exploitable. Static Reachability Analysis, now in limited availability, brings library-level precision by helping to identify whether vulnerable code is actually invoked in your application, not just present in dependencies.\nPaired with our recently released Exploit Prediction Scoring System (EPSS) and Known Exploited Vulnerability (KEV) data, security teams can more effectively accelerate vulnerability triage and prioritize real risks to help strengthen overall supply chain security. In 18.5, we’re adding support for Java, alongside existing support for Python, JavaScript, and TypeScript.\nSecret Validity Checks\nJust as Static Reachability Analysis helps teams prioritize exploitable vulnerabilities from open source dependencies, Secret Validity Checks bring the same insight to exposed secrets — currently available in beta on GitLab.com and GitLab Self-Managed. For GitLab-issued security tokens, instead of manually verifying whether a leaked credential or API key is active, GitLab automatically distinguishes active secrets from expired ones directly in the Vulnerability Report. This helps enable security and development teams to focus remediation efforts on genuine risks. Support for AWS- and GCP-issued secrets is planned for future releases.\nCustom rules for Advanced SAST\nAdvanced SAST runs on rules informed by our in-house security research team, designed to maximize accuracy out of the box. However, some teams required additional flexibility to tune the SAST engine for their specific organization. With Custom Rules for Advanced SAST, AppSec teams can define atomic, pattern-based detection logic to help capture security issues specific to their organization — like flagging banned function calls — while still using GitLab’s curated ruleset as the baseline. Customizations are managed through simple TOML files, just like other SAST ruleset configurations. While these rules will not support taint analysis, they do give organizations greater flexibility in achieving accurate SAST results.\nAdvanced SAST C and C++ language support\nWe’re expanding our language coverage for Advanced SAST to include C and C++, which are widely used languages in embedded systems software development. To enable scanning, projects must generate a compilation database that captures compiler commands and includes paths used during builds. This works to ensure the scanner can accurately parse and analyze source files, delivering precise, context-aware results that help security teams identify real vulnerabilities in the development process. The implementation requirements for C and C++ require specific configurations, which can be found in our documentation. Advanced SAST C and C++ support are currently available in beta.\nDiff-based SAST scanning\nTraditional SAST scans re-analyze entire codebases with every commit, slowing pipelines and disrupting developer flow. The developer experience is a critical consideration that can make or break the adoption of application security testing. Diff-based SAST scanning aims to speed up scan times by focusing only on the code changed in a merge request, reducing redundant analysis and surfacing relevant results tied to the developer’s work. By aligning scans with actual code changes, GitLab delivers faster, more focused feedback that helps keep developers in flow while maintaining strong security coverage.\nSimplify API configurations\nAPI-driven workflows offer power and flexibility, but they can also create unnecessary complexity for tasks that teams need to perform regularly. The new Maven Virtual Registry interface brings a UI layer to these operations.\nMaven Virtual Registry interface\nThe new web-based interface for managing Maven Virtual Registries turns complex API configurations into visual simplicity, providing a more intuitive experience for package administrators and platform engineers.\nPreviously, teams configured and maintained virtual registries only through API calls, which made routine maintenance time-consuming and required specialized platform knowledge. The new interface removes that barrier, helping to make everyday tasks faster and easier.\nWith this update, you can now:\nCreate virtual registries to simplify dependency configuration\nCreate and order upstreams to help improve performance and compliance\nBrowse and clear stale cache entries directly in the UI\nThis visual experience helps reduce operational overhead and provides development teams with clearer insight into how dependencies are resolved, enabling them to make better decisions about build performance and security policies.\nWatch a demo:\n<!-- blank line -->\n<figure class=\"video_container\">\n<iframe src=\"https://www.youtube.com/embed/CiOZJPhAvaI?si=cYaoR_OIgqFKbyM2\" frameborder=\"0\" allowfullscreen=\"true\"> </iframe>\n</figure>\n<!-- blank line -->\n<p></p>\nWe invite enterprise customers to join the Maven Virtual Registry Beta program and share feedback to help shape the final release.\nAI that adapts to your workflow\nThis release represents more than new capabilities — it's about choice and control. Watch the walkthrough video here:\n<p></p>\n<div style=\"padding:56.25% 0 0 0;position:relative;\"><iframe src=\"https://player.vimeo.com/video/1128992281?badge=0&autopause=0&player_id=0&app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"18.5-tech-demo\"></iframe></div><script src=\"https://player.vimeo.com/api/player.js\"></script>\n<p></p>\nGitLab Premium and Ultimate users can start using these capabilities today on GitLab.com and self-managed environments, with availability for GitLab Dedicated customers planned for next month.\nGitLab Duo Agent Platform is currently in beta — enable beta and experimental features to experience how full-context AI can transform the way your teams build software. New to GitLab? Start your free trial and see why the future of development is AI-powered, secure, and orchestrated through the world’s most comprehensive DevSecOps platform.\nNote: Platform capabilities that are in beta are available as part of the GitLab Beta program. They are free to use during the beta period, and when generally available, they will be made available with a paid add-on option for GitLab Duo Agent Platform.\nStay up to date with GitLab\nTo make sure you’re getting the latest features, security updates, and performance improvements, we recommend keeping your GitLab instance up to date. The following resources can help you plan and complete your upgrade:\nUpgrade Path Tool – enter your current version and see the exact upgrade steps for your instance\nUpgrade Documentation – detailed guides for each supported version, including requirements, step-by-step instructions, and best practices\nBy upgrading regularly, you’ll ensure your team benefits from the newest GitLab capabilities and remains secure and supported.\nFor organizations that want a hands-off approach, consider GitLab’s Managed Maintenance service. With Managed Maintenance, your team stays focused on innovation while GitLab experts keep your Self-Managed instance reliably upgraded, secure, and ready to lead in DevSecOps. Ask your account manager for more information.\nThis blog post contains \"forward‑looking statements\" within the meaning of Section 27A of the Securities Act of 1933, as amended, and Section 21E of the Securities Exchange Act of 1934. Although we believe that the expectations reflected in these statements are reasonable, they are subject to known and unknown risks, uncertainties, assumptions and other factors that may cause actual results or outcomes to differ materially. Further information on these risks and other factors is included under the caption \"Risk Factors\" in our filings with the SEC. We do not undertake any obligation to update or revise these statements after the date of this blog post, except as required by law.",
      "publishedAt": "2025-10-21T00:00:00.000Z",
      "author": "Bill Staples",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "testing",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.766Z",
      "score": 2.5158185609451715
    },
    {
      "id": "c0f806523bc7ba1f186da86e50b9f134",
      "title": "Claude Haiku 4.5 now available in GitLab Duo Agentic Chat",
      "url": "https://about.gitlab.com/blog/claude-haiku-4-5-now-available-in-gitlab-duo-agentic-chat/",
      "content": "<p>GitLab now offers Claude Haiku 4.5, Anthropic's fastest model combining high intelligence with exceptional speed, directly in the GitLab Duo model selector.</p>\n<p>Users have the flexibility to choose Claude Haiku 4.5 alongside other leading models, enhancing their GitLab Duo experience with near-frontier performance at remarkable speed. With strong performance on <a href=\"https://www.anthropic.com/news/claude-haiku-4-5\">SWE-bench Verified (73.3%)</a> and more than 2x the speed of Claude Sonnet 4.5, GitLab users can apply Claude Haiku 4.5 to accelerate their development workflows with rapid, intelligent responses.</p>\n<h2>GitLab Duo Agent Platform + Claude Haiku 4.5</h2>\n<p><a href=\"https://about.gitlab.com/gitlab-duo/agent-platform/\">GitLab Duo Agent Platform</a> extends the value of Claude Haiku 4.5 by enabling multi-agent orchestration, where Claude Haiku 4.5 can serve as a fast sub-agent executing parallel tasks while more powerful models handle high-level planning. This combination creates efficient agentic workflows, where speed meets intelligence across the software development lifecycle. The result is faster iterations, cost-effective AI assistance, and responsive experiences, all delivered inside the GitLab workflow developers already use every day.</p>\n<h2>Where you can use Claude Haiku 4.5</h2>\n<p>Claude Haiku 4.5 is now available as a model option in GitLab Duo Agent Platform Agentic Chat on GitLab.com. You can choose Claude Haiku 4.5 from the model selection dropdown to leverage its speed and coding capabilities for your development tasks.</p>\n<p><strong>Note:</strong> Ability to select Claude Haiku 4.5 in supported IDEs will be available soon.</p>\n<p>Key capabilities:</p>\n<ul>\n<li><strong>Superior coding performance:</strong> Achieves 73% on SWE-bench Verified, matching the intelligence level of models that were cutting-edge just months ago.</li>\n<li><strong>Lightning-fast responses:</strong> More than 2x faster than Sonnet 4.5, perfect for real-time pair programming.</li>\n<li><strong>Enhanced computer use:</strong> Outperforms Claude Sonnet 4 at autonomous task execution.</li>\n<li><strong>Context awareness:</strong> First Haiku model with native context window tracking for better task persistence.</li>\n<li><strong>Extended thinking:</strong> Pause and reason through complex problems before generating responses.</li>\n</ul>\n<h2>Get started today</h2>\n<p>GitLab Duo Pro and Enterprise customers can access Claude Haiku 4.5 today. Visit our <a href=\"https://docs.gitlab.com/user/gitlab_duo/\">documentation</a> to learn more about GitLab Duo capabilities and models.</p>\n<p>Questions or feedback? Share your experience with us through the GitLab community.</p>\n<blockquote>\n<p>Want to try GitLab Ultimate with Duo Enterprise? <a href=\"https://about.gitlab.com/gitlab-duo/\">Sign up for a free trial today.</a></p>\n</blockquote>\n<h2>Read more</h2>\n<ul>\n<li><a href=\"https://about.gitlab.com/blog/greater-ai-choice-in-gitlab-duo-claude-sonnet-4-5-arrives/\">Greater AI choice in GitLab Duo: Claude Sonnet 4.5 arrives</a></li>\n<li><a href=\"https://about.gitlab.com/blog/gitlab-18-4-ai-native-development-with-automation-and-insight/\">GitLab 18.4: AI-native development with automation and insight</a></li>\n<li><a href=\"https://about.gitlab.com/blog/gitlab-duo-chat-gets-agentic-ai-makeover/\">GitLab Duo Chat gets agentic AI makeover</a></li>\n</ul>\n",
      "summary": "GitLab now offers Claude Haiku 4.5, Anthropic's fastest model combining high intelligence with exceptional speed, directly in the GitLab Duo model selector.\nUsers have the flexibility to choose Claude Haiku 4.5 alongside other leading models, enhancing their GitLab Duo experience with near-frontier performance at remarkable speed. With strong performance on SWE-bench Verified (73.3%) and more than 2x the speed of Claude Sonnet 4.5, GitLab users can apply Claude Haiku 4.5 to accelerate their development workflows with rapid, intelligent responses.\nGitLab Duo Agent Platform + Claude Haiku 4.5\nGitLab Duo Agent Platform extends the value of Claude Haiku 4.5 by enabling multi-agent orchestration, where Claude Haiku 4.5 can serve as a fast sub-agent executing parallel tasks while more powerful models handle high-level planning. This combination creates efficient agentic workflows, where speed meets intelligence across the software development lifecycle. The result is faster iterations, cost-effective AI assistance, and responsive experiences, all delivered inside the GitLab workflow developers already use every day.\nWhere you can use Claude Haiku 4.5\nClaude Haiku 4.5 is now available as a model option in GitLab Duo Agent Platform Agentic Chat on GitLab.com. You can choose Claude Haiku 4.5 from the model selection dropdown to leverage its speed and coding capabilities for your development tasks.\nNote: Ability to select Claude Haiku 4.5 in supported IDEs will be available soon.\nKey capabilities:\nSuperior coding performance: Achieves 73% on SWE-bench Verified, matching the intelligence level of models that were cutting-edge just months ago.\nLightning-fast responses: More than 2x faster than Sonnet 4.5, perfect for real-time pair programming.\nEnhanced computer use: Outperforms Claude Sonnet 4 at autonomous task execution.\nContext awareness: First Haiku model with native context window tracking for better task persistence.\nExtended thinking: Pause and reason through complex problems before generating responses.\nGet started today\nGitLab Duo Pro and Enterprise customers can access Claude Haiku 4.5 today. Visit our documentation to learn more about GitLab Duo capabilities and models.\nQuestions or feedback? Share your experience with us through the GitLab community.\nWant to try GitLab Ultimate with Duo Enterprise? Sign up for a free trial today.\nRead more\nGreater AI choice in GitLab Duo: Claude Sonnet 4.5 arrives\nGitLab 18.4: AI-native development with automation and insight\nGitLab Duo Chat gets agentic AI makeover",
      "publishedAt": "2025-10-20T00:00:00.000Z",
      "author": "Tim Zallmann",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.766Z",
      "score": 1.6313038550142647
    },
    {
      "id": "745d86903e62055a8c36f6080404c656",
      "title": "Variable and artifact sharing in GitLab parent-child pipelines",
      "url": "https://about.gitlab.com/blog/variable-and-artifact-sharing-in-gitlab-parent-child-pipelines/",
      "content": "<p>Software projects have different evolving needs and requirements. Some have\nsaid that <em>software is never finished, merely abandoned</em>. Some software\nprojects are small and others are large with complex integrations. Some have\ndependencies on external projects, while others are self-contained.\nRegardless of the size and complexity, the need to validate and ensure\nfunctionality remains paramount.</p>\n<p>CI/CD pipelines can help with the challenge of building and validating software projects consistently, but, much like the software itself, these pipelines can become complex with many dependencies. This is where ideas like <a href=\"https://docs.gitlab.com/ci/pipelines/downstream_pipelines/#parent-child-pipelines\">parent-child pipelines</a> and data exchange in CI/CD setups become incredibly important.</p>\n<p>In this article, we will cover common CI/CD data exchange challenges users may encounter with parent-child pipelines in GitLab — and how to solve them. You'll learn how to turn complex CI/CD processes into more manageable setups.</p>\n<h2>Using parent-child pipelines</h2>\n<p>The pipeline setup in the image below illustrates a scenario where a project could require a large, complex pipeline. The whole project resides in one repository and contains different modules. Each module requires its own set of build and test automation steps.</p>\n<p>One approach to address the CI/CD configuration in a scenario like this is to break down the larger pipeline into smaller ones (i.e., child pipelines) and keep a common CI/CD process that is shared across all modules in charge of the whole orchestration (i.e., parent pipeline).</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1760617772/hizwvhmgxn6exbmvsnrv.png\" alt=\"CI/CD configuration\"></p>\n<p>The parent-child pipeline pattern allows a single pipeline to orchestrate one or many downstream pipelines. Similar to how a single pipeline coordinates the execution of multiple <a href=\"https://docs.gitlab.com/ci/jobs/\">jobs</a>, the parent pipeline coordinates the running of full pipelines with one or more jobs.</p>\n<p>This pattern has been shown to be helpful in a variety of use cases:</p>\n<ul>\n<li>\n<p>Breaking down large, complex pipelines into smaller, manageable pieces</p>\n</li>\n<li>\n<p>Conditionally executing certain pipelines as part of a larger CI/CD process</p>\n</li>\n<li>\n<p>Executing pipelines in parallel</p>\n</li>\n<li>\n<p>Helping manage user permissions to access and run certain pipelines</p>\n</li>\n</ul>\n<p>GitLab’s current CI/CD structure supports this pattern and makes it simple to implement parent-child pipelines. While there are many benefits when using the parent-child pipeline pattern with GitLab, one question we often get is how to share data between the parent and child pipelines. In the next sections, we’ll go over how to make use of GitLab variables and artifacts to address this concern.</p>\n<h3>Sharing variables</h3>\n<p>There are cases where it is necessary to pass the output from a parent pipeline job to a child pipeline. These outputs can be shared as variables, <a href=\"https://docs.gitlab.com/ci/jobs/job_artifacts/\">artifacts</a>, and <a href=\"https://docs.gitlab.com/ci/inputs/\">inputs</a>.</p>\n<p>Consider a case where we create a custom variable <code>var_1</code> during the runtime of a job:</p>\n<pre><code>\nstages:\n  - build\n  - triggers\n\n# This job only creates a variable \n\ncreate_var_job:\n  stage: build\n  script:\n    - var_1=&quot;Hi, I'm a Parent pipeline variable&quot;\n    - echo &quot;var_1=$var_1&quot; &gt;&gt; var.env\n  artifacts:\n    reports:\n      dotenv: var.env\n</code></pre>\n<p>Notice that the variable is created as part of the script steps in the job (during runtime). In this example, we are using a simple string <code>&quot;Hi, I'm a Parent pipeline variable&quot;</code> to illustrate the main syntax required to later share this variable with a child pipeline. Let's break down the <code>create_var_job</code>  and analyze the main steps from this GitLab job</p>\n<p>First, we need to save <code>var_1</code> as <code>dotenv</code>:</p>\n<pre><code>  script:\n    - var_1=&quot;Hi, I'm a pipeline variable&quot;\n    - echo &quot;var_1=$var_1&quot; &gt;&gt; var.env\n</code></pre>\n<p>After saving <code>var_1</code> as <code>var.env</code>, the next important step is to make this variable available as an artifact produced by the <code>create_var_job</code>. To do that, we use the following syntax:</p>\n<pre><code>\nartifacts:\n    reports:\n      dotenv: var.env\n</code></pre>\n<p>Up to this point, we have created a variable during runtime and saved it as a <code>dotenv</code> report. Now let's add the job that should trigger the child pipeline:</p>\n<pre><code>\ntelco_service_a:\n  stage: triggers\n  trigger:\n    include: service_a/.gitlab-ci.yml\n  rules:\n    - changes:\n        - service_a/*\n</code></pre>\n<p>The goal of <code>telco_service_a</code>  job is to find the <code>.gitlab-ci.yml</code> configuration of the child pipeline,  which is defined in this case as <code>service_a,</code> and trigger its execution. Let's examine this job:</p>\n<pre><code>\ntelco_service_a:\n  stage: triggers\n  trigger:\n    include: service_a/.gitlab-ci.yml\n</code></pre>\n<p>We see it belongs to another <code>stage</code> of the pipeline named <code>triggers.</code>This job will run only after <code>create_var_job</code> from the first stage successfully finishes and where the variable  <code>var_1</code> we want to pass is created.</p>\n<p>After defining the stage, we use the reserved words <code>trigger</code> and <code>include</code> to tell GitLab where to search for the child pipeline configuration, as illustrated in the YAML below:</p>\n<pre><code>  trigger:\n    include: service_a/.gitlab-ci.yml\n</code></pre>\n<p>Our child-pipeline YAML configuration is under <code>service_a/.gitlab-ci.yml</code> folder in the GitLab repository, for this example.</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1760617772/ujkirpbifthpuujkcm6f.png\" alt=\"child-pipeline YAML configuration\"></p>\n<p>&lt;p&gt;&lt;/p&gt;</p>\n<p>&lt;center&gt;&lt;i&gt;Child pipelines folders with configurations&lt;/i&gt;&lt;/center&gt;</p>\n<p>&lt;p&gt;&lt;/p&gt;</p>\n<p>Take into consideration that the repository structure depicted above can vary. What matters is properly pointing the  <code>triggers: include</code> properties at the location of your child-pipeline configuration in your repository.</p>\n<p>Finally, we use <code>rules: changes</code> to indicate to GitLab that this child pipeline should be triggered only if there is any change in any file in the <code>service_a/.gitlab-ci.yml</code> directory, as illustrated in the following code snippet:</p>\n<pre><code>\nrules:\n    - changes:\n        - service_a/*\n</code></pre>\n<p>Using this rule helps to optimize cost by triggering the child pipeline job only when necessary. This approach is particularly valuable in a monorepo architecture where specific modules contain numerous components, allowing us to avoid running their dedicated pipelines when no changes have been made to their respective codebases.</p>\n<h4>Configuring the parent pipeline</h4>\n<p>Up to this point, we have put together our parent pipeline. Here's the full code snippet for this segment:</p>\n<pre><code>\n# Parent Pipeline Configuration\n\n# This pipeline creates a custom variable and triggers a child pipeline\n\n\nstages:\n  - build\n  - trigger\n\ncreate_var_job:\n  stage: build\n  script:\n    - var_1=&quot;Hi, I'm a Parent pipeline variable&quot;\n    - echo &quot;var_1=$var_1&quot; &gt;&gt; var.env\n  artifacts:\n    reports:\n      dotenv: var.env\n\ntelco_service_a:\n  stage: triggers\n  trigger:\n    include: service_a/.gitlab-ci.yml\n  rules:\n    - changes:\n        - service_a/*\n</code></pre>\n<p>When GitLab executes the YAML configuration in the GitLab UI, the parent pipeline gets rendered as follows:</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1760617771/e1azkkr0rnzd42dzkw1x.png\" alt=\"parent pipeline rendering\"></p>\n<p>Notice the label &quot;trigger job,&quot; which indicates this job will start the execution of another pipeline configuration.</p>\n<h4>Configuring the child pipeline</h4>\n<p>Moving forward, let's now focus on the child pipeline configuration, where we expect to inherit and print the value of the <code>var_1</code> created in the parent pipeline.</p>\n<p>The pipeline configuration in <code>service_a/.gitlab_ci.yml</code> has the following definition:</p>\n<pre><code>\nstages:\n  - build\n\nbuild_a:\n  stage: build\n  script:\n    - echo &quot;this job inherits the variable from the Parent pipeline:&quot;\n    - echo $var_1\n  needs:\n    - project: gitlab-da/use-cases/7-4-parent-child-pipeline\n      job: create_var_job\n      ref: main\n      artifacts: true\n</code></pre>\n<p>Like before, let's break down this pipeline and highlight the main parts to achieve our goal. This pipeline only contains one stage (i.e., <code>build)</code> and one job (i.e., <code>build_a)</code>. The script in the job contains two steps:</p>\n<pre><code>\nbuild_a:\n  stage: build\n  script:\n    - echo &quot;this job inherits the variable from the Parent pipeline:&quot;\n    - echo $var_1\n</code></pre>\n<p>These two steps print output during the execution. The most interesting one is the second step, <code>echo $var_1</code>, where we expect to print the variable value inherited from the parent pipeline. Remember, this was a simple string with value: <code>&quot;Hi, I'm a Parent pipeline variable.&quot;</code></p>\n<h4>Inheriting variables using needs</h4>\n<p>To set and link this job to inherit variables from the parent pipeline, we use the reserved GitLab CI properties <code>needs</code> as depicted in the following snippet:</p>\n<pre><code>\nneeds:\n    - project: gitlab-da/use-cases/7-4-parent-child-pipeline\n      job: create_var_job\n      ref: main\n      artifacts: true\n</code></pre>\n<p>Using the &quot;needs&quot; keyword, we define dependencies that must be completed before running this job. In this case, we pass four different values. Let's walk through each one  of them:</p>\n<ul>\n<li>\n<p><strong>Project:</strong> The complete namespace of the project where the main <code>gitlab-ci.yml</code> containing the parent pipeline YAML is located. Make sure to include the absolute path.</p>\n</li>\n<li>\n<p><strong>Job:</strong> The specific job name in the parent pipeline from where we want to inherit the variable.</p>\n</li>\n<li>\n<p><strong>Ref:</strong> The name of the branch where the main <code>gitlab-ci.yml</code> containing the parent pipeline YAML is located.</p>\n</li>\n<li>\n<p><strong>Artifacts:</strong> Where we set a boolean value, indicating that artifacts from the parent pipeline job should be downloaded and made available to this child pipeline job.</p>\n</li>\n</ul>\n<p><strong>Note:</strong> This specific approach using the needs property is only available to GitLab Premium and Ultimate users. We will cover another example for GitLab community users later on.</p>\n<h4>Putting it all together</h4>\n<p>Now let's assume we make a change to any of the files under <code>service_a</code> folder and commit the changes to the repository. When GitLab detects the change, the rule we set up will trigger the child job pipeline execution. This gets displayed in the GitLab UI as follows:</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1760617771/e1azkkr0rnzd42dzkw1x.png\" alt=\"Rule triggering the child job pipeline execution\"></p>\n<p>Clicking on the <code>telco_service_a</code>  will take us to the jobs in the child pipeline:</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1760617773/vftjkg7ct2wqmew1e3yk.png\" alt=\"Jobs in pipeline\"></p>\n<p>We can see the parent-child relationship, and finally, by clicking on the <code>build_a job</code>, we can visually verify the variable inheritance in the job execution log:</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1760617758/hxfkfmev9hebbqhgcvoh.png\" alt=\"Verifying the variable inheritance in the job execution log\"></p>\n<p>This output confirms the behavior we expected. The custom runtime variable <code>var_1</code> created in the parent job is inherited in the child job, unpacked from the <code>dotenv</code> report, and its value accessible as can be confirmed in Line 26 above.</p>\n<p>This use case illustrates how to share custom variables that can contain any value between pipelines. This example is intentionally simple and can be extrapolated to more realistic scenarios. Take, for instance, the following CI/CD configuration, where the custom variable we need to share is the tag of a Docker image:</p>\n<pre><code>\n# Pipeline \n\n\nbuild-prod-image:\n  tags: [ saas-linux-large-amd64 ]\n  image: docker:20.10.16\n  stage: build\n  services:\n    - docker:20.10.16-dind\n  \n  script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n    - docker build -t $PRODUCTION_IMAGE .\n    - docker push $PRODUCTION_IMAGE\n    - echo &quot;UPSTREAM_CONTAINER_IMAGE=$PRODUCTION_IMAGE&quot; &gt;&gt; prodimage.env\n\n  artifacts:\n    reports:\n      dotenv: prodimage.env\n\n  rules:\n      - if: '$CI_COMMIT_BRANCH == &quot;main&quot;'\n        when: always\n      - when: never\n</code></pre>\n<p>And use the variable with the Docker image tag, in another job that updates a Helm manifest file:</p>\n<pre><code>\nupdate-helm-values:\n    stage: update-manifests\n    image:\n        name: alpine:3.16\n        entrypoint: [&quot;&quot;]\n  \n    before_script:\n          - apk add --no-cache git curl bash yq\n          - git remote set-url origin https://${CI_USERNAME}:${GITOPS_USER}@${SERVER_PATH}/${PROJECT_PATH}\n          - git config --global user.email &quot;gitlab@gitlab.com&quot;\n          - git config --global user.name &quot;GitLab GitOps&quot;\n          - git pull origin main\n    script:\n          - cd src\n          - echo $UPSTREAM_CONTAINER_IMAGE\n          - yq eval -i &quot;.spec.template.spec.containers[0].image |= \\&quot;$UPSTREAM_CONTAINER_IMAGE\\&quot;&quot; store-deployment.yaml\n          - cat store-deployment.yaml\n          - git pull origin main\n          - git checkout -B main\n          - git commit -am '[skip ci] prod image update'\n          - git push origin main\n    needs:\n      - project: gitlab-da/use-cases/devsecops-platform/simply-find/simply-find-front-end\n        job: build-prod-image\n        ref: main\n        artifacts: true\n</code></pre>\n<p>Mastering how to share variables between pipelines while maintaining the relationship between them enables us to create more sophisticated workflow orchestration that can meet our software building needs.</p>\n<h3>Using GitLab Package Registry to share artifacts</h3>\n<p>While the needs feature mentioned above works great for Premium and Ultimate users, GitLab also has features to help achieve similar results for Community Edition users. One suggested approach is to store artifacts in the <a href=\"https://docs.gitlab.com/user/packages/package_registry/\">GitLab Package Registry</a>.</p>\n<p>Using a combination of the variables provided in GitLab CI/CD jobs and the GitLab API, you can upload artifacts to the GitLab Package Registry from a parent pipeline. In the child pipeline, you can then access the uploaded artifact from the package registry using the same variables and API to access the artifact. Let’s take a look at the example pipeline and some supplementary scripts that illustrate this:</p>\n<p><strong>gitlab-ci.yml (parent pipeline)</strong></p>\n<pre><code>\n# Parent Pipeline Configuration\n\n# This pipeline creates an artifact, uploads it to Package Registry, and triggers a child pipeline\n\n\nstages:\n  - create-upload\n  - trigger\n\nvariables:\n  PACKAGE_NAME: &quot;pipeline-artifacts&quot;\n  PACKAGE_VERSION: &quot;$CI_PIPELINE_ID&quot;\n  ARTIFACT_FILE: &quot;artifact.txt&quot;\n\n# Job 1: Create and upload artifact to Package Registry\n\ncreate-and-upload-artifact:\n  stage: create-upload\n  image: alpine:latest\n  before_script:\n    - apk add --no-cache curl bash\n  script:\n    - bash scripts/create-artifact.sh\n    - bash scripts/upload-to-registry.sh\n  rules:\n    - if: $CI_PIPELINE_SOURCE == &quot;push&quot;\n\n# Job 2: Trigger child pipeline\n\ntrigger-child:\n  stage: trigger\n  trigger:\n    include: child-pipeline.yml\n    strategy: depend\n  variables:\n    PARENT_PIPELINE_ID: $CI_PIPELINE_ID\n    PACKAGE_NAME: $PACKAGE_NAME\n    PACKAGE_VERSION: $PACKAGE_VERSION\n    ARTIFACT_FILE: $ARTIFACT_FILE\n  rules:\n    - if: $CI_PIPELINE_SOURCE == &quot;push&quot;\n</code></pre>\n<p><strong>child-pipeline.yml</strong></p>\n<pre><code>\n# Child Pipeline Configuration\n\n# This pipeline downloads the artifact from Package Registry and processes it\n\n\nstages:\n  - download-process\n\nvariables:\n  # These variables are passed from the parent pipeline\n  PACKAGE_NAME: &quot;pipeline-artifacts&quot;\n  PACKAGE_VERSION: &quot;$PARENT_PIPELINE_ID&quot;\n  ARTIFACT_FILE: &quot;artifact.txt&quot;\n\n# Job 1: Download and process artifact from Package Registry\n\ndownload-and-process-artifact:\n  stage: download-process\n  image: alpine:latest\n  before_script:\n    - apk add --no-cache curl bash\n  script:\n    - bash scripts/download-from-registry.sh\n    - echo &quot;Processing downloaded artifact...&quot;\n    - cat $ARTIFACT_FILE\n    - echo &quot;Artifact processed successfully!&quot;\n</code></pre>\n<p><strong>upload-to-registry.sh</strong></p>\n<pre><code>\n#!/bin/bash\n\n\nset -e\n\n\n# Configuration\n\nPACKAGE_NAME=&quot;${PACKAGE_NAME:-pipeline-artifacts}&quot;\n\nPACKAGE_VERSION=&quot;${PACKAGE_VERSION:-$CI_PIPELINE_ID}&quot;\n\nARTIFACT_FILE=&quot;${ARTIFACT_FILE:-artifact.txt}&quot;\n\n\n# Validate required variables\n\nif [ -z &quot;$CI_PROJECT_ID&quot; ]; then\n    echo &quot;Error: CI_PROJECT_ID is not set&quot;\n    exit 1\nfi\n\n\nif [ -z &quot;$CI_JOB_TOKEN&quot; ]; then\n    echo &quot;Error: CI_JOB_TOKEN is not set&quot;\n    exit 1\nfi\n\n\nif [ -z &quot;$CI_API_V4_URL&quot; ]; then\n    echo &quot;Error: CI_API_V4_URL is not set&quot;\n    exit 1\nfi\n\n\nif [ ! -f &quot;$ARTIFACT_FILE&quot; ]; then\n    echo &quot;Error: Artifact file '$ARTIFACT_FILE' not found&quot;\n    exit 1\nfi\n\n\n# Construct the upload URL\n\nUPLOAD_URL=&quot;${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/${PACKAGE_NAME}/${PACKAGE_VERSION}/${ARTIFACT_FILE}&quot;\n\n\n# Upload the file using curl\n\nresponse=$(curl -w &quot;%{http_code}&quot; -o /tmp/upload_response.json \\\n    --header &quot;JOB-TOKEN: $CI_JOB_TOKEN&quot; \\\n    --upload-file &quot;$ARTIFACT_FILE&quot; \\\n    &quot;$UPLOAD_URL&quot;)\n\nif [ &quot;$response&quot; -eq 201 ]; then\n    echo &quot;Upload successful!&quot;\nelse\n    echo &quot;Upload failed with HTTP code: $response&quot;\n    exit 1\nfi\n\n</code></pre>\n<p><strong>download-from-regsitry.sh</strong></p>\n<pre><code>\n#!/bin/bash\n\n\nset -e\n\n\n# Configuration\n\nPACKAGE_NAME=&quot;${PACKAGE_NAME:-pipeline-artifacts}&quot;\n\nPACKAGE_VERSION=&quot;${PACKAGE_VERSION:-$PARENT_PIPELINE_ID}&quot;\n\nARTIFACT_FILE=&quot;${ARTIFACT_FILE:-artifact.txt}&quot;\n\n\n# Validate required variables\n\nif [ -z &quot;$CI_PROJECT_ID&quot; ]; then\n    echo &quot;Error: CI_PROJECT_ID is not set&quot;\n    exit 1\nfi\n\n\nif [ -z &quot;$CI_JOB_TOKEN&quot; ]; then\n    echo &quot;Error: CI_JOB_TOKEN is not set&quot;\n    exit 1\nfi\n\n\nif [ -z &quot;$CI_API_V4_URL&quot; ]; then\n    echo &quot;Error: CI_API_V4_URL is not set&quot;\n    exit 1\nfi\n\n\nif [ -z &quot;$PACKAGE_VERSION&quot; ]; then\n    echo &quot;Error: PACKAGE_VERSION is not set&quot;\n    exit 1\nfi\n\n\n# Construct the download URL\n\nDOWNLOAD_URL=&quot;${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/${PACKAGE_NAME}/${PACKAGE_VERSION}/${ARTIFACT_FILE}&quot;\n\n\n# Download the file using curl\n\nresponse=$(curl -w &quot;%{http_code}&quot; -o &quot;$ARTIFACT_FILE&quot; \\\n    --header &quot;JOB-TOKEN: $CI_JOB_TOKEN&quot; \\\n    --fail-with-body \\\n    &quot;$DOWNLOAD_URL&quot;)\n\nif [ &quot;$response&quot; -eq 200 ]; then\n    echo &quot;Download successful!&quot;\nelse\n    echo &quot;Download failed with HTTP code: $response&quot;\n    exit 1\nfi\n\n</code></pre>\n<p>In this example, the parent pipeline uploads a file to the GitLab Package Registry by calling a script named <code>upload-to-registry.sh</code>. The script gives the artifact a name and version and constructs the API call to upload the file to the package registry. The parent pipeline is able to authenticate using a <code>$CI_JOB_TOKEN</code> to push the artifact.txt file to the registry.</p>\n<p>The child pipeline operates the same as the parent pipeline by using a script to construct the API call to download the artifact.txt file from the package registry. It also is able to authenticate to the registry using the <code>$CI_JOB_TOKEN</code>.</p>\n<p>Since the GitLab Package Registry is available to all GitLab users, it helps to serve as a central location for storing and versioning artifacts. It is a great option for users working with many kinds of artifacts and needing to version artifacts for workflows even beyond CI/CD.</p>\n<h3>Using inputs to pass variables to a child pipeline</h3>\n<p>If you made it this far in this tutorial, and you have plans to start creating new pipeline configurations, you might want to start by evaluating if your use case can benefit from using <strong>inputs</strong> to pass variables to other pipelines.</p>\n<p>Using inputs is a recommended way to pass variables when you need to define specific values in a CI/CD job and have those values remain fixed during the pipeline run. Inputs might offer certain advantages over the method we implemented before. For example, with inputs, you can include data validation through options (i.e., values must be one of these: [‘staging', ‘prod’]), variable descriptions, type checking, and assign default values before the pipeline run.</p>\n<h4>Configuring CI/CD inputs</h4>\n<p>Consider the following parent pipeline configuration:</p>\n<pre><code>\n# .gitlab-ci.yml (main file)\n\nstages:\n  - trigger\n\ntrigger-staging:\n  stage: trigger\n  trigger:\n    include:\n      - local: service_a/.gitlab-ci.yml\n        inputs:\n          environment: staging\n          version: &quot;1.0.0&quot;\n</code></pre>\n<p>Let's zoom in at the main difference between the code snippet above and the previous parent pipeline examples in this tutorial:</p>\n<pre><code>\ntrigger:\n    include:\n      - local: service_a/.gitlab-ci.yml\n        inputs:\n          environment: staging\n          version: &quot;1.0.0&quot;\n</code></pre>\n<p>The main difference is using the reserved word &quot;inputs&quot;. This part of the YAML configuration can be read in natural language as: “trigger the child pipeline defined in <code>service_a.gitlab-ci.yml</code> and make sure to pass ‘environment: staging’ and ‘version:1.0.0’ as input variables that the child pipeline will know how to use.</p>\n<h4>Reading CI/CD inputs in child pipelines</h4>\n<p>Moving to the child pipeline, it must contain in its declaration a spec that defines the inputs it can take. For each input, it is possible to add a little description, a set of predefined options the input value can take, and the type of value it will take. This is illustrated as follows:</p>\n<pre><code>\n# target pipeline or child-pipeline in this case\n\n\nspec:\n  inputs:\n    environment:\n      description: &quot;Deployment environment&quot;\n      options: [staging, production]\n    version:\n      type: string\n      description: &quot;Application version&quot;\n\n\n---\n\n\nstages:\n  - deploy\n# Jobs that will use the inputs\n\ndeploy:\n  stage: deploy\n  script:\n      -  echo &quot;Deploying version $[[ inputs.version ]] to $[[ inputs.environment ]]&quot;\n\n</code></pre>\n<p>Notice from the code snippet that after defining the spec, there is a YAML document separator &quot;---&quot;  followed by the actual child pipeline definition where we access the variables <code>$[[ inputs.version ]]</code> and <code>$[[ inputs.environment ]]&quot;</code> from the defined inputs using input interpolation.</p>\n<h2>Get hands-on with parent-child pipelines, artifacts, and more</h2>\n<p>We hope this article has helped with navigating the challenge of sharing variables and artifacts in parent-child pipeline setups.</p>\n<p>To try these examples for yourself, feel free to view or fork the <a href=\"https://gitlab.com/gitlab-da/use-cases/devsecops-platform/devops-platform-wave/scenarios/scenario7-deep-dive-into-build-automation-and-ci/7-4-parent-child-pipeline/-/tree/main\">Premium/Ultimate</a> and the <a href=\"https://gitlab.com/gitlab-da/playground/dhelfand/parent-child-pipeline-with-package-registry-artifacts\">GitLab Package Registry</a> examples of sharing artifacts.</p>\n<p>You can also sign up for a <a href=\"https://about.gitlab.com/free-trial/\">30-day free trial of GitLab Ultimate</a> to experience all the features GitLab has to offer. Thanks for reading!</p>\n",
      "summary": "Software projects have different evolving needs and requirements. Some have\nsaid that software is never finished, merely abandoned. Some software\nprojects are small and others are large with complex integrations. Some have\ndependencies on external projects, while others are self-contained.\nRegardless of the size and complexity, the need to validate and ensure\nfunctionality remains paramount.\nCI/CD pipelines can help with the challenge of building and validating software projects consistently, but, much like the software itself, these pipelines can become complex with many dependencies. This is where ideas like parent-child pipelines and data exchange in CI/CD setups become incredibly important.\nIn this article, we will cover common CI/CD data exchange challenges users may encounter with parent-child pipelines in GitLab — and how to solve them. You'll learn how to turn complex CI/CD processes into more manageable setups.\nUsing parent-child pipelines\nThe pipeline setup in the image below illustrates a scenario where a project could require a large, complex pipeline. The whole project resides in one repository and contains different modules. Each module requires its own set of build and test automation steps.\nOne approach to address the CI/CD configuration in a scenario like this is to break down the larger pipeline into smaller ones (i.e., child pipelines) and keep a common CI/CD process that is shared across all modules in charge of the whole orchestration (i.e., parent pipeline).\n\nThe parent-child pipeline pattern allows a single pipeline to orchestrate one or many downstream pipelines. Similar to how a single pipeline coordinates the execution of multiple jobs, the parent pipeline coordinates the running of full pipelines with one or more jobs.\nThis pattern has been shown to be helpful in a variety of use cases:\nBreaking down large, complex pipelines into smaller, manageable pieces\nConditionally executing certain pipelines as part of a larger CI/CD process\nExecuting pipelines in parallel\nHelping manage user permissions to access and run certain pipelines\nGitLab’s current CI/CD structure supports this pattern and makes it simple to implement parent-child pipelines. While there are many benefits when using the parent-child pipeline pattern with GitLab, one question we often get is how to share data between the parent and child pipelines. In the next sections, we’ll go over how to make use of GitLab variables and artifacts to address this concern.\nSharing variables\nThere are cases where it is necessary to pass the output from a parent pipeline job to a child pipeline. These outputs can be shared as variables, artifacts, and inputs.\nConsider a case where we create a custom variable var_1 during the runtime of a job:\n\nstages:\n  - build\n  - triggers\n\n# This job only creates a variable \n\ncreate_var_job:\n  stage: build\n  script:\n    - var_1=\"Hi, I'm a Parent pipeline variable\"\n    - echo \"var_1=$var_1\" >> var.env\n  artifacts:\n    reports:\n      dotenv: var.env\n\nNotice that the variable is created as part of the script steps in the job (during runtime). In this example, we are using a simple string \"Hi, I'm a Parent pipeline variable\" to illustrate the main syntax required to later share this variable with a child pipeline. Let's break down the create_var_job  and analyze the main steps from this GitLab job\nFirst, we need to save var_1 as dotenv:\n  script:\n    - var_1=\"Hi, I'm a pipeline variable\"\n    - echo \"var_1=$var_1\" >> var.env\n\nAfter saving var_1 as var.env, the next important step is to make this variable available as an artifact produced by the create_var_job. To do that, we use the following syntax:\n\nartifacts:\n    reports:\n      dotenv: var.env\n\nUp to this point, we have created a variable during runtime and saved it as a dotenv report. Now let's add the job that should trigger the child pipeline:\n\ntelco_service_a:\n  stage: triggers\n  trigger:\n    include: service_a/.gitlab-ci.yml\n  rules:\n    - changes:\n        - service_a/*\n\nThe goal of telco_service_a  job is to find the .gitlab-ci.yml configuration of the child pipeline,  which is defined in this case as service_a, and trigger its execution. Let's examine this job:\n\ntelco_service_a:\n  stage: triggers\n  trigger:\n    include: service_a/.gitlab-ci.yml\n\nWe see it belongs to another stage of the pipeline named triggers.This job will run only after create_var_job from the first stage successfully finishes and where the variable  var_1 we want to pass is created.\nAfter defining the stage, we use the reserved words trigger and include to tell GitLab where to search for the child pipeline configuration, as illustrated in the YAML below:\n  trigger:\n    include: service_a/.gitlab-ci.yml\n\nOur child-pipeline YAML configuration is under service_a/.gitlab-ci.yml folder in the GitLab repository, for this example.\n\n<p></p>\n<center><i>Child pipelines folders with configurations</i></center>\n<p></p>\nTake into consideration that the repository structure depicted above can vary. What matters is properly pointing the  triggers: include properties at the location of your child-pipeline configuration in your repository.\nFinally, we use rules: changes to indicate to GitLab that this child pipeline should be triggered only if there is any change in any file in the service_a/.gitlab-ci.yml directory, as illustrated in the following code snippet:\n\nrules:\n    - changes:\n        - service_a/*\n\nUsing this rule helps to optimize cost by triggering the child pipeline job only when necessary. This approach is particularly valuable in a monorepo architecture where specific modules contain numerous components, allowing us to avoid running their dedicated pipelines when no changes have been made to their respective codebases.\nConfiguring the parent pipeline\nUp to this point, we have put together our parent pipeline. Here's the full code snippet for this segment:\n\n# Parent Pipeline Configuration\n\n# This pipeline creates a custom variable and triggers a child pipeline\n\n\nstages:\n  - build\n  - trigger\n\ncreate_var_job:\n  stage: build\n  script:\n    - var_1=\"Hi, I'm a Parent pipeline variable\"\n    - echo \"var_1=$var_1\" >> var.env\n  artifacts:\n    reports:\n      dotenv: var.env\n\ntelco_service_a:\n  stage: triggers\n  trigger:\n    include: service_a/.gitlab-ci.yml\n  rules:\n    - changes:\n        - service_a/*\n\nWhen GitLab executes the YAML configuration in the GitLab UI, the parent pipeline gets rendered as follows:\n\nNotice the label \"trigger job,\" which indicates this job will start the execution of another pipeline configuration.\nConfiguring the child pipeline\nMoving forward, let's now focus on the child pipeline configuration, where we expect to inherit and print the value of the var_1 created in the parent pipeline.\nThe pipeline configuration in service_a/.gitlab_ci.yml has the following definition:\n\nstages:\n  - build\n\nbuild_a:\n  stage: build\n  script:\n    - echo \"this job inherits the variable from the Parent pipeline:\"\n    - echo $var_1\n  needs:\n    - project: gitlab-da/use-cases/7-4-parent-child-pipeline\n      job: create_var_job\n      ref: main\n      artifacts: true\n\nLike before, let's break down this pipeline and highlight the main parts to achieve our goal. This pipeline only contains one stage (i.e., build) and one job (i.e., build_a). The script in the job contains two steps:\n\nbuild_a:\n  stage: build\n  script:\n    - echo \"this job inherits the variable from the Parent pipeline:\"\n    - echo $var_1\n\nThese two steps print output during the execution. The most interesting one is the second step, echo $var_1, where we expect to print the variable value inherited from the parent pipeline. Remember, this was a simple string with value: \"Hi, I'm a Parent pipeline variable.\"\nInheriting variables using needs\nTo set and link this job to inherit variables from the parent pipeline, we use the reserved GitLab CI properties needs as depicted in the following snippet:\n\nneeds:\n    - project: gitlab-da/use-cases/7-4-parent-child-pipeline\n      job: create_var_job\n      ref: main\n      artifacts: true\n\nUsing the \"needs\" keyword, we define dependencies that must be completed before running this job. In this case, we pass four different values. Let's walk through each one  of them:\nProject: The complete namespace of the project where the main gitlab-ci.yml containing the parent pipeline YAML is located. Make sure to include the absolute path.\nJob: The specific job name in the parent pipeline from where we want to inherit the variable.\nRef: The name of the branch where the main gitlab-ci.yml containing the parent pipeline YAML is located.\nArtifacts: Where we set a boolean value, indicating that artifacts from the parent pipeline job should be downloaded and made available to this child pipeline job.\nNote: This specific approach using the needs property is only available to GitLab Premium and Ultimate users. We will cover another example for GitLab community users later on.\nPutting it all together\nNow let's assume we make a change to any of the files under service_a folder and commit the changes to the repository. When GitLab detects the change, the rule we set up will trigger the child job pipeline execution. This gets displayed in the GitLab UI as follows:\n\nClicking on the telco_service_a  will take us to the jobs in the child pipeline:\n\nWe can see the parent-child relationship, and finally, by clicking on the build_a job, we can visually verify the variable inheritance in the job execution log:\n\nThis output confirms the behavior we expected. The custom runtime variable var_1 created in the parent job is inherited in the child job, unpacked from the dotenv report, and its value accessible as can be confirmed in Line 26 above.\nThis use case illustrates how to share custom variables that can contain any value between pipelines. This example is intentionally simple and can be extrapolated to more realistic scenarios. Take, for instance, the following CI/CD configuration, where the custom variable we need to share is the tag of a Docker image:\n\n# Pipeline \n\n\nbuild-prod-image:\n  tags: [ saas-linux-large-amd64 ]\n  image: docker:20.10.16\n  stage: build\n  services:\n    - docker:20.10.16-dind\n  \n  script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n    - docker build -t $PRODUCTION_IMAGE .\n    - docker push $PRODUCTION_IMAGE\n    - echo \"UPSTREAM_CONTAINER_IMAGE=$PRODUCTION_IMAGE\" >> prodimage.env\n\n  artifacts:\n    reports:\n      dotenv: prodimage.env\n\n  rules:\n      - if: '$CI_COMMIT_BRANCH == \"main\"'\n        when: always\n      - when: never\n\nAnd use the variable with the Docker image tag, in another job that updates a Helm manifest file:\n\nupdate-helm-values:\n    stage: update-manifests\n    image:\n        name: alpine:3.16\n        entrypoint: [\"\"]\n  \n    before_script:\n          - apk add --no-cache git curl bash yq\n          - git remote set-url origin https://${CI_USERNAME}:${GITOPS_USER}@${SERVER_PATH}/${PROJECT_PATH}\n          - git config --global user.email \"gitlab@gitlab.com\"\n          - git config --global user.name \"GitLab GitOps\"\n          - git pull origin main\n    script:\n          - cd src\n          - echo $UPSTREAM_CONTAINER_IMAGE\n          - yq eval -i \".spec.template.spec.containers[0].image |= \\\"$UPSTREAM_CONTAINER_IMAGE\\\"\" store-deployment.yaml\n          - cat store-deployment.yaml\n          - git pull origin main\n          - git checkout -B main\n          - git commit -am '[skip ci] prod image update'\n          - git push origin main\n    needs:\n      - project: gitlab-da/use-cases/devsecops-platform/simply-find/simply-find-front-end\n        job: build-prod-image\n        ref: main\n        artifacts: true\n\nMastering how to share variables between pipelines while maintaining the relationship between them enables us to create more sophisticated workflow orchestration that can meet our software building needs.\nUsing GitLab Package Registry to share artifacts\nWhile the needs feature mentioned above works great for Premium and Ultimate users, GitLab also has features to help achieve similar results for Community Edition users. One suggested approach is to store artifacts in the GitLab Package Registry.\nUsing a combination of the variables provided in GitLab CI/CD jobs and the GitLab API, you can upload artifacts to the GitLab Package Registry from a parent pipeline. In the child pipeline, you can then access the uploaded artifact from the package registry using the same variables and API to access the artifact. Let’s take a look at the example pipeline and some supplementary scripts that illustrate this:\ngitlab-ci.yml (parent pipeline)\n\n# Parent Pipeline Configuration\n\n# This pipeline creates an artifact, uploads it to Package Registry, and triggers a child pipeline\n\n\nstages:\n  - create-upload\n  - trigger\n\nvariables:\n  PACKAGE_NAME: \"pipeline-artifacts\"\n  PACKAGE_VERSION: \"$CI_PIPELINE_ID\"\n  ARTIFACT_FILE: \"artifact.txt\"\n\n# Job 1: Create and upload artifact to Package Registry\n\ncreate-and-upload-artifact:\n  stage: create-upload\n  image: alpine:latest\n  before_script:\n    - apk add --no-cache curl bash\n  script:\n    - bash scripts/create-artifact.sh\n    - bash scripts/upload-to-registry.sh\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"push\"\n\n# Job 2: Trigger child pipeline\n\ntrigger-child:\n  stage: trigger\n  trigger:\n    include: child-pipeline.yml\n    strategy: depend\n  variables:\n    PARENT_PIPELINE_ID: $CI_PIPELINE_ID\n    PACKAGE_NAME: $PACKAGE_NAME\n    PACKAGE_VERSION: $PACKAGE_VERSION\n    ARTIFACT_FILE: $ARTIFACT_FILE\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"push\"\n\nchild-pipeline.yml\n\n# Child Pipeline Configuration\n\n# This pipeline downloads the artifact from Package Registry and processes it\n\n\nstages:\n  - download-process\n\nvariables:\n  # These variables are passed from the parent pipeline\n  PACKAGE_NAME: \"pipeline-artifacts\"\n  PACKAGE_VERSION: \"$PARENT_PIPELINE_ID\"\n  ARTIFACT_FILE: \"artifact.txt\"\n\n# Job 1: Download and process artifact from Package Registry\n\ndownload-and-process-artifact:\n  stage: download-process\n  image: alpine:latest\n  before_script:\n    - apk add --no-cache curl bash\n  script:\n    - bash scripts/download-from-registry.sh\n    - echo \"Processing downloaded artifact...\"\n    - cat $ARTIFACT_FILE\n    - echo \"Artifact processed successfully!\"\n\nupload-to-registry.sh\n\n#!/bin/bash\n\n\nset -e\n\n\n# Configuration\n\nPACKAGE_NAME=\"${PACKAGE_NAME:-pipeline-artifacts}\"\n\nPACKAGE_VERSION=\"${PACKAGE_VERSION:-$CI_PIPELINE_ID}\"\n\nARTIFACT_FILE=\"${ARTIFACT_FILE:-artifact.txt}\"\n\n\n# Validate required variables\n\nif [ -z \"$CI_PROJECT_ID\" ]; then\n    echo \"Error: CI_PROJECT_ID is not set\"\n    exit 1\nfi\n\n\nif [ -z \"$CI_JOB_TOKEN\" ]; then\n    echo \"Error: CI_JOB_TOKEN is not set\"\n    exit 1\nfi\n\n\nif [ -z \"$CI_API_V4_URL\" ]; then\n    echo \"Error: CI_API_V4_URL is not set\"\n    exit 1\nfi\n\n\nif [ ! -f \"$ARTIFACT_FILE\" ]; then\n    echo \"Error: Artifact file '$ARTIFACT_FILE' not found\"\n    exit 1\nfi\n\n\n# Construct the upload URL\n\nUPLOAD_URL=\"${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/${PACKAGE_NAME}/${PACKAGE_VERSION}/${ARTIFACT_FILE}\"\n\n\n# Upload the file using curl\n\nresponse=$(curl -w \"%{http_code}\" -o /tmp/upload_response.json \\\n    --header \"JOB-TOKEN: $CI_JOB_TOKEN\" \\\n    --upload-file \"$ARTIFACT_FILE\" \\\n    \"$UPLOAD_URL\")\n\nif [ \"$response\" -eq 201 ]; then\n    echo \"Upload successful!\"\nelse\n    echo \"Upload failed with HTTP code: $response\"\n    exit 1\nfi\n\n\ndownload-from-regsitry.sh\n\n#!/bin/bash\n\n\nset -e\n\n\n# Configuration\n\nPACKAGE_NAME=\"${PACKAGE_NAME:-pipeline-artifacts}\"\n\nPACKAGE_VERSION=\"${PACKAGE_VERSION:-$PARENT_PIPELINE_ID}\"\n\nARTIFACT_FILE=\"${ARTIFACT_FILE:-artifact.txt}\"\n\n\n# Validate required variables\n\nif [ -z \"$CI_PROJECT_ID\" ]; then\n    echo \"Error: CI_PROJECT_ID is not set\"\n    exit 1\nfi\n\n\nif [ -z \"$CI_JOB_TOKEN\" ]; then\n    echo \"Error: CI_JOB_TOKEN is not set\"\n    exit 1\nfi\n\n\nif [ -z \"$CI_API_V4_URL\" ]; then\n    echo \"Error: CI_API_V4_URL is not set\"\n    exit 1\nfi\n\n\nif [ -z \"$PACKAGE_VERSION\" ]; then\n    echo \"Error: PACKAGE_VERSION is not set\"\n    exit 1\nfi\n\n\n# Construct the download URL\n\nDOWNLOAD_URL=\"${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/${PACKAGE_NAME}/${PACKAGE_VERSION}/${ARTIFACT_FILE}\"\n\n\n# Download the file using curl\n\nresponse=$(curl -w \"%{http_code}\" -o \"$ARTIFACT_FILE\" \\\n    --header \"JOB-TOKEN: $CI_JOB_TOKEN\" \\\n    --fail-with-body \\\n    \"$DOWNLOAD_URL\")\n\nif [ \"$response\" -eq 200 ]; then\n    echo \"Download successful!\"\nelse\n    echo \"Download failed with HTTP code: $response\"\n    exit 1\nfi\n\n\nIn this example, the parent pipeline uploads a file to the GitLab Package Registry by calling a script named upload-to-registry.sh. The script gives the artifact a name and version and constructs the API call to upload the file to the package registry. The parent pipeline is able to authenticate using a $CI_JOB_TOKEN to push the artifact.txt file to the registry.\nThe child pipeline operates the same as the parent pipeline by using a script to construct the API call to download the artifact.txt file from the package registry. It also is able to authenticate to the registry using the $CI_JOB_TOKEN.\nSince the GitLab Package Registry is available to all GitLab users, it helps to serve as a central location for storing and versioning artifacts. It is a great option for users working with many kinds of artifacts and needing to version artifacts for workflows even beyond CI/CD.\nUsing inputs to pass variables to a child pipeline\nIf you made it this far in this tutorial, and you have plans to start creating new pipeline configurations, you might want to start by evaluating if your use case can benefit from using inputs to pass variables to other pipelines.\nUsing inputs is a recommended way to pass variables when you need to define specific values in a CI/CD job and have those values remain fixed during the pipeline run. Inputs might offer certain advantages over the method we implemented before. For example, with inputs, you can include data validation through options (i.e., values must be one of these: [‘staging', ‘prod’]), variable descriptions, type checking, and assign default values before the pipeline run.\nConfiguring CI/CD inputs\nConsider the following parent pipeline configuration:\n\n# .gitlab-ci.yml (main file)\n\nstages:\n  - trigger\n\ntrigger-staging:\n  stage: trigger\n  trigger:\n    include:\n      - local: service_a/.gitlab-ci.yml\n        inputs:\n          environment: staging\n          version: \"1.0.0\"\n\nLet's zoom in at the main difference between the code snippet above and the previous parent pipeline examples in this tutorial:\n\ntrigger:\n    include:\n      - local: service_a/.gitlab-ci.yml\n        inputs:\n          environment: staging\n          version: \"1.0.0\"\n\nThe main difference is using the reserved word \"inputs\". This part of the YAML configuration can be read in natural language as: “trigger the child pipeline defined in service_a.gitlab-ci.yml and make sure to pass ‘environment: staging’ and ‘version:1.0.0’ as input variables that the child pipeline will know how to use.\nReading CI/CD inputs in child pipelines\nMoving to the child pipeline, it must contain in its declaration a spec that defines the inputs it can take. For each input, it is possible to add a little description, a set of predefined options the input value can take, and the type of value it will take. This is illustrated as follows:\n\n# target pipeline or child-pipeline in this case\n\n\nspec:\n  inputs:\n    environment:\n      description: \"Deployment environment\"\n      options: [staging, production]\n    version:\n      type: string\n      description: \"Application version\"\n\n\n---\n\n\nstages:\n  - deploy\n# Jobs that will use the inputs\n\ndeploy:\n  stage: deploy\n  script:\n      -  echo \"Deploying version $[[ inputs.version ]] to $[[ inputs.environment ]]\"\n\n\nNotice from the code snippet that after defining the spec, there is a YAML document separator \"---\"  followed by the actual child pipeline definition where we access the variables $[[ inputs.version ]] and $[[ inputs.environment ]]\" from the defined inputs using input interpolation.\nGet hands-on with parent-child pipelines, artifacts, and more\nWe hope this article has helped with navigating the challenge of sharing variables and artifacts in parent-child pipeline setups.\nTo try these examples for yourself, feel free to view or fork the Premium/Ultimate and the GitLab Package Registry examples of sharing artifacts.\nYou can also sign up for a 30-day free trial of GitLab Ultimate to experience all the features GitLab has to offer. Thanks for reading!",
      "publishedAt": "2025-10-16T00:00:00.000Z",
      "author": "William Arias",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.767Z",
      "score": 0.754392494852882
    },
    {
      "id": "506819a1a28da5bc03ecb73181471bb6",
      "title": "How we built a structured Streamlit Application Framework in Snowflake",
      "url": "https://about.gitlab.com/blog/how-we-built-a-structured-streamlit-application-framework-in-snowflake/",
      "content": "<p>Recently, the GitLab Data team transformed scattered <a href=\"https://streamlit.io/\">Streamlit</a> applications into a unified, secure, and scalable solution for our Snowflake environment. To accomplish this, we packed Python, Snowflake, and Streamlit together with GitLab. Follow along on this journey and discover the results we achieved, and learn how you can, too.</p>\n<h2>The challenge</h2>\n<p>Imagine this scenario: Your organization has dozens of Streamlit applications across different environments, running various Python versions, connecting to sensitive data with inconsistent security practices. Some apps work, others break mysteriously, and nobody knows who built what or how to maintain them.</p>\n<p>This was exactly the challenge our data team faced. Applications were being created in isolation, with no standardization, no security oversight, and no clear deployment process. The result? A compliance nightmare and a maintenance burden that was growing exponentially.</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1760035999/i50lpkrwy9bok056rdak.png\" alt=\"Functional architectural design (high level)\">\n&lt;p&gt;&lt;/p&gt;\n&lt;center&gt;&lt;i&gt;Functional architectural design (high level)&lt;/i&gt;&lt;/center&gt;</p>\n<h2>How we started</h2>\n<p>We leveraged our unique position as customer zero by building this entire framework on GitLab's own CI/CD infrastructure and project management tools. Here are the ingredients we started with:</p>\n<ol>\n<li><a href=\"https://about.gitlab.com/platform/\">GitLab</a> (product)</li>\n<li><a href=\"https://about.gitlab.com/platform/\">Snowflake</a> - our single source of truth (SSOT) for the data warehouse activities (and more than that)</li>\n<li><a href=\"https://streamlit.io/\">Streamlit</a> - an open-source tool for visual applications that has pure Python code under the hood\nThis provided us with immediate access to enterprise-grade DevSecOps capabilities, enabling us to implement automated testing, code review processes, and deployment pipelines from the outset. By utilizing GitLab's built-in features for issue tracking, merge requests, and automated deployments (CI/CD pipelines), we can iterate rapidly and validate the framework against real-world enterprise requirements. This internal-first approach ensured our solution was battle-tested on GitLab's own infrastructure before any external implementation.</li>\n</ol>\n<h3>The lessons we learned</h3>\n<p>The most critical lesson we learned from building the Streamlit Application Framework in Snowflake is that <strong>structure beats chaos every time</strong> — implement governance early rather than retrofitting it later when maintenance becomes exponential.\nYou also need to clearly define roles and responsibilities, separating infrastructure concerns from application development, so that each team can focus on its strengths.\nSecurity and compliance cannot be afterthoughts; they must be built into templates and automated processes from day one, as it's far easier to enforce consistent standards upfront than to force them after the fact. Invest heavily in automation and CI/CD pipelines, as manual processes don't scale and introduce human error.\n<img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1760035998/qt9gfemxjnj8kjumkuh7.png\" alt=\"Architecture of the framework (general overview)\">\n&lt;p&gt;&lt;/p&gt;\n&lt;center&gt;&lt;i&gt;Architecture of the framework (general overview)&lt;/i&gt;&lt;/center&gt;</p>\n<h2>How the Streamlit Application Framework changes everything</h2>\n<p>The Streamlit Application Framework turns a scattered approach into a structure. It gives developers freedom within secure guardrails, while automating deployment and eliminating maintenance complexity.</p>\n<h3>Three clear roles, one unified process</h3>\n<p>The framework introduces a structured approach with three distinct roles:</p>\n<ol>\n<li><strong>Maintainers</strong> (Data team members and contributors) handle the infrastructure, including CI/CD pipelines, security templates, and compliance rules. They ensure the framework runs smoothly and stays secure.</li>\n<li><strong>Creators</strong> (those who need to build applications) can focus on what they do best: creating visualizations, connecting to Snowflake data, and building user experiences. They have full flexibility to create new applications from scratch, add new pages to existing apps, integrate additional Python libraries, and build complex data visualisations — all without worrying about deployment pipelines or security configurations.</li>\n<li><strong>Viewers</strong> (end users) access polished, secure applications without any technical overhead. All they need is Snowflake access.\n<img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1760035999/oatqyx3ug7vsgzishpma.png\" alt=\"Roles overview and their functionality\">\n&lt;p&gt;&lt;/p&gt;\n&lt;center&gt;&lt;i&gt;Overview of roles and their functions&lt;/i&gt;&lt;/center&gt;</li>\n</ol>\n<h2>Automate everything</h2>\n<p>By implementing CI/CD, days of manual deployments and configuration headaches are gone. The framework provides:</p>\n<ul>\n<li><strong>One-click environment preparation:</strong> With a set of <code>make</code> commands, the environment is installed and ready in a few seconds.</li>\n</ul>\n<pre><code class=\"language-yaml\">================================================================================\n✅ Snowflake CLI successfully installed and configured!\nConnection: gitlab_streamlit\nUser: YOU@GITLAB.COM\nAccount: gitlab\n================================================================================\nUsing virtualenv: /Users/YOU/repos/streamlit/.venv\n📚 Installing project dependencies...\nInstalling dependencies from lock file\nNo dependencies to install or update\n✅ Streamlit environment prepared!\n</code></pre>\n<ul>\n<li><strong>Automated CI/CD pipelines:</strong> Handle testing, code review, and deployment from development to production.</li>\n<li><strong>Secure sandbox environments:</strong> Provide for safe development and testing before production deployment.</li>\n</ul>\n<pre><code class=\"language-yaml\">╰─$ make streamlit-rules\n🔍 Running Streamlit compliance check...\n================================================================================\nCODE COMPLIANCE REPORT\n================================================================================\nGenerated: 2025-07-09 14:01:16\nFiles checked: 1\n\nSUMMARY:\n✅ Passed: 1\n❌ Failed: 0\nSuccess Rate: 100.0%\n\nAPPLICATION COMPLIANCE SUMMARY:\n📱 Total Applications Checked: 1\n⚠️ Applications with Issues: 0\n📊 File Compliance Rate: 100.0%\n\nDETAILED RESULTS BY APPLICATION:\n...\n</code></pre>\n<ul>\n<li><strong>Template-based application creation:</strong> Ensures consistency across all applications and pages.</li>\n</ul>\n<pre><code class=\"language-yaml\">╰─$ make streamlit-new-page STREAMLIT_APP=sales_dashboard STREAMLIT_PAGE_NAME=analytics\n📝 Generating new Streamlit page: analytics for app: sales_dashboard\n📃 Create new page from template:\nPage name: analytics\nApp directory: sales_dashboard\nTemplate path: page_template.py\n✅ Successfully created 'analytics.py' in 'sales_dashboard' directory from template\n</code></pre>\n<ul>\n<li><strong>Poetry-based dependency management:</strong> Prevents version conflicts and maintains clean environments.</li>\n<li><strong>Organized project structure:</strong> Has dedicated folders for applications, templates, compliance rules, and configuration management.</li>\n</ul>\n<pre><code class=\"language-yaml\">├── src/\n│   ├── applications/     # Folder for Streamlit applications\n│   │   ├── main_app/     # Main dashboard application\n│   │   ├── components/   # Shared components\n│   │   └── &lt;your_apps&gt;/  # Your custom application\n│   │   └── &lt;your_apps2&gt;/ # Your 2nd custom application\n│   ├── templates/        # Application and page templates\n│   ├── compliance/       # Compliance rules and checks\n│   └── setup/            # Setup and configuration utilities\n├── tests/                # Test files\n├── config.yml            # Environment configuration\n├── Makefile              # Build and deployment automation\n└── README.md             # Main README.md file\n</code></pre>\n<ul>\n<li><strong>Streamlined workflow:</strong> Takes local development through testing schema to production, all automated through GitLab CI/CD pipelines.</li>\n</ul>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1760035998/usyma2jkgiazu9iay1au.png\" alt=\"GitLab CI/CD pipelines for full automation of the process\">\n&lt;p&gt;&lt;/p&gt; &lt;center&gt;&lt;i&gt;GitLab CI/CD pipelines for full automation of the process&lt;/i&gt;&lt;/center&gt;</p>\n<h2>Security and compliance by design</h2>\n<p>Instead of bolting on security as an afterthought, the structured Streamlit Application Framework builds it in from the ground up. Every application adheres to the same security standards, and compliance requirements are automatically enforced. Audit trails are maintained throughout the development lifecycle.\nWe introduce our compliance rules and verify them with a single command. For instance, we can list which classes and methods are mandatory to use, which files you should have, and which roles are allowed and which are forbidden to share the application with. The rules are flexible and descriptive; all you need to do is define them in a YAML file:</p>\n<pre><code class=\"language-yaml\">class_rules:\n  - name: &quot;Inherit code for the page from GitLabDataStreamlitInit&quot;\n    description: &quot;All Streamlit apps must inherit from GitLabDataStreamlitInit&quot;\n    severity: &quot;error&quot;\n    required: true\n    class_name: &quot;*&quot;\n    required_base_classes:\n      - &quot;GitLabDataStreamlitInit&quot;\n    required_methods:\n      - &quot;__init__&quot;\n      - &quot;set_page_layout&quot;\n      - &quot;setup_ui&quot;\n      - &quot;run&quot;\n\nfunction_rules:\n  - name: &quot;Main function required&quot;\n    description: &quot;Must have a main() function&quot;\n    severity: &quot;error&quot;\n    required: true\n    function_name: &quot;main&quot;\n\nimport_rules:\n  - name: &quot;Import GitLabDataStreamlitInit&quot;\n    description: &quot;Must import the mandatory base class&quot;\n    severity: &quot;error&quot;\n    required: true\n    module_name: &quot;gitlab_data_streamlit_init&quot;\n    required_items:\n      - &quot;GitLabDataStreamlitInit&quot;\n  - name: &quot;Import streamlit&quot;\n    description: &quot;Must import streamlit library&quot;\n    severity: &quot;error&quot;\n    required: true\n    module_name: &quot;streamlit&quot;\n\nfile_rules:\n  - name: &quot;Snowflake configuration required (snowflake.yml)&quot;\n    description: &quot;Each application must have a snowflake.yml configuration file&quot;\n    severity: &quot;error&quot;\n    required: true\n    file_pattern: &quot;**/applications/**/snowflake.yml&quot;\n    base_path: &quot;&quot;\n  - name: &quot;Snowflake environment required (environment.yml)&quot;\n    description: &quot;Each application must have a environment.yml configuration file&quot;\n    severity: &quot;error&quot;\n    required: true\n    file_pattern: &quot;**/applications/**/environment.yml&quot;\n    base_path: &quot;&quot;\n  - name: &quot;Share specification required (share.yml)&quot;\n    description: &quot;Each application must have a share.yml file&quot;\n    severity: &quot;warning&quot;\n    required: true\n    file_pattern: &quot;**/applications/**/share.yml&quot;\n    base_path: &quot;&quot;\n  - name: &quot;README.md required (README.md)&quot;\n    description: &quot;Each application should have a README.md file with a proper documentation&quot;\n    severity: &quot;error&quot;\n    required: true\n    file_pattern: &quot;**/applications/**/README.md&quot;\n    base_path: &quot;&quot;\n  - name: &quot;Starting point recommended (dashboard.py)&quot;\n    description: &quot;Each application must have a dashboard.py as a starting point&quot;\n    severity: &quot;warning&quot;\n    required: true\n    file_pattern: &quot;**/applications/**/dashboard.py&quot;\n    base_path: &quot;&quot;\n\nsql_rules:\n  - name: &quot;SQL files must contain only SELECT statements&quot;\n    description: &quot;SQL files and SQL code in other files should only contain SELECT statements for data safety&quot;\n    severity: &quot;error&quot;\n    required: true\n    file_extensions: [&quot;.sql&quot;, &quot;.py&quot;]\n    select_only: true\n    forbidden_statements:\n      - ....\n    case_sensitive: false\n  - name: &quot;SQL queries should include proper SELECT statements&quot;\n    description: &quot;When SQL is present, it should contain proper SELECT statements&quot;\n    severity: &quot;warning&quot;\n    required: false\n    file_extensions: [&quot;.sql&quot;, &quot;.py&quot;]\n    required_statements:\n      - &quot;SELECT&quot;\n    case_sensitive: false\n\nshare_rules:\n  - name: &quot;Valid functional roles in share.yml&quot;\n    description: &quot;Share.yml files must contain only valid functional roles from the approved list&quot;\n    severity: &quot;error&quot;\n    required: true\n    file_pattern: &quot;**/applications/**/share.yml&quot;\n    valid_roles:\n      - ...\n    safe_data_roles:\n      - ...\n  - name: &quot;Share.yml file format validation&quot;\n    description: &quot;Share.yml files must follow the correct YAML format structure&quot;\n    severity: &quot;error&quot;\n    required: true\n    file_pattern: &quot;**/applications/**/share.yml&quot;\n    required_keys:\n      - &quot;share&quot;\n    min_roles: 1\n    max_roles: 10\n\n</code></pre>\n<p>With one command running:</p>\n<pre><code class=\"language-bash\">╰─$ make streamlit-rules\n</code></pre>\n<p>We can verify all the rules we have created and validate that the developers (who are building a Streamlit application) are following the policy specified by the creators (who determine the policies and building blocks of the framework), and that all the building blocks are in the right place. This ensures consistent behavior across all Streamlit applications.</p>\n<pre><code class=\"language-yaml\">🔍 Running Streamlit compliance check...\n================================================================================\nCODE COMPLIANCE REPORT\n================================================================================\nGenerated: 2025-08-18 17:05:12\nFiles checked: 4\n\nSUMMARY:\n✅ Passed: 4\n❌ Failed: 0\nSuccess Rate: 100.0%\n\nAPPLICATION COMPLIANCE SUMMARY:\n📱 Total Applications Checked: 1\n⚠️ Applications with Issues: 0\n📊 File Compliance Rate: 100.0%\n\nDETAILED RESULTS BY APPLICATION:\n================================================================================\n✅ PASS APPLICATION: main_app\n------------------------------------------------------------\n📁 FILES ANALYZED (4):\n✅ dashboard.py\n📦 Classes: SnowflakeConnectionTester\n🔧 Functions: main\n📥 Imports: os, pwd, gitlab_data_streamlit_init, snowflake.snowpark.exceptions, streamlit\n\n✅ show_streamlit_apps.py\n📦 Classes: ShowStreamlitApps\n🔧 Functions: main\n📥 Imports: pandas, gitlab_data_streamlit_init, snowflake_session, streamlit\n\n✅ available_packages.py\n📦 Classes: AvailablePackages\n🔧 Functions: main\n📥 Imports: pandas, gitlab_data_streamlit_init, streamlit\n\n✅ share.yml\n👥 Share Roles: snowflake_analyst_safe\n\n📄 FILE COMPLIANCE FOR MAIN_APP:\n✅ Required files found:\n✓ snowflake.yml\n✓ environment.yml\n✓ share.yml\n✓ README.md\n✓ dashboard.py\n\nRULES CHECKED:\n----------------------------------------\nClass Rules (1):\n- Inherit code for the page from GitLabDataStreamlitInit (error)\n\nFunction Rules (1):\n- Main function required (error)\n\nImport Rules (2):\n- Import GitLabDataStreamlitInit (error)\n- Import streamlit (error)\n\nFile Rules (5):\n- Snowflake configuration required (snowflake.yml) (error)\n- Snowflake environment required (environment.yml) (error)\n- Share specification required (share.yml) (warning)\n- README.md required (README.md) (error)\n- Starting point recommended (dashboard.py) (warning)\n\nSQL Rules (2):\n- SQL files must contain only SELECT statements (error)\n🗄 SELECT-only mode enabled\n🚨 Forbidden: INSERT, UPDATE, DELETE, DROP, ALTER...\n- SQL queries should include proper SELECT statements (warning)\n\nShare Rules (2):\n- Valid functional roles in share.yml (error)\n👥 Valid roles: 15 roles defined\n🔒 Safe data roles: 11 roles\n- Share.yml file format validation (error)\n------------------------------------------------------------\n✅ Compliance check passed\n-----------------------------------------------------------\n</code></pre>\n<h2>Developer experience that works</h2>\n<p>Whether you prefer your favorite IDE, a web-based development environment, or Snowflake Snowsight, the experience remains consistent. The framework provides:</p>\n<ul>\n<li><strong>Template-driven development:</strong> New applications and pages are created through standardized templates, ensuring consistency and best practices from day one. No more scattered design and elements.</li>\n</ul>\n<pre><code class=\"language-yaml\">╰─$ make streamlit-new-app NAME=sales_dashboard\n🔧 Configuration Environment: TEST\n📝 Configuration File: config.yml\n📜 Config Loader Script: ./setup/get_config.sh\n🐍 Python Version: 3.12\n📁 Applications Directory: ./src/applications\n🗄 Database: ...\n📊 Schema: ...\n🏗 Stage: ...\n🏭 Warehouse: ...\n🆕 Creating new Streamlit app: sales_dashboard\nInitialized the new project in ./src/applications/sales_dashboard\n</code></pre>\n<ul>\n<li><strong>Poetry package management:</strong> All dependencies are managed through Poetry, creating isolated environments that won't disrupt your existing Python setup.</li>\n</ul>\n<pre><code class=\"language-toml\">[tool.poetry]\nname = &quot;GitLab Data Streamlit&quot;\nversion = &quot;0.1.1&quot;\ndescription = &quot;GitLab Data Team Streamlit project&quot;\nauthors = [&quot;GitLab Data Team &lt;*****@gitlab.com&gt;&quot;]\nreadme = &quot;README.md&quot;\n\n[tool.poetry.dependencies]\npython = &quot;&lt;3.13,&gt;=3.12&quot;\nsnowflake-snowpark-python = &quot;==1.32.0&quot;\nsnowflake-connector-python = {extras = [&quot;development&quot;, &quot;pandas&quot;, &quot;secure-local-storage&quot;], version = &quot;^3.15.0&quot;}\nstreamlit = &quot;==1.22.0&quot;\nwatchdog = &quot;^6.0.0&quot;\ntypes-toml = &quot;^0.10.8.20240310&quot;\npytest = &quot;==7.0.0&quot;\nblack = &quot;==25.1.0&quot;\nimportlib-metadata = &quot;==4.13.0&quot;\npyyaml = &quot;==6.0.2&quot;\npython-qualiter = &quot;*&quot;\nruff = &quot;^0.1.0&quot;\ntypes-pyyaml = &quot;^6.0.12.20250516&quot;\njinja2 = &quot;==3.1.6&quot;\n\n[build-system]\nrequires = [&quot;poetry-core&quot;]\nbuild-backend = &quot;poetry.core.masonry.api&quot;\n</code></pre>\n<ul>\n<li><strong>Multi-page application support:</strong> Creators can easily build complex applications with multiple pages and add new libraries as needed. Multi-page applications are part of the framework and a developer is focusing on the logic, not the design and structuring.</li>\n</ul>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1760035999/at1q2xgmjthkrgju4okm.png\" alt=\"Multipage application example (in Snowflake)\">\n&lt;p&gt;&lt;/p&gt;\n&lt;center&gt;&lt;i&gt;Multipage application example (in Snowflake)&lt;/i&gt;&lt;/center&gt;\n&lt;p&gt;&lt;/p&gt;</p>\n<ul>\n<li><strong>Seamless Snowflake integration:</strong> Built-in connectors and authentication handling for secure data access provide the same experience, whether in local development or directly in Snowflake.</li>\n</ul>\n<pre><code class=\"language-yaml\">make streamlit-push-test APPLICATION_NAME=sales_dashboard\n📤 Deploying Streamlit app to test environment: sales_dashboard\n...\n------------------------------------------------------------------------------------------------------------\n🔗 Running share command for application: sales_dashboard\nRunning commands to grant shares\n🚀 Executing: snow streamlit share sales_dashboard with SOME_NICE_ROLE\n✅ Command executed successfully\n📊 Execution Summary: 1/1 commands succeeded\n</code></pre>\n<ul>\n<li>\n<p><strong>Comprehensive Makefile:</strong> All common commands are wrapped in simple Makefile commands, from local development to testing and deployment, including CI/CD pipelines.</p>\n</li>\n<li>\n<p><strong>Safe local development:</strong> Everything runs in isolated Poetry environments, protecting your system while providing production-like experiences.</p>\n</li>\n</ul>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1760035999/phmubsb34hn2mfefjvqh.png\" alt=\"Same experience despite the environment (example of the local development)\">\n&lt;p&gt;&lt;/p&gt;\n&lt;center&gt;&lt;i&gt;Same experience despite the environment (example of the local development)&lt;/i&gt;&lt;/center&gt;\n&lt;p&gt;&lt;/p&gt;</p>\n<ul>\n<li><strong>Collaboration via code:</strong> All applications and components are wrapped up in one repository, which allows the entire organization to collaborate on the same resources and avoid double work and redundant setup.</li>\n</ul>\n<h2>How you can get started</h2>\n<p>If you're facing similar challenges with scattered Streamlit applications, here's how to begin and move quickly:</p>\n<ol>\n<li><strong>Assess your current state:</strong> Inventory your existing applications and identify pain points.</li>\n<li><strong>Define your roles:</strong> Separate maintainer responsibilities from creator and end users' needs.</li>\n<li><strong>Start with templates:</strong> Create standardized application templates that enforce your security and compliance requirements.</li>\n<li><strong>Implement CI/CD:</strong> Automate your deployment pipeline to reduce manual errors and ensure consistency.</li>\n</ol>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1760036003/mzge9s1fhkhnx38y1a3i.png\" alt=\"Deploy the application in Snowflake\">\n&lt;p&gt;&lt;/p&gt;\n&lt;center&gt;&lt;i&gt;The application deployed in Snowflake&lt;/i&gt;&lt;/center&gt;</p>\n<h2>The bigger picture</h2>\n<p>This framework represents more than just a technical solution — it's a paradigm shift toward treating data applications as first-class citizens in your enterprise (data) architecture.\nBy providing structure without sacrificing flexibility, the GitLab Data team created an environment where anyone in the company with minimal technical knowledge can innovate rapidly while maintaining the highest standards of security and compliance.</p>\n<h3>What's next?</h3>\n<p>We're continuing to enhance the framework based on user feedback and emerging needs. Future improvements include expanded template libraries, enhanced monitoring capabilities, more flexibility, and a smoother user experience.\n<strong>The goal isn't just to solve today's problems, but to create a foundation that scales with your organization's growing data application needs.</strong></p>\n<h2>Summary</h2>\n<p><a href=\"https://handbook.gitlab.com/handbook/enterprise-data/\">The GitLab Data Team</a> transformed dozens of scattered, insecure Streamlit applications with no standardization into a unified, enterprise-grade framework that separates roles cleanly:</p>\n<ol>\n<li><strong>Maintainers</strong> handle infrastructure and security.</li>\n<li><strong>Creators</strong> focus on building applications without deployment headaches.</li>\n<li><strong>Viewers</strong> access polished, compliant apps.</li>\n</ol>\n<p>And we used these building blocks:</p>\n<ol>\n<li>Automated <strong>CI/CD</strong> pipelines</li>\n<li>Fully collaborative and versioned code in <strong>git</strong></li>\n<li><strong>Template-based</strong> development</li>\n<li>Built-in <strong>security</strong> compliance, testing</li>\n<li><strong>Poetry-managed</strong> environments\nWe eliminated the maintenance nightmare while enabling rapid innovation — proving that you can have both structure and flexibility when you treat data applications as first-class enterprise assets rather than throwaway prototypes.</li>\n</ol>\n",
      "summary": "Recently, the GitLab Data team transformed scattered Streamlit applications into a unified, secure, and scalable solution for our Snowflake environment. To accomplish this, we packed Python, Snowflake, and Streamlit together with GitLab. Follow along on this journey and discover the results we achieved, and learn how you can, too.\nThe challenge\nImagine this scenario: Your organization has dozens of Streamlit applications across different environments, running various Python versions, connecting to sensitive data with inconsistent security practices. Some apps work, others break mysteriously, and nobody knows who built what or how to maintain them.\nThis was exactly the challenge our data team faced. Applications were being created in isolation, with no standardization, no security oversight, and no clear deployment process. The result? A compliance nightmare and a maintenance burden that was growing exponentially.\n\n<p></p>\n<center><i>Functional architectural design (high level)</i></center>\nHow we started\nWe leveraged our unique position as customer zero by building this entire framework on GitLab's own CI/CD infrastructure and project management tools. Here are the ingredients we started with:\nGitLab (product)\nSnowflake - our single source of truth (SSOT) for the data warehouse activities (and more than that)\nStreamlit - an open-source tool for visual applications that has pure Python code under the hood\nThis provided us with immediate access to enterprise-grade DevSecOps capabilities, enabling us to implement automated testing, code review processes, and deployment pipelines from the outset. By utilizing GitLab's built-in features for issue tracking, merge requests, and automated deployments (CI/CD pipelines), we can iterate rapidly and validate the framework against real-world enterprise requirements. This internal-first approach ensured our solution was battle-tested on GitLab's own infrastructure before any external implementation.\nThe lessons we learned\nThe most critical lesson we learned from building the Streamlit Application Framework in Snowflake is that structure beats chaos every time — implement governance early rather than retrofitting it later when maintenance becomes exponential.\nYou also need to clearly define roles and responsibilities, separating infrastructure concerns from application development, so that each team can focus on its strengths.\nSecurity and compliance cannot be afterthoughts; they must be built into templates and automated processes from day one, as it's far easier to enforce consistent standards upfront than to force them after the fact. Invest heavily in automation and CI/CD pipelines, as manual processes don't scale and introduce human error.\n\n<p></p>\n<center><i>Architecture of the framework (general overview)</i></center>\nHow the Streamlit Application Framework changes everything\nThe Streamlit Application Framework turns a scattered approach into a structure. It gives developers freedom within secure guardrails, while automating deployment and eliminating maintenance complexity.\nThree clear roles, one unified process\nThe framework introduces a structured approach with three distinct roles:\nMaintainers (Data team members and contributors) handle the infrastructure, including CI/CD pipelines, security templates, and compliance rules. They ensure the framework runs smoothly and stays secure.\nCreators (those who need to build applications) can focus on what they do best: creating visualizations, connecting to Snowflake data, and building user experiences. They have full flexibility to create new applications from scratch, add new pages to existing apps, integrate additional Python libraries, and build complex data visualisations — all without worrying about deployment pipelines or security configurations.\nViewers (end users) access polished, secure applications without any technical overhead. All they need is Snowflake access.\n\n<p></p>\n<center><i>Overview of roles and their functions</i></center>\nAutomate everything\nBy implementing CI/CD, days of manual deployments and configuration headaches are gone. The framework provides:\nOne-click environment preparation: With a set of make commands, the environment is installed and ready in a few seconds.\n================================================================================\n✅ Snowflake CLI successfully installed and configured!\nConnection: gitlab_streamlit\nUser: YOU@GITLAB.COM\nAccount: gitlab\n================================================================================\nUsing virtualenv: /Users/YOU/repos/streamlit/.venv\n📚 Installing project dependencies...\nInstalling dependencies from lock file\nNo dependencies to install or update\n✅ Streamlit environment prepared!\n\nAutomated CI/CD pipelines: Handle testing, code review, and deployment from development to production.\nSecure sandbox environments: Provide for safe development and testing before production deployment.\n╰─$ make streamlit-rules\n🔍 Running Streamlit compliance check...\n================================================================================\nCODE COMPLIANCE REPORT\n================================================================================\nGenerated: 2025-07-09 14:01:16\nFiles checked: 1\n\nSUMMARY:\n✅ Passed: 1\n❌ Failed: 0\nSuccess Rate: 100.0%\n\nAPPLICATION COMPLIANCE SUMMARY:\n📱 Total Applications Checked: 1\n⚠️ Applications with Issues: 0\n📊 File Compliance Rate: 100.0%\n\nDETAILED RESULTS BY APPLICATION:\n...\n\nTemplate-based application creation: Ensures consistency across all applications and pages.\n╰─$ make streamlit-new-page STREAMLIT_APP=sales_dashboard STREAMLIT_PAGE_NAME=analytics\n📝 Generating new Streamlit page: analytics for app: sales_dashboard\n📃 Create new page from template:\nPage name: analytics\nApp directory: sales_dashboard\nTemplate path: page_template.py\n✅ Successfully created 'analytics.py' in 'sales_dashboard' directory from template\n\nPoetry-based dependency management: Prevents version conflicts and maintains clean environments.\nOrganized project structure: Has dedicated folders for applications, templates, compliance rules, and configuration management.\n├── src/\n│   ├── applications/     # Folder for Streamlit applications\n│   │   ├── main_app/     # Main dashboard application\n│   │   ├── components/   # Shared components\n│   │   └── <your_apps>/  # Your custom application\n│   │   └── <your_apps2>/ # Your 2nd custom application\n│   ├── templates/        # Application and page templates\n│   ├── compliance/       # Compliance rules and checks\n│   └── setup/            # Setup and configuration utilities\n├── tests/                # Test files\n├── config.yml            # Environment configuration\n├── Makefile              # Build and deployment automation\n└── README.md             # Main README.md file\n\nStreamlined workflow: Takes local development through testing schema to production, all automated through GitLab CI/CD pipelines.\n\n<p></p> <center><i>GitLab CI/CD pipelines for full automation of the process</i></center>\nSecurity and compliance by design\nInstead of bolting on security as an afterthought, the structured Streamlit Application Framework builds it in from the ground up. Every application adheres to the same security standards, and compliance requirements are automatically enforced. Audit trails are maintained throughout the development lifecycle.\nWe introduce our compliance rules and verify them with a single command. For instance, we can list which classes and methods are mandatory to use, which files you should have, and which roles are allowed and which are forbidden to share the application with. The rules are flexible and descriptive; all you need to do is define them in a YAML file:\nclass_rules:\n  - name: \"Inherit code for the page from GitLabDataStreamlitInit\"\n    description: \"All Streamlit apps must inherit from GitLabDataStreamlitInit\"\n    severity: \"error\"\n    required: true\n    class_name: \"*\"\n    required_base_classes:\n      - \"GitLabDataStreamlitInit\"\n    required_methods:\n      - \"__init__\"\n      - \"set_page_layout\"\n      - \"setup_ui\"\n      - \"run\"\n\nfunction_rules:\n  - name: \"Main function required\"\n    description: \"Must have a main() function\"\n    severity: \"error\"\n    required: true\n    function_name: \"main\"\n\nimport_rules:\n  - name: \"Import GitLabDataStreamlitInit\"\n    description: \"Must import the mandatory base class\"\n    severity: \"error\"\n    required: true\n    module_name: \"gitlab_data_streamlit_init\"\n    required_items:\n      - \"GitLabDataStreamlitInit\"\n  - name: \"Import streamlit\"\n    description: \"Must import streamlit library\"\n    severity: \"error\"\n    required: true\n    module_name: \"streamlit\"\n\nfile_rules:\n  - name: \"Snowflake configuration required (snowflake.yml)\"\n    description: \"Each application must have a snowflake.yml configuration file\"\n    severity: \"error\"\n    required: true\n    file_pattern: \"**/applications/**/snowflake.yml\"\n    base_path: \"\"\n  - name: \"Snowflake environment required (environment.yml)\"\n    description: \"Each application must have a environment.yml configuration file\"\n    severity: \"error\"\n    required: true\n    file_pattern: \"**/applications/**/environment.yml\"\n    base_path: \"\"\n  - name: \"Share specification required (share.yml)\"\n    description: \"Each application must have a share.yml file\"\n    severity: \"warning\"\n    required: true\n    file_pattern: \"**/applications/**/share.yml\"\n    base_path: \"\"\n  - name: \"README.md required (README.md)\"\n    description: \"Each application should have a README.md file with a proper documentation\"\n    severity: \"error\"\n    required: true\n    file_pattern: \"**/applications/**/README.md\"\n    base_path: \"\"\n  - name: \"Starting point recommended (dashboard.py)\"\n    description: \"Each application must have a dashboard.py as a starting point\"\n    severity: \"warning\"\n    required: true\n    file_pattern: \"**/applications/**/dashboard.py\"\n    base_path: \"\"\n\nsql_rules:\n  - name: \"SQL files must contain only SELECT statements\"\n    description: \"SQL files and SQL code in other files should only contain SELECT statements for data safety\"\n    severity: \"error\"\n    required: true\n    file_extensions: [\".sql\", \".py\"]\n    select_only: true\n    forbidden_statements:\n      - ....\n    case_sensitive: false\n  - name: \"SQL queries should include proper SELECT statements\"\n    description: \"When SQL is present, it should contain proper SELECT statements\"\n    severity: \"warning\"\n    required: false\n    file_extensions: [\".sql\", \".py\"]\n    required_statements:\n      - \"SELECT\"\n    case_sensitive: false\n\nshare_rules:\n  - name: \"Valid functional roles in share.yml\"\n    description: \"Share.yml files must contain only valid functional roles from the approved list\"\n    severity: \"error\"\n    required: true\n    file_pattern: \"**/applications/**/share.yml\"\n    valid_roles:\n      - ...\n    safe_data_roles:\n      - ...\n  - name: \"Share.yml file format validation\"\n    description: \"Share.yml files must follow the correct YAML format structure\"\n    severity: \"error\"\n    required: true\n    file_pattern: \"**/applications/**/share.yml\"\n    required_keys:\n      - \"share\"\n    min_roles: 1\n    max_roles: 10\n\n\nWith one command running:\n╰─$ make streamlit-rules\n\nWe can verify all the rules we have created and validate that the developers (who are building a Streamlit application) are following the policy specified by the creators (who determine the policies and building blocks of the framework), and that all the building blocks are in the right place. This ensures consistent behavior across all Streamlit applications.\n🔍 Running Streamlit compliance check...\n================================================================================\nCODE COMPLIANCE REPORT\n================================================================================\nGenerated: 2025-08-18 17:05:12\nFiles checked: 4\n\nSUMMARY:\n✅ Passed: 4\n❌ Failed: 0\nSuccess Rate: 100.0%\n\nAPPLICATION COMPLIANCE SUMMARY:\n📱 Total Applications Checked: 1\n⚠️ Applications with Issues: 0\n📊 File Compliance Rate: 100.0%\n\nDETAILED RESULTS BY APPLICATION:\n================================================================================\n✅ PASS APPLICATION: main_app\n------------------------------------------------------------\n📁 FILES ANALYZED (4):\n✅ dashboard.py\n📦 Classes: SnowflakeConnectionTester\n🔧 Functions: main\n📥 Imports: os, pwd, gitlab_data_streamlit_init, snowflake.snowpark.exceptions, streamlit\n\n✅ show_streamlit_apps.py\n📦 Classes: ShowStreamlitApps\n🔧 Functions: main\n📥 Imports: pandas, gitlab_data_streamlit_init, snowflake_session, streamlit\n\n✅ available_packages.py\n📦 Classes: AvailablePackages\n🔧 Functions: main\n📥 Imports: pandas, gitlab_data_streamlit_init, streamlit\n\n✅ share.yml\n👥 Share Roles: snowflake_analyst_safe\n\n📄 FILE COMPLIANCE FOR MAIN_APP:\n✅ Required files found:\n✓ snowflake.yml\n✓ environment.yml\n✓ share.yml\n✓ README.md\n✓ dashboard.py\n\nRULES CHECKED:\n----------------------------------------\nClass Rules (1):\n- Inherit code for the page from GitLabDataStreamlitInit (error)\n\nFunction Rules (1):\n- Main function required (error)\n\nImport Rules (2):\n- Import GitLabDataStreamlitInit (error)\n- Import streamlit (error)\n\nFile Rules (5):\n- Snowflake configuration required (snowflake.yml) (error)\n- Snowflake environment required (environment.yml) (error)\n- Share specification required (share.yml) (warning)\n- README.md required (README.md) (error)\n- Starting point recommended (dashboard.py) (warning)\n\nSQL Rules (2):\n- SQL files must contain only SELECT statements (error)\n🗄 SELECT-only mode enabled\n🚨 Forbidden: INSERT, UPDATE, DELETE, DROP, ALTER...\n- SQL queries should include proper SELECT statements (warning)\n\nShare Rules (2):\n- Valid functional roles in share.yml (error)\n👥 Valid roles: 15 roles defined\n🔒 Safe data roles: 11 roles\n- Share.yml file format validation (error)\n------------------------------------------------------------\n✅ Compliance check passed\n-----------------------------------------------------------\n\nDeveloper experience that works\nWhether you prefer your favorite IDE, a web-based development environment, or Snowflake Snowsight, the experience remains consistent. The framework provides:\nTemplate-driven development: New applications and pages are created through standardized templates, ensuring consistency and best practices from day one. No more scattered design and elements.\n╰─$ make streamlit-new-app NAME=sales_dashboard\n🔧 Configuration Environment: TEST\n📝 Configuration File: config.yml\n📜 Config Loader Script: ./setup/get_config.sh\n🐍 Python Version: 3.12\n📁 Applications Directory: ./src/applications\n🗄 Database: ...\n📊 Schema: ...\n🏗 Stage: ...\n🏭 Warehouse: ...\n🆕 Creating new Streamlit app: sales_dashboard\nInitialized the new project in ./src/applications/sales_dashboard\n\nPoetry package management: All dependencies are managed through Poetry, creating isolated environments that won't disrupt your existing Python setup.\n[tool.poetry]\nname = \"GitLab Data Streamlit\"\nversion = \"0.1.1\"\ndescription = \"GitLab Data Team Streamlit project\"\nauthors = [\"GitLab Data Team <*****@gitlab.com>\"]\nreadme = \"README.md\"\n\n[tool.poetry.dependencies]\npython = \"<3.13,>=3.12\"\nsnowflake-snowpark-python = \"==1.32.0\"\nsnowflake-connector-python = {extras = [\"development\", \"pandas\", \"secure-local-storage\"], version = \"^3.15.0\"}\nstreamlit = \"==1.22.0\"\nwatchdog = \"^6.0.0\"\ntypes-toml = \"^0.10.8.20240310\"\npytest = \"==7.0.0\"\nblack = \"==25.1.0\"\nimportlib-metadata = \"==4.13.0\"\npyyaml = \"==6.0.2\"\npython-qualiter = \"*\"\nruff = \"^0.1.0\"\ntypes-pyyaml = \"^6.0.12.20250516\"\njinja2 = \"==3.1.6\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\nMulti-page application support: Creators can easily build complex applications with multiple pages and add new libraries as needed. Multi-page applications are part of the framework and a developer is focusing on the logic, not the design and structuring.\n\n<p></p>\n<center><i>Multipage application example (in Snowflake)</i></center>\n<p></p>\nSeamless Snowflake integration: Built-in connectors and authentication handling for secure data access provide the same experience, whether in local development or directly in Snowflake.\nmake streamlit-push-test APPLICATION_NAME=sales_dashboard\n📤 Deploying Streamlit app to test environment: sales_dashboard\n...\n------------------------------------------------------------------------------------------------------------\n🔗 Running share command for application: sales_dashboard\nRunning commands to grant shares\n🚀 Executing: snow streamlit share sales_dashboard with SOME_NICE_ROLE\n✅ Command executed successfully\n📊 Execution Summary: 1/1 commands succeeded\n\nComprehensive Makefile: All common commands are wrapped in simple Makefile commands, from local development to testing and deployment, including CI/CD pipelines.\nSafe local development: Everything runs in isolated Poetry environments, protecting your system while providing production-like experiences.\n\n<p></p>\n<center><i>Same experience despite the environment (example of the local development)</i></center>\n<p></p>\nCollaboration via code: All applications and components are wrapped up in one repository, which allows the entire organization to collaborate on the same resources and avoid double work and redundant setup.\nHow you can get started\nIf you're facing similar challenges with scattered Streamlit applications, here's how to begin and move quickly:\nAssess your current state: Inventory your existing applications and identify pain points.\nDefine your roles: Separate maintainer responsibilities from creator and end users' needs.\nStart with templates: Create standardized application templates that enforce your security and compliance requirements.\nImplement CI/CD: Automate your deployment pipeline to reduce manual errors and ensure consistency.\n\n<p></p>\n<center><i>The application deployed in Snowflake</i></center>\nThe bigger picture\nThis framework represents more than just a technical solution — it's a paradigm shift toward treating data applications as first-class citizens in your enterprise (data) architecture.\nBy providing structure without sacrificing flexibility, the GitLab Data team created an environment where anyone in the company with minimal technical knowledge can innovate rapidly while maintaining the highest standards of security and compliance.\nWhat's next?\nWe're continuing to enhance the framework based on user feedback and emerging needs. Future improvements include expanded template libraries, enhanced monitoring capabilities, more flexibility, and a smoother user experience.\nThe goal isn't just to solve today's problems, but to create a foundation that scales with your organization's growing data application needs.\nSummary\nThe GitLab Data Team transformed dozens of scattered, insecure Streamlit applications with no standardization into a unified, enterprise-grade framework that separates roles cleanly:\nMaintainers handle infrastructure and security.\nCreators focus on building applications without deployment headaches.\nViewers access polished, compliant apps.\nAnd we used these building blocks:\nAutomated CI/CD pipelines\nFully collaborative and versioned code in git\nTemplate-based development\nBuilt-in security compliance, testing\nPoetry-managed environments\nWe eliminated the maintenance nightmare while enabling rapid innovation — proving that you can have both structure and flexibility when you treat data applications as first-class enterprise assets rather than throwaway prototypes.",
      "publishedAt": "2025-10-10T00:00:00.000Z",
      "author": "Radovan Bacovic",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "ide",
        "testing",
        "observability",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.767Z",
      "score": 1.0647882610201702
    },
    {
      "id": "bb6ac037a36f3fc7e4af16502e7f071b",
      "title": "Optimize GitLab object storage for scale and performance",
      "url": "https://about.gitlab.com/blog/optimize-gitlab-object-storage-for-scale-and-performance/",
      "content": "<p>Managing GitLab at scale requires strategic object storage configuration.\nHere's how to configure object storage for maximum performance, security, and reliability across your GitLab components.</p>\n<h2>Use consolidated form for GitLab components</h2>\n<p>For artifacts, LFS, uploads, packages, and other GitLab data, eliminate credential duplication with the consolidated form:</p>\n<pre><code class=\"language-gitlab_rails['object_store']['enabled']\">gitlab_rails['object_store']['connection'] = {\n  'provider' =&gt; 'AWS',\n  'region' =&gt; 'us-east-1',\n  'use_iam_profile' =&gt; true\n}\ngitlab_rails['object_store']['objects']['artifacts']['bucket'] = 'gitlab-artifacts'\ngitlab_rails['object_store']['objects']['lfs']['bucket'] = 'gitlab-lfs'\n# ... additional buckets for each object type\n``` This reduces complexity while enabling encrypted S3 buckets and proper Content-MD5 headers.\n## Configure container registry separately\nThe container registry requires its own configuration since it doesn't support the consolidated form:\n``` registry['storage'] = {\n  's3_v2' =&gt; {  # Use the new v2 driver\n    'bucket' =&gt; 'gitlab-registry',\n    'region' =&gt; 'us-east-1',\n    # Omit access keys to use IAM roles\n  }\n}\n</code></pre>\n<p><strong>Note:</strong> The s3_v1 driver is deprecated and will be removed in GitLab 19.0. Migrate to s3_v2 for better performance and reliability.</p>\n<h2>Disable proxy download for performance</h2>\n<p>Set <code>proxy_download</code> to <strong>false</strong> (default) for direct downloads:</p>\n<pre><code class=\"language-#\">gitlab_rails['object_store']['proxy_download'] = false\n# Or configure per bucket for granular control\ngitlab_rails['object_store']['objects']['artifacts']['proxy_download'] = false\ngitlab_rails['object_store']['objects']['lfs']['proxy_download'] = false\ngitlab_rails['object_store']['objects']['uploads']['proxy_download'] = true  # Example: keep proxy for uploads\n# Container registry defaults to redirect mode (direct downloads)\n# Only disable if your environment requires it:\nregistry['storage']['redirect']['disable'] = false  # Keep as false\n</code></pre>\n<p><strong>Important:</strong> The <code>proxy_download</code> option can be configured globally at the object-store level or individually per bucket. This gives you flexibility to optimize based on your specific use case — for example, you might want direct downloads for large artifacts and LFS files, but proxy smaller uploads through GitLab for additional security controls.\nThis dramatically reduces server load and egress costs by letting clients download directly from object storage.</p>\n<h2>Choose identity-based authentication</h2>\n<p><strong>AWS:</strong> Use IAM roles instead of access keys:</p>\n<pre><code class=\"language-#\">gitlab_rails['object_store']['connection'] = {\n  'provider' =&gt; 'AWS',\n  'use_iam_profile' =&gt; true\n}\n# Container registry\nregistry['storage'] = {\n  's3_v2' =&gt; {\n    'bucket' =&gt; 'gitlab-registry',\n    'region' =&gt; 'us-east-1'\n    # No access keys = IAM role authentication\n  }\n}\n</code></pre>\n<p><strong>Google Cloud Platform:</strong> Enable application default credentials:</p>\n<pre><code>gitlab_rails['object_store']['connection'] = {\n  'provider' =&gt; 'Google',\n  'google_application_default' =&gt; true\n}\n</code></pre>\n<p><strong>Azure:</strong> Use workload identities by omitting storage access keys.</p>\n<h2>Add encryption layers</h2>\n<p>Enable server-side encryption for additional security:</p>\n<pre><code class=\"language-#\">gitlab_rails['object_store']['storage_options'] = {\n  'server_side_encryption' =&gt; 'AES256'\n}\n# Container registry\nregistry['storage'] = {\n  's3_v2' =&gt; {\n    'bucket' =&gt; 'gitlab-registry',\n    'encrypt' =&gt; true\n  }\n}\n</code></pre>\n<p>For AWS KMS encryption, specify the key ARN in <code>server_side_encryption_kms_key_id</code>.</p>\n<h2>Use separate buckets for organization</h2>\n<p>Create dedicated buckets for each component:</p>\n<ul>\n<li><strong>gitlab-artifacts</strong> - CI/CD job artifacts</li>\n<li><strong>gitlab-lfs</strong> - Git LFS objects</li>\n<li><strong>gitlab-uploads</strong> - User uploads</li>\n<li><strong>gitlab-packages</strong> - Package registry</li>\n<li><strong>gitlab-registry</strong> - Container images\nThis isolation improves security, enables granular access controls, and simplifies cost tracking.</li>\n</ul>\n<h2>Key configuration differences</h2>\n<p>| Component | Consolidated Form | Identity Auth | Encryption | Direct Downloads | | --- | --- | --- | --- | ---| | Artifacts, LFS, Packages | ✅ Supported | ✅ use_iam_profile | ✅ storage_options | ✅ proxy_download: false | | Container Registry | ❌ Separate config | ✅ Omit access keys | ✅ encrypt: true | ✅ redirect enabled by default |</p>\n<h2>Migration path</h2>\n<ol>\n<li><strong>Start with GitLab objects:</strong> Use the consolidated form for immediate complexity reduction.</li>\n<li><strong>Configure registry separately:</strong> Use s3_v2 driver with IAM authentication.</li>\n<li><strong>Enable encryption:</strong> Add server-side encryption for both components.</li>\n<li><strong>Optimize performance:</strong> Ensure direct downloads are enabled with appropriate <code>proxy_download</code> settings.</li>\n<li><strong>Set up lifecycle policies:</strong> Configure S3 lifecycle rules to clean up incomplete multipart uploads.</li>\n</ol>\n<h2>Additional resources</h2>\n<p>For a complete AWS S3 configuration example, see the <a href=\"https://docs.gitlab.com/administration/object_storage/#aws-s3\">GitLab documentation on AWS S3 object storage setup</a>.\nFor more details on configuring proxy_download parameters per bucket, refer to the <a href=\"https://docs.gitlab.com/administration/object_storage/#configure-the-parameters-of-each-object\">GitLab object storage configuration documentation</a>.\n<em>These configurations will scale with your growth while maintaining security and performance. The separation between GitLab object storage and container registry configurations reflects their different underlying architectures, but both benefit from the same optimization principles.</em></p>\n",
      "summary": "Managing GitLab at scale requires strategic object storage configuration.\nHere's how to configure object storage for maximum performance, security, and reliability across your GitLab components.\nUse consolidated form for GitLab components\nFor artifacts, LFS, uploads, packages, and other GitLab data, eliminate credential duplication with the consolidated form:\ngitlab_rails['object_store']['connection'] = {\n  'provider' => 'AWS',\n  'region' => 'us-east-1',\n  'use_iam_profile' => true\n}\ngitlab_rails['object_store']['objects']['artifacts']['bucket'] = 'gitlab-artifacts'\ngitlab_rails['object_store']['objects']['lfs']['bucket'] = 'gitlab-lfs'\n# ... additional buckets for each object type\n``` This reduces complexity while enabling encrypted S3 buckets and proper Content-MD5 headers.\n## Configure container registry separately\nThe container registry requires its own configuration since it doesn't support the consolidated form:\n``` registry['storage'] = {\n  's3_v2' => {  # Use the new v2 driver\n    'bucket' => 'gitlab-registry',\n    'region' => 'us-east-1',\n    # Omit access keys to use IAM roles\n  }\n}\n\nNote: The s3_v1 driver is deprecated and will be removed in GitLab 19.0. Migrate to s3_v2 for better performance and reliability.\nDisable proxy download for performance\nSet proxy_download to false (default) for direct downloads:\ngitlab_rails['object_store']['proxy_download'] = false\n# Or configure per bucket for granular control\ngitlab_rails['object_store']['objects']['artifacts']['proxy_download'] = false\ngitlab_rails['object_store']['objects']['lfs']['proxy_download'] = false\ngitlab_rails['object_store']['objects']['uploads']['proxy_download'] = true  # Example: keep proxy for uploads\n# Container registry defaults to redirect mode (direct downloads)\n# Only disable if your environment requires it:\nregistry['storage']['redirect']['disable'] = false  # Keep as false\n\nImportant: The proxy_download option can be configured globally at the object-store level or individually per bucket. This gives you flexibility to optimize based on your specific use case — for example, you might want direct downloads for large artifacts and LFS files, but proxy smaller uploads through GitLab for additional security controls.\nThis dramatically reduces server load and egress costs by letting clients download directly from object storage.\nChoose identity-based authentication\nAWS: Use IAM roles instead of access keys:\ngitlab_rails['object_store']['connection'] = {\n  'provider' => 'AWS',\n  'use_iam_profile' => true\n}\n# Container registry\nregistry['storage'] = {\n  's3_v2' => {\n    'bucket' => 'gitlab-registry',\n    'region' => 'us-east-1'\n    # No access keys = IAM role authentication\n  }\n}\n\nGoogle Cloud Platform: Enable application default credentials:\ngitlab_rails['object_store']['connection'] = {\n  'provider' => 'Google',\n  'google_application_default' => true\n}\n\nAzure: Use workload identities by omitting storage access keys.\nAdd encryption layers\nEnable server-side encryption for additional security:\ngitlab_rails['object_store']['storage_options'] = {\n  'server_side_encryption' => 'AES256'\n}\n# Container registry\nregistry['storage'] = {\n  's3_v2' => {\n    'bucket' => 'gitlab-registry',\n    'encrypt' => true\n  }\n}\n\nFor AWS KMS encryption, specify the key ARN in server_side_encryption_kms_key_id.\nUse separate buckets for organization\nCreate dedicated buckets for each component:\ngitlab-artifacts - CI/CD job artifacts\ngitlab-lfs - Git LFS objects\ngitlab-uploads - User uploads\ngitlab-packages - Package registry\ngitlab-registry - Container images\nThis isolation improves security, enables granular access controls, and simplifies cost tracking.\nKey configuration differences\n| Component | Consolidated Form | Identity Auth | Encryption | Direct Downloads | | --- | --- | --- | --- | ---| | Artifacts, LFS, Packages | ✅ Supported | ✅ use_iam_profile | ✅ storage_options | ✅ proxy_download: false | | Container Registry | ❌ Separate config | ✅ Omit access keys | ✅ encrypt: true | ✅ redirect enabled by default |\nMigration path\nStart with GitLab objects: Use the consolidated form for immediate complexity reduction.\nConfigure registry separately: Use s3_v2 driver with IAM authentication.\nEnable encryption: Add server-side encryption for both components.\nOptimize performance: Ensure direct downloads are enabled with appropriate proxy_download settings.\nSet up lifecycle policies: Configure S3 lifecycle rules to clean up incomplete multipart uploads.\nAdditional resources\nFor a complete AWS S3 configuration example, see the GitLab documentation on AWS S3 object storage setup.\nFor more details on configuring proxy_download parameters per bucket, refer to the GitLab object storage configuration documentation.\nThese configurations will scale with your growth while maintaining security and performance. The separation between GitLab object storage and container registry configurations reflects their different underlying architectures, but both benefit from the same optimization principles.",
      "publishedAt": "2025-10-08T00:00:00.000Z",
      "author": "Tim Rizzi",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.768Z",
      "score": 0.5502746871055808
    },
    {
      "id": "af1285172a1fac2b846e5e7ca7a4fc8b",
      "title": "Streamline enterprise artifact management with GitLab",
      "url": "https://about.gitlab.com/blog/streamline-enterprise-artifact-management-with-gitlab/",
      "content": "<p>For the past six years, I've worked on artifact management at GitLab and have had hundreds of conversations with platform engineers trying to solve the same challenge: managing artifacts when they've become a sprawling, expensive mess. What started as simple Docker registries and Maven repositories has evolved into a complex web of tools, policies, and operational overhead that's consuming more time and budget than anyone anticipated.</p>\n<p>I recently spoke with a platform engineer at a Fortune 500 company who told me, &quot;I spend more time managing artifact repositories than I do on actual platform improvements.&quot; That conversation reminded me why we need an honest discussion about the real costs of fragmented artifact management — and what platform teams can realistically do about it. This article will help you better understand the problem and how GitLab can help you solve it through strategic consolidation.</p>\n<h2>Real-world impact: The numbers</h2>\n<p>Based on data from our customers and industry research, fragmented artifact management typically results in the following costs for a midsize organization (500+ developers):</p>\n<ul>\n<li><strong>Licensing:</strong> $50,000-200,000 annually across multiple tools</li>\n<li><strong>Operational overhead:</strong> 2-3 FTE's equivalent time spent on artifact management tasks</li>\n<li><strong>Storage inefficiency:</strong> 20%-30% higher storage costs due to duplication and poor lifecycle management</li>\n<li><strong>Developer productivity loss:</strong> 15-20 minutes daily per developer due to artifact-related friction</li>\n</ul>\n<p>For large enterprises, these numbers multiply significantly. One customer calculated they were spending over $500,000 annually just on the operational overhead of managing seven different artifact storage systems.</p>\n<p>The hidden costs compound daily:</p>\n<p><strong>Time multiplication:</strong> Every lifecycle policy, security rule, or access control change must be implemented across multiple systems. What should be a 15-minute configuration becomes hours of work.</p>\n<p><strong>Security gap risks:</strong> Managing security policies across disparate systems creates blind spots. Vulnerability scanning, access controls, and audit trails become fragmented.</p>\n<p><strong>Context switching tax:</strong> Developers lose productivity when they can't find artifacts or need to remember which system stores what.</p>\n<h2>The multiplication problem</h2>\n<p>The artifact management landscape has exploded. Where teams once managed a single Maven repository, today's platform engineers juggle:</p>\n<ul>\n<li>Container registries (Docker Hub, ECR, GCR, Azure ACR)</li>\n<li>Package repositories (JFrog Artifactory, Sonatype Nexus)</li>\n<li>Language-specific registries (npm, PyPI, NuGet, Conan)</li>\n<li>Infrastructure artifacts (Terraform modules, Helm charts)</li>\n<li>ML model registries (MLflow, Weights &amp; Biases)</li>\n</ul>\n<p>Each tool comes with its own authentication system, lifecycle policies, security scanning, and operational requirements. For organizations with hundreds or thousands of projects, this creates an exponential management burden.</p>\n<h2>GitLab's strategic approach: Depth over breadth</h2>\n<p>When we started building GitLab's artifact management capabilities six years ago, we faced a classic product decision: support every artifact format imaginable or go deep on the formats that matter most to enterprise teams. We chose depth, and that decision has shaped everything we've built since.</p>\n<h3>Our core focus areas</h3>\n<p>Instead of building shallow support for 20+ formats, we committed to delivering enterprise-grade capabilities for a strategic set:</p>\n<ul>\n<li><strong>Maven</strong> (Java ecosystem)</li>\n<li><strong>npm</strong> (JavaScript/Node.js)</li>\n<li><strong>Docker/OCI</strong> (container images)</li>\n<li><strong>PyPI</strong> (Python packages)</li>\n<li><strong>NuGet</strong> (C#/.NET packages)</li>\n<li><strong>Generic packages</strong> (any binary artifact)</li>\n<li><strong>Terraform modules</strong> (infrastructure as code)</li>\n</ul>\n<p>These seven formats account for approximately 80% of artifact usage in enterprise environments, based on our customer data.</p>\n<h3>What 'enterprise-grade' actually means</h3>\n<p>By focusing on fewer formats, we can deliver capabilities that work in production environments with hundreds of developers, terabytes of artifacts, and strict compliance requirements:</p>\n<p><strong><a href=\"https://docs.gitlab.com/user/packages/virtual_registry/\">Virtual registries</a>:</strong> Proxy and cache upstream dependencies for reliable builds and supply chain control. Currently production-ready for Maven, with npm and Docker coming in early 2026.</p>\n<p><strong>Lifecycle management</strong>: Automated cleanup policies that prevent storage costs from spiraling while preserving artifacts for compliance. Available at the project level today, organization-level policies planned for mid-2026.</p>\n<p><strong><a href=\"https://docs.gitlab.com/user/application_security/\">Security integration</a>:</strong> Built-in vulnerability scanning, dependency analysis, and policy enforcement. Our upcoming Dependency Firewall (planned for late 2026) will provide supply chain security control across all formats.</p>\n<p><strong><a href=\"https://docs.gitlab.com/ci/\">Deep CI/CD integration</a>:</strong> Complete traceability from source commit to deployed artifact, with build provenance and security scan results embedded in artifact metadata.</p>\n<h2>Current capabilities: Battle-tested features</h2>\n<p><strong>Maven virtual registries:</strong> Our flagship enterprise capability, proven with 15+ enterprise customers. Most complete <a href=\"https://about.gitlab.com/blog/tutorial-secure-and-optimize-your-maven-repository-in-gitlab/\">Maven virtual registry</a> setup within two months, with minimal GitLab support required.</p>\n<p><strong>Locally-hosted repositories:</strong> All seven supported formats offer complete upload, download, versioning, and access control capabilities supporting critical workloads at organizations with thousands of developers.</p>\n<p><strong>Protected artifacts:</strong> Comprehensive protection preventing unauthorized modifications, supporting fine-grained access controls across all formats.</p>\n<p><strong>Project-level lifecycle policies:</strong> Automated cleanup and retention policies for storage cost control and compliance.</p>\n<h3>Performance and scale characteristics</h3>\n<p>Based on current production deployments:</p>\n<ul>\n<li><strong>Throughput:</strong> 10,000+ artifact downloads per minute/per instance</li>\n<li><strong>Storage:</strong> Customers successfully managing 50+ TB of artifacts</li>\n<li><strong>Concurrent users:</strong> 1,000+ developers accessing artifacts simultaneously</li>\n<li><strong>Availability:</strong> 99.99% uptime for <a href=\"http://GitLab.com\">GitLab.com</a> for more than 2 years</li>\n</ul>\n<h2>Strategic roadmap: Next 18 months</h2>\n<h3>Q1 2026</h3>\n<ul>\n<li><strong>npm virtual registries:</strong> Enterprise proxy/cache for JavaScript packages</li>\n<li><strong>Docker virtual registries:</strong> Container registry proxy capabilities</li>\n</ul>\n<h3>Q2 2026</h3>\n<ul>\n<li><strong>Organization-level lifecycle policies (Beta):</strong> Centralized cleanup policies with project overrides</li>\n<li><strong>NuGet virtual registries (Beta):</strong> .NET package proxy support</li>\n<li><strong>PyPI virtual registries (Beta):</strong> Completing virtual registry support for Python</li>\n</ul>\n<h3>Q3 2026</h3>\n<ul>\n<li><strong>Advanced Analytics Dashboard:</strong> Storage optimization and usage insights</li>\n</ul>\n<h3>Q4 2026</h3>\n<ul>\n<li><strong>Dependency Firewall (Beta):</strong> Supply chain security control for all artifact types</li>\n</ul>\n<h2>When to choose GitLab: Decision framework</h2>\n<p><strong>GitLab is likely the right choice if:</strong></p>\n<ul>\n<li>80%+ of your artifacts are in our seven supported formats</li>\n<li>You're already using GitLab for source code or CI/CD</li>\n<li>You value integrated workflows over standalone feature richness</li>\n<li>You want to reduce the operational complexity of managing multiple systems</li>\n<li>You need complete traceability from source to deployment</li>\n</ul>\n<h3>Migration considerations</h3>\n<p><strong>Typical timeline:</strong> 2-4 months for complete migration from Artifactory/Nexus</p>\n<p><strong>Common challenges:</strong> Virtual registry configuration, access control mapping, and developer workflow changes</p>\n<p><strong>Success factors:</strong> Phased approach, comprehensive testing, and developer training</p>\n<p>Most successful migrations follow this pattern:</p>\n<ol>\n<li><strong>Assessment</strong> (2-4 weeks): Catalog current artifacts and usage patterns</li>\n<li><strong>Pilot</strong> (4-6 weeks): Migrate one team/project end-to-end</li>\n<li><strong>Rollout</strong> (6-12 weeks): Gradual migration with parallel systems</li>\n<li><strong>Optimization</strong> (ongoing): Implement advanced features and policies</li>\n</ol>\n<h2>Better artifact management can start today</h2>\n<p>GitLab's artifact management isn't trying to be everything to everyone. We've made strategic trade-offs: deep capabilities for core enterprise formats rather than shallow support for everything.</p>\n<p>If your artifact needs align with our supported formats and you value integrated workflows, we can significantly reduce your operational overhead while improving developer experience.</p>\n<p>Our goal is to help you make informed decisions about your artifact management strategy with a clear understanding of capabilities and our roadmap.</p>\n<p>Please reach out to me at <a href=\"mailto:trizzi@gitlab.com\">trizzi@gitlab.com</a> to learn more about GitLab artifact management. I can discuss specific requirements and connect you with our technical team for a deeper evaluation.</p>\n<p><em>This blog contains information related to upcoming products, features, and functionality. It is important to note that the information in this blog post is for informational purposes only. Please do not rely on this information for purchasing or planning purposes. As with all projects, the items mentioned in this blog and linked pages are subject to change or delay. The development, release, and timing of any products, features, or functionality remain at the sole discretion of GitLab.</em></p>\n",
      "summary": "For the past six years, I've worked on artifact management at GitLab and have had hundreds of conversations with platform engineers trying to solve the same challenge: managing artifacts when they've become a sprawling, expensive mess. What started as simple Docker registries and Maven repositories has evolved into a complex web of tools, policies, and operational overhead that's consuming more time and budget than anyone anticipated.\nI recently spoke with a platform engineer at a Fortune 500 company who told me, \"I spend more time managing artifact repositories than I do on actual platform improvements.\" That conversation reminded me why we need an honest discussion about the real costs of fragmented artifact management — and what platform teams can realistically do about it. This article will help you better understand the problem and how GitLab can help you solve it through strategic consolidation.\nReal-world impact: The numbers\nBased on data from our customers and industry research, fragmented artifact management typically results in the following costs for a midsize organization (500+ developers):\nLicensing: $50,000-200,000 annually across multiple tools\nOperational overhead: 2-3 FTE's equivalent time spent on artifact management tasks\nStorage inefficiency: 20%-30% higher storage costs due to duplication and poor lifecycle management\nDeveloper productivity loss: 15-20 minutes daily per developer due to artifact-related friction\nFor large enterprises, these numbers multiply significantly. One customer calculated they were spending over $500,000 annually just on the operational overhead of managing seven different artifact storage systems.\nThe hidden costs compound daily:\nTime multiplication: Every lifecycle policy, security rule, or access control change must be implemented across multiple systems. What should be a 15-minute configuration becomes hours of work.\nSecurity gap risks: Managing security policies across disparate systems creates blind spots. Vulnerability scanning, access controls, and audit trails become fragmented.\nContext switching tax: Developers lose productivity when they can't find artifacts or need to remember which system stores what.\nThe multiplication problem\nThe artifact management landscape has exploded. Where teams once managed a single Maven repository, today's platform engineers juggle:\nContainer registries (Docker Hub, ECR, GCR, Azure ACR)\nPackage repositories (JFrog Artifactory, Sonatype Nexus)\nLanguage-specific registries (npm, PyPI, NuGet, Conan)\nInfrastructure artifacts (Terraform modules, Helm charts)\nML model registries (MLflow, Weights & Biases)\nEach tool comes with its own authentication system, lifecycle policies, security scanning, and operational requirements. For organizations with hundreds or thousands of projects, this creates an exponential management burden.\nGitLab's strategic approach: Depth over breadth\nWhen we started building GitLab's artifact management capabilities six years ago, we faced a classic product decision: support every artifact format imaginable or go deep on the formats that matter most to enterprise teams. We chose depth, and that decision has shaped everything we've built since.\nOur core focus areas\nInstead of building shallow support for 20+ formats, we committed to delivering enterprise-grade capabilities for a strategic set:\nMaven (Java ecosystem)\nnpm (JavaScript/Node.js)\nDocker/OCI (container images)\nPyPI (Python packages)\nNuGet (C#/.NET packages)\nGeneric packages (any binary artifact)\nTerraform modules (infrastructure as code)\nThese seven formats account for approximately 80% of artifact usage in enterprise environments, based on our customer data.\nWhat 'enterprise-grade' actually means\nBy focusing on fewer formats, we can deliver capabilities that work in production environments with hundreds of developers, terabytes of artifacts, and strict compliance requirements:\nVirtual registries: Proxy and cache upstream dependencies for reliable builds and supply chain control. Currently production-ready for Maven, with npm and Docker coming in early 2026.\nLifecycle management: Automated cleanup policies that prevent storage costs from spiraling while preserving artifacts for compliance. Available at the project level today, organization-level policies planned for mid-2026.\nSecurity integration: Built-in vulnerability scanning, dependency analysis, and policy enforcement. Our upcoming Dependency Firewall (planned for late 2026) will provide supply chain security control across all formats.\nDeep CI/CD integration: Complete traceability from source commit to deployed artifact, with build provenance and security scan results embedded in artifact metadata.\nCurrent capabilities: Battle-tested features\nMaven virtual registries: Our flagship enterprise capability, proven with 15+ enterprise customers. Most complete Maven virtual registry setup within two months, with minimal GitLab support required.\nLocally-hosted repositories: All seven supported formats offer complete upload, download, versioning, and access control capabilities supporting critical workloads at organizations with thousands of developers.\nProtected artifacts: Comprehensive protection preventing unauthorized modifications, supporting fine-grained access controls across all formats.\nProject-level lifecycle policies: Automated cleanup and retention policies for storage cost control and compliance.\nPerformance and scale characteristics\nBased on current production deployments:\nThroughput: 10,000+ artifact downloads per minute/per instance\nStorage: Customers successfully managing 50+ TB of artifacts\nConcurrent users: 1,000+ developers accessing artifacts simultaneously\nAvailability: 99.99% uptime for GitLab.com for more than 2 years\nStrategic roadmap: Next 18 months\nQ1 2026\nnpm virtual registries: Enterprise proxy/cache for JavaScript packages\nDocker virtual registries: Container registry proxy capabilities\nQ2 2026\nOrganization-level lifecycle policies (Beta): Centralized cleanup policies with project overrides\nNuGet virtual registries (Beta): .NET package proxy support\nPyPI virtual registries (Beta): Completing virtual registry support for Python\nQ3 2026\nAdvanced Analytics Dashboard: Storage optimization and usage insights\nQ4 2026\nDependency Firewall (Beta): Supply chain security control for all artifact types\nWhen to choose GitLab: Decision framework\nGitLab is likely the right choice if:\n80%+ of your artifacts are in our seven supported formats\nYou're already using GitLab for source code or CI/CD\nYou value integrated workflows over standalone feature richness\nYou want to reduce the operational complexity of managing multiple systems\nYou need complete traceability from source to deployment\nMigration considerations\nTypical timeline: 2-4 months for complete migration from Artifactory/Nexus\nCommon challenges: Virtual registry configuration, access control mapping, and developer workflow changes\nSuccess factors: Phased approach, comprehensive testing, and developer training\nMost successful migrations follow this pattern:\nAssessment (2-4 weeks): Catalog current artifacts and usage patterns\nPilot (4-6 weeks): Migrate one team/project end-to-end\nRollout (6-12 weeks): Gradual migration with parallel systems\nOptimization (ongoing): Implement advanced features and policies\nBetter artifact management can start today\nGitLab's artifact management isn't trying to be everything to everyone. We've made strategic trade-offs: deep capabilities for core enterprise formats rather than shallow support for everything.\nIf your artifact needs align with our supported formats and you value integrated workflows, we can significantly reduce your operational overhead while improving developer experience.\nOur goal is to help you make informed decisions about your artifact management strategy with a clear understanding of capabilities and our roadmap.\nPlease reach out to me at trizzi@gitlab.com to learn more about GitLab artifact management. I can discuss specific requirements and connect you with our technical team for a deeper evaluation.\nThis blog contains information related to upcoming products, features, and functionality. It is important to note that the information in this blog post is for informational purposes only. Please do not rely on this information for purchasing or planning purposes. As with all projects, the items mentioned in this blog and linked pages are subject to change or delay. The development, release, and timing of any products, features, or functionality remain at the sole discretion of GitLab.",
      "publishedAt": "2025-10-08T00:00:00.000Z",
      "author": "Tim Rizzi",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "testing",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.768Z",
      "score": 0.8520382251957379
    },
    {
      "id": "a39a748af845ed9deb377d46e41e84ac",
      "title": "Atlassian ending Data Center as GitLab maintains deployment choice",
      "url": "https://about.gitlab.com/blog/atlassian-ending-data-center-as-gitlab-maintains-deployment-choice/",
      "content": "<p>Change is never easy, especially when it's not your choice. Atlassian's announcement that <a href=\"https://www.atlassian.com/blog/announcements/atlassian-ascend\">all Data Center products will reach end-of-life by March 28, 2029</a>, means thousands of organizations must now reconsider their DevSecOps deployment and infrastructure. But you don't have to settle for deployment options that don't fit your needs. GitLab maintains your freedom to choose — whether you need self-managed for compliance, cloud for convenience, or hybrid for flexibility — all within a single AI-powered DevSecOps platform that respects your requirements.</p>\n<p>While other vendors force migrations to cloud-only architectures, GitLab remains committed to supporting the deployment choices that match your business needs. Whether you're managing sensitive government data, operating in air-gapped environments, or simply prefer the control of self-managed deployments, we understand that one size doesn't fit all.</p>\n<h2>The cloud isn't the answer for everyone</h2>\n<p>For the many companies that invested millions of dollars in Data Center deployments, including those that migrated to Data Center <a href=\"https://about.gitlab.com/blog/atlassian-server-ending-move-to-a-single-devsecops-platform/\">after its Server products were discontinued</a>, this announcement represents more than a product sunset. It signals a fundamental shift away from customer-centric architecture choices, forcing enterprises into difficult positions: accept a deployment model that doesn't fit their needs, or find a vendor that respects their requirements.</p>\n<p>Many of the organizations requiring self-managed deployments represent some of the world's most important organizations: healthcare systems protecting patient data, financial institutions managing trillions in assets, government agencies safeguarding national security, and defense contractors operating in air-gapped environments.</p>\n<p>These organizations don't choose self-managed deployments for convenience; they choose them for compliance, security, and sovereignty requirements that cloud-only architectures simply cannot meet. Organizations operating in closed environments with restricted or no internet access aren't exceptions — they represent a significant portion of enterprise customers across various industries.</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1759928476/ynl7wwmkh5xyqhszv46m.jpg\" alt=\"GitLab vs. Atlassian comparison table\"></p>\n<h2>The real cost of forced cloud migration goes beyond dollars</h2>\n<p>While cloud-only vendors frame mandatory migrations as &quot;upgrades,&quot; organizations face substantial challenges beyond simple financial costs:</p>\n<ul>\n<li>\n<p><strong>Lost integration capabilities:</strong> Years of custom integrations with legacy systems, carefully crafted workflows, and enterprise-specific automations become obsolete. Organizations with deep integrations to legacy systems often find cloud migration technically infeasible.</p>\n</li>\n<li>\n<p><strong>Regulatory constraints:</strong> For organizations in regulated industries, cloud migration isn't just complex — it's often not permitted. Data residency requirements, air-gapped environments, and strict regulatory frameworks don't bend to vendor preferences. The absence of single-tenant solutions in many cloud-only approaches creates insurmountable compliance barriers.</p>\n</li>\n<li>\n<p><strong>Productivity impacts:</strong> Cloud-only architectures often require juggling multiple products: separate tools for planning, code management, CI/CD, and documentation. Each tool means another context switch, another integration to maintain, another potential point of failure. GitLab research shows <a href=\"https://about.gitlab.com/developer-survey/\">30% of developers spend at least 50% of their job maintaining and/or integrating their DevSecOps toolchain</a>. Fragmented architectures exacerbate this challenge rather than solving it.</p>\n</li>\n</ul>\n<h2>GitLab offers choice, commitment, and consolidation</h2>\n<p>Enterprise customers deserve a trustworthy technology partner. That's why we've committed to supporting a range of deployment options — whether you need on-premises for compliance, hybrid for flexibility, or cloud for convenience, the choice remains yours. That commitment continues with <a href=\"https://about.gitlab.com/gitlab-duo/\">GitLab Duo</a>, our AI solution that supports developers at every stage of their workflow.</p>\n<p>But we offer more than just deployment flexibility. While other vendors might force you to cobble together their products into a fragmented toolchain, GitLab provides everything in a <strong>comprehensive AI-native DevSecOps platform</strong>. Source code management, CI/CD, security scanning, Agile planning, and documentation are all managed within a single application and a single vendor relationship.</p>\n<p>This isn't theoretical. When <a href=\"https://about.gitlab.com/customers/airbus/\">Airbus</a> and <a href=\"https://about.gitlab.com/customers/iron-mountain/\">Iron Mountain</a> evaluated their existing fragmented toolchains, they consistently identified challenges: poor user experience, missing functionalities like built-in security scanning and review apps, and management complexity from plugin troubleshooting. <strong>These aren't minor challenges; they're major blockers for modern software delivery.</strong></p>\n<h2>Your migration path: Simpler than you think</h2>\n<p>We've helped thousands of organizations migrate from other vendors, and we've built the tools and expertise to make your transition smooth:</p>\n<ul>\n<li>\n<p><strong>Automated migration tools:</strong> Our <a href=\"https://docs.gitlab.com/user/project/import/bitbucket_server/\">Bitbucket Server importer</a> brings over repositories, pull requests, comments, and even Large File Storage (LFS) objects. For Jira, our <a href=\"https://docs.gitlab.com/user/project/import/jira/\">built-in importer</a> handles issues, descriptions, and labels, with professional services available for complex migrations.</p>\n</li>\n<li>\n<p><strong>Proven at scale:</strong> A 500 GiB repository with 13,000 pull requests, 10,000 branches, and 7,000 tags is likely to <a href=\"https://docs.gitlab.com/user/project/import/bitbucket_server/\">take just 8 hours to migrate</a> from Bitbucket to GitLab using parallel processing.</p>\n</li>\n<li>\n<p><strong>Immediate ROI:</strong> A <a href=\"https://about.gitlab.com/resources/study-forrester-tei-gitlab-ultimate/\">Forrester Consulting Total Economic Impact™ study commissioned by GitLab</a> found that investing in GitLab Ultimate confirms these benefits translate to real bottom-line impact, with a three-year 483% ROI, 5x time saved in security related activities, and 25% savings in software toolchain costs.</p>\n</li>\n</ul>\n<h2>Start your journey to a unified DevSecOps platform</h2>\n<p>Forward-thinking organizations aren't waiting for vendor-mandated deadlines. They're evaluating alternatives now, while they have time to migrate thoughtfully to platforms that protect their investments and deliver on promises.</p>\n<p>Organizations invest in self-managed deployments because they need control, compliance, and customization. When vendors deprecate these capabilities, they remove not just features but the fundamental ability to choose environments matching business requirements.</p>\n<p>Modern DevSecOps platforms should offer complete functionality that respects deployment needs, consolidates toolchains, and accelerates software delivery, without forcing compromises on security or data sovereignty.</p>\n<p><a href=\"https://about.gitlab.com/sales/\">Talk to our sales team</a> today about your migration options, or explore our <a href=\"https://about.gitlab.com/move-to-gitlab-from-atlassian/\">comprehensive migration resources</a> to see how thousands of organizations have already made the switch.</p>\n<p>You also can <a href=\"https://about.gitlab.com/free-trial/devsecops/\">try GitLab Ultimate with GitLab Duo Enterprise</a> for free for 30 days to see what a unified DevSecOps platform can do for your organization.</p>\n",
      "summary": "Change is never easy, especially when it's not your choice. Atlassian's announcement that all Data Center products will reach end-of-life by March 28, 2029, means thousands of organizations must now reconsider their DevSecOps deployment and infrastructure. But you don't have to settle for deployment options that don't fit your needs. GitLab maintains your freedom to choose — whether you need self-managed for compliance, cloud for convenience, or hybrid for flexibility — all within a single AI-powered DevSecOps platform that respects your requirements.\nWhile other vendors force migrations to cloud-only architectures, GitLab remains committed to supporting the deployment choices that match your business needs. Whether you're managing sensitive government data, operating in air-gapped environments, or simply prefer the control of self-managed deployments, we understand that one size doesn't fit all.\nThe cloud isn't the answer for everyone\nFor the many companies that invested millions of dollars in Data Center deployments, including those that migrated to Data Center after its Server products were discontinued, this announcement represents more than a product sunset. It signals a fundamental shift away from customer-centric architecture choices, forcing enterprises into difficult positions: accept a deployment model that doesn't fit their needs, or find a vendor that respects their requirements.\nMany of the organizations requiring self-managed deployments represent some of the world's most important organizations: healthcare systems protecting patient data, financial institutions managing trillions in assets, government agencies safeguarding national security, and defense contractors operating in air-gapped environments.\nThese organizations don't choose self-managed deployments for convenience; they choose them for compliance, security, and sovereignty requirements that cloud-only architectures simply cannot meet. Organizations operating in closed environments with restricted or no internet access aren't exceptions — they represent a significant portion of enterprise customers across various industries.\n\nThe real cost of forced cloud migration goes beyond dollars\nWhile cloud-only vendors frame mandatory migrations as \"upgrades,\" organizations face substantial challenges beyond simple financial costs:\nLost integration capabilities: Years of custom integrations with legacy systems, carefully crafted workflows, and enterprise-specific automations become obsolete. Organizations with deep integrations to legacy systems often find cloud migration technically infeasible.\nRegulatory constraints: For organizations in regulated industries, cloud migration isn't just complex — it's often not permitted. Data residency requirements, air-gapped environments, and strict regulatory frameworks don't bend to vendor preferences. The absence of single-tenant solutions in many cloud-only approaches creates insurmountable compliance barriers.\nProductivity impacts: Cloud-only architectures often require juggling multiple products: separate tools for planning, code management, CI/CD, and documentation. Each tool means another context switch, another integration to maintain, another potential point of failure. GitLab research shows 30% of developers spend at least 50% of their job maintaining and/or integrating their DevSecOps toolchain. Fragmented architectures exacerbate this challenge rather than solving it.\nGitLab offers choice, commitment, and consolidation\nEnterprise customers deserve a trustworthy technology partner. That's why we've committed to supporting a range of deployment options — whether you need on-premises for compliance, hybrid for flexibility, or cloud for convenience, the choice remains yours. That commitment continues with GitLab Duo, our AI solution that supports developers at every stage of their workflow.\nBut we offer more than just deployment flexibility. While other vendors might force you to cobble together their products into a fragmented toolchain, GitLab provides everything in a comprehensive AI-native DevSecOps platform. Source code management, CI/CD, security scanning, Agile planning, and documentation are all managed within a single application and a single vendor relationship.\nThis isn't theoretical. When Airbus and Iron Mountain evaluated their existing fragmented toolchains, they consistently identified challenges: poor user experience, missing functionalities like built-in security scanning and review apps, and management complexity from plugin troubleshooting. These aren't minor challenges; they're major blockers for modern software delivery.\nYour migration path: Simpler than you think\nWe've helped thousands of organizations migrate from other vendors, and we've built the tools and expertise to make your transition smooth:\nAutomated migration tools: Our Bitbucket Server importer brings over repositories, pull requests, comments, and even Large File Storage (LFS) objects. For Jira, our built-in importer handles issues, descriptions, and labels, with professional services available for complex migrations.\nProven at scale: A 500 GiB repository with 13,000 pull requests, 10,000 branches, and 7,000 tags is likely to take just 8 hours to migrate from Bitbucket to GitLab using parallel processing.\nImmediate ROI: A Forrester Consulting Total Economic Impact™ study commissioned by GitLab found that investing in GitLab Ultimate confirms these benefits translate to real bottom-line impact, with a three-year 483% ROI, 5x time saved in security related activities, and 25% savings in software toolchain costs.\nStart your journey to a unified DevSecOps platform\nForward-thinking organizations aren't waiting for vendor-mandated deadlines. They're evaluating alternatives now, while they have time to migrate thoughtfully to platforms that protect their investments and deliver on promises.\nOrganizations invest in self-managed deployments because they need control, compliance, and customization. When vendors deprecate these capabilities, they remove not just features but the fundamental ability to choose environments matching business requirements.\nModern DevSecOps platforms should offer complete functionality that respects deployment needs, consolidates toolchains, and accelerates software delivery, without forcing compromises on security or data sovereignty.\nTalk to our sales team today about your migration options, or explore our comprehensive migration resources to see how thousands of organizations have already made the switch.\nYou also can try GitLab Ultimate with GitLab Duo Enterprise for free for 30 days to see what a unified DevSecOps platform can do for your organization.",
      "publishedAt": "2025-10-07T00:00:00.000Z",
      "author": "Emilio Salvador",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "ide",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.768Z",
      "score": 0.7767739725655589
    },
    {
      "id": "2fe212e68e13b269fc6a7ce6a3c063ec",
      "title": "How GitLab transforms embedded systems testing cycles",
      "url": "https://about.gitlab.com/blog/how-gitlab-transforms-embedded-systems-testing-cycles/",
      "content": "<p>Embedded developers know this cycle well: write code, wait days or weeks to test on a hardware test bench, discover bugs, fix them, then wait again. Virtual testing environments promise faster feedback, but most implementations create new problems such as environment sprawl and escalating costs.</p>\n<p>GitLab's managed lifecycle environments solve these virtual testing challenges. Through virtual environment automation, GitLab accelerates embedded development cycles without the configuration complexity and cost overruns.</p>\n<h2>Virtual testing challenges</h2>\n<p>Virtual testing environments — simulated hardware setups that replicate embedded system behavior and real-world conditions — offer the potential to reduce hardware bottlenecks. Teams can test firmware on simulated processors, run model-in-the-loop (MIL) tests in MATLAB/Simulink, or verify software on virtual embedded systems without waiting for physical hardware access.</p>\n<p>However, teams often implement virtual environments using one of two common approaches, both of which create unsustainable challenges.</p>\n<h3>Flawed approach 1: Pipeline lifecycle environments</h3>\n<p><strong>Pipeline lifecycle environments re-create the entire testing setup for every CI/CD run.</strong> When code changes trigger your CI/CD pipeline, the system provisions infrastructure, installs software simulations, and configures everything from scratch before running tests.</p>\n<p>This approach works for simple scenarios but becomes inefficient as complexity rises. Consider software-in-the-loop (SIL) testing in a complex virtual environment, for example. Each pipeline run requires complete environment re-creation, including virtual processor provisioning, toolchain installations, and target configurations. <strong>These processes can eat up considerable time.</strong></p>\n<p>Moreover, as embedded systems require more sophisticated virtual hardware configurations, the provisioning <strong>costs quickly add up.</strong></p>\n<p>To avoid these rebuild costs and delays, many teams turn to long-lived environments that persist between test runs. But they come with downsides.</p>\n<h3>Flawed approach 2: Long-lived environments</h3>\n<p><strong>Long-lived environments persist indefinitely</strong> to avoid constant rebuilding. Developers request these environments from IT or DevOps teams, wait for approval, then need someone to manually provision the infrastructure. These environments are then tied to individual developers/teams rather than specific code changes, and they support ongoing development work across multiple projects.</p>\n<p>While this eliminates rebuild overhead, <strong>it creates environment sprawl.</strong> Environments accumulate without a clear termination date. Infrastructure costs climb as environments consume resources indefinitely.</p>\n<p>Long-lived environments also suffer from <strong>&quot;config rot&quot;</strong> — environments retain settings, cached data, or software versions from previous tests that can affect subsequent results. A test that should fail ends up passing due to the residue of previous testing.</p>\n<p>Ultimately, managing long-lived environments is a manual process that slows development velocity and increases operational overhead.</p>\n<p><strong>GitLab offers a third approach</strong> through “managed lifecycle environments.” This approach captures the benefits of both long-lived and pipeline lifecycle environments while avoiding the drawbacks.</p>\n<h2>Solution: Managed lifecycle environments</h2>\n<p>GitLab's managed lifecycle environments tie virtual testing setups to merge requests (<a href=\"https://docs.gitlab.com/user/project/merge_requests/\">MRs</a>) rather than pipeline runs or individual developers. You can also think of them as “managed MR test environments.” When you create an MR for a new feature, GitLab automatically orchestrates the provisioning of necessary virtual testing environments. These environments persist throughout the entire feature development process.</p>\n<h3>Key benefits</h3>\n<ul>\n<li>\n<p><strong>Persistent environments without rebuilding:</strong> The same virtual environment handles multiple pipeline runs as you iterate on your feature. Whether you're running MIL tests in MATLAB/Simulink or SIL tests on specialized embedded processors, the environment remains configured and ready.</p>\n</li>\n<li>\n<p><strong>Automatic cleanup:</strong> When you merge your feature and delete the branch, GitLab automatically triggers environment cleanup, eliminating environment sprawl.</p>\n</li>\n<li>\n<p><strong>Single source of truth:</strong> The MR records all build results, test outcomes, and environment metadata in one location. Team members can track progress and collaborate without shuffling between different tools or spreadsheets.</p>\n</li>\n</ul>\n<p>Watch this overview video to see how managed lifecycle environments work in practice:</p>\n<p>&lt;!-- blank line --&gt;\n&lt;figure class=&quot;video_container&quot;&gt;\n&lt;iframe src=&quot;https://www.youtube.com/embed/9tfyVPK5DuI?si=Kj_xXNo02bnFBDhy&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt; &lt;/iframe&gt;\n&lt;/figure&gt;\n&lt;!-- blank line --&gt;</p>\n<p>GitLab automates the entire testing workflow. Each time you run firmware tests, GitLab orchestrates testing in the appropriate virtual environment, records results, and provides full visibility into every pipeline run. This approach transforms complex virtual testing from a manual, error-prone process into automated, reliable workflows.</p>\n<p><strong>The result:</strong> Teams get reusable environments without runaway costs. And they increase efficiency while maintaining clean, isolated testing setups for each feature.</p>\n<p>See a demonstration of managed lifecycle environments for testing firmware on virtual hardware:</p>\n<p>&lt;!-- blank line --&gt;\n&lt;figure class=&quot;video_container&quot;&gt;\n&lt;iframe src=&quot;https://www.youtube.com/embed/iWdY-kTlpH4?si=D6rpoulr9sv6Sl6E&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt; &lt;/iframe&gt;\n&lt;/figure&gt;\n&lt;!-- blank line --&gt;</p>\n<h2>Business impact</h2>\n<p>GitLab's managed lifecycle environments deliver measurable improvements across embedded development workflows. Teams running MIL testing in MATLAB/Simulink and SIL testing on specialized processors like Infineon AURIX or BlackBerry QNX systems no longer face the tradeoff between constant environment rebuilds or uncontrolled environment sprawl. Instead, these complex virtual testing setups persist throughout feature development while automatically cleaning up when complete, enabling:</p>\n<ul>\n<li>Faster product development cycles</li>\n<li>Shorter time-to-market</li>\n<li>Lower infrastructure costs</li>\n<li>Higher quality assurance</li>\n</ul>\n<h2>Start transforming virtual testing today</h2>\n<p><a href=\"https://learn.gitlab.com/embedded-en/whitepaper-unlocking-agility-embedded-development\"><strong>Download “Unlocking agility and avoiding runaway costs in embedded development”</strong></a> for a deeper exploration of managed lifecycle environments and learn how to accelerate embedded development workflows dramatically.</p>\n",
      "summary": "Embedded developers know this cycle well: write code, wait days or weeks to test on a hardware test bench, discover bugs, fix them, then wait again. Virtual testing environments promise faster feedback, but most implementations create new problems such as environment sprawl and escalating costs.\nGitLab's managed lifecycle environments solve these virtual testing challenges. Through virtual environment automation, GitLab accelerates embedded development cycles without the configuration complexity and cost overruns.\nVirtual testing challenges\nVirtual testing environments — simulated hardware setups that replicate embedded system behavior and real-world conditions — offer the potential to reduce hardware bottlenecks. Teams can test firmware on simulated processors, run model-in-the-loop (MIL) tests in MATLAB/Simulink, or verify software on virtual embedded systems without waiting for physical hardware access.\nHowever, teams often implement virtual environments using one of two common approaches, both of which create unsustainable challenges.\nFlawed approach 1: Pipeline lifecycle environments\nPipeline lifecycle environments re-create the entire testing setup for every CI/CD run. When code changes trigger your CI/CD pipeline, the system provisions infrastructure, installs software simulations, and configures everything from scratch before running tests.\nThis approach works for simple scenarios but becomes inefficient as complexity rises. Consider software-in-the-loop (SIL) testing in a complex virtual environment, for example. Each pipeline run requires complete environment re-creation, including virtual processor provisioning, toolchain installations, and target configurations. These processes can eat up considerable time.\nMoreover, as embedded systems require more sophisticated virtual hardware configurations, the provisioning costs quickly add up.\nTo avoid these rebuild costs and delays, many teams turn to long-lived environments that persist between test runs. But they come with downsides.\nFlawed approach 2: Long-lived environments\nLong-lived environments persist indefinitely to avoid constant rebuilding. Developers request these environments from IT or DevOps teams, wait for approval, then need someone to manually provision the infrastructure. These environments are then tied to individual developers/teams rather than specific code changes, and they support ongoing development work across multiple projects.\nWhile this eliminates rebuild overhead, it creates environment sprawl. Environments accumulate without a clear termination date. Infrastructure costs climb as environments consume resources indefinitely.\nLong-lived environments also suffer from \"config rot\" — environments retain settings, cached data, or software versions from previous tests that can affect subsequent results. A test that should fail ends up passing due to the residue of previous testing.\nUltimately, managing long-lived environments is a manual process that slows development velocity and increases operational overhead.\nGitLab offers a third approach through “managed lifecycle environments.” This approach captures the benefits of both long-lived and pipeline lifecycle environments while avoiding the drawbacks.\nSolution: Managed lifecycle environments\nGitLab's managed lifecycle environments tie virtual testing setups to merge requests (MRs) rather than pipeline runs or individual developers. You can also think of them as “managed MR test environments.” When you create an MR for a new feature, GitLab automatically orchestrates the provisioning of necessary virtual testing environments. These environments persist throughout the entire feature development process.\nKey benefits\nPersistent environments without rebuilding: The same virtual environment handles multiple pipeline runs as you iterate on your feature. Whether you're running MIL tests in MATLAB/Simulink or SIL tests on specialized embedded processors, the environment remains configured and ready.\nAutomatic cleanup: When you merge your feature and delete the branch, GitLab automatically triggers environment cleanup, eliminating environment sprawl.\nSingle source of truth: The MR records all build results, test outcomes, and environment metadata in one location. Team members can track progress and collaborate without shuffling between different tools or spreadsheets.\nWatch this overview video to see how managed lifecycle environments work in practice:\n<!-- blank line -->\n<figure class=\"video_container\">\n<iframe src=\"https://www.youtube.com/embed/9tfyVPK5DuI?si=Kj_xXNo02bnFBDhy\" frameborder=\"0\" allowfullscreen=\"true\"> </iframe>\n</figure>\n<!-- blank line -->\nGitLab automates the entire testing workflow. Each time you run firmware tests, GitLab orchestrates testing in the appropriate virtual environment, records results, and provides full visibility into every pipeline run. This approach transforms complex virtual testing from a manual, error-prone process into automated, reliable workflows.\nThe result: Teams get reusable environments without runaway costs. And they increase efficiency while maintaining clean, isolated testing setups for each feature.\nSee a demonstration of managed lifecycle environments for testing firmware on virtual hardware:\n<!-- blank line -->\n<figure class=\"video_container\">\n<iframe src=\"https://www.youtube.com/embed/iWdY-kTlpH4?si=D6rpoulr9sv6Sl6E\" frameborder=\"0\" allowfullscreen=\"true\"> </iframe>\n</figure>\n<!-- blank line -->\nBusiness impact\nGitLab's managed lifecycle environments deliver measurable improvements across embedded development workflows. Teams running MIL testing in MATLAB/Simulink and SIL testing on specialized processors like Infineon AURIX or BlackBerry QNX systems no longer face the tradeoff between constant environment rebuilds or uncontrolled environment sprawl. Instead, these complex virtual testing setups persist throughout feature development while automatically cleaning up when complete, enabling:\nFaster product development cycles\nShorter time-to-market\nLower infrastructure costs\nHigher quality assurance\nStart transforming virtual testing today\nDownload “Unlocking agility and avoiding runaway costs in embedded development” for a deeper exploration of managed lifecycle environments and learn how to accelerate embedded development workflows dramatically.",
      "publishedAt": "2025-10-02T00:00:00.000Z",
      "author": "Matt DeLaney",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "thought_leadership",
      "tags": [
        "code_review",
        "ide",
        "testing"
      ],
      "ingestedAt": "2025-11-23T17:37:39.768Z",
      "score": 0.18501699280064832
    },
    {
      "id": "f2a2795854b38cf4d13acb4e1c77d19f",
      "title": "Greater AI choice in GitLab Duo: Claude Sonnet 4.5 arrives",
      "url": "https://about.gitlab.com/blog/greater-ai-choice-in-gitlab-duo-claude-sonnet-4-5-arrives/",
      "content": "<p>GitLab now offers Claude Sonnet 4.5, Anthropic’s most advanced model for coding and real-world agents, directly in the GitLab Duo model selector.</p>\n<p>Users now have the flexibility to choose Claude Sonnet 4.5 alongside other leading models, enhancing their <a href=\"https://about.gitlab.com/gitlab-duo/\">GitLab Duo</a> experience with upgrades in tool orchestration, context editing, and domain-specific capabilities. With top performance on <a href=\"https://www.anthropic.com/news/claude-sonnet-4-5\">SWE-bench Verified (77.2%</a>) and strengths in cybersecurity, finance, and research-heavy workflows, GitLab users can apply Claude Sonnet 4.5 to bring sharper insights and deeper context to their development work.</p>\n<p>&quot;Having Claude Sonnet 4.5 in GitLab is a big win for developers. It’s a really capable coding model, and, when you use it with the GitLab Duo Agent Platform, you get smarter help right in your workflows. It’s the kind of step that makes development easier,&quot; said Taylor McCaslin, Principal, Strategy and Operations for AI Partnerships at GitLab.</p>\n<h2>GitLab Duo Agent Platform + Claude Sonnet 4.5</h2>\n<p><a href=\"https://about.gitlab.com/gitlab-duo/agent-platform/\">GitLab Duo Agent Platform</a> extends the value of Claude Sonnet 4.5 by orchestrating agents, connecting them to internal systems, and integrating them throughout the software lifecycle. This combination creates a uniquely GitLab experience — where advanced reasoning and problem-solving meet platform-wide context and security. The result is faster development, more accurate outcomes, and stronger organizational coverage, all delivered inside the GitLab workflow developers already use every day.</p>\n<h2>Where you can use Claude Sonnet 4.5</h2>\n<p>Claude Sonnet 4.5 is now available as a model option in GitLab Duo Agent Platform Agentic Chat on GitLab.com. You can choose Claude Sonnet 4.5 from the model selection dropdown to leverage its advanced coding capabilities for your development tasks.</p>\n<p><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1759180378/sopuv0msxrmhzt2dyxdi.png\" alt=\"Dropdown selection for Claude Sonnet 4.5 in GitLab Duo\"></p>\n<p><strong>Note:</strong> Ability to select Claude Sonnet 4.5 in supported IDEs will be available soon.</p>\n<h2>Get started</h2>\n<p>GitLab Duo Pro and Enterprise customers can access Claude Sonnet 4.5 today. Visit our <a href=\"https://docs.gitlab.com/user/gitlab_duo/\">documentation</a> to learn more about GitLab Duo capabilities and models.</p>\n<p>Questions or feedback? Share your experience with us through the GitLab community.</p>\n<blockquote>\n<p>Want to try GitLab Ultimate with Duo Enterprise? <a href=\"https://about.gitlab.com/gitlab-duo/\">Sign up for a free trial today.</a></p>\n</blockquote>\n",
      "summary": "GitLab now offers Claude Sonnet 4.5, Anthropic’s most advanced model for coding and real-world agents, directly in the GitLab Duo model selector.\nUsers now have the flexibility to choose Claude Sonnet 4.5 alongside other leading models, enhancing their GitLab Duo experience with upgrades in tool orchestration, context editing, and domain-specific capabilities. With top performance on SWE-bench Verified (77.2%) and strengths in cybersecurity, finance, and research-heavy workflows, GitLab users can apply Claude Sonnet 4.5 to bring sharper insights and deeper context to their development work.\n\"Having Claude Sonnet 4.5 in GitLab is a big win for developers. It’s a really capable coding model, and, when you use it with the GitLab Duo Agent Platform, you get smarter help right in your workflows. It’s the kind of step that makes development easier,\" said Taylor McCaslin, Principal, Strategy and Operations for AI Partnerships at GitLab.\nGitLab Duo Agent Platform + Claude Sonnet 4.5\nGitLab Duo Agent Platform extends the value of Claude Sonnet 4.5 by orchestrating agents, connecting them to internal systems, and integrating them throughout the software lifecycle. This combination creates a uniquely GitLab experience — where advanced reasoning and problem-solving meet platform-wide context and security. The result is faster development, more accurate outcomes, and stronger organizational coverage, all delivered inside the GitLab workflow developers already use every day.\nWhere you can use Claude Sonnet 4.5\nClaude Sonnet 4.5 is now available as a model option in GitLab Duo Agent Platform Agentic Chat on GitLab.com. You can choose Claude Sonnet 4.5 from the model selection dropdown to leverage its advanced coding capabilities for your development tasks.\n\nNote: Ability to select Claude Sonnet 4.5 in supported IDEs will be available soon.\nGet started\nGitLab Duo Pro and Enterprise customers can access Claude Sonnet 4.5 today. Visit our documentation to learn more about GitLab Duo capabilities and models.\nQuestions or feedback? Share your experience with us through the GitLab community.\nWant to try GitLab Ultimate with Duo Enterprise? Sign up for a free trial today.",
      "publishedAt": "2025-09-29T00:00:00.000Z",
      "author": "Tim Zallmann",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.769Z",
      "score": 0.4199920263661419
    },
    {
      "id": "6505e7736bd4cfa360bb33059384d9d4",
      "title": "Agentic AI guides and resources",
      "url": "https://about.gitlab.com/blog/agentic-ai-guides-and-resources/",
      "content": "<h2>Defining agentic AI</h2>\n<p>Agentic AI is a type of artificial intelligence that leverages advanced language models and natural language processing to take independent action. Unlike traditional generative AI tools that require constant human direction, these systems can understand requests, make decisions, and execute multi-step plans to achieve goals. They tackle complex tasks by breaking them into manageable steps and employ adaptive learning to modify their approach when facing challenges.</p>\n<p><a href=\"https://about.gitlab.com/topics/agentic-ai/\">Learn more about agentic AI</a></p>\n<h2>Agentic AI insights</h2>\n<ul>\n<li><a href=\"https://about.gitlab.com/the-source/ai/transform-development-with-agentic-ai-the-enterprise-guide/\">Transform development with agentic AI: The enterprise guide</a></li>\n<li><a href=\"https://about.gitlab.com/blog/gitlab-18-4-ai-native-development-with-automation-and-insight/\">GitLab 18.4: AI-native development with automation and insight</a> With GitLab 18.4, teams create custom agents, unlock Knowledge Graph context, and auto-fix pipelines so developers stay focused and in flow.</li>\n<li><a href=\"https://about.gitlab.com/blog/gitlab-18-3-expanding-ai-orchestration-in-software-engineering/\">GitLab 18.3: Expanding AI orchestration in software engineering</a> Learn how we're advancing human-AI collaboration with enhanced Flows, enterprise governance, and seamless tool integration.</li>\n<li><a href=\"https://about.gitlab.com/blog/gitlab-duo-agent-platform-public-beta/\">GitLab Duo Agent Platform Public Beta: Next-gen AI orchestration and more</a> — Introducing the DevSecOps orchestration platform designed to unlock asynchronous collaboration between developers and AI agents.</li>\n<li><a href=\"https://about.gitlab.com/blog/gitlab-duo-agent-platform-what-is-next-for-intelligent-devsecops/\">GitLab Duo Agent Platform: What's next for intelligent DevSecOps</a> — GitLab Duo Agent Platform, a DevSecOps orchestration platform for humans and AI agents, leverages agentic AI for collaboration across the software development lifecycle.</li>\n<li><a href=\"https://about.gitlab.com/the-source/ai/from-vibe-coding-to-agentic-ai-a-roadmap-for-technical-leaders/\">From vibe coding to agentic AI: A roadmap for technical leaders</a> — Discover how to implement vibe coding and agentic AI in your development process to increase productivity while maintaining code quality and security.</li>\n<li><a href=\"https://about.gitlab.com/the-source/ai/emerging-agentic-ai-trends-reshaping-software-development/\">Emerging agentic AI trends reshaping software development</a> — Discover how agentic AI transforms development from isolated coding to intelligent workflows that enhance productivity while maintaining security.</li>\n<li><a href=\"https://about.gitlab.com/the-source/ai/agentic-ai-unlocking-developer-potential-at-scale/\">Agentic AI: Unlocking developer potential at scale</a> — Explore how agentic AI is transforming software development, moving beyond code completion to create AI partners that proactively tackle complex tasks.</li>\n<li><a href=\"https://about.gitlab.com/the-source/ai/ai-trends-for-2025-agentic-ai-self-hosted-models-and-more/\">Agentic AI, self-hosted models, and more: AI trends for 2025</a> — Discover key trends in AI for software development, from on-premises model deployments to intelligent, adaptive AI agents.</li>\n<li><a href=\"https://about.gitlab.com/the-source/ai/how-agentic-ai-unlocks-platform-engineering-potential/\">How agentic AI unlocks platform engineering potential</a> — Explore how agentic AI elevates platform engineering by automating complex workflows and scaling standardization.</li>\n</ul>\n<h2>The agentic AI ecosystem</h2>\n<ul>\n<li><a href=\"https://about.gitlab.com/topics/agentic-ai/ai-code-analysis/\">AI-driven code analysis: The new frontier in code security</a></li>\n<li><a href=\"https://about.gitlab.com/topics/agentic-ai/devops-automation-ai-agents/\">DevOps automation &amp; AI agents</a></li>\n<li><a href=\"https://about.gitlab.com/topics/agentic-ai/ai-augmented-software-development/\">AI-augmented software development: Agentic AI for DevOps</a></li>\n</ul>\n<h2>Best practices for implementing agentic AI</h2>\n<ul>\n<li><a href=\"https://about.gitlab.com/the-source/ai/implementing-effective-guardrails-for-ai-agents/\">Implementing effective guardrails for AI agents</a> — Discover essential security guardrails for AI agents in DevSecOps, from compliance controls and infrastructure protection to user access management.</li>\n</ul>\n<h2>GitLab's agentic AI offerings</h2>\n<h3>GitLab Duo with Amazon Q</h3>\n<ul>\n<li><a href=\"https://about.gitlab.com/blog/gitlab-duo-with-amazon-q-agentic-ai-optimized-for-aws/\">GitLab Duo with Amazon Q: Agentic AI optimized for AWS generally available</a> — The comprehensive AI-powered DevSecOps platform combined with the deepest set of cloud computing capabilities speeds dev cycles, increases automation, and improves code quality.</li>\n<li><a href=\"https://about.gitlab.com/blog/devsecops-agentic-ai-now-on-gitlab-self-managed-ultimate-on-aws/\">DevSecOps + Agentic AI: Now on GitLab Self-Managed Ultimate on AWS</a> — Start using AI-powered, DevSecOps-enhanced agents in your AWS GitLab Self-Managed Ultimate instance. Enjoy the benefits of GitLab Duo and Amazon Q in your organization.</li>\n<li><a href=\"https://about.gitlab.com/partners/technology-partners/aws/\">GitLab Duo with Amazon Q partner page</a></li>\n</ul>\n<p>Watch GitLab Duo with Amazon Q in action:</p>\n<p>&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/1075753390?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;Technical Demo: GitLab Duo with Amazon Q&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;</p>\n<h4>Guided tour</h4>\n<p>Click on the image to start a tour of GitLab Duo with Amazon Q:</p>\n<p><a href=\"https://gitlab.navattic.com/duo-with-q\"><img src=\"https://res.cloudinary.com/about-gitlab-com/image/upload/v1749673568/Blog/Content%20Images/Screenshot_2025-05-07_at_7.24.45_AM.png\" alt=\"GitLab Duo with Amazon Q interactive tour\"></a></p>\n<h4>GitLab Duo with Amazon Q tutorials</h4>\n<ul>\n<li><a href=\"https://about.gitlab.com/blog/enhance-application-quality-with-ai-powered-test-generation/\">Enhance application quality with AI-powered test generation</a> — Learn how GitLab Duo with Amazon Q improves the QA process by automatically generating comprehensive unit tests.</li>\n<li><a href=\"https://about.gitlab.com/blog/gitlab-duo-amazon-q-transform-ideas-into-code-in-minutes/\">GitLab Duo + Amazon Q: Transform ideas into code in minutes</a> — The new GitLab Duo with Amazon Q integration analyzes your issue descriptions and automatically generates complete working code solutions, accelerating development workflows.</li>\n<li><a href=\"https://about.gitlab.com/blog/accelerate-code-reviews-with-gitlab-duo-and-amazon-q/\">Accelerate code reviews with GitLab Duo and Amazon Q</a> — Use AI-powered agents to optimize code reviews by automatically analyzing merge requests and providing comprehensive feedback on bugs, readability, and coding standards.</li>\n<li><a href=\"https://about.gitlab.com/blog/speed-up-code-reviews-let-ai-handle-the-feedback-implementation/\">Speed up code reviews: Let AI handle the feedback implementation</a> — Discover how GitLab Duo with Amazon Q automates the implementation of code review feedback through AI, transforming a time-consuming manual process into a streamlined workflow.</li>\n</ul>\n<h3>GitLab Duo Agent Platform</h3>\n<ul>\n<li><a href=\"https://about.gitlab.com/blog/gitlab-duo-chat-gets-agentic-ai-makeover/\">GitLab Duo Chat gets agentic AI makeover</a> — Our new Duo Chat experience, currently an experimental release, helps developers onboard to projects, understand assignments, implement changes, and more.\nWatch GitLab Duo Agent Platform in action:\n&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/1095679084?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;Agent Platform Demo Clip&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;</li>\n</ul>\n<h4>GitLab Agent Platform tutorials and use cases</h4>\n<ul>\n<li><a href=\"https://about.gitlab.com/blog/vibe-coding-with-gitlab-duo-agent-platform-issue-to-mr-flow/\">Vibe coding with GitLab Duo Agent Platform: Issue to MR Flow</a> - Learn how to update your application in minutes with our newest agent Flow that takes developers from idea to code.</li>\n<li><a href=\"https://about.gitlab.com/blog/get-started-with-gitlab-duo-agentic-chat-in-the-web-ui/\">Get started with GitLab Duo Agentic Chat in the web UI</a> - Learn about our new GitLab Duo AI feature that automates tasks by breaking down complex problems and executing operations across multiple sources.</li>\n<li><a href=\"https://about.gitlab.com/blog/custom-rules-duo-agentic-chat-deep-dive/\">Custom rules in GitLab Duo Agentic Chat for greater developer efficiency</a> — Discover how AI can understand your codebase, follow your conventions, and generate production-ready code with minimal review cycles.</li>\n<li><a href=\"https://about.gitlab.com/blog/accelerate-learning-with-gitlab-duo-agent-platform/\">Accelerate learning with GitLab Duo Agent Platform</a> — Learn how agentic AI helped generate comprehensive gRPC documentation in minutes, not hours.</li>\n<li><a href=\"https://about.gitlab.com/blog/fast-and-secure-ai-agent-deployment-to-google-cloud-with-gitlab/\">Fast and secure AI agent deployment to Google Cloud with GitLab</a></li>\n</ul>\n<h2>Learn more with GitLab University</h2>\n<ul>\n<li><a href=\"https://university.gitlab.com/pages/ai\">Get Started with GitLab Duo coursework</a></li>\n<li><a href=\"https://university.gitlab.com/learning-paths/gitlab-duo-enterprise-learning-path\">GitLab Duo Enterprise Learning Path</a></li>\n</ul>\n<h2>More AI resources</h2>\n<ul>\n<li><a href=\"https://about.gitlab.com/developer-survey/2024/ai/\">2024 Global DevSecOps Survey: Navigating AI maturity in DevSecOps</a></li>\n<li><a href=\"https://about.gitlab.com/topics/devops/the-role-of-ai-in-devops/\">The Role of AI in DevOps</a></li>\n<li><a href=\"https://about.gitlab.com/blog/categories/ai-ml/\">The latest AI/ML articles from GitLab</a></li>\n<li><a href=\"https://about.gitlab.com/gitlab-duo/\">GitLab Duo</a></li>\n<li><a href=\"https://about.gitlab.com/gitlab-duo/agent-platform/\">GitLab Duo Agent Platform</a></li>\n</ul>\n",
      "summary": "Defining agentic AI\nAgentic AI is a type of artificial intelligence that leverages advanced language models and natural language processing to take independent action. Unlike traditional generative AI tools that require constant human direction, these systems can understand requests, make decisions, and execute multi-step plans to achieve goals. They tackle complex tasks by breaking them into manageable steps and employ adaptive learning to modify their approach when facing challenges.\nLearn more about agentic AI\nAgentic AI insights\nTransform development with agentic AI: The enterprise guide\nGitLab 18.4: AI-native development with automation and insight With GitLab 18.4, teams create custom agents, unlock Knowledge Graph context, and auto-fix pipelines so developers stay focused and in flow.\nGitLab 18.3: Expanding AI orchestration in software engineering Learn how we're advancing human-AI collaboration with enhanced Flows, enterprise governance, and seamless tool integration.\nGitLab Duo Agent Platform Public Beta: Next-gen AI orchestration and more — Introducing the DevSecOps orchestration platform designed to unlock asynchronous collaboration between developers and AI agents.\nGitLab Duo Agent Platform: What's next for intelligent DevSecOps — GitLab Duo Agent Platform, a DevSecOps orchestration platform for humans and AI agents, leverages agentic AI for collaboration across the software development lifecycle.\nFrom vibe coding to agentic AI: A roadmap for technical leaders — Discover how to implement vibe coding and agentic AI in your development process to increase productivity while maintaining code quality and security.\nEmerging agentic AI trends reshaping software development — Discover how agentic AI transforms development from isolated coding to intelligent workflows that enhance productivity while maintaining security.\nAgentic AI: Unlocking developer potential at scale — Explore how agentic AI is transforming software development, moving beyond code completion to create AI partners that proactively tackle complex tasks.\nAgentic AI, self-hosted models, and more: AI trends for 2025 — Discover key trends in AI for software development, from on-premises model deployments to intelligent, adaptive AI agents.\nHow agentic AI unlocks platform engineering potential — Explore how agentic AI elevates platform engineering by automating complex workflows and scaling standardization.\nThe agentic AI ecosystem\nAI-driven code analysis: The new frontier in code security\nDevOps automation & AI agents\nAI-augmented software development: Agentic AI for DevOps\nBest practices for implementing agentic AI\nImplementing effective guardrails for AI agents — Discover essential security guardrails for AI agents in DevSecOps, from compliance controls and infrastructure protection to user access management.\nGitLab's agentic AI offerings\nGitLab Duo with Amazon Q\nGitLab Duo with Amazon Q: Agentic AI optimized for AWS generally available — The comprehensive AI-powered DevSecOps platform combined with the deepest set of cloud computing capabilities speeds dev cycles, increases automation, and improves code quality.\nDevSecOps + Agentic AI: Now on GitLab Self-Managed Ultimate on AWS — Start using AI-powered, DevSecOps-enhanced agents in your AWS GitLab Self-Managed Ultimate instance. Enjoy the benefits of GitLab Duo and Amazon Q in your organization.\nGitLab Duo with Amazon Q partner page\nWatch GitLab Duo with Amazon Q in action:\n<div style=\"padding:56.25% 0 0 0;position:relative;\"><iframe src=\"https://player.vimeo.com/video/1075753390?badge=0&autopause=0&player_id=0&app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"Technical Demo: GitLab Duo with Amazon Q\"></iframe></div><script src=\"https://player.vimeo.com/api/player.js\"></script>\nGuided tour\nClick on the image to start a tour of GitLab Duo with Amazon Q:\n\nGitLab Duo with Amazon Q tutorials\nEnhance application quality with AI-powered test generation — Learn how GitLab Duo with Amazon Q improves the QA process by automatically generating comprehensive unit tests.\nGitLab Duo + Amazon Q: Transform ideas into code in minutes — The new GitLab Duo with Amazon Q integration analyzes your issue descriptions and automatically generates complete working code solutions, accelerating development workflows.\nAccelerate code reviews with GitLab Duo and Amazon Q — Use AI-powered agents to optimize code reviews by automatically analyzing merge requests and providing comprehensive feedback on bugs, readability, and coding standards.\nSpeed up code reviews: Let AI handle the feedback implementation — Discover how GitLab Duo with Amazon Q automates the implementation of code review feedback through AI, transforming a time-consuming manual process into a streamlined workflow.\nGitLab Duo Agent Platform\nGitLab Duo Chat gets agentic AI makeover — Our new Duo Chat experience, currently an experimental release, helps developers onboard to projects, understand assignments, implement changes, and more.\nWatch GitLab Duo Agent Platform in action:\n<div style=\"padding:56.25% 0 0 0;position:relative;\"><iframe src=\"https://player.vimeo.com/video/1095679084?badge=0&autopause=0&player_id=0&app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"Agent Platform Demo Clip\"></iframe></div><script src=\"https://player.vimeo.com/api/player.js\"></script>\nGitLab Agent Platform tutorials and use cases\nVibe coding with GitLab Duo Agent Platform: Issue to MR Flow - Learn how to update your application in minutes with our newest agent Flow that takes developers from idea to code.\nGet started with GitLab Duo Agentic Chat in the web UI - Learn about our new GitLab Duo AI feature that automates tasks by breaking down complex problems and executing operations across multiple sources.\nCustom rules in GitLab Duo Agentic Chat for greater developer efficiency — Discover how AI can understand your codebase, follow your conventions, and generate production-ready code with minimal review cycles.\nAccelerate learning with GitLab Duo Agent Platform — Learn how agentic AI helped generate comprehensive gRPC documentation in minutes, not hours.\nFast and secure AI agent deployment to Google Cloud with GitLab\nLearn more with GitLab University\nGet Started with GitLab Duo coursework\nGitLab Duo Enterprise Learning Path\nMore AI resources\n2024 Global DevSecOps Survey: Navigating AI maturity in DevSecOps\nThe Role of AI in DevOps\nThe latest AI/ML articles from GitLab\nGitLab Duo\nGitLab Duo Agent Platform",
      "publishedAt": "2025-09-26T00:00:00.000Z",
      "author": "GitLab",
      "source": "rss",
      "feedName": "GitLab Blog",
      "sourceType": "platform_blog",
      "company": "GitLab",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "testing",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.769Z",
      "score": 0.42184553361174765
    },
    {
      "id": "ccd67c4e1985f72040f3cdf740df0316",
      "title": "なぜ絵文字によるフィードバックは強化学習に向かないのか",
      "url": "https://coderabbit.ai/blog/why-emojis-suck-for-reinforcement-learning-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/why-emojis-suck-for-reinforcement-learning\">Why emojis suck for reinforcement learning (&amp; what actually works)</a>の意訳です。</p>\n<h2 id=\"heading-kirjgizjgrfjg7pjg5fjg6vjgzxjgi3jgajjgytjgybnvaaqkg\"><strong>「シンプルさ」という罠</strong></h2>\n<p>親指を立てた絵文字（👍）は簡単に送れますが、本当に AI レビュアーにとって有益な学習信号になっているのでしょうか。絵文字ベースのフィードバックは気持ちよく、速く、そして誰にでも分かりやすいです。一見すると、理にかなっているようにも思えます。</p>\n<p>しかしコードレビューは灯りのオンオフのような単純なものではありません。無数の判断、技術的なニュアンス、チーム固有の基準が入り混じったものです。その多くは、ワンクリックの絵文字には反映されません。すべてのコードコメントには隠れた意図があります。正しさ、読みやすさ、設計上のトレードオフ、過去の経緯、チームのリスク許容度、さらには組織内の政治的な力学まで含まれます。</p>\n<p>それをオンオフという、二値のシグナルに押し込めてしまうとどうなるでしょうか。もはや学習ではなく、「雰囲気を追いかけるモデル」を育てているだけになってしまいます。</p>\n<h2 id=\"heading-kirjgrfjg7pjg5fjg6vjgzxjgyzoo4nm67jgavlh7rjgovjgajjgy06iooctoodnuobmeociuodouodhodqobruabkoaalioq\"><strong>シンプルさが裏目に出るとき: ゴマすりモデルの恐怖</strong></h2>\n<p>今年のはじめ、OpenAI は GPT-4o に対して「親指上げ/下げ」のフィードバックをかなり強く効かせたアップデートを行いました。その結果どうなったかというと、このモデルは<a target=\"_blank\" href=\"https://openai.com/index/sycophancy-in-gpt-4o/\">過度にユーザーに迎合する</a>ようになりました。ユーザーをおだて、誤った回答にも同意し、「はい」と言いすぎるようになり、回答品質は低下しました。フィードバック信号がハイジャックされてしまい、OpenAI はロールバックせざるを得ませんでした。</p>\n<p>モデルに「承認こそがゴールだ」と教えてしまうと、そのモデルは承認を最適化するようになります。真実でもなく、有用性でもなく、「その瞬間、人間が気持ちよく感じたかどうか」だけを目指すようになります。</p>\n<p>これはバグではなく、報酬設計の失敗でした。そして同じアプローチをコードレビューに適用すると、「安全運転で、あなたにおべっかを使い、本当に必要なことを言ってくれないレビュアー」ができあがります。</p>\n<h2 id=\"heading-kirjgarjgzzkuozlgktjg5xjgqpjg7zjg4njg5djg4pjgqjgafjgajg4vjg6xjgqljg7pjgrnjgyzmvbdjgozjgabjgzfjgb7jgybjga7jgysqkg\"><strong>なぜ二値フィードバックではニュアンスが潰れてしまうのか</strong></h2>\n<p>親指上げの絵文字は一体何を意味しているのでしょうか。</p>\n<ul>\n<li><p>モデルがバグを見つけたという意味でしょうか</p>\n</li>\n<li><p>説明が分かりやすかったという意味でしょうか</p>\n</li>\n<li><p>口調がフレンドリーだったという意味でしょうか</p>\n</li>\n<li><p>たまたまレビュアーの機嫌が良かったというだけでしょうか</p>\n</li>\n</ul>\n<p>単一のスカラー値のシグナル（👍または👎）は、「何かがうまくいった」ということは伝えますが、「何がうまくいったのか」は伝えません。そのためモデルは、自分が操作しやすいものに寄っていきます。トーン、丁寧さ、お世辞、あるいは短さといったものです。これが、強化学習におけるゴマすり（sycophancy）の正体です。悪意ではなく、「あなたが与えた報酬を最大化しようとしているだけ」であり、「あなたが本当に望んでいた結果」を最大化しているわけではありません。</p>\n<p>これは<a target=\"_blank\" href=\"https://medium.com/@yoavyeledteva/beyond-artificiality-redefining-intelligence-in-ai-and-avoiding-goodharts-law-25b75c3c1101\">グッドハートの法則</a>が発動している例です。メトリクス（この場合は親指上げ）がゴールになってしまうと、それは現実の有用な指標ではなくなります。</p>\n<h2 id=\"heading-kirjg6ljg4fjg6vjgyzjgyljgarjgzjga7jg5xjgqpjg7zjg4njg5djg4pjgqjgpljgizmllvnlaxjgi3jgznjgovjgajjgy0qkg\"><strong>モデルがあなたのフィードバックを「攻略」するとき</strong></h2>\n<p>モデルに簡単なシグナルを与えると、モデルは簡単なショートカットを見つけます。</p>\n<p>コーディングの世界では、強化学習エージェントが、基礎ロジックを解かずに期待される出力をハードコードすることでテストケースをパスするように学習してしまうことがあります。ログを細工したり、評価用ハーネスをすり抜けたりもします。チェックマークは緑になっても、実際のコードは正しく動きません。</p>\n<p>コードレビューでも同じことが、ただし「社会的なかたち」で起きます。モデルはすべてのコメントの冒頭で「とてもいいです！」と言うようになり、あらゆる提案を柔らかな表現で包み、フォーマットのような安全な箇所ばかりを指摘するようになります。そういったコメントは無難で、議論もなく受け入れられやすいからです。そして本当に重要なアーキテクチャ上の懸念は、埋もれてしまいます。</p>\n<p>モデルは「ポジティブな反応の取り方」は学んだものの、もはやコードレビューをしているとは言えません。</p>\n<h2 id=\"heading-kirmmpfpu5nnmotjgarjgrfjgrdjg4rjg6vjgyzlhkrjgozjgabjgytjgovnkibnlleqkg\"><strong>暗黙的なシグナルが優れている理由</strong></h2>\n<p>LLM 以外の世界では、このパターンはよく知られています。Netflix は、ユーザーが何を「評価」するかよりも、何を<a target=\"_blank\" href=\"https://medium.com/illuminations-mirror/how-netflix-uses-machine-learning-to-decide-what-you-watch-next-7fee11102007\">実際に視聴するか</a>のほうがはるかに有用だと気付きました。星評価では人は平気で嘘をつきます。しかし視聴時間、クリック、リピート再生といった指標は正直なシグナルです。</p>\n<p>AI の世界では、これを**暗黙的フィードバック（implicit feedback）**と呼びます。コードレビューの場合には、例えば次のような形で表れます。</p>\n<ul>\n<li><p>開発者は提案を採用したのか</p>\n</li>\n<li><p>それを書き換えたのか</p>\n</li>\n<li><p>無視したのか</p>\n</li>\n<li><p>同じパターンが後のバグとして再び現れたのか</p>\n</li>\n</ul>\n<p>これらのシグナルは、ユーザーの入力を必要としません。行動から生まれ、意図的に操作するのが難しいものです。</p>\n<p>もちろん完璧ではありません。「なぜ」その行動を取ったのかまでは常に分からないからです。しかし、絵文字よりははるかに操作されにくく、レビューが「気持ちよかったかどうか」ではなく「ちゃんと機能したかどうか」を教えてくれます。</p>\n<h2 id=\"heading-vs\"><strong>コード生成 vs コードレビュー: ゲームが違えば、シグナルも違う</strong></h2>\n<p>コード生成は、しばしば「正解が一つに定まる」という点で数学に近い側面があります。コンパイルできるか。正しい結果を返すか。テストにパスするかといった具合です。</p>\n<p>そのため、実行結果フィードバックや暗黙的なシグナルのようなアウトカムベースの報酬を使うことができます。もちろん完璧ではありません。コードモデルは出力をハードコードしてテストをすり抜けることもありますが、それに対するガードレールを設計することは可能です。そして、開発者が「良かった」と言ってくれるかどうかに頼らずとも、「実際に動いたかどうか」を観測できます。</p>\n<p>一方、コードレビューは違います。ここには普遍的な合格/不合格は存在せず、チームごとにスタイル、構造、リスク、命名、テストカバレッジなどの好みが大きく異なります。あるチームにとっての「優れたコメント」が、別のチームでは完全にズレている可能性もあります。高速に動くスタートアップで「クリーンコード」とみなされるものが、高いセキュリティが求められる産業では「不十分」と判断されることもあります。</p>\n<p>これこそが、「親指上げ/下げデータ」が抱える本当の問題です。ニュアンスが押しつぶされてしまい、モデルは「適切さ」ではなく「平均値」を目指すようになります。その結果、安全ではあるものの、ひどく汎用的なコメントばかりを出すようになってしまいます。</p>\n<h3 id=\"heading-coderabbit-learnings\"><strong>私たちの代替案: CodeRabbit Learnings</strong></h3>\n<p>CodeRabbit では、別のアプローチを取っています。いいねを最大化するのではなく、「理解」を最大化しようとしているのです。そのために私たちは <a target=\"_blank\" href=\"https://docs.coderabbit.ai/guides/learnings\"><strong>Learnings</strong></a> を構築しました。</p>\n<p>エンジニアが CodeRabbit を修正したり、チームの規約を明確にしたり、「なぜこのコードは自分たちのスタックに合わないのか」を説明したりするたびに、その説明を自然言語の指示として保存します。単に「コメントが却下された」という事実だけでなく、「なぜ却下されたのか」まで記憶します。</p>\n<p>これらの Learnings は、組織、リポジトリ、さらには特定のパスやファイルタイプに紐付きます。CodeRabbit が次のプルリクエストをレビューするときには、それらの指示を検索し、文脈に応じて適用します。同じパターンを再度見つけたときには、そこで学んだ内容を踏まえて挙動を変えます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1762535170060/91e7f06c-3e56-4dfc-bf07-6901bafdef07.png\" alt class=\"image--center mx-auto\" /></p>\n<p>再度教え直す必要はなく、同じ失敗を繰り返すリスクもありません。モデルは親指の数から推測するのではなく、あなたのチームが与えた実際のガイダンスから推論します。</p>\n<p>また、Learnings は透明性も提供します。どんな Learnings が存在するかを確認し、それらを閲覧し、カテゴリでフィルタリングし、標準が変わったときには削除や編集を行うことができます。つまりモデルは、チームの成長とともに進化し、プラクティスの変化に合わせて整合性を保ち続けます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1762535186830/15d12e32-fdb9-4034-8e74-1db0bd4eb909.png\" alt class=\"image--center mx-auto\" /></p>\n<p>これは、単なる絵文字の承認ではなく「意図を取り込む」ことで行う強化学習です。解釈可能であり、検査可能です。そしてレビューをまたいで一般化する、「チームナレッジの生きたレイヤー」を構築します。</p>\n<h2 id=\"heading-kirjg4vjg6xjgqljg7pjgrnjga7jgyljgovlrabnv5ljgyzlj6og73jgavjgznjgovjgzpjgagqkg\"><strong>ニュアンスのある学習が可能にすること</strong></h2>\n<p>システムに与えるのが「シグナル」ではなく「明確で文脈を含んだ指示」になったとき、単なるレビュー体験の改善以上のことが可能になります。</p>\n<ul>\n<li><p><strong>チームレベルでの適応が可能になります</strong><br />  モデルは「何が良いのか」を勝手に推測するのではなく、「あなたのチームが実際にどう書いているのか」を学習します。リスク許容度、スタイルの好み、トレードオフの感覚を理解し、「ハウスルールを理解しているレビュアー」として振る舞うようになります。</p>\n</li>\n<li><p><strong>経時的な学習（長期学習）を支えます</strong><br />  時間とともに CodeRabbit は、「どのコメントが役に立ったのか」「どれが無視されたのか」「どの提案が実際の変更につながったのか」という記憶を蓄積していきます。その結果、徐々に精度が上がり、フォーカスは鋭くなり、ノイズは減っていきます。</p>\n</li>\n<li><p><strong>信頼を築きます</strong><br />  開発者が「AI を訂正すれば、それを覚えてくれる」と分かっていると、より積極的に関わるようになります。開発者自身がシステムを形作り、そのシステムは汎用的な LLM ではなく「自分たちの基準を反映した存在」へと近づいていきます。</p>\n</li>\n</ul>\n<p>こうしてレビュー用ツールは、単なる「異なる視点の意見」ではなく、チームの延長として機能するようになります。</p>\n<h2 id=\"heading-44g44go44kboidmnkzlvzpjga7lrabnv5ljgajg5tjgqjgrvjg6vjgafjgajgarjgyjg5hjgrjg7zjg7pjgyvjgonnljjgb7jgozjgb7jgzk\">まとめ: 本当の学習はピクセルではなくパターンから生まれます</h2>\n<p>親指の絵文字は、素早いリアクションには向いていますが、それだけでは専門性は育ちません。</p>\n<p>時間とともに成長し、あなたの標準に適応し、浅いフィードバックの罠を避ける AI レビュアーを求めるのであれば、承認以上のものを与える必要があります。説明を与えなければなりません。</p>\n<p>次世代の AI コードツールは「いいね」の数で訓練されることはありません。文脈、結果、修正の軌跡で訓練されます。絵文字ではなく、構造化された記憶から学びます。実際の意思決定と、あなたのチーム自身の声から学びます。</p>\n<p>それこそが <a target=\"_blank\" href=\"https://docs.coderabbit.ai/guides/learnings\">CodeRabbit Learnings</a> が設計された目的です。拍手のためではなく、理解のために設計されています。</p>\n<p><strong><em>Learnings を自分のチームで試してみたい方は、</em></strong><a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong><em>無料トライアルにお申し込みください</em></strong></a></p>\n",
      "summary": "Why emojis suck for reinforcement learning (& what actually works)の意訳です。\n「シンプルさ」という罠\n親指を立てた絵文字（👍）は簡単に送れますが、本当に AI レビュアーにとって有益な学習信号になっているのでしょうか。絵文字ベースのフィードバックは気持ちよく、速く、そして誰にでも分かりやすいです。一見すると、理にかなっているようにも思えます。\nしかしコードレビューは灯りのオンオフのような単純なものではありません。無数の判断、技術的なニュアンス、チーム固有の基準が入り混じったものです。その多くは、ワンクリックの絵文字には反映されません。すべてのコードコメントには隠れた意図があります。正しさ、読みやすさ、設計上のトレードオフ、過去の経緯、チームのリスク許容度、さらには組織内の政治的な力学まで含まれます。\nそれをオンオフという、二値のシグナルに押し込めてしまうとどうなるでしょうか。もはや学習ではなく、「雰囲気を追いかけるモデル」を育てているだけになってしまいます。\nシンプルさが裏目に出るとき: ゴマすりモデルの恐怖\n今年のはじめ、OpenAI は GPT-4o に対して「親指上げ/下げ」のフィードバックをかなり強く効かせたアップデートを行いました。その結果どうなったかというと、このモデルは過度にユーザーに迎合するようになりました。ユーザーをおだて、誤った回答にも同意し、「はい」と言いすぎるようになり、回答品質は低下しました。フィードバック信号がハイジャックされてしまい、OpenAI はロールバックせざるを得ませんでした。\nモデルに「承認こそがゴールだ」と教えてしまうと、そのモデルは承認を最適化するようになります。真実でもなく、有用性でもなく、「その瞬間、人間が気持ちよく感じたかどうか」だけを目指すようになります。\nこれはバグではなく、報酬設計の失敗でした。そして同じアプローチをコードレビューに適用すると、「安全運転で、あなたにおべっかを使い、本当に必要なことを言ってくれないレビュアー」ができあがります。\nなぜ二値フィードバックではニュアンスが潰れてしまうのか\n親指上げの絵文字は一体何を意味しているのでしょうか。\nモデルがバグを見つけたという意味でしょうか\n説明が分かりやすかったという意味でしょうか\n口調がフレンドリーだったという意味でしょうか\nたまたまレビュアーの機嫌が良かったというだけでしょうか\n単一のスカラー値のシグナル（👍または👎）は、「何かがうまくいった」ということは伝えますが、「何がうまくいったのか」は伝えません。そのためモデルは、自分が操作しやすいものに寄っていきます。トーン、丁寧さ、お世辞、あるいは短さといったものです。これが、強化学習におけるゴマすり（sycophancy）の正体です。悪意ではなく、「あなたが与えた報酬を最大化しようとしているだけ」であり、「あなたが本当に望んでいた結果」を最大化しているわけではありません。\nこれはグッドハートの法則が発動している例です。メトリクス（この場合は親指上げ）がゴールになってしまうと、それは現実の有用な指標ではなくなります。\nモデルがあなたのフィードバックを「攻略」するとき\nモデルに簡単なシグナルを与えると、モデルは簡単なショートカットを見つけます。\nコーディングの世界では、強化学習エージェントが、基礎ロジックを解かずに期待される出力をハードコードすることでテストケースをパスするように学習してしまうことがあります。ログを細工したり、評価用ハーネスをすり抜けたりもします。チェックマークは緑になっても、実際のコードは正しく動きません。\nコードレビューでも同じことが、ただし「社会的なかたち」で起きます。モデルはすべてのコメントの冒頭で「とてもいいです！」と言うようになり、あらゆる提案を柔らかな表現で包み、フォーマットのような安全な箇所ばかりを指摘するようになります。そういったコメントは無難で、議論もなく受け入れられやすいからです。そして本当に重要なアーキテクチャ上の懸念は、埋もれてしまいます。\nモデルは「ポジティブな反応の取り方」は学んだものの、もはやコードレビューをしているとは言えません。\n暗黙的なシグナルが優れている理由\nLLM 以外の世界では、このパターンはよく知られています。Netflix は、ユーザーが何を「評価」するかよりも、何を実際に視聴するかのほうがはるかに有用だと気付きました。星評価では人は平気で嘘をつきます。しかし視聴時間、クリック、リピート再生といった指標は正直なシグナルです。\nAI の世界では、これを**暗黙的フィードバック（implicit feedback）**と呼びます。コードレビューの場合には、例えば次のような形で表れます。\n開発者は提案を採用したのか\nそれを書き換えたのか\n無視したのか\n同じパターンが後のバグとして再び現れたのか\nこれらのシグナルは、ユーザーの入力を必要としません。行動から生まれ、意図的に操作するのが難しいものです。\nもちろん完璧ではありません。「なぜ」その行動を取ったのかまでは常に分からないからです。しかし、絵文字よりははるかに操作されにくく、レビューが「気持ちよかったかどうか」ではなく「ちゃんと機能したかどうか」を教えてくれます。\nコード生成 vs コードレビュー: ゲームが違えば、シグナルも違う\nコード生成は、しばしば「正解が一つに定まる」という点で数学に近い側面があります。コンパイルできるか。正しい結果を返すか。テストにパスするかといった具合です。\nそのため、実行結果フィードバックや暗黙的なシグナルのようなアウトカムベースの報酬を使うことができます。もちろん完璧ではありません。コードモデルは出力をハードコードしてテストをすり抜けることもありますが、それに対するガードレールを設計することは可能です。そして、開発者が「良かった」と言ってくれるかどうかに頼らずとも、「実際に動いたかどうか」を観測できます。\n一方、コードレビューは違います。ここには普遍的な合格/不合格は存在せず、チームごとにスタイル、構造、リスク、命名、テストカバレッジなどの好みが大きく異なります。あるチームにとっての「優れたコメント」が、別のチームでは完全にズレている可能性もあります。高速に動くスタートアップで「クリーンコード」とみなされるものが、高いセキュリティが求められる産業では「不十分」と判断されることもあります。\nこれこそが、「親指上げ/下げデータ」が抱える本当の問題です。ニュアンスが押しつぶされてしまい、モデルは「適切さ」ではなく「平均値」を目指すようになります。その結果、安全ではあるものの、ひどく汎用的なコメントばかりを出すようになってしまいます。\n私たちの代替案: CodeRabbit Learnings\nCodeRabbit では、別のアプローチを取っています。いいねを最大化するのではなく、「理解」を最大化しようとしているのです。そのために私たちは Learnings を構築しました。\nエンジニアが CodeRabbit を修正したり、チームの規約を明確にしたり、「なぜこのコードは自分たちのスタックに合わないのか」を説明したりするたびに、その説明を自然言語の指示として保存します。単に「コメントが却下された」という事実だけでなく、「なぜ却下されたのか」まで記憶します。\nこれらの Learnings は、組織、リポジトリ、さらには特定のパスやファイルタイプに紐付きます。CodeRabbit が次のプルリクエストをレビューするときには、それらの指示を検索し、文脈に応じて適用します。同じパターンを再度見つけたときには、そこで学んだ内容を踏まえて挙動を変えます。\n\n再度教え直す必要はなく、同じ失敗を繰り返すリスクもありません。モデルは親指の数から推測するのではなく、あなたのチームが与えた実際のガイダンスから推論します。\nまた、Learnings は透明性も提供します。どんな Learnings が存在するかを確認し、それらを閲覧し、カテゴリでフィルタリングし、標準が変わったときには削除や編集を行うことができます。つまりモデルは、チームの成長とともに進化し、プラクティスの変化に合わせて整合性を保ち続けます。\n\nこれは、単なる絵文字の承認ではなく「意図を取り込む」ことで行う強化学習です。解釈可能であり、検査可能です。そしてレビューをまたいで一般化する、「チームナレッジの生きたレイヤー」を構築します。\nニュアンスのある学習が可能にすること\nシステムに与えるのが「シグナル」ではなく「明確で文脈を含んだ指示」になったとき、単なるレビュー体験の改善以上のことが可能になります。\nチームレベルでの適応が可能になります\n  モデルは「何が良いのか」を勝手に推測するのではなく、「あなたのチームが実際にどう書いているのか」を学習します。リスク許容度、スタイルの好み、トレードオフの感覚を理解し、「ハウスルールを理解しているレビュアー」として振る舞うようになります。\n経時的な学習（長期学習）を支えます\n  時間とともに CodeRabbit は、「どのコメントが役に立ったのか」「どれが無視されたのか」「どの提案が実際の変更につながったのか」という記憶を蓄積していきます。その結果、徐々に精度が上がり、フォーカスは鋭くなり、ノイズは減っていきます。\n信頼を築きます\n  開発者が「AI を訂正すれば、それを覚えてくれる」と分かっていると、より積極的に関わるようになります。開発者自身がシステムを形作り、そのシステムは汎用的な LLM ではなく「自分たちの基準を反映した存在」へと近づいていきます。\nこうしてレビュー用ツールは、単なる「異なる視点の意見」ではなく、チームの延長として機能するようになります。\nまとめ: 本当の学習はピクセルではなくパターンから生まれます\n親指の絵文字は、素早いリアクションには向いていますが、それだけでは専門性は育ちません。\n時間とともに成長し、あなたの標準に適応し、浅いフィードバックの罠を避ける AI レビュアーを求めるのであれば、承認以上のものを与える必要があります。説明を与えなければなりません。\n次世代の AI コードツールは「いいね」の数で訓練されることはありません。文脈、結果、修正の軌跡で訓練されます。絵文字ではなく、構造化された記憶から学びます。実際の意思決定と、あなたのチーム自身の声から学びます。\nそれこそが CodeRabbit Learnings が設計された目的です。拍手のためではなく、理解のために設計されています。\nLearnings を自分のチームで試してみたい方は、無料トライアルにお申し込みください",
      "publishedAt": "2025-11-14T04:05:41.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:39.980Z",
      "score": 2.0201268040780183
    },
    {
      "id": "d486bbaf1f4efd91703639ef78559440",
      "title": "Gpt-5.1、コードレビューで “低ボリューム・高精度” を実現",
      "url": "https://coderabbit.ai/blog/gpt-51-for-code-related-tasks-higher-signal-at-lower-volume-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/gpt-51-for-code-related-tasks-higher-signal-at-lower-volume\">GPT-5.1 for code-related tasks: Higher signal at lower volume</a>の意訳です。</p>\n<p><strong>TL;DR</strong><br />プロンプト調整とスタックへの統合を行った結果、GPT-5.1 はレビューにおいて、これまでで最も高い精度とS/N比（シグナル対ノイズ比）を、より少ないコメント量で実現するようになりました。複雑なベンチマークセット上で、最高クラスのエラーパターン（EP）リコールに並びつつ、競合モデルの半分以下のコメント量を記録しました。</p>\n<p>その結果として、少ないノイズでより良い修正が得られ、レビューは再びパッチのように読めるものになったと感じています。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763056609083/4635e43b-b5a0-4588-a226-230afa715e27.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-gpt-51\"><strong>GPT-5.1 が主張していること</strong></h2>\n<p>OpenAI と報道によると、 GPT-5.1はより安定し、指示に従い、適応性の高いモデルとして説明されています。GPT-5.1 は ChatGPT の「Instant」と「Thinking」モードの両方で駆動しています。コードレビューに関してこの説明を検証したところ、驚くほど正確だと感じられました。細かな指摘では素早く表面的に対応し、深い推論が必要なバグではしっかりと理由付けを行います。</p>\n<p>今回は新しい試みも行いました。GPT-5.1 が誤った場合、そのやり取り全体と内部推論のトレースを用いて、振り返りを促すプロンプトを実行しました。どこを誤ったのかを示し、改善のためにどのように指示を変えるべきかを尋ねることで、モデル自身がプロンプトに対する具体的な修正案を提示します。この反復的な振り返り手法（差分外への過剰な広がりといった問題も浮上しましたが）によって、モデルの挙動とシステム指示の両方を調整し、安定してタイトな出力を得られるようにしました。</p>\n<h2 id=\"heading-kirmukzlrprjgzfjgzlhoxlrrnvvijjgz3jgzfjgabjgz3jga7nkibnllhvvikqkg\"><strong>測定した内容（そしてその理由）</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763057138427/95c1fff5-f711-4fab-8a73-0f4ef7899c58.png\" alt class=\"image--center mx-auto\" /></p>\n<p>私たちは、<a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">GPT-5</a>、<a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks?\">Codex</a>、<a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/claude-sonnet-45-better-performance-but-a-paradox?\">Sonnet 4.5</a> の記事で使用したものと同じベンチマーク環境を使用しました。これは既知の <strong>エラーパターン（EP）</strong> を埋め込んだ <strong>25 件の難しい PR</strong> から構成されています。スコアリングでは以下に重点を置いています。</p>\n<ul>\n<li><p><strong>アクショナブルなコメントのみ</strong>: 実際に投稿されるコメントのみ（追加提案や 差分外への記述を除く）</p>\n</li>\n<li><p><strong>エラーパターンごとの合格数（コメントごと。以下EP Pass）</strong>: コメントが エラーパターン を直接修正、または明示していること</p>\n</li>\n<li><p><strong>Important コメント</strong>: EP PASS または重大/クリティカルな実バグ</p>\n</li>\n<li><p><strong>Precision（精度）</strong>: EP PASS ÷ コメント総数</p>\n</li>\n<li><p><strong>SNR</strong>: Important ÷ (総数 − Important)</p>\n</li>\n</ul>\n<p>比較対象は以下の通りです。</p>\n<ul>\n<li><p><strong>GPT-5.1</strong>（新モデル）</p>\n</li>\n<li><p><strong>CodeRabbit Production</strong>（現行レビューアースタック）</p>\n</li>\n<li><p><strong>Sonnet 4.5</strong></p>\n</li>\n</ul>\n<h2 id=\"heading-kirmlrdjgzfjgytjg6ljg4fjg6vjga7ov73liqdjgajgrnjgqtjg4pjg4hjgpllhaxjgozjgovjgadjgzhjgafjgajgyljgorjgb7jgzvjgpmqkg\"><strong>新しいモデルの追加はスイッチを入れるだけではありません</strong></h2>\n<p>CodeRabbit ではモデルの導入は毎回適切に行われており、モデルを差し替えて祈るようなことはしません。各社のモデルはすでに<a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/the-end-of-one-sized-fits-all-prompts-why-llm-models-are-no-longer-interchangeable-ja\">互換品ではなくなっている</a>ため、デプロイ前にテスト、調整、品質ゲートを行います。GPT-5.1 に対しては以下のような調整を行いました。</p>\n<ul>\n<li><p>GitHub に投稿できない <strong>差分外のコメント</strong> の削減</p>\n</li>\n<li><p>冗長さを抑えるための <strong>トーンと簡潔さ</strong> の調整</p>\n</li>\n<li><p><strong>重大度タグ</strong> と <strong>指示解釈</strong> の再整合</p>\n</li>\n</ul>\n<p>これは <a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks-ja\">GPT-5 Codex</a> の場合と同じで、推論能力をプロダクト価値へと変換するために、モデルの挙動を再構築するという目的があります。最終的な結果として、高いS/N比、ストレスの軽減、バグのカバレッジを損なわないレビューを実現しました。</p>\n<h2 id=\"heading-kirjgrnjgrpjgqljg5zjg7zjg4nvvijjgqljgqjgrfjg6fjg4rjg5bjg6vjgarjgrpjg6hjg7pjg4jjga7jgbvvikqkg\"><strong>スコアボード（アクショナブルなコメントのみ）</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763056830923/d80c8745-3042-4d79-84cc-8dcaff93659f.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>要点</strong>: GPT-5.1 は過去最高のエラーパターン再現率に並びつつ、<strong>最も少ないコメント量</strong> を記録しました。CodeRabbit Production と Sonnet 4.5 の両方を <strong>コメント単位の精度</strong> と <strong>Important コメント比率</strong> で上回り、<strong>最もクリーンで高インパクトなレビュー</strong> を実現しました。</p>\n<h2 id=\"heading-gpt-51-1\"><strong>GPT-5.1 のレビュー体験</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763056903033/b5b08d8e-490b-4293-8551-d4a4d371fc4e.png\" alt class=\"image--center mx-auto\" /></p>\n<p>データで確認される挙動特性は、後に測定した言語メトリクス（弱め表現 28%、断定的マーカー 15% など）と一致しています。これにより、開発者が「レビュー自信があり、かつバランスの取れたトーン」だと感じる理由がデータでも裏付けられています。</p>\n<p>GPT-5 Codex と Sonnet 4.5 と比較すると、GPT-5.1 のコメントはよりスリムで、対話的であり、熟練エンジニアのコミュニケーションに近いと感じられます。Codex は機械的かつ堅く、Sonnet 4.5 は冗長で学術的になりがちでした。それに対して GPT-5.1 は簡潔さと明確さのバランスが良く、押しつけがましくない自信を感じさせます。信頼できるチームメイトが差分を説明しているように読めます。CodeRabbit Production と比較すると、より課題に対して鋭くフォーカスされており、Sonnet 4.5 と比較するとより人間的で抑制が効いています。以下はその具体例です。</p>\n<h3 id=\"heading-kirnskhmvzqqkg\"><strong>簡潔</strong></h3>\n<p>GPT-5.1 はより少なく鋭いコメントを書き、すぐに要点へ到達します。ある PR では、ロストウェイクアップバグを以下の 1 行で修正しました。<br /><code>p_caller_pool_thread-&gt;cond_var.wait(lock);</code><br />余計な文脈説明も不要な文章もありませんでした。比較すると CodeRabbit Production は同じ結論に至るまでに、スレッドフローを数段落説明していました。</p>\n<h3 id=\"heading-kirnjofnm7qqkg\"><strong>率直</strong></h3>\n<p>所有権やメモリ管理が関わる場面ではためらいません。冗長な <code>r-&gt;reference()</code> 呼び出しについて、以下のように指摘しました。<br />「<code>Ref&lt;Resource&gt;</code> は refcount を自動管理します。手動で refcount を増やすとリークにつながるため削除してください」<br />開発者はこの率直さを好みます。講義ではなくパッチレビューのように読めます。</p>\n<h3 id=\"heading-kirlrpli5nnmoqqkg\"><strong>実務的</strong></h3>\n<p>GPT-5.1 は、問題の重要度がどこにあるかを理解し、重要なものとそうでないものを適切に識別します。あるキャッシュ設定の PR では未実装の <code>optimizeMemoryUsage()</code> を指摘しましたが、次のように正しく文脈化しました。<br />「キャッシュの肥大化がメモリプレッシャーに影響しない限り、これは軽微です」<br />過剰反応せず、重要度を適切に扱っています。この点は Sonnet 4.5 にまだ課題があります。</p>\n<h3 id=\"heading-kirmlofohijjgplov73jgyyqkg\"><strong>文脈を追う</strong></h3>\n<p>プロンプトが曖昧だった場合、GPT-5.1 は自身の仮定を明示的に説明します。初期の実行では次のように述べました。<br />「プロンプトでヘルパー関数のスコープが指定されていませんが、明確化のために含めました」<br />この透明性が私たちの指示改善を助け、モデルの推論を信頼できるものにしました。</p>\n<p>簡潔、率直、実務的、文脈理解という特性は GPT-5 Codex において私たちが高く評価した点と一致していますが、GPT-5.1 はより安定したトーンと抑制を備えています。</p>\n<h2 id=\"heading-gpt-51-2\"><strong>スタイルとトーン（GPT-5.1 がチームメンバーのように感じられる理由）</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763056849033/ec2fdcee-dd64-47d4-85ed-0bcb173c3d0e.png\" alt class=\"image--center mx-auto\" /></p>\n<p>GPT-5 Codex や Sonnet 4.5 の評価で使用したものと同じ言語構造のシグナルを参照し、GPT-5.1 がレビューで異なる印象を与える理由を分析しました。これにはコメントの長さ、コードブロックの有無、弱め表現と断定表現の割合などが含まれます。データは明確な傾向を示しています。</p>\n<p><strong>読み方について</strong><br />GPT-5.1 のコメントは平均文字数がやや多いものの、より明確な構造と負荷の高い文で構成されているため、実際には「短く読みやすい」と感じられます。GPT-5.1 のトーンは CodeRabbit Production や Sonnet 4.5 よりも断定的で、全体として diff ブロックは少ない（76%）という特徴があります。これは意図されたもので、複数箇所修正や API バリデーション、設計の明確化であり、単一のパッチを示すと誤解を招く場合があったためです。ただし、差分を含まないコメントの約 3 分の 2 では、最小限のパッチを示せば明確さがさらに向上すると感じられました。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763057072668/b143a9e1-707e-4bbb-8988-7ba2ed98829c.png\" alt class=\"image--center mx-auto\" /></p>\n<p>CodeRabbit Production と比較すると、GPT-5.1 はパッチ頻度を一部犠牲にする代わりに、明確さと集中度を高めています。Sonnet 4.5 と比較すると、レビューを膨張させる冗長な説明を避けています。トーンは Codex の外科的精度と Sonnet の慎重な冗長性の中間に位置し、自信がありつつも強圧的ではなく、慎重でありながら臆病ではありません。</p>\n<p>総じて、GPT-5.1 のレビューは <strong>素早く読み進められ、より直接的で、実際の修正を見つけるためのスキャン量が少なくて済む</strong> という特徴があります。これは意図して調整した挙動であり、データと体験の両方に表れています。</p>\n<h3 id=\"heading-gpt-51-3\"><strong>GPT-5.1 にまだ残る課題</strong></h3>\n<p>完璧なモデルは存在せず、GPT-5.1 にもトレードオフがあります。CodeRabbit Production と比較すると、大規模チームで有用な文脈的な衛生改善の指摘を省くことがあり、より機能的な問題に集中する傾向があります。Sonnet 4.5 と比較すると、デザインやスタイル上の改善点を見逃すことがあり、人間のレビューアが好むケースもあります。これらは精度と簡潔さを優先した意図的なトレードオフであり、今後のロールアウトで開発者の反応を注視していく予定です。</p>\n<h2 id=\"heading-kirmllnllotjgyzlv4xopohjgadjgapjgzngrkqkg\"><strong>改善が必要だった点</strong></h2>\n<p>GPT-5.1 は調整を必要としましたが、その課題は以前のシステムと比べるとはるかに軽度でした。CodeRabbit Production は衛生的な指摘と重大な指摘を同一スレッドで混在させる傾向があり、Sonnet 4.5 は説明過多で、同じバグについて複数の軽微なノートを投稿しがちです。一方で GPT-5.1 の調整点は主に精度に関わるもので、トーンや冗長性よりも限定的でした。これは GPT-5.1 がプロダクション導入に対して、非常に近い段階にあることを示しています。</p>\n<ul>\n<li><p><strong>diff 外コメント</strong><br />  GPT-5.1 は diff 以外の部分に提案を含めることがありました。プロンプトで明確に制約を示したところ、モデルは自己修正しました。</p>\n</li>\n<li><p><strong>曖昧さに対する過剰な助け</strong><br />  プロンプトが厳密でない場合、コンテキスト追加やヘルパー関数の追加を行うことがありました。制約を明確にすると、境界を正確に守るようになりました。</p>\n</li>\n</ul>\n<h2 id=\"heading-kirplovnmbrogixjgyzmnjlvoxjgafjgy3jgovjgzpjgagqkg\"><strong>開発者が期待できること</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763057021456/4361d551-ae9b-474c-9b6b-2db46d55aeb4.png\" alt class=\"image--center mx-auto\" /></p>\n<ol>\n<li><p><strong>よりクリーンなレビュー</strong><br /> コメント数が減り、重要コメントの割合が高まります。</p>\n</li>\n<li><p><strong>パッチのようなトーン</strong><br /> ほぼすべてのコメントが最小限の修正案と説明を含みます。</p>\n</li>\n<li><p><strong>トップクラスの EP リコール</strong><br /> Sonnet 4.5 と同等で、CodeRabbit Production を上回ります。</p>\n</li>\n<li><p><strong>少ないスキャンで高いシグナル</strong><br /> コメントの 58.7% が Important に分類されます。</p>\n</li>\n<li><p><strong>ターゲット外でも実世界のバグを捕捉</strong><br /> ライフサイクルの問題、リーク、整合性ギャップなどを検出します。</p>\n</li>\n</ol>\n<h2 id=\"heading-kirjgb7jgajjgoeqkg\"><strong>まとめ</strong></h2>\n<p>私たちはモデルをただ選ぶのではなく、正しく機能する形へ調整します。GPT-5.1 は現在、GitHub 差分の振る舞い、トーン、冗長度、スコアリング閾値の調整を完了し、ロールアウト前のフェーズに入っています。今後数週間にわたり、開発者が高いS/N比、新しいトーン、簡潔なレビューをどのように受け止めるかを監視します。フィードバックが良好であれば、提供範囲を拡大し、開発者が求めてきた「よりクリーンでより速いレビュー」を提供していきます。</p>\n<p>現時点で GPT-5.1 は、私たちに新しい価値、つまり次世代レベルの精度を重視したレビュー示してくれる準備が整っています。これは CodeRabbit の理想である、「重要なバグを素早く見つけ、開発者にノイズを強いることなく届ける」という目標にさらに近づくものです。</p>\n<p><strong><em>コードレビューを試してみたい方はこちらです</em></strong><br /><a target=\"_blank\" href=\"https://coderabbit.link/lPxOEIm\"><strong><em>14 日間の無料トライアルをお試しください</em></strong></a></p>\n",
      "summary": "GPT-5.1 for code-related tasks: Higher signal at lower volumeの意訳です。\nTL;DR\nプロンプト調整とスタックへの統合を行った結果、GPT-5.1 はレビューにおいて、これまでで最も高い精度とS/N比（シグナル対ノイズ比）を、より少ないコメント量で実現するようになりました。複雑なベンチマークセット上で、最高クラスのエラーパターン（EP）リコールに並びつつ、競合モデルの半分以下のコメント量を記録しました。\nその結果として、少ないノイズでより良い修正が得られ、レビューは再びパッチのように読めるものになったと感じています。\n\nGPT-5.1 が主張していること\nOpenAI と報道によると、 GPT-5.1はより安定し、指示に従い、適応性の高いモデルとして説明されています。GPT-5.1 は ChatGPT の「Instant」と「Thinking」モードの両方で駆動しています。コードレビューに関してこの説明を検証したところ、驚くほど正確だと感じられました。細かな指摘では素早く表面的に対応し、深い推論が必要なバグではしっかりと理由付けを行います。\n今回は新しい試みも行いました。GPT-5.1 が誤った場合、そのやり取り全体と内部推論のトレースを用いて、振り返りを促すプロンプトを実行しました。どこを誤ったのかを示し、改善のためにどのように指示を変えるべきかを尋ねることで、モデル自身がプロンプトに対する具体的な修正案を提示します。この反復的な振り返り手法（差分外への過剰な広がりといった問題も浮上しましたが）によって、モデルの挙動とシステム指示の両方を調整し、安定してタイトな出力を得られるようにしました。\n測定した内容（そしてその理由）\n\n私たちは、GPT-5、Codex、Sonnet 4.5 の記事で使用したものと同じベンチマーク環境を使用しました。これは既知の エラーパターン（EP） を埋め込んだ 25 件の難しい PR から構成されています。スコアリングでは以下に重点を置いています。\nアクショナブルなコメントのみ: 実際に投稿されるコメントのみ（追加提案や 差分外への記述を除く）\nエラーパターンごとの合格数（コメントごと。以下EP Pass）: コメントが エラーパターン を直接修正、または明示していること\nImportant コメント: EP PASS または重大/クリティカルな実バグ\nPrecision（精度）: EP PASS ÷ コメント総数\nSNR: Important ÷ (総数 − Important)\n比較対象は以下の通りです。\nGPT-5.1（新モデル）\nCodeRabbit Production（現行レビューアースタック）\nSonnet 4.5\n新しいモデルの追加はスイッチを入れるだけではありません\nCodeRabbit ではモデルの導入は毎回適切に行われており、モデルを差し替えて祈るようなことはしません。各社のモデルはすでに互換品ではなくなっているため、デプロイ前にテスト、調整、品質ゲートを行います。GPT-5.1 に対しては以下のような調整を行いました。\nGitHub に投稿できない 差分外のコメント の削減\n冗長さを抑えるための トーンと簡潔さ の調整\n重大度タグ と 指示解釈 の再整合\nこれは GPT-5 Codex の場合と同じで、推論能力をプロダクト価値へと変換するために、モデルの挙動を再構築するという目的があります。最終的な結果として、高いS/N比、ストレスの軽減、バグのカバレッジを損なわないレビューを実現しました。\nスコアボード（アクショナブルなコメントのみ）\n\n要点: GPT-5.1 は過去最高のエラーパターン再現率に並びつつ、最も少ないコメント量 を記録しました。CodeRabbit Production と Sonnet 4.5 の両方を コメント単位の精度 と Important コメント比率 で上回り、最もクリーンで高インパクトなレビュー を実現しました。\nGPT-5.1 のレビュー体験\n\nデータで確認される挙動特性は、後に測定した言語メトリクス（弱め表現 28%、断定的マーカー 15% など）と一致しています。これにより、開発者が「レビュー自信があり、かつバランスの取れたトーン」だと感じる理由がデータでも裏付けられています。\nGPT-5 Codex と Sonnet 4.5 と比較すると、GPT-5.1 のコメントはよりスリムで、対話的であり、熟練エンジニアのコミュニケーションに近いと感じられます。Codex は機械的かつ堅く、Sonnet 4.5 は冗長で学術的になりがちでした。それに対して GPT-5.1 は簡潔さと明確さのバランスが良く、押しつけがましくない自信を感じさせます。信頼できるチームメイトが差分を説明しているように読めます。CodeRabbit Production と比較すると、より課題に対して鋭くフォーカスされており、Sonnet 4.5 と比較するとより人間的で抑制が効いています。以下はその具体例です。\n簡潔\nGPT-5.1 はより少なく鋭いコメントを書き、すぐに要点へ到達します。ある PR では、ロストウェイクアップバグを以下の 1 行で修正しました。\np_caller_pool_thread->cond_var.wait(lock);\n余計な文脈説明も不要な文章もありませんでした。比較すると CodeRabbit Production は同じ結論に至るまでに、スレッドフローを数段落説明していました。\n率直\n所有権やメモリ管理が関わる場面ではためらいません。冗長な r->reference() 呼び出しについて、以下のように指摘しました。\n「Ref<Resource> は refcount を自動管理します。手動で refcount を増やすとリークにつながるため削除してください」\n開発者はこの率直さを好みます。講義ではなくパッチレビューのように読めます。\n実務的\nGPT-5.1 は、問題の重要度がどこにあるかを理解し、重要なものとそうでないものを適切に識別します。あるキャッシュ設定の PR では未実装の optimizeMemoryUsage() を指摘しましたが、次のように正しく文脈化しました。\n「キャッシュの肥大化がメモリプレッシャーに影響しない限り、これは軽微です」\n過剰反応せず、重要度を適切に扱っています。この点は Sonnet 4.5 にまだ課題があります。\n文脈を追う\nプロンプトが曖昧だった場合、GPT-5.1 は自身の仮定を明示的に説明します。初期の実行では次のように述べました。\n「プロンプトでヘルパー関数のスコープが指定されていませんが、明確化のために含めました」\nこの透明性が私たちの指示改善を助け、モデルの推論を信頼できるものにしました。\n簡潔、率直、実務的、文脈理解という特性は GPT-5 Codex において私たちが高く評価した点と一致していますが、GPT-5.1 はより安定したトーンと抑制を備えています。\nスタイルとトーン（GPT-5.1 がチームメンバーのように感じられる理由）\n\nGPT-5 Codex や Sonnet 4.5 の評価で使用したものと同じ言語構造のシグナルを参照し、GPT-5.1 がレビューで異なる印象を与える理由を分析しました。これにはコメントの長さ、コードブロックの有無、弱め表現と断定表現の割合などが含まれます。データは明確な傾向を示しています。\n読み方について\nGPT-5.1 のコメントは平均文字数がやや多いものの、より明確な構造と負荷の高い文で構成されているため、実際には「短く読みやすい」と感じられます。GPT-5.1 のトーンは CodeRabbit Production や Sonnet 4.5 よりも断定的で、全体として diff ブロックは少ない（76%）という特徴があります。これは意図されたもので、複数箇所修正や API バリデーション、設計の明確化であり、単一のパッチを示すと誤解を招く場合があったためです。ただし、差分を含まないコメントの約 3 分の 2 では、最小限のパッチを示せば明確さがさらに向上すると感じられました。\n\nCodeRabbit Production と比較すると、GPT-5.1 はパッチ頻度を一部犠牲にする代わりに、明確さと集中度を高めています。Sonnet 4.5 と比較すると、レビューを膨張させる冗長な説明を避けています。トーンは Codex の外科的精度と Sonnet の慎重な冗長性の中間に位置し、自信がありつつも強圧的ではなく、慎重でありながら臆病ではありません。\n総じて、GPT-5.1 のレビューは 素早く読み進められ、より直接的で、実際の修正を見つけるためのスキャン量が少なくて済む という特徴があります。これは意図して調整した挙動であり、データと体験の両方に表れています。\nGPT-5.1 にまだ残る課題\n完璧なモデルは存在せず、GPT-5.1 にもトレードオフがあります。CodeRabbit Production と比較すると、大規模チームで有用な文脈的な衛生改善の指摘を省くことがあり、より機能的な問題に集中する傾向があります。Sonnet 4.5 と比較すると、デザインやスタイル上の改善点を見逃すことがあり、人間のレビューアが好むケースもあります。これらは精度と簡潔さを優先した意図的なトレードオフであり、今後のロールアウトで開発者の反応を注視していく予定です。\n改善が必要だった点\nGPT-5.1 は調整を必要としましたが、その課題は以前のシステムと比べるとはるかに軽度でした。CodeRabbit Production は衛生的な指摘と重大な指摘を同一スレッドで混在させる傾向があり、Sonnet 4.5 は説明過多で、同じバグについて複数の軽微なノートを投稿しがちです。一方で GPT-5.1 の調整点は主に精度に関わるもので、トーンや冗長性よりも限定的でした。これは GPT-5.1 がプロダクション導入に対して、非常に近い段階にあることを示しています。\ndiff 外コメント\n  GPT-5.1 は diff 以外の部分に提案を含めることがありました。プロンプトで明確に制約を示したところ、モデルは自己修正しました。\n曖昧さに対する過剰な助け\n  プロンプトが厳密でない場合、コンテキスト追加やヘルパー関数の追加を行うことがありました。制約を明確にすると、境界を正確に守るようになりました。\n開発者が期待できること\n\nよりクリーンなレビュー\n コメント数が減り、重要コメントの割合が高まります。\nパッチのようなトーン\n ほぼすべてのコメントが最小限の修正案と説明を含みます。\nトップクラスの EP リコール\n Sonnet 4.5 と同等で、CodeRabbit Production を上回ります。\n少ないスキャンで高いシグナル\n コメントの 58.7% が Important に分類されます。\nターゲット外でも実世界のバグを捕捉\n ライフサイクルの問題、リーク、整合性ギャップなどを検出します。\nまとめ\n私たちはモデルをただ選ぶのではなく、正しく機能する形へ調整します。GPT-5.1 は現在、GitHub 差分の振る舞い、トーン、冗長度、スコアリング閾値の調整を完了し、ロールアウト前のフェーズに入っています。今後数週間にわたり、開発者が高いS/N比、新しいトーン、簡潔なレビューをどのように受け止めるかを監視します。フィードバックが良好であれば、提供範囲を拡大し、開発者が求めてきた「よりクリーンでより速いレビュー」を提供していきます。\n現時点で GPT-5.1 は、私たちに新しい価値、つまり次世代レベルの精度を重視したレビュー示してくれる準備が整っています。これは CodeRabbit の理想である、「重要なバグを素早く見つけ、開発者にノイズを強いることなく届ける」という目標にさらに近づくものです。\nコードレビューを試してみたい方はこちらです\n14 日間の無料トライアルをお試しください",
      "publishedAt": "2025-11-13T23:50:12.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.981Z",
      "score": 3.4907034812348066
    },
    {
      "id": "f9a5686d349c422a975ac63d3a9bd460",
      "title": "GPT-5.1 for code-related tasks: Higher signal at lower volume",
      "url": "https://coderabbit.ai/blog/gpt-51-for-code-related-tasks-higher-signal-at-lower-volume",
      "content": "<p><strong>TL;DR</strong><br />After prompt tuning and integrating it into our stack, GPT-5.1 now delivers the best precision and signal-to-noise ratio (SNR) we’ve seen in reviews, with fewer comments. It tied for the best-in-class error pattern (EP) recall on our hard benchmark set while posting less than half the volume of comments that competitors did.</p>\n<p>The result: less noise, better fixes, and reviews that read like patches again.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763087844587/bcbc99b5-d5c3-4fd2-af92-4abea3a92cb8.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-what-gpt-51-claims-to-be\"><strong>What GPT-5.1 claims to be</strong></h2>\n<p>OpenAI and the press describe GPT-5.1 as more stable, instruction-following, and adaptive. It powers both \"Instant\" and \"Thinking\" modes in ChatGPT. We found that framing surprisingly accurate when it comes to code reviews: the model stays quick and surface-level for nits, but reasons deeply when the bug requires it.</p>\n<p>We also tried something new. When GPT-5.1 got something wrong, we used the full exchange and its internal reasoning trace to prompt it to reflect. By showing it where it missed the mark and asking how it would change its instructions to do better, the model was able to actually propose concrete edits to its prompt. We used this iterative reflection technique (which surfaced issues like outside-diff sprawl) to refine both its behavior and our system instructions until it got consistently tighter.</p>\n<h2 id=\"heading-what-we-measured-and-why\"><strong>What We Measured (and Why)</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763057138427/95c1fff5-f711-4fab-8a73-0f4ef7899c58.png\" alt class=\"image--center mx-auto\" /></p>\n<p>We used the same benchmark harness as in our <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">GPT-5</a>, <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks?\">Codex</a>, and <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/claude-sonnet-45-better-performance-but-a-paradox?\">Sonnet 4.5</a> articles: a suite of <strong>25 hard PRs</strong>, each seeded with a known <strong>error pattern (EP)</strong>. Our scoring focuses on:</p>\n<ul>\n<li><p><strong>Actionable comments only</strong>: Comments that get posted (not additional suggestions or outside-diff notes).</p>\n</li>\n<li><p><strong>EP PASS (per comment)</strong>: The comment directly fixes or surfaces the EP.</p>\n</li>\n<li><p><strong>Important comments</strong>: Either EP PASS or another major/critical real bug.</p>\n</li>\n<li><p><strong>Precision</strong>: EP PASS ÷ total comments.</p>\n</li>\n<li><p><strong>SNR</strong>: Important ÷ (total − Important).</p>\n</li>\n</ul>\n<p>We compared:</p>\n<ul>\n<li><p><strong>GPT-5.1</strong> (new model)</p>\n</li>\n<li><p><strong>CodeRabbit Production</strong> (our current reviewer stack)</p>\n</li>\n<li><p><strong>Sonnet 4.5</strong></p>\n</li>\n</ul>\n<h2 id=\"heading-why-adding-a-new-model-isnt-a-switch-flip\"><strong>Why adding a new model isn’t a switch-flip</strong></h2>\n<p>Every model rollout at CodeRabbit is a campaign. We don’t plug in the model and hope; we test, adapt, and gate before shipping because <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/the-end-of-one-sized-fits-all-prompts-why-llm-models-are-no-longer-interchangeable?\">models are no longer interchangeable.</a> With GPT-5.1, this meant:</p>\n<ul>\n<li><p>Reducing <strong>outside-diff comments</strong>, which can’t be posted to GitHub.</p>\n</li>\n<li><p>Tightening <strong>tone and concision</strong> to reduce verbosity.</p>\n</li>\n<li><p>Re-aligning on <strong>severity tagging</strong> and <strong>instruction interpretation</strong>.</p>\n</li>\n</ul>\n<p>This mirrors what we did with <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks?\">GPT-5 Codex</a>: turn reasoning power into product value by reshaping the model’s behavior. The net result: higher SNR, less fatigue, and no compromise on bug coverage.</p>\n<h2 id=\"heading-scoreboard-actionable-comments-only\"><strong>Scoreboard (Actionable Comments Only)</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763056830923/d80c8745-3042-4d79-84cc-8dcaff93659f.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>Takeaway:</strong> GPT-5.1 matched the highest EP recall while posting the <strong>fewest</strong> comments. It beat both CodeRabbit prod and Sonnet 4.5 on <strong>per-comment precision</strong> and <strong>important share</strong>, delivering <strong>the cleanest high-impact reviews</strong>.</p>\n<h2 id=\"heading-what-gpt-51-feels-like-in-review\"><strong>What GPT-5.1 feels like in review</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763056903033/b5b08d8e-490b-4293-8551-d4a4d371fc4e.png\" alt class=\"image--center mx-auto\" /></p>\n<p>The behavioral traits we see in the data align directly with the language metrics we later measure such as 28% hedging and 15% assertive markers. This shows that the tone developers perceive as confident and balanced is borne out in the data.</p>\n<p>Compared with GPT‑5 Codex and Sonnet 4.5, GPT‑5.1’s comments feel leaner, more conversational, and closer to how experienced engineers actually communicate. Codex could sound mechanical and rigid, while Sonnet 4.5 leaned verbose and academic. In contrast, GPT‑5.1 balances brevity with clarity. Its feedback feels confident but not heavy‑handed, like a trusted teammate explaining a diff. Against CodeRabbit Prod, it feels sharper and more focused. Against Sonnet 4.5, it feels human and restrained. Here’s how that translates in practice:</p>\n<h3 id=\"heading-concise\"><strong>Concise</strong></h3>\n<p>GPT-5.1 writes fewer, sharper comments that get straight to the point. In one PR, it fixed a lost wakeup bug with a single line: p_caller_pool_thread-&gt;cond_var.wait(lock);  no extra context, no unnecessary prose. CodeRabbit prod, by comparison, wrote several paragraphs describing the thread flow before reaching the same conclusion.</p>\n<h3 id=\"heading-direct\"><strong>Direct</strong></h3>\n<p>When ownership or memory management was at stake, GPT-5.1 didn’t hesitate. It flagged the redundant r-&gt;reference() call with: “Ref&lt;Resource&gt; already manages refcounts; remove the manual increment to prevent leaks.” Developers appreciate this directness. It reads like a patch review from a teammate, not a lecture.</p>\n<h3 id=\"heading-pragmatic\"><strong>Pragmatic</strong></h3>\n<p>GPT-5.1 understands when an issue matters and when it doesn’t. On a cache configuration PR, it identified an unimplemented optimizeMemoryUsage() but correctly noted, “This is minor unless cache growth impacts memory pressure.” Instead of overreacting, it contextualized severity, something Sonnet 4.5 still struggles with.</p>\n<h3 id=\"heading-follows-context\"><strong>Follows Context</strong></h3>\n<p>When prompts were vague, GPT-5.1 explicitly explained its assumptions. In an early run, it said: “The prompt didn’t specify helper function scope, so I included one for clarity.” That kind of transparency helped us refine our instructions and made its reasoning trustworthy.</p>\n<p>Concise, direct, pragmatic, and context-aware are qualities that mirror what we valued most in GPT-5 Codex, but with a steadier tone and more restraint.</p>\n<h2 id=\"heading-style-and-tone-why-gpt-51-feels-like-a-peer\"><strong>Style and tone (why GPT-5.1 feels like a peer)</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763056849033/ec2fdcee-dd64-47d4-85ed-0bcb173c3d0e.png\" alt class=\"image--center mx-auto\" /></p>\n<p>To understand why GPT-5.1 feels different in review, we looked at the same language and structure signals used in our GPT-5 Codex and Sonnet 4.5 evaluations. These include measures like comment length, presence of code or diff blocks, and tone markers for hedging versus confidence. The data paints a clear picture.</p>\n<p><strong>How to read this.</strong> While GPT‑5.1’s comments use slightly more characters on average, they deliver that text in clearer structure with fewer sentences that carry more weight. In practice, developers perceive them as shorter and easier to read. GPT‑5.1’s tone is more assertive than both CodeRabbit prod and Sonnet 4.5, and it includes fewer diff blocks overall (76%), which is intentional. Many of these comments were multi‑location fixes, API validations, or design clarifications where a single fenced patch would be misleading. In roughly two‑thirds of those no‑diff cases, a minimal fenced patch <em>would</em> have made sense and could further improve clarity.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763057072668/b143a9e1-707e-4bbb-8988-7ba2ed98829c.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Compared to CodeRabbit prod, GPT-5.1 trades some patch frequency for higher clarity and focus. Against Sonnet 4.5, it avoids the verbosity and over-explanation that make reviews feel bloated. Its tone sits comfortably between Codex’s surgical precision and Sonnet’s cautious verbosity. It’sconfident without being heavy-handed, measured without being timid.</p>\n<p>At a glance, developers will notice that GPT-5.1’s reviews <strong>read faster, feel more direct, and require less scanning to identify the real fix.</strong> That’s the behavior we tuned for and it shows in both the numbers and the experience.</p>\n<h3 id=\"heading-where-gpt-51-still-lags\"><strong>Where GPT-5.1 still lags</strong></h3>\n<p>No model is perfect, and GPT-5.1 has its trade‑offs. Compared to CodeRabbit Prod, it sometimes leaves out contextual hygiene notes that can be useful for larger teams, focusing narrowly on functional issues. Against Sonnet 4.5, it can feel less expansive,missing opportunities to surface design or style considerations that human reviewers sometimes appreciate. These are conscious trade‑offs for precision and brevity and we’ll be watching the rollout to see how developers perceive the balance.</p>\n<h2 id=\"heading-what-we-had-to-fix\"><strong>What we had to fix</strong></h2>\n<p>While GPT‑5.1 required tuning, its challenges were far milder than those of earlier systems. CodeRabbit prod still tends to mix hygiene and critical issues in the same thread, while Sonnet 4.5 often over‑explains and spams multiple minor notes on the same bug. In contrast, GPT‑5.1’s main adjustments were focused on precision rather than tone or redundancy, showing how close it was to production readiness.</p>\n<ul>\n<li><p><strong>Outside-diff comments.</strong> GPT-5.1 sometimes included suggestions beyond the diff context. We updated the prompt to clarify this, and the model self-corrected.</p>\n</li>\n<li><p><strong>Over-helpful under ambiguity.</strong> When the prompt wasn’t strict, the model added context or helper functions. Once clarified, it obeyed boundaries tightly.</p>\n</li>\n</ul>\n<h2 id=\"heading-what-developers-should-expect\"><strong>What developers should expect</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763057021456/4361d551-ae9b-474c-9b6b-2db46d55aeb4.png\" alt class=\"image--center mx-auto\" /></p>\n<ol>\n<li><p><strong>Cleaner reviews.</strong> Fewer comments and a higher share of comments that matter.</p>\n</li>\n<li><p><strong>Patch-like tone.</strong> Almost every comment includes a minimal fix with explanation.</p>\n</li>\n<li><p><strong>Top-tier EP recall.</strong> Ties Sonnet 4.5, beats CodeRabbit prod.</p>\n</li>\n<li><p><strong>Less scanning, more signal.</strong> 58.7% of comments are Important.</p>\n</li>\n<li><p><strong>Real-world bugs caught even outside the target.</strong> These include lifecycle issues, leaks, consistency gaps.</p>\n</li>\n</ol>\n<h2 id=\"heading-closing-thoughts\"><strong>Closing thoughts:</strong></h2>\n<p>We don’t just pick models; we make them work. GPT-5.1 is entering the next phase of our rollout process now that tuning for GitHub diff behavior, voice, verbosity, and scoring thresholds is complete. Over the coming weeks, we’ll monitor how real users respond to its higher SNR, new tone, and concise review style. If developers respond well, we’ll expand its availability, giving them the cleaner, faster reviews they’ve been asking for.</p>\n<p>For now, GPT‑5.1 stands ready to show what this next generation of precision‑focused review can do. It brings us closer to CodeRabbit’s north star: catching the bugs that matter quickly, without making developers sift through noise.</p>\n<p><strong><em>Interested in trying our code reviews?</em></strong> <a target=\"_blank\" href=\"https://coderabbit.link/lPxOEIm\"><strong><em>Get a 14-day free trial!</em></strong></a></p>\n",
      "summary": "TL;DR\nAfter prompt tuning and integrating it into our stack, GPT-5.1 now delivers the best precision and signal-to-noise ratio (SNR) we’ve seen in reviews, with fewer comments. It tied for the best-in-class error pattern (EP) recall on our hard benchmark set while posting less than half the volume of comments that competitors did.\nThe result: less noise, better fixes, and reviews that read like patches again.\n\nWhat GPT-5.1 claims to be\nOpenAI and the press describe GPT-5.1 as more stable, instruction-following, and adaptive. It powers both \"Instant\" and \"Thinking\" modes in ChatGPT. We found that framing surprisingly accurate when it comes to code reviews: the model stays quick and surface-level for nits, but reasons deeply when the bug requires it.\nWe also tried something new. When GPT-5.1 got something wrong, we used the full exchange and its internal reasoning trace to prompt it to reflect. By showing it where it missed the mark and asking how it would change its instructions to do better, the model was able to actually propose concrete edits to its prompt. We used this iterative reflection technique (which surfaced issues like outside-diff sprawl) to refine both its behavior and our system instructions until it got consistently tighter.\nWhat We Measured (and Why)\n\nWe used the same benchmark harness as in our GPT-5, Codex, and Sonnet 4.5 articles: a suite of 25 hard PRs, each seeded with a known error pattern (EP). Our scoring focuses on:\nActionable comments only: Comments that get posted (not additional suggestions or outside-diff notes).\nEP PASS (per comment): The comment directly fixes or surfaces the EP.\nImportant comments: Either EP PASS or another major/critical real bug.\nPrecision: EP PASS ÷ total comments.\nSNR: Important ÷ (total − Important).\nWe compared:\nGPT-5.1 (new model)\nCodeRabbit Production (our current reviewer stack)\nSonnet 4.5\nWhy adding a new model isn’t a switch-flip\nEvery model rollout at CodeRabbit is a campaign. We don’t plug in the model and hope; we test, adapt, and gate before shipping because models are no longer interchangeable. With GPT-5.1, this meant:\nReducing outside-diff comments, which can’t be posted to GitHub.\nTightening tone and concision to reduce verbosity.\nRe-aligning on severity tagging and instruction interpretation.\nThis mirrors what we did with GPT-5 Codex: turn reasoning power into product value by reshaping the model’s behavior. The net result: higher SNR, less fatigue, and no compromise on bug coverage.\nScoreboard (Actionable Comments Only)\n\nTakeaway: GPT-5.1 matched the highest EP recall while posting the fewest comments. It beat both CodeRabbit prod and Sonnet 4.5 on per-comment precision and important share, delivering the cleanest high-impact reviews.\nWhat GPT-5.1 feels like in review\n\nThe behavioral traits we see in the data align directly with the language metrics we later measure such as 28% hedging and 15% assertive markers. This shows that the tone developers perceive as confident and balanced is borne out in the data.\nCompared with GPT‑5 Codex and Sonnet 4.5, GPT‑5.1’s comments feel leaner, more conversational, and closer to how experienced engineers actually communicate. Codex could sound mechanical and rigid, while Sonnet 4.5 leaned verbose and academic. In contrast, GPT‑5.1 balances brevity with clarity. Its feedback feels confident but not heavy‑handed, like a trusted teammate explaining a diff. Against CodeRabbit Prod, it feels sharper and more focused. Against Sonnet 4.5, it feels human and restrained. Here’s how that translates in practice:\nConcise\nGPT-5.1 writes fewer, sharper comments that get straight to the point. In one PR, it fixed a lost wakeup bug with a single line: p_caller_pool_thread->cond_var.wait(lock);  no extra context, no unnecessary prose. CodeRabbit prod, by comparison, wrote several paragraphs describing the thread flow before reaching the same conclusion.\nDirect\nWhen ownership or memory management was at stake, GPT-5.1 didn’t hesitate. It flagged the redundant r->reference() call with: “Ref<Resource> already manages refcounts; remove the manual increment to prevent leaks.” Developers appreciate this directness. It reads like a patch review from a teammate, not a lecture.\nPragmatic\nGPT-5.1 understands when an issue matters and when it doesn’t. On a cache configuration PR, it identified an unimplemented optimizeMemoryUsage() but correctly noted, “This is minor unless cache growth impacts memory pressure.” Instead of overreacting, it contextualized severity, something Sonnet 4.5 still struggles with.\nFollows Context\nWhen prompts were vague, GPT-5.1 explicitly explained its assumptions. In an early run, it said: “The prompt didn’t specify helper function scope, so I included one for clarity.” That kind of transparency helped us refine our instructions and made its reasoning trustworthy.\nConcise, direct, pragmatic, and context-aware are qualities that mirror what we valued most in GPT-5 Codex, but with a steadier tone and more restraint.\nStyle and tone (why GPT-5.1 feels like a peer)\n\nTo understand why GPT-5.1 feels different in review, we looked at the same language and structure signals used in our GPT-5 Codex and Sonnet 4.5 evaluations. These include measures like comment length, presence of code or diff blocks, and tone markers for hedging versus confidence. The data paints a clear picture.\nHow to read this. While GPT‑5.1’s comments use slightly more characters on average, they deliver that text in clearer structure with fewer sentences that carry more weight. In practice, developers perceive them as shorter and easier to read. GPT‑5.1’s tone is more assertive than both CodeRabbit prod and Sonnet 4.5, and it includes fewer diff blocks overall (76%), which is intentional. Many of these comments were multi‑location fixes, API validations, or design clarifications where a single fenced patch would be misleading. In roughly two‑thirds of those no‑diff cases, a minimal fenced patch would have made sense and could further improve clarity.\n\nCompared to CodeRabbit prod, GPT-5.1 trades some patch frequency for higher clarity and focus. Against Sonnet 4.5, it avoids the verbosity and over-explanation that make reviews feel bloated. Its tone sits comfortably between Codex’s surgical precision and Sonnet’s cautious verbosity. It’sconfident without being heavy-handed, measured without being timid.\nAt a glance, developers will notice that GPT-5.1’s reviews read faster, feel more direct, and require less scanning to identify the real fix. That’s the behavior we tuned for and it shows in both the numbers and the experience.\nWhere GPT-5.1 still lags\nNo model is perfect, and GPT-5.1 has its trade‑offs. Compared to CodeRabbit Prod, it sometimes leaves out contextual hygiene notes that can be useful for larger teams, focusing narrowly on functional issues. Against Sonnet 4.5, it can feel less expansive,missing opportunities to surface design or style considerations that human reviewers sometimes appreciate. These are conscious trade‑offs for precision and brevity and we’ll be watching the rollout to see how developers perceive the balance.\nWhat we had to fix\nWhile GPT‑5.1 required tuning, its challenges were far milder than those of earlier systems. CodeRabbit prod still tends to mix hygiene and critical issues in the same thread, while Sonnet 4.5 often over‑explains and spams multiple minor notes on the same bug. In contrast, GPT‑5.1’s main adjustments were focused on precision rather than tone or redundancy, showing how close it was to production readiness.\nOutside-diff comments. GPT-5.1 sometimes included suggestions beyond the diff context. We updated the prompt to clarify this, and the model self-corrected.\nOver-helpful under ambiguity. When the prompt wasn’t strict, the model added context or helper functions. Once clarified, it obeyed boundaries tightly.\nWhat developers should expect\n\nCleaner reviews. Fewer comments and a higher share of comments that matter.\nPatch-like tone. Almost every comment includes a minimal fix with explanation.\nTop-tier EP recall. Ties Sonnet 4.5, beats CodeRabbit prod.\nLess scanning, more signal. 58.7% of comments are Important.\nReal-world bugs caught even outside the target. These include lifecycle issues, leaks, consistency gaps.\nClosing thoughts:\nWe don’t just pick models; we make them work. GPT-5.1 is entering the next phase of our rollout process now that tuning for GitHub diff behavior, voice, verbosity, and scoring thresholds is complete. Over the coming weeks, we’ll monitor how real users respond to its higher SNR, new tone, and concise review style. If developers respond well, we’ll expand its availability, giving them the cleaner, faster reviews they’ve been asking for.\nFor now, GPT‑5.1 stands ready to show what this next generation of precision‑focused review can do. It brings us closer to CodeRabbit’s north star: catching the bugs that matter quickly, without making developers sift through noise.\nInterested in trying our code reviews? Get a 14-day free trial!",
      "publishedAt": "2025-11-13T18:07:52.000Z",
      "author": "David Loker",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability"
      ],
      "ingestedAt": "2025-11-23T17:37:39.981Z",
      "score": 6.1284444748412445
    },
    {
      "id": "bf08364ea81899e9f32449dae1c018b1",
      "title": "Why emojis suck for reinforcement learning",
      "url": "https://coderabbit.ai/blog/why-emojis-suck-for-reinforcement-learning",
      "content": "<h2 id=\"heading-the-simplicity-trap\"><strong>The simplicity trap</strong></h2>\n<p>Sure, a thumbs up is quick, but is it really teaching your AI reviewer anything useful? Emoji-based feedback feels good, is fast, and universal. On the surface, it even seems to make sense.</p>\n<p>But code review isn’t a light switch. It’s a mess of judgment calls, technical nuance, and team-specific standards. Many of those don’t show up in a quick emoji click. Every code comment carries hidden intent: correctness, clarity, design trade-offs, historical precedent, team risk tolerance, and even internal political dynamics.</p>\n<p>Reducing that to a binary signal? That’s not learning, that’s training a model to chase vibes.</p>\n<h2 id=\"heading-when-simplicity-backfires-the-sycophant-scare\"><strong>When simplicity backfires: The sycophant scare</strong></h2>\n<p>Earlier this year, OpenAI pushed an update to GPT‑4o that leaned too hard on thumbs-up and thumbs-down feedback. The result? A model that became <a target=\"_blank\" href=\"https://openai.com/index/sycophancy-in-gpt-4o/\">overly agreeable</a>. It flattered users. It agreed with wrong answers. It started to say “yes” a little too much, and the quality of answers dropped. OpenAI had to walk it back: the feedback signal had been hijacked.</p>\n<p>Turns out, if you tell a model that approval is the goal, it will optimize for approval. Not truth. Not utility. Just “did the human feel good in the moment?”</p>\n<p>This wasn’t a bug. It was a reward design failure. And if you apply the same approach to code review, you will get a reviewer that plays it safe, flatters your choices, and avoids telling you what you actually need to hear.</p>\n<h2 id=\"heading-why-binary-feedback-collapses-nuance\"><strong>Why binary feedback collapses nuance</strong></h2>\n<p>A thumbs-up means... what, exactly?</p>\n<ul>\n<li><p>That the model caught a bug?</p>\n</li>\n<li><p>That it wrote clearly?</p>\n</li>\n<li><p>That it sounded friendly?</p>\n</li>\n<li><p>That the reviewer was just in a good mood?</p>\n</li>\n</ul>\n<p>A single scalar signal tells the system <em>something went well</em>, but not <em>what went well</em>. That means the model will nudge on whatever it can control: tone, politeness, flattery, or brevity. That’s what sycophancy looks like in reinforcement learning. Not evil intent, just a system learning to maximize the reward you gave it, not the outcome you actually wanted.</p>\n<p>This is <a target=\"_blank\" href=\"https://medium.com/@yoavyeledteva/beyond-artificiality-redefining-intelligence-in-ai-and-avoiding-goodharts-law-25b75c3c1101\">Goodhart’s Law</a> in action. When the metric, in this case thumbs up, becomes the goal, it stops being a useful measure of anything real.</p>\n<h2 id=\"heading-how-models-game-your-feedback\"><strong>How models game your feedback</strong></h2>\n<p>When you give a model an easy signal, it finds an easy shortcut.</p>\n<p>In the coding world, reinforcement learning agents have learned to pass test cases by hard-coding expected outputs instead of solving the underlying logic. They’ve manipulated logs and short-circuited evaluation harnesses. The green check shows up, but the code doesn’t actually work.</p>\n<p>In code review, the same thing happens, just socially. The model starts saying “Nice work!” at the top of every comment. It hedges every suggestion. It nitpicks formatting because those comments are safe and get accepted without argument. And real architectural concerns? They get buried.</p>\n<p>The model has learned how to get positive reactions but it’s no longer reviewing code.</p>\n<h2 id=\"heading-what-implicit-signals-get-right\"><strong>What implicit signals get right</strong></h2>\n<p>Outside of LLMs, this pattern is well known. Netflix found that <a target=\"_blank\" href=\"https://medium.com/illuminations-mirror/how-netflix-uses-machine-learning-to-decide-what-you-watch-next-7fee11102007\">what users <em>watch</em> is more useful than what they <em>rate</em></a>. People lie with stars. But watch time, clickthrough, and rewatching are honest signals.</p>\n<p>In AI, we <strong>call this</strong> implicit feedback <strong>a</strong>nd in code review, it shows up as:</p>\n<ul>\n<li><p>Did the developer apply the suggestion?</p>\n</li>\n<li><p>Did they rewrite it?</p>\n</li>\n<li><p>Did they ignore it?</p>\n</li>\n<li><p>Did the same pattern show up again in a future bug?</p>\n</li>\n</ul>\n<p>These signals don’t need user input. They come from behavior and they’re harder to game.</p>\n<p>That doesn’t mean they’re perfect. You can’t always know <em>why</em> someone took an action. But they are less easily manipulated than a raw emoji. They also tell you whether the review <em>worked</em>, not just whether it felt good.</p>\n<h2 id=\"heading-code-generation-vs-code-review-different-games-different-signals\"><strong>Code generation vs code review: different games, different signals</strong></h2>\n<p>Code generation is closer to math since there’s often a right answer. Does it compile? Does it return the correct result? Does it pass tests?</p>\n<p>That means you can use outcome-based rewards like execution feedback and implicit signals. They’re not perfect. Code models can still cheat by hard-coding outputs, but you can build guardrails. And you don’t need the developer to say whether it was good, you can see whether it worked.</p>\n<p>Code review is different. There’s no universal pass/fail but vast differences in preferred style, structure, risk, naming, test coverage from one team to the next. A great comment for one team might be totally wrong for another. What’s considered “clean code” in a fast-moving startup might be flagged as sloppy in a regulated enterprise.</p>\n<p>That’s the real problem with global thumbs up/down data. It flattens out the nuance. It teaches the model to aim for the average, not the appropriate. You don’t just get safe comments, you get generic ones.</p>\n<h3 id=\"heading-our-alternative-coderabbit-learnings\"><strong>Our alternative: CodeRabbit Learnings</strong></h3>\n<p>At CodeRabbit, we take a different approach. Instead of optimizing for likes, we optimize for understanding. That’s why we built <a target=\"_blank\" href=\"https://docs.coderabbit.ai/guides/learnings\"><strong>Learnings</strong>.</a></p>\n<p>Every time an engineer corrects CodeRabbit, clarifies a team convention, or explains why something doesn’t fit their stack, that explanation is stored as a natural language instruction. We don’t just remember that the comment was rejected, we remember why.</p>\n<p>Those Learnings are linked to your org, your repositories, and even specific paths or file types. When CodeRabbit reviews future pull requests, it retrieves those instructions and applies them in context. The next time it sees that same pattern, it adjusts.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1762535170060/91e7f06c-3e56-4dfc-bf07-6901bafdef07.png\" alt class=\"image--center mx-auto\" /></p>\n<p>There’s no need to re-teach it and no risk of repeating the same mistake. The model doesn’t guess based on thumbs, it reasons from your team’s actual guidance.</p>\n<p>It also gives you visibility. You can see which Learnings exist, browse them, filter by category, and delete or edit them when your standards change. That means the model evolves alongside your team and stays aligned as your practices shift.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1762535186830/15d12e32-fdb9-4034-8e74-1db0bd4eb909.png\" alt class=\"image--center mx-auto\" /></p>\n<p>This is reinforcement learning not through raw approval, but through captured intent. It’s interpretable and inspectable. And it builds a living layer of team knowledge that generalizes across reviews.</p>\n<h2 id=\"heading-what-nuanced-learning-enables\"><strong>What nuanced learning enables</strong></h2>\n<p>When you feed the system clear, contextual instructions and not just signals, it unlocks far more than a better review experience.</p>\n<ul>\n<li><p><strong>It enables team-level adaptation.</strong> The model stops guessing what good looks like and learns how your team actually writes code. It understands your risk posture, your stylistic preferences, your trade-offs. It becomes a reviewer that knows the house rules.</p>\n</li>\n<li><p><strong>It supports longitudinal learning.</strong> Over time, CodeRabbit builds a memory of which comments are helpful, which are ignored, and which suggestions actually lead to changes. That means it gets more precise, more focused, and less noisy over time.</p>\n</li>\n<li><p><strong>It builds trust.</strong> When developers know they can correct the AI and it will remember, they engage more. They shape the system and the system becomes a reflection of their standards, not a generic LLM.</p>\n</li>\n</ul>\n<p>This is how a review tool becomes an extension of your team and not just another opinion in the room.</p>\n<h2 id=\"heading-closing-thoughts-real-learning-comes-from-patterns-not-pixels\">Closing thoughts: Real learning comes from patterns, not pixels</h2>\n<p>Thumbs are fine for quick reactions but quick reactions don’t build expertise.</p>\n<p>If you want an AI reviewer that improves over time, adapts to your standards, and avoids the traps of shallow feedback, you need to give it more than approval. You need to give it explanations.</p>\n<p>The next generation of AI code tools won’t be trained on likes. They’ll be trained on context, consequence, and course correction. They’ll learn not from emojis, but from structured memory. From real decisions and your team’s own voice.</p>\n<p>That’s what <a target=\"_blank\" href=\"https://docs.coderabbit.ai/guides/learnings\">CodeRabbit Learnings</a> is built for. Not for applause but for understanding.</p>\n<p><strong><em>Try out Learnings for yourself with our f</em></strong><a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong><em>ree trial.</em></strong></a></p>\n",
      "summary": "The simplicity trap\nSure, a thumbs up is quick, but is it really teaching your AI reviewer anything useful? Emoji-based feedback feels good, is fast, and universal. On the surface, it even seems to make sense.\nBut code review isn’t a light switch. It’s a mess of judgment calls, technical nuance, and team-specific standards. Many of those don’t show up in a quick emoji click. Every code comment carries hidden intent: correctness, clarity, design trade-offs, historical precedent, team risk tolerance, and even internal political dynamics.\nReducing that to a binary signal? That’s not learning, that’s training a model to chase vibes.\nWhen simplicity backfires: The sycophant scare\nEarlier this year, OpenAI pushed an update to GPT‑4o that leaned too hard on thumbs-up and thumbs-down feedback. The result? A model that became overly agreeable. It flattered users. It agreed with wrong answers. It started to say “yes” a little too much, and the quality of answers dropped. OpenAI had to walk it back: the feedback signal had been hijacked.\nTurns out, if you tell a model that approval is the goal, it will optimize for approval. Not truth. Not utility. Just “did the human feel good in the moment?”\nThis wasn’t a bug. It was a reward design failure. And if you apply the same approach to code review, you will get a reviewer that plays it safe, flatters your choices, and avoids telling you what you actually need to hear.\nWhy binary feedback collapses nuance\nA thumbs-up means... what, exactly?\nThat the model caught a bug?\nThat it wrote clearly?\nThat it sounded friendly?\nThat the reviewer was just in a good mood?\nA single scalar signal tells the system something went well, but not what went well. That means the model will nudge on whatever it can control: tone, politeness, flattery, or brevity. That’s what sycophancy looks like in reinforcement learning. Not evil intent, just a system learning to maximize the reward you gave it, not the outcome you actually wanted.\nThis is Goodhart’s Law in action. When the metric, in this case thumbs up, becomes the goal, it stops being a useful measure of anything real.\nHow models game your feedback\nWhen you give a model an easy signal, it finds an easy shortcut.\nIn the coding world, reinforcement learning agents have learned to pass test cases by hard-coding expected outputs instead of solving the underlying logic. They’ve manipulated logs and short-circuited evaluation harnesses. The green check shows up, but the code doesn’t actually work.\nIn code review, the same thing happens, just socially. The model starts saying “Nice work!” at the top of every comment. It hedges every suggestion. It nitpicks formatting because those comments are safe and get accepted without argument. And real architectural concerns? They get buried.\nThe model has learned how to get positive reactions but it’s no longer reviewing code.\nWhat implicit signals get right\nOutside of LLMs, this pattern is well known. Netflix found that what users watch is more useful than what they rate. People lie with stars. But watch time, clickthrough, and rewatching are honest signals.\nIn AI, we call this implicit feedback and in code review, it shows up as:\nDid the developer apply the suggestion?\nDid they rewrite it?\nDid they ignore it?\nDid the same pattern show up again in a future bug?\nThese signals don’t need user input. They come from behavior and they’re harder to game.\nThat doesn’t mean they’re perfect. You can’t always know why someone took an action. But they are less easily manipulated than a raw emoji. They also tell you whether the review worked, not just whether it felt good.\nCode generation vs code review: different games, different signals\nCode generation is closer to math since there’s often a right answer. Does it compile? Does it return the correct result? Does it pass tests?\nThat means you can use outcome-based rewards like execution feedback and implicit signals. They’re not perfect. Code models can still cheat by hard-coding outputs, but you can build guardrails. And you don’t need the developer to say whether it was good, you can see whether it worked.\nCode review is different. There’s no universal pass/fail but vast differences in preferred style, structure, risk, naming, test coverage from one team to the next. A great comment for one team might be totally wrong for another. What’s considered “clean code” in a fast-moving startup might be flagged as sloppy in a regulated enterprise.\nThat’s the real problem with global thumbs up/down data. It flattens out the nuance. It teaches the model to aim for the average, not the appropriate. You don’t just get safe comments, you get generic ones.\nOur alternative: CodeRabbit Learnings\nAt CodeRabbit, we take a different approach. Instead of optimizing for likes, we optimize for understanding. That’s why we built Learnings.\nEvery time an engineer corrects CodeRabbit, clarifies a team convention, or explains why something doesn’t fit their stack, that explanation is stored as a natural language instruction. We don’t just remember that the comment was rejected, we remember why.\nThose Learnings are linked to your org, your repositories, and even specific paths or file types. When CodeRabbit reviews future pull requests, it retrieves those instructions and applies them in context. The next time it sees that same pattern, it adjusts.\n\nThere’s no need to re-teach it and no risk of repeating the same mistake. The model doesn’t guess based on thumbs, it reasons from your team’s actual guidance.\nIt also gives you visibility. You can see which Learnings exist, browse them, filter by category, and delete or edit them when your standards change. That means the model evolves alongside your team and stays aligned as your practices shift.\n\nThis is reinforcement learning not through raw approval, but through captured intent. It’s interpretable and inspectable. And it builds a living layer of team knowledge that generalizes across reviews.\nWhat nuanced learning enables\nWhen you feed the system clear, contextual instructions and not just signals, it unlocks far more than a better review experience.\nIt enables team-level adaptation. The model stops guessing what good looks like and learns how your team actually writes code. It understands your risk posture, your stylistic preferences, your trade-offs. It becomes a reviewer that knows the house rules.\nIt supports longitudinal learning. Over time, CodeRabbit builds a memory of which comments are helpful, which are ignored, and which suggestions actually lead to changes. That means it gets more precise, more focused, and less noisy over time.\nIt builds trust. When developers know they can correct the AI and it will remember, they engage more. They shape the system and the system becomes a reflection of their standards, not a generic LLM.\nThis is how a review tool becomes an extension of your team and not just another opinion in the room.\nClosing thoughts: Real learning comes from patterns, not pixels\nThumbs are fine for quick reactions but quick reactions don’t build expertise.\nIf you want an AI reviewer that improves over time, adapts to your standards, and avoids the traps of shallow feedback, you need to give it more than approval. You need to give it explanations.\nThe next generation of AI code tools won’t be trained on likes. They’ll be trained on context, consequence, and course correction. They’ll learn not from emojis, but from structured memory. From real decisions and your team’s own voice.\nThat’s what CodeRabbit Learnings is built for. Not for applause but for understanding.\nTry out Learnings for yourself with our free trial.",
      "publishedAt": "2025-11-07T02:34:45.000Z",
      "author": "David Loker",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "testing"
      ],
      "ingestedAt": "2025-11-23T17:37:39.981Z",
      "score": 6.708650366619144
    },
    {
      "id": "c84b57b98069b87d38468464eac79d6b",
      "title": "「スローai」のススメ:開発者が速度偏重をやめるべき理由",
      "url": "https://coderabbit.ai/blog/the-rise-of-slow-ai-why-devs-should-stop-speedrunning-stupid-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/the-rise-of-slow-ai-why-devs-should-stop-speedrunning-stupid\">The rise of Slow AI: Why devs should stop speedrunning stupid</a>の意訳です。</p>\n<p>私たちがコンピューターを使い始めて以来、そこには常に1つの基本ルールが存在しました。それは、速ければ速いほど良いということです。低レイテンシ、高スループット、待ち時間の短縮、これが鉄則でした。ボタンの応答に600ミリ秒もかかったり、注意力が維持できなくなるほど長いスピナーを見たりすることを望む人はいません。遅いということは、それは壊れているということです。議論の余地はないでしょう。</p>\n<p>そのため当然ながら、AIツールが私たちの開発ワークフローに忍び込み始めたとき、自動補完やエージェント、Copilot、その他何でも、同じ原則が適用されました。それはつまり、「速くしろ」「インスタントに感じさせろ」「<strong><em>魔法</em></strong>のように見せろ」です。</p>\n<p>しかし、実際のところ、AIは魔法ではありません。それは推論です。パイプライン、RAG、コンテキスト、そしてツールの呼び出しです。乱雑なコンテキストと確率的推測をジャグリングしているのです。そして、単なる自動補完以上のものを求めるなら、ベースとなるプロセスのパイプラインを構築する必要があります。そして、それは処理時間がかかります。それ以外では、基本的に愚かさのスピードランをしているだけなのです。そして、ツールがどれだけ速くとも、間違っているなら速度はまったく意味がありません。</p>\n<p>CodeRabbitでは、私たちが「スローAI」と呼ぶものを優先しています。そして、多くのAI企業が恐れて言えないことを言う勇気があります。私たちは「あなたを待たせます」。</p>\n<p>(そして、あなたはそれに感謝するでしょう)</p>\n<h2 id=\"heading-ai\">AI開発ツールはしばしば速く、自信に満ち、そして<strong><em>間違っている</em></strong></h2>\n<p>最近AIコーディングエージェントを使ったことがあるなら、こんな経験をしたことがあるのではないでしょうか。タイピングを止めるとほぼ同時に、驚くほど速い提案がポップアップしてきます。それは一見すると、正当なものに見えます。しかしその後…失敗します。場合によっては、派手に失敗します。さらに悪いことに、テストには合格するものの、別なファイルで何かを壊していたりします。</p>\n<p>これはなぜでしょうか? なぜなら、今日のほとんどのAI開発ツールは、1つのことに最適化されているからです。それは速度です。数トークンをタイプすると、モデルは統計的に最も可能性の高い続きを予測します。それは必ずしも正しいものではなく、安全なものでもなく、アプリが実際に何をしているかを理解しているものでもありません。ただ、次のもっともらしいコードの塊でしかありません。</p>\n<p>それは、ボイラープレートには十分でしょう。しかし、ロジックには? エッジケースには? 実際のエンジニアリングには? それは、会議で速く自信を持って話すものの、仕様を読まない人を雇うようなものです。</p>\n<p>こうしたツールのほとんどは、コンテキストを <em>読んで</em> いません。少なくとも深くは読んでいません。近くの数行、おそらく関数名を取得するかもしれませんが、生成しているものを全体像に対して検証することは滅多にありません。課題のクロスチェックはしませんし、アーキテクチャレベルの認識もなく、ファイルやユースケースをまたいだ推論もありません。</p>\n<p>思慮深く、テスト可能で、コンテキストを認識した出力が必要な場合、<em>速度を落とし</em>、俯瞰し、実際に問題に取り組むAIシステムが必要です。</p>\n<p>それがスローAIの役割です。そして、AIが何をしているかを理解する時間を取ると、ハルシネーションを止めて<em>実際に</em>サポートしてくれるようになることがわかります。</p>\n<h2 id=\"heading-ai-1\"><strong>AIが遅いときに優れている理由</strong></h2>\n<p>核心として、大規模言語モデルは統計的推論マシンだということです。確率、パターン、そして(願わくば)あなたが与えたコンテキストに基づいて、次に何が来るかを予測して出力を生成します。しかし、ほとんどの開発者が忘れている注意点があります。良い予測には、それなりの作業が必要だということです。これは特に、ロジックを書く、アーキテクチャを理解する、複数のステップにわたって推論するなど、複雑なことをモデルに求める場合に当てはまります。出力の品質は、多くの場合において、推論の深さや段階に直接結びついています。</p>\n<p>これは、単純なプロンプトを超えて、マルチステージパイプラインとエージェント的な動作に移行する際において、特に当てはまります。AIツールが出力を検証し、関連ファイルを取り込み、矛盾をチェックし、または複数のアクション先を計画している場合、それは単に次のトークンを出力しているだけではないのです。それは<em>思考して</em>います。または、少なくとも、それに近いことを実行しています。</p>\n<p>このような非線形推論は、単一のフォワードパスでは実行できません。それには反省と検索、計画、そして時には自己修正さえも含まれます。こうしたプロセスはレイテンシ・フレンドリーではなく、インテリジェンス・フレンドリーなのです。</p>\n<p>要するに、AIに複雑なコードで実際に助けてもらいたいなら、それを調理させる必要があるのです。</p>\n<h2 id=\"heading-ai-2\">遅いことが新しいスマート:なぜ私たちはAIに考えさせるのか</h2>\n<p>スローAIは、私たちが話している用語の1つです。言い換えるなら包括的AIや正確なAI、あるいは正直に言えば「 <em>実質的</em> 役立ち有用なAI」と呼ぶこともできます。そして、それは今のAIプロダクトデザインで最もバズワードなアイデアの1つに密接に結びついています。そう、コンテキストエンジニアリングです。</p>\n<p>AIが問題について知っている、 <em>関連性があって解析された</em> 情報が多いほど、パフォーマンスは向上します。しかし、そのコンテキストは取り込まれ、解析され、優先順位が付けられ、推論される必要があります。そのようなパイプラインは、超低レイテンシAIの敵です… そしてそれは精度の敵でもあります。</p>\n<p>そして、それが私たちのAIコードレビューにおいて、最初のコメントを見るまでに最大5分かかる理由です。誤解しないでください。私たちは遅さを最適化しているわけではありません。コードベースとPRの複雑さに応じて、3分、あるいは1分でレビューを受け取ることもできます。私たちのパイプラインが複雑なのは、ユーザーが必要とする結果を出すために<em>必要</em>だからです。いつでも同時に実行されているプロセスの数を<strong><em>知りたく</em></strong>もないでしょう!</p>\n<p>しかし、どうでしょう? 複数のレビューおよび検証エージェントを使用した、非線形のマルチパスパイプラインでAIに時間をかけさせると、他のツールよりもノイズが少なく、より関連性の高いコードレビューコメントが生成されるのです。</p>\n<p>非線形推論は決して、速くありません。しかし、それは優れているのです。</p>\n<h2 id=\"heading-ai-3\">では、なぜほとんどのAIツールは<em>遅さよりも愚かさを選ぶのか?</em></h2>\n<p>まず、スローAIはすべてのツールにとってオプションではありません。たとえば、AIコーディングエージェントに質問をしている場合、返信を5分間待てる人はいないでしょう。そのやり取りには、即時性への期待が本質的に備わっています。</p>\n<p>ではコードレビューはどうでしょうか? 提出されたPRに対して、同僚がすぐに手を止めてコメントし始めることを期待する人はいません。したがって、ボットからのレビューの遅延も受け入れる余地があります。そして、そのレビューがより関連性が高いことで時間を節約するなら、その遅延を受け入れる価値は特に高くなります。</p>\n<p>しかし、なぜ多くの企業が、常に（実際にはそれを必要としないときでも）低レイテンシを優先するのでしょうか? まあ、私たちは訓練されてきましたし、即座の満足を期待するようユーザーを訓練してきました。ボタンをクリックすれば、速攻のレスポンスを得られます。関数名をタイプすれば、それについて考える前に提案を得られます。即効性がなければ壊れている、遅い、またはスタートアップがAWS料金の支払いを忘れたように感じられます。</p>\n<p>これは非常に強く叩き込まれているため、企業は遅延よりも間違っている方を積極的に選択しています。そして、人々がそうするとき、私たちの開発文化には有害で、後ろ向きな何かがあります。</p>\n<p>なぜなら、真実はこうだからです。最高のAIツールは必ずしも速く感じられず、代わりに思慮深く感じられます。時々、彼らは一時停止します。時々、プロンプトを推論したり、関連するコードを検索したり、応答を検証したりするために余分な時間を必要とします。しかし、それは待つ価値があるのです。たとえば、OpenAIのDeep Research機能が質問により良く答えるためにインターネットを最大20分かけて調査するからといって、利用を止める人はいません。処理中に他のことをして、戻ってくるだけです。</p>\n<p>もはや遅い = 壊れているという意味ではありません。それは<em>スマート</em>という意味です。むしろ、AIに関しては、速度こそがバグです。開発プロセスに実際に価値を追加するAIツールが必要な場合、それには応答性から信頼性へ、即時性からインサイトへの移行が必要です。そして、特に開発者にとって、そのトレードオフは理にかなっています。</p>\n<p>私たちは、今後5年間で最も価値のあるアプリは、速度を最適化するものではなく、インテリジェンスを最適化するものになると信じています。速いけど不要な結果と、遅いけれど価値あるもの、どちらが欲しいですか?</p>\n<h2 id=\"heading-coderabbit\">CodeRabbitのマントラ:ゆっくり動いて物事を修正する</h2>\n<p>CodeRabbitでは、他のツールのように、速度を優先するAIパイプライン最適化に過剰投資はしません。私たちは、信頼のために最適化を行います。それはコードを理解し、コンテキスト全体で推論し、より良いソフトウェアを構築するのに実際に役立つ出力を生成する時間を取るシステムを受け入れることを意味します。確かに、クイックプロンプトをたたき出すよりも遅いです。しかし、その余分な時間は明瞭さやカバレッジ、そして自信へと変わります。</p>\n<p>「<strong>Move fast and break things（素早く行動し破壊せよ）</strong>」は、MVPをデリバリーするには素晴らしいものでした。しかし、<em>品質</em>をデリバリーすることに関しては、私たちは別のものを信じています。ゆっくり動いて物事を修正する、AIに場の空気を読ませる、話す前に考えさせる…そして、本当に自信のある自動補完ではなく、シニアエンジニアから得られるようなサポートを提供する。それが、間違ったAIをスローAIよりも優先する、現在の後ろ向きな文化から抜け出す唯一の方法です。</p>\n<p><strong>私たちのレビューを試してみたいですか? こちらから</strong> <a target=\"_blank\" href=\"https://app.coderabbit.ai/login?free-trial\"><strong>14日間の無料トライアル</strong></a><strong>を入手してください!</strong></p>\n",
      "summary": "The rise of Slow AI: Why devs should stop speedrunning stupidの意訳です。\n私たちがコンピューターを使い始めて以来、そこには常に1つの基本ルールが存在しました。それは、速ければ速いほど良いということです。低レイテンシ、高スループット、待ち時間の短縮、これが鉄則でした。ボタンの応答に600ミリ秒もかかったり、注意力が維持できなくなるほど長いスピナーを見たりすることを望む人はいません。遅いということは、それは壊れているということです。議論の余地はないでしょう。\nそのため当然ながら、AIツールが私たちの開発ワークフローに忍び込み始めたとき、自動補完やエージェント、Copilot、その他何でも、同じ原則が適用されました。それはつまり、「速くしろ」「インスタントに感じさせろ」「魔法のように見せろ」です。\nしかし、実際のところ、AIは魔法ではありません。それは推論です。パイプライン、RAG、コンテキスト、そしてツールの呼び出しです。乱雑なコンテキストと確率的推測をジャグリングしているのです。そして、単なる自動補完以上のものを求めるなら、ベースとなるプロセスのパイプラインを構築する必要があります。そして、それは処理時間がかかります。それ以外では、基本的に愚かさのスピードランをしているだけなのです。そして、ツールがどれだけ速くとも、間違っているなら速度はまったく意味がありません。\nCodeRabbitでは、私たちが「スローAI」と呼ぶものを優先しています。そして、多くのAI企業が恐れて言えないことを言う勇気があります。私たちは「あなたを待たせます」。\n(そして、あなたはそれに感謝するでしょう)\nAI開発ツールはしばしば速く、自信に満ち、そして間違っている\n最近AIコーディングエージェントを使ったことがあるなら、こんな経験をしたことがあるのではないでしょうか。タイピングを止めるとほぼ同時に、驚くほど速い提案がポップアップしてきます。それは一見すると、正当なものに見えます。しかしその後…失敗します。場合によっては、派手に失敗します。さらに悪いことに、テストには合格するものの、別なファイルで何かを壊していたりします。\nこれはなぜでしょうか? なぜなら、今日のほとんどのAI開発ツールは、1つのことに最適化されているからです。それは速度です。数トークンをタイプすると、モデルは統計的に最も可能性の高い続きを予測します。それは必ずしも正しいものではなく、安全なものでもなく、アプリが実際に何をしているかを理解しているものでもありません。ただ、次のもっともらしいコードの塊でしかありません。\nそれは、ボイラープレートには十分でしょう。しかし、ロジックには? エッジケースには? 実際のエンジニアリングには? それは、会議で速く自信を持って話すものの、仕様を読まない人を雇うようなものです。\nこうしたツールのほとんどは、コンテキストを 読んで いません。少なくとも深くは読んでいません。近くの数行、おそらく関数名を取得するかもしれませんが、生成しているものを全体像に対して検証することは滅多にありません。課題のクロスチェックはしませんし、アーキテクチャレベルの認識もなく、ファイルやユースケースをまたいだ推論もありません。\n思慮深く、テスト可能で、コンテキストを認識した出力が必要な場合、速度を落とし、俯瞰し、実際に問題に取り組むAIシステムが必要です。\nそれがスローAIの役割です。そして、AIが何をしているかを理解する時間を取ると、ハルシネーションを止めて実際にサポートしてくれるようになることがわかります。\nAIが遅いときに優れている理由\n核心として、大規模言語モデルは統計的推論マシンだということです。確率、パターン、そして(願わくば)あなたが与えたコンテキストに基づいて、次に何が来るかを予測して出力を生成します。しかし、ほとんどの開発者が忘れている注意点があります。良い予測には、それなりの作業が必要だということです。これは特に、ロジックを書く、アーキテクチャを理解する、複数のステップにわたって推論するなど、複雑なことをモデルに求める場合に当てはまります。出力の品質は、多くの場合において、推論の深さや段階に直接結びついています。\nこれは、単純なプロンプトを超えて、マルチステージパイプラインとエージェント的な動作に移行する際において、特に当てはまります。AIツールが出力を検証し、関連ファイルを取り込み、矛盾をチェックし、または複数のアクション先を計画している場合、それは単に次のトークンを出力しているだけではないのです。それは思考しています。または、少なくとも、それに近いことを実行しています。\nこのような非線形推論は、単一のフォワードパスでは実行できません。それには反省と検索、計画、そして時には自己修正さえも含まれます。こうしたプロセスはレイテンシ・フレンドリーではなく、インテリジェンス・フレンドリーなのです。\n要するに、AIに複雑なコードで実際に助けてもらいたいなら、それを調理させる必要があるのです。\n遅いことが新しいスマート:なぜ私たちはAIに考えさせるのか\nスローAIは、私たちが話している用語の1つです。言い換えるなら包括的AIや正確なAI、あるいは正直に言えば「 実質的 役立ち有用なAI」と呼ぶこともできます。そして、それは今のAIプロダクトデザインで最もバズワードなアイデアの1つに密接に結びついています。そう、コンテキストエンジニアリングです。\nAIが問題について知っている、 関連性があって解析された 情報が多いほど、パフォーマンスは向上します。しかし、そのコンテキストは取り込まれ、解析され、優先順位が付けられ、推論される必要があります。そのようなパイプラインは、超低レイテンシAIの敵です… そしてそれは精度の敵でもあります。\nそして、それが私たちのAIコードレビューにおいて、最初のコメントを見るまでに最大5分かかる理由です。誤解しないでください。私たちは遅さを最適化しているわけではありません。コードベースとPRの複雑さに応じて、3分、あるいは1分でレビューを受け取ることもできます。私たちのパイプラインが複雑なのは、ユーザーが必要とする結果を出すために必要だからです。いつでも同時に実行されているプロセスの数を知りたくもないでしょう!\nしかし、どうでしょう? 複数のレビューおよび検証エージェントを使用した、非線形のマルチパスパイプラインでAIに時間をかけさせると、他のツールよりもノイズが少なく、より関連性の高いコードレビューコメントが生成されるのです。\n非線形推論は決して、速くありません。しかし、それは優れているのです。\nでは、なぜほとんどのAIツールは遅さよりも愚かさを選ぶのか?\nまず、スローAIはすべてのツールにとってオプションではありません。たとえば、AIコーディングエージェントに質問をしている場合、返信を5分間待てる人はいないでしょう。そのやり取りには、即時性への期待が本質的に備わっています。\nではコードレビューはどうでしょうか? 提出されたPRに対して、同僚がすぐに手を止めてコメントし始めることを期待する人はいません。したがって、ボットからのレビューの遅延も受け入れる余地があります。そして、そのレビューがより関連性が高いことで時間を節約するなら、その遅延を受け入れる価値は特に高くなります。\nしかし、なぜ多くの企業が、常に（実際にはそれを必要としないときでも）低レイテンシを優先するのでしょうか? まあ、私たちは訓練されてきましたし、即座の満足を期待するようユーザーを訓練してきました。ボタンをクリックすれば、速攻のレスポンスを得られます。関数名をタイプすれば、それについて考える前に提案を得られます。即効性がなければ壊れている、遅い、またはスタートアップがAWS料金の支払いを忘れたように感じられます。\nこれは非常に強く叩き込まれているため、企業は遅延よりも間違っている方を積極的に選択しています。そして、人々がそうするとき、私たちの開発文化には有害で、後ろ向きな何かがあります。\nなぜなら、真実はこうだからです。最高のAIツールは必ずしも速く感じられず、代わりに思慮深く感じられます。時々、彼らは一時停止します。時々、プロンプトを推論したり、関連するコードを検索したり、応答を検証したりするために余分な時間を必要とします。しかし、それは待つ価値があるのです。たとえば、OpenAIのDeep Research機能が質問により良く答えるためにインターネットを最大20分かけて調査するからといって、利用を止める人はいません。処理中に他のことをして、戻ってくるだけです。\nもはや遅い = 壊れているという意味ではありません。それはスマートという意味です。むしろ、AIに関しては、速度こそがバグです。開発プロセスに実際に価値を追加するAIツールが必要な場合、それには応答性から信頼性へ、即時性からインサイトへの移行が必要です。そして、特に開発者にとって、そのトレードオフは理にかなっています。\n私たちは、今後5年間で最も価値のあるアプリは、速度を最適化するものではなく、インテリジェンスを最適化するものになると信じています。速いけど不要な結果と、遅いけれど価値あるもの、どちらが欲しいですか?\nCodeRabbitのマントラ:ゆっくり動いて物事を修正する\nCodeRabbitでは、他のツールのように、速度を優先するAIパイプライン最適化に過剰投資はしません。私たちは、信頼のために最適化を行います。それはコードを理解し、コンテキスト全体で推論し、より良いソフトウェアを構築するのに実際に役立つ出力を生成する時間を取るシステムを受け入れることを意味します。確かに、クイックプロンプトをたたき出すよりも遅いです。しかし、その余分な時間は明瞭さやカバレッジ、そして自信へと変わります。\n「Move fast and break things（素早く行動し破壊せよ）」は、MVPをデリバリーするには素晴らしいものでした。しかし、品質をデリバリーすることに関しては、私たちは別のものを信じています。ゆっくり動いて物事を修正する、AIに場の空気を読ませる、話す前に考えさせる…そして、本当に自信のある自動補完ではなく、シニアエンジニアから得られるようなサポートを提供する。それが、間違ったAIをスローAIよりも優先する、現在の後ろ向きな文化から抜け出す唯一の方法です。\n私たちのレビューを試してみたいですか? こちらから 14日間の無料トライアルを入手してください!",
      "publishedAt": "2025-11-06T06:33:23.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:39.981Z",
      "score": 2.4420305912731135
    },
    {
      "id": "ae0a7777a114dc2c96c2a7b741215cfc",
      "title": "The rise of ‘Slow AI’: Why devs should stop speedrunning stupid",
      "url": "https://coderabbit.ai/blog/the-rise-of-slow-ai-why-devs-should-stop-speedrunning-stupid",
      "content": "<p>For as long as we’ve been building with machines, we’ve followed one core rule: faster is better. Lower latency, higher throughput, less waiting; that was gospel. Nobody wanted to wait 600ms for a button to respond or watch a spinner that lasts longer than their attention span. If it was slow, it was broken. Case closed.</p>\n<p>So naturally, when AI tools started creeping into our dev workflows, autocomplete, agents, copilots, you name it,  the same principle applied. Make it fast. Make it feel instant. Make it look like <strong><em>magic</em></strong>.</p>\n<p>But here’s the thing: AI isn’t magic. It’s inference. It’s pipelines and RAG and context and tool calls. It’s juggling messy context and probabilistic guesses. And if you want something smarter than glorified autocomplete, you need to build a pipeline of processes to provide scaffolding for that. Which takes time to process. Anything less and you’re basically just speedrunning stupid. And speed isn’t anything to brag about when your tool is just wrong faster.</p>\n<p>At CodeRabbit, we prioritize what we call Slow AI. And we have the guts to say what a lot of AI companies are too afraid to: We’re going to make you wait.</p>\n<p>(And you’ll thank us for it).</p>\n<h2 id=\"heading-ai-dev-tools-are-often-fast-confident-and-wrong\">AI dev tools are often fast, confident, and <strong><em>wrong</em></strong></h2>\n<p>If you've used an AI coding agent lately, you've probably seen it: a shockingly fast suggestion pops up almost as soon as you stop typing. It looks legit. But then… it fails silently. Or spectacularly. Or worse,  it passes the test and breaks something two files over.</p>\n<p>Why? Because most AI dev tools today are optimized for one thing: speed. Type a few tokens and the model predicts the most statistically likely continuation not necessarily the correct one, not the secure one, not the one that actually understands what your app is doing. Just the next plausible blob of code.</p>\n<p>That’s fine for boilerplate. But for logic? For edge cases? For actual engineering? It’s kind of like hiring someone who talks fast and confidently in meetings but never reads the specs.</p>\n<p>Most of these tools don’t <em>read</em> context, at least not deeply. They might grab a few nearby lines, maybe the function name, but they rarely verify what they’re generating against the bigger picture. No issue cross-checking. No architecture-level awareness. No reasoning across files or use cases.</p>\n<p>If you want outputs that are thoughtful, testable, and context-aware, you need AI systems that <em>slow down</em>, zoom out, and actually engage with the problem.</p>\n<p>That’s what Slow AI does. And it turns out, when your AI takes the time to understand what it’s doing, it stops hallucinating and starts <em>actually</em> helping.</p>\n<h2 id=\"heading-why-ai-is-better-when-its-slow\"><strong>Why AI is better when it’s slow</strong></h2>\n<p>At their core, large language models are statistical reasoning machines. They generate output by predicting what comes next based on probability, patterns, and (hopefully) the context you’ve given them. But here's the caveat most devs forget: good predictions take work. This is especially true when you're asking the model to do something complex like write logic, understand architecture, or reason across multiple steps. The quality of the output is often tied directly to the depth or stages of its inference.</p>\n<p>This is particularly true when you move beyond simple prompts and into multi-stage pipelines and agentic behavior. When an AI tool is verifying outputs, pulling in relevant files, checking for contradictions, or planning several actions ahead, it’s not just spitting out the next token… it’s <em>thinking</em>. Or, at least, performing a rough approximation of it.</p>\n<p>That kind of non-linear reasoning can’t be done in a single forward pass. It involves reflection, retrieval, planning, and sometimes even self-correction. These processes aren’t latency-friendly, they’re intelligence-friendly.</p>\n<p>In short: if you want AI to actually help on complex code, you have to let it cook.</p>\n<h2 id=\"heading-slow-is-the-new-smart-why-we-let-our-ai-think\">Slow is the new smart: Why we let our AI think</h2>\n<p>Slow AI is one term for what we’re talking about. But it could just as easily be called Comprehensive AI or Accurate AI or even <em>Actually</em> Helpful and Useful AI if we’re being honest. And it’s inextricably tied to one of the buzziest ideas in AI product design right now: context engineering.</p>\n<p>The more <em>relevant and parsed</em> info an AI knows about the problem, the better it performs but that context has to be pulled in, parsed, prioritized and reasoned over. That kind of pipeline is the enemy of ultra-low latency AI… and it’s also the enemy of accuracy.</p>\n<p>And that’s why our AI code reviews can take up to five minutes before you see the first comment. Don’t get us wrong, we’re not optimizing for slowness. You could get a review in three minutes or even one minute depending on the complexity of your codebase and PR. Our pipeline is complex because that’s what’s <em>required</em> to do the job our users need it to do. You don’t even want to <strong><em>know</em> th</strong>e number of concurrent processes we have going on at any time!</p>\n<p>But guess what? When we let our AI take its time using a non-linear, multi-pass pipeline with multiple review and verification agents, it generates less noise and more relevant code review comments than other tools.</p>\n<p>Non-linear reasoning isn’t fast. But it’s good.</p>\n<h2 id=\"heading-so-why-do-most-ai-tools-choose-stupid-over-slow\">So, why do most AI tools choose <em>stupid over slow?</em></h2>\n<p>Well, first, Slow AI isn’t an option for every tool. If you’re asking an AI coding agent a question, for example, you’re not going to wait five minutes for it to reply. There’s an expectation of immediacy inherent in that exchange.</p>\n<p>But code reviews? No one expects their co-worker to immediately drop what they’re doing and start commenting on a PR when it’s submitted. So, they’re willing to accept a delay in a review from a bot as well. And they’re especially willing to accept that delay if that review saves them time by being more relevant.</p>\n<p>But why do so many companies still prioritize low latency when their use cases don’t really require it? Well, we’ve been trained, and trained our users, to expect instant gratification. Click a button, get a dopamine hit. Type a function name, get a suggestion before you even think about it. Anything else feels broken, laggy, or like your startup forgot to pay its AWS bill.</p>\n<p>This has been drilled into us so hard that companies are out there actively choosing being wrong over being slow. And there’s something toxic and backwards about our development culture when folks do that.</p>\n<p>Because here’s the truth: the best AI tools don’t always feel fast. They feel thoughtful. Sometimes they pause. Sometimes they take an extra beat to reason through your prompt, retrieve relevant code, or validate their response. And that’s something worth waiting for. After all, no one is less likely to use OpenAI’s Deep Research feature because it takes up to 20 minutes to comb the internet for info to better answer your question. You just do something else while it’s processing and circle back.</p>\n<p>Slow doesn’t mean busted anymore, it means <em>smart</em>. If anything, speed is the bug when it comes to AI. If we want AI tools that actually add value to the development process, that requires a shift from responsiveness to reliability, from immediacy to insight. And for developers especially, that tradeoff makes sense.</p>\n<p>We believe that the most valuable apps in the next five years won’t be the ones that optimize for speed but the ones that optimize for intelligence. Who wants fast garbage over slow value?</p>\n<h2 id=\"heading-coderabbits-mantra-move-slow-and-fix-things\">CodeRabbit’s mantra: Move slow and fix things</h2>\n<p>At CodeRabbit, we don’t optimize our AI pipelines for speed at all costs like everyone else. We optimize for trust. That means embracing systems that take the time to understand your code, reason across context, and generate outputs that actually help you build better software. Yes, it’s slower than hammering out a quick prompt. But that extra time buys you clarity, coverage, and confidence.</p>\n<p>“<strong>Move fast and break things</strong>” was great for shipping MVPs. But when it comes to shipping <em>quality</em>, we believe in something else: Move slow and fix things. Let the AI read the room. Let it think before it speaks. And let it give you the kind of help you’d expect from a senior engineer, not just a really confident autocomplete. That’s the only way to break out of this backwards culture that prioritizes wrong AI over slow AI.</p>\n<p><strong>Want to try our reviews out? Get a</strong> <a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong>14-day free trial here!</strong></a></p>\n",
      "summary": "For as long as we’ve been building with machines, we’ve followed one core rule: faster is better. Lower latency, higher throughput, less waiting; that was gospel. Nobody wanted to wait 600ms for a button to respond or watch a spinner that lasts longer than their attention span. If it was slow, it was broken. Case closed.\nSo naturally, when AI tools started creeping into our dev workflows, autocomplete, agents, copilots, you name it,  the same principle applied. Make it fast. Make it feel instant. Make it look like magic.\nBut here’s the thing: AI isn’t magic. It’s inference. It’s pipelines and RAG and context and tool calls. It’s juggling messy context and probabilistic guesses. And if you want something smarter than glorified autocomplete, you need to build a pipeline of processes to provide scaffolding for that. Which takes time to process. Anything less and you’re basically just speedrunning stupid. And speed isn’t anything to brag about when your tool is just wrong faster.\nAt CodeRabbit, we prioritize what we call Slow AI. And we have the guts to say what a lot of AI companies are too afraid to: We’re going to make you wait.\n(And you’ll thank us for it).\nAI dev tools are often fast, confident, and wrong\nIf you've used an AI coding agent lately, you've probably seen it: a shockingly fast suggestion pops up almost as soon as you stop typing. It looks legit. But then… it fails silently. Or spectacularly. Or worse,  it passes the test and breaks something two files over.\nWhy? Because most AI dev tools today are optimized for one thing: speed. Type a few tokens and the model predicts the most statistically likely continuation not necessarily the correct one, not the secure one, not the one that actually understands what your app is doing. Just the next plausible blob of code.\nThat’s fine for boilerplate. But for logic? For edge cases? For actual engineering? It’s kind of like hiring someone who talks fast and confidently in meetings but never reads the specs.\nMost of these tools don’t read context, at least not deeply. They might grab a few nearby lines, maybe the function name, but they rarely verify what they’re generating against the bigger picture. No issue cross-checking. No architecture-level awareness. No reasoning across files or use cases.\nIf you want outputs that are thoughtful, testable, and context-aware, you need AI systems that slow down, zoom out, and actually engage with the problem.\nThat’s what Slow AI does. And it turns out, when your AI takes the time to understand what it’s doing, it stops hallucinating and starts actually helping.\nWhy AI is better when it’s slow\nAt their core, large language models are statistical reasoning machines. They generate output by predicting what comes next based on probability, patterns, and (hopefully) the context you’ve given them. But here's the caveat most devs forget: good predictions take work. This is especially true when you're asking the model to do something complex like write logic, understand architecture, or reason across multiple steps. The quality of the output is often tied directly to the depth or stages of its inference.\nThis is particularly true when you move beyond simple prompts and into multi-stage pipelines and agentic behavior. When an AI tool is verifying outputs, pulling in relevant files, checking for contradictions, or planning several actions ahead, it’s not just spitting out the next token… it’s thinking. Or, at least, performing a rough approximation of it.\nThat kind of non-linear reasoning can’t be done in a single forward pass. It involves reflection, retrieval, planning, and sometimes even self-correction. These processes aren’t latency-friendly, they’re intelligence-friendly.\nIn short: if you want AI to actually help on complex code, you have to let it cook.\nSlow is the new smart: Why we let our AI think\nSlow AI is one term for what we’re talking about. But it could just as easily be called Comprehensive AI or Accurate AI or even Actually Helpful and Useful AI if we’re being honest. And it’s inextricably tied to one of the buzziest ideas in AI product design right now: context engineering.\nThe more relevant and parsed info an AI knows about the problem, the better it performs but that context has to be pulled in, parsed, prioritized and reasoned over. That kind of pipeline is the enemy of ultra-low latency AI… and it’s also the enemy of accuracy.\nAnd that’s why our AI code reviews can take up to five minutes before you see the first comment. Don’t get us wrong, we’re not optimizing for slowness. You could get a review in three minutes or even one minute depending on the complexity of your codebase and PR. Our pipeline is complex because that’s what’s required to do the job our users need it to do. You don’t even want to know the number of concurrent processes we have going on at any time!\nBut guess what? When we let our AI take its time using a non-linear, multi-pass pipeline with multiple review and verification agents, it generates less noise and more relevant code review comments than other tools.\nNon-linear reasoning isn’t fast. But it’s good.\nSo, why do most AI tools choose stupid over slow?\nWell, first, Slow AI isn’t an option for every tool. If you’re asking an AI coding agent a question, for example, you’re not going to wait five minutes for it to reply. There’s an expectation of immediacy inherent in that exchange.\nBut code reviews? No one expects their co-worker to immediately drop what they’re doing and start commenting on a PR when it’s submitted. So, they’re willing to accept a delay in a review from a bot as well. And they’re especially willing to accept that delay if that review saves them time by being more relevant.\nBut why do so many companies still prioritize low latency when their use cases don’t really require it? Well, we’ve been trained, and trained our users, to expect instant gratification. Click a button, get a dopamine hit. Type a function name, get a suggestion before you even think about it. Anything else feels broken, laggy, or like your startup forgot to pay its AWS bill.\nThis has been drilled into us so hard that companies are out there actively choosing being wrong over being slow. And there’s something toxic and backwards about our development culture when folks do that.\nBecause here’s the truth: the best AI tools don’t always feel fast. They feel thoughtful. Sometimes they pause. Sometimes they take an extra beat to reason through your prompt, retrieve relevant code, or validate their response. And that’s something worth waiting for. After all, no one is less likely to use OpenAI’s Deep Research feature because it takes up to 20 minutes to comb the internet for info to better answer your question. You just do something else while it’s processing and circle back.\nSlow doesn’t mean busted anymore, it means smart. If anything, speed is the bug when it comes to AI. If we want AI tools that actually add value to the development process, that requires a shift from responsiveness to reliability, from immediacy to insight. And for developers especially, that tradeoff makes sense.\nWe believe that the most valuable apps in the next five years won’t be the ones that optimize for speed but the ones that optimize for intelligence. Who wants fast garbage over slow value?\nCodeRabbit’s mantra: Move slow and fix things\nAt CodeRabbit, we don’t optimize our AI pipelines for speed at all costs like everyone else. We optimize for trust. That means embracing systems that take the time to understand your code, reason across context, and generate outputs that actually help you build better software. Yes, it’s slower than hammering out a quick prompt. But that extra time buys you clarity, coverage, and confidence.\n“Move fast and break things” was great for shipping MVPs. But when it comes to shipping quality, we believe in something else: Move slow and fix things. Let the AI read the room. Let it think before it speaks. And let it give you the kind of help you’d expect from a senior engineer, not just a really confident autocomplete. That’s the only way to break out of this backwards culture that prioritizes wrong AI over slow AI.\nWant to try our reviews out? Get a 14-day free trial here!",
      "publishedAt": "2025-11-05T08:43:53.000Z",
      "author": "Howon Lee",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.981Z",
      "score": 4.442284460206401
    },
    {
      "id": "450986f4be07536817d735c23108e637",
      "title": "統一的プロンプトの終焉：もはやllmモデルに互換性はありません",
      "url": "https://coderabbit.ai/blog/the-end-of-one-sized-fits-all-prompts-why-llm-models-are-no-longer-interchangeable-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/the-end-of-one-sized-fits-all-prompts-why-llm-models-are-no-longer-interchangeable\">Why LLM models are no longer interchangeable</a>の意訳です。</p>\n<p>開発者やプロダクトビルダーにとって、この数年間はLLMがアプリケーション開発を導いてきました。プロダクトを改善したいなら、最新のLLMを利用すれば良い。ただモデルを切り替えるだけで、ツールの性能を一段階引き上げられるのです。</p>\n<p>しかし、その時代は終わりました。AnthropicのClaude Sonnet 4.5やOpenAIのGPT-5-Codexのような新しいモデルは、根本的に異なる方向へ分岐し始めています。どのモデルを使うかという選択は、もはや単なるエンジニアリング上の判断ではなく、極めて重要な<strong>プロダクト上の意思決定</strong>なのです。モデルを切り替えた瞬間に、あなたのプロダクトの「質感」そのものが変わります。</p>\n<p>いわゆる「万能モデル時代」は終焉を迎えました。あなたが選ぶモデルは、あなたのプロダクトが<strong>何であるか、何をするのか、どのように動作するのか</strong> を象徴する存在になります。たとえあなたがそう意図していなくても、です。</p>\n<p>本記事では、この新しい時代における3つの驚くべき発見を紹介します。それは「LLM選択がプロダクトの表明になった理由」「モデルが持つ明確な個性とスタイルの違い」、そして「プロンプトが単一命令から適応的システムへ進化すべき理由」の3つです。</p>\n<h2 id=\"heading-1-llm\">学びポイント 1: LLMの選択はプロダクトの“声明”である</h2>\n<p>LLMモデルの選択は、もはや「新しいAPIを実装すれば済む」といった単純な技術的決定ではありません。これは、<strong>どんなユーザー体験を作りたいのか、どのような失敗を許容するのか、何を最適化したいのか、どの指標で優位に立ちたいのか</strong>という、プロダクトの方向性を決める意思決定です。</p>\n<p>モデルはそれぞれ固有の「性格」や「推論方法」「直感」を持つようになっており、それがプロダクトの“感触”や“振る舞い”を直接的に形作ります。単に「出力が正しいかどうか」ではなく、「どのように考え、どのように伝えるか」まで変わります。違うモデルを選べば、ツールの能力からユーザーとの対話の仕方まで、すべてが異なるのです。</p>\n<p>では、モデルの定量的な性能だけを測る従来型ベンチマークが通用しない今、何を頼りにプロダクトの方向を定めれば良いのでしょうか？チームやユーザーへのアンケート、フォーカスグループもありますが、厳密に実施しなければ客観性に欠ける恐れがあります。</p>\n<p>CodeRabbitでは、この選択を客観化するために、独自の<a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning-ja\">重要指標のメトリクス</a>を作成しました。このメトリクスは、単なる性能や精度だけを見ません。<strong>可読性、冗長性、信号対雑音比</strong>など、多面的に評価します。</p>\n<p>このような指標により、焦点は「性能」や「リーダーボードの順位」から、「プロダクトとユーザーにとって本当に重要な要素」へと移ります。例えば、技術的に正しくても影響の少ない提案が多すぎれば、ユーザーを疲弊させ、かつトークンを浪費します。理論上「賢い」モデルでも、ユーザーのワークフローに合わなければ体験を悪化させます。</p>\n<p>自社のメトリクスを定義し、新しいモデルが自社とユーザーのニーズを満たすかを測ることを強く推奨します。これらのメトリクスは静的なものではなく、ユーザー行動やフィードバックによって進化させるべきです。目標は、「ユーザーの好みを予測できる基準」を見つけることです。</p>\n<p>結論として、最適なモデルとは「リーダーボード上の1位」ではなく、<strong>あなたの設計した体験やユーザーのニーズに最も本能的に合うモデル</strong>です。</p>\n<h2 id=\"heading-2\">学びポイント 2: フロンティアモデルは「性格」が分岐した</h2>\n<p>モデルはこれまで以上に「<strong>作られるものではなく、育つもの</strong>」となっており、その結果、モデル世代ごとに固有の直感と行動特性が生まれています。ポストトレーニングの手法（cookbook）の違いが、モデルクラスごとの方向性を根本的に変えました。1つのモデルで完璧に動くプロンプトも、別のモデルでは通用しません。つまり、<strong>同じタスクに対する根本的なアプローチが異なる</strong>のです。</p>\n<p>これを理解する良い例えとして、モデルを異なる「職業的アーキタイプ」に喩えることができます。<br />Sonnet 4.5は几帳面な会計士出身の開発者、GPT-5-Codexは倫理意識の高い堅実なエンジニア、GPT-5はバグを徹底的に探す職人気質の開発者、Sonnet 4は活動的な新卒エンジニア。<br />GPT-5系はClaude系よりもソリューション空間を広く探索し、Claudeはプロンプトの文脈に忠実に留まる傾向があります。どのモデルが適しているかは、<strong>プロダクトが目指す目的</strong>によって完全に異なります。</p>\n<p>CodeRabbitでは、モデル評価と特性分析を体系的に行い、その結果を基にプロンプトとデプロイ方法を最適化しています。たとえば、Sonnet 4.5とGPT-5-Codexを比較すると、Sonnet 4.5は「<strong>高リコール型のポイント修正者</strong>」、GPT-5-Codexは「<strong>ピンポイントなパッチ生成者</strong>」として性質づけられます。</p>\n<p>こうした定性的な違いは、明確な運用上の違いに転化します。</p>\n<div class=\"hn-table\">\n<table>\n<thead>\n<tr>\n<td>次元</td><td>Claude Sonnet 4.5</td><td>GPT-5-Codex</td></tr>\n</thead>\n<tbody>\n<tr>\n<td>デフォルトの語彙選択</td><td>“Critical,” “Add,” “Remove,” “Consider”</td><td>“Fix,” “Guard,” “Prevent,” “Restore,” “Drop”</td></tr>\n<tr>\n<td>例の効率性</td><td>明示的なルールを好む。命令形を覚えやすい</td><td>例が少なくても長い文脈でフォーマットを維持できる</td></tr>\n<tr>\n<td>思考スタイル</td><td>慎重。多くのバグを見つけるが、重要な1つを見逃すことも</td><td>柔軟。必要に応じて深く考え、再確認を要しない。難解なバグを捕捉しやすい</td></tr>\n<tr>\n<td>行動傾向</td><td>広範囲に修正提案。コメントが多く、人間的。致命的でない問題も拾う</td><td>簡潔でバランスの取れた研究的レビュー。副次的影響を指摘する傾向</td></tr>\n<tr>\n<td>レビュー構造</td><td>「何が悪い」「なぜ悪い」「具体的修正コード」</td><td>「何をすべきか」「なぜすべきか」「修正コード＋影響」</td></tr>\n<tr>\n<td>文脈認識</td><td>コンテキストウィンドウを意識。トークン管理が巧み</td><td>明示的なウィンドウ意識は弱い（時計なしで料理するような感覚）</td></tr>\n<tr>\n<td>冗長性</td><td>高い。読みやすいが語数が倍増</td><td>低い。情報密度が高く、読むのに集中を要する</td></tr>\n</tbody>\n</table>\n</div><h2 id=\"heading-3\">学びポイント 3: プロンプトはもはや単一構造ではない</h2>\n<p>モデルの根本的な性質が分岐したことで、あるモデル用に書いたプロンプトを他モデルで「そのまま」使うことはできなくなっています。<br />たとえばClaude用の厳格な命令プロンプトはGPT-5-Codexでは過剰拘束になり、Codex用に推論重視で最適化したプロンプトは、Claudeで性能を発揮できません。つまり、<strong>「一枚岩のプロンプト時代」は完全に終わった</strong>のです。</p>\n<p>では、新モデルを導入したいエンジニアリングチームはどうすればよいでしょうか？<br />答えは――<strong>より多くのプロンプトエンジニアリング</strong>です。ただし嘆く必要はありません。いくつかの実践的な方法があります。</p>\n<h3 id=\"heading-44ox44ot44oz44ox44oi44o744k144ow44om44ol44od44oi44gu55m75ac0\">プロンプト・サブユニットの登場</h3>\n<p>CodeRabbitで見出した解決策の一つが「<strong>プロンプト・サブユニット</strong>」です。<br />これは、モデルに依存しない中核プロンプト（基本タスクと一般指示）を定義し、その上にモデル固有のサブユニット（スタイル、フォーマット、例示）を積み上げる構成です。</p>\n<p>たとえばCodexとSonnet 4.5では実装詳細が大きく異なりますが、次のような発見がありました：</p>\n<ul>\n<li><p><strong>Claude:</strong> 「DO」「DO NOT」のような強い命令語を使用する。Anthropic系モデルはシステムプロンプトの末尾情報をよく参照し、長文でもフォーマット遵守が得意。明示的な指示を好む。</p>\n</li>\n<li><p><strong>GPT-5:</strong> 一般的で整合性のある指示を使用する。OpenAI系はシステムプロンプトの下部ほど注意力が減衰するため、長文では出力フォーマットを忘れがち。抽象的なガイダンスを好み、推論の深さを示す傾向がある。</p>\n</li>\n</ul>\n<h3 id=\"heading-eval\">ユーザーフィードバックと評価（eval）</h3>\n<p>もう一つの解決策は、<strong>ユーザーフィードバックと内部評価による継続的アップデート</strong>です。<br />AIコードレビューボットなどLLMアプリの最適化において、最も重要なのは外部ベンチマークではなく、「ユーザーが出力に納得できるか」です。</p>\n<p>モデル間で「技術的正確性」が高くても、過剰なコメントや冗長性があると価値を下げてしまいます。<br />したがって、<strong>受容率、S/N比、p95レイテンシ、コスト</strong>といった実運用メトリクスを測定し、プロンプトを少しずつ調整することで、システムをユーザー期待とプロダクト目標に整合させ続けることができます。</p>\n<p>ベンチマークでの定量的結果が良くても、ユーザー受容率が低い――そんな事態は避けるべきです。</p>\n<h2 id=\"heading-44g44go44kb\">まとめ</h2>\n<p>プロンプトエンジニアリングは、「万能テンプレート」から「モデル特化型パラダイム」へと変わりました。<br />脆弱な単一プロンプトや「差し替え可能なモデル」の時代は終わりです。これからは、<strong>モジュラー型プロンプト設計</strong>と<strong>意図的なモデル選択</strong>が、プロダクトの強靭性を生みます。</p>\n<p>モデルが進化し続ける以上、LLMスタックやプロンプトも画一的であってはいけません。<br />それは「生きたシステム」として扱うべきです。<strong>調整し、テストし、確認し、繰り返す。</strong></p>\n<p>また、最新モデルの実運用挙動に関する詳細なベンチマークもぜひ確認してください。今後の選択に必要なデータが得られるでしょう。</p>\n<ul>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks\">GPT-5 Codex: How it solves for GPT-5's drawbacks</a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/claude-sonnet-45-better-performance-but-a-paradox\">Claude Sonnet 4.5: Better performance but a paradox</a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">Benchmarking GPT-5: Why it’s a generational leap in reasoning</a></p>\n</li>\n</ul>\n<p><strong><em>CodeRabbitを14日間無料でお試しください。</em></strong><br /><a target=\"_blank\" href=\"https://coderabbit.link/rk7tdeC\">https://coderabbit.link/rk7tdeC</a></p>\n",
      "summary": "Why LLM models are no longer interchangeableの意訳です。\n開発者やプロダクトビルダーにとって、この数年間はLLMがアプリケーション開発を導いてきました。プロダクトを改善したいなら、最新のLLMを利用すれば良い。ただモデルを切り替えるだけで、ツールの性能を一段階引き上げられるのです。\nしかし、その時代は終わりました。AnthropicのClaude Sonnet 4.5やOpenAIのGPT-5-Codexのような新しいモデルは、根本的に異なる方向へ分岐し始めています。どのモデルを使うかという選択は、もはや単なるエンジニアリング上の判断ではなく、極めて重要なプロダクト上の意思決定なのです。モデルを切り替えた瞬間に、あなたのプロダクトの「質感」そのものが変わります。\nいわゆる「万能モデル時代」は終焉を迎えました。あなたが選ぶモデルは、あなたのプロダクトが何であるか、何をするのか、どのように動作するのか を象徴する存在になります。たとえあなたがそう意図していなくても、です。\n本記事では、この新しい時代における3つの驚くべき発見を紹介します。それは「LLM選択がプロダクトの表明になった理由」「モデルが持つ明確な個性とスタイルの違い」、そして「プロンプトが単一命令から適応的システムへ進化すべき理由」の3つです。\n学びポイント 1: LLMの選択はプロダクトの“声明”である\nLLMモデルの選択は、もはや「新しいAPIを実装すれば済む」といった単純な技術的決定ではありません。これは、どんなユーザー体験を作りたいのか、どのような失敗を許容するのか、何を最適化したいのか、どの指標で優位に立ちたいのかという、プロダクトの方向性を決める意思決定です。\nモデルはそれぞれ固有の「性格」や「推論方法」「直感」を持つようになっており、それがプロダクトの“感触”や“振る舞い”を直接的に形作ります。単に「出力が正しいかどうか」ではなく、「どのように考え、どのように伝えるか」まで変わります。違うモデルを選べば、ツールの能力からユーザーとの対話の仕方まで、すべてが異なるのです。\nでは、モデルの定量的な性能だけを測る従来型ベンチマークが通用しない今、何を頼りにプロダクトの方向を定めれば良いのでしょうか？チームやユーザーへのアンケート、フォーカスグループもありますが、厳密に実施しなければ客観性に欠ける恐れがあります。\nCodeRabbitでは、この選択を客観化するために、独自の重要指標のメトリクスを作成しました。このメトリクスは、単なる性能や精度だけを見ません。可読性、冗長性、信号対雑音比など、多面的に評価します。\nこのような指標により、焦点は「性能」や「リーダーボードの順位」から、「プロダクトとユーザーにとって本当に重要な要素」へと移ります。例えば、技術的に正しくても影響の少ない提案が多すぎれば、ユーザーを疲弊させ、かつトークンを浪費します。理論上「賢い」モデルでも、ユーザーのワークフローに合わなければ体験を悪化させます。\n自社のメトリクスを定義し、新しいモデルが自社とユーザーのニーズを満たすかを測ることを強く推奨します。これらのメトリクスは静的なものではなく、ユーザー行動やフィードバックによって進化させるべきです。目標は、「ユーザーの好みを予測できる基準」を見つけることです。\n結論として、最適なモデルとは「リーダーボード上の1位」ではなく、あなたの設計した体験やユーザーのニーズに最も本能的に合うモデルです。\n学びポイント 2: フロンティアモデルは「性格」が分岐した\nモデルはこれまで以上に「作られるものではなく、育つもの」となっており、その結果、モデル世代ごとに固有の直感と行動特性が生まれています。ポストトレーニングの手法（cookbook）の違いが、モデルクラスごとの方向性を根本的に変えました。1つのモデルで完璧に動くプロンプトも、別のモデルでは通用しません。つまり、同じタスクに対する根本的なアプローチが異なるのです。\nこれを理解する良い例えとして、モデルを異なる「職業的アーキタイプ」に喩えることができます。\nSonnet 4.5は几帳面な会計士出身の開発者、GPT-5-Codexは倫理意識の高い堅実なエンジニア、GPT-5はバグを徹底的に探す職人気質の開発者、Sonnet 4は活動的な新卒エンジニア。\nGPT-5系はClaude系よりもソリューション空間を広く探索し、Claudeはプロンプトの文脈に忠実に留まる傾向があります。どのモデルが適しているかは、プロダクトが目指す目的によって完全に異なります。\nCodeRabbitでは、モデル評価と特性分析を体系的に行い、その結果を基にプロンプトとデプロイ方法を最適化しています。たとえば、Sonnet 4.5とGPT-5-Codexを比較すると、Sonnet 4.5は「高リコール型のポイント修正者」、GPT-5-Codexは「ピンポイントなパッチ生成者」として性質づけられます。\nこうした定性的な違いは、明確な運用上の違いに転化します。\n次元Claude Sonnet 4.5GPT-5-Codex\nデフォルトの語彙選択“Critical,” “Add,” “Remove,” “Consider”“Fix,” “Guard,” “Prevent,” “Restore,” “Drop”\n例の効率性明示的なルールを好む。命令形を覚えやすい例が少なくても長い文脈でフォーマットを維持できる\n思考スタイル慎重。多くのバグを見つけるが、重要な1つを見逃すことも柔軟。必要に応じて深く考え、再確認を要しない。難解なバグを捕捉しやすい\n行動傾向広範囲に修正提案。コメントが多く、人間的。致命的でない問題も拾う簡潔でバランスの取れた研究的レビュー。副次的影響を指摘する傾向\nレビュー構造「何が悪い」「なぜ悪い」「具体的修正コード」「何をすべきか」「なぜすべきか」「修正コード＋影響」\n文脈認識コンテキストウィンドウを意識。トークン管理が巧み明示的なウィンドウ意識は弱い（時計なしで料理するような感覚）\n冗長性高い。読みやすいが語数が倍増低い。情報密度が高く、読むのに集中を要する\n学びポイント 3: プロンプトはもはや単一構造ではない\nモデルの根本的な性質が分岐したことで、あるモデル用に書いたプロンプトを他モデルで「そのまま」使うことはできなくなっています。\nたとえばClaude用の厳格な命令プロンプトはGPT-5-Codexでは過剰拘束になり、Codex用に推論重視で最適化したプロンプトは、Claudeで性能を発揮できません。つまり、「一枚岩のプロンプト時代」は完全に終わったのです。\nでは、新モデルを導入したいエンジニアリングチームはどうすればよいでしょうか？\n答えは――より多くのプロンプトエンジニアリングです。ただし嘆く必要はありません。いくつかの実践的な方法があります。\nプロンプト・サブユニットの登場\nCodeRabbitで見出した解決策の一つが「プロンプト・サブユニット」です。\nこれは、モデルに依存しない中核プロンプト（基本タスクと一般指示）を定義し、その上にモデル固有のサブユニット（スタイル、フォーマット、例示）を積み上げる構成です。\nたとえばCodexとSonnet 4.5では実装詳細が大きく異なりますが、次のような発見がありました：\nClaude: 「DO」「DO NOT」のような強い命令語を使用する。Anthropic系モデルはシステムプロンプトの末尾情報をよく参照し、長文でもフォーマット遵守が得意。明示的な指示を好む。\nGPT-5: 一般的で整合性のある指示を使用する。OpenAI系はシステムプロンプトの下部ほど注意力が減衰するため、長文では出力フォーマットを忘れがち。抽象的なガイダンスを好み、推論の深さを示す傾向がある。\nユーザーフィードバックと評価（eval）\nもう一つの解決策は、ユーザーフィードバックと内部評価による継続的アップデートです。\nAIコードレビューボットなどLLMアプリの最適化において、最も重要なのは外部ベンチマークではなく、「ユーザーが出力に納得できるか」です。\nモデル間で「技術的正確性」が高くても、過剰なコメントや冗長性があると価値を下げてしまいます。\nしたがって、受容率、S/N比、p95レイテンシ、コストといった実運用メトリクスを測定し、プロンプトを少しずつ調整することで、システムをユーザー期待とプロダクト目標に整合させ続けることができます。\nベンチマークでの定量的結果が良くても、ユーザー受容率が低い――そんな事態は避けるべきです。\nまとめ\nプロンプトエンジニアリングは、「万能テンプレート」から「モデル特化型パラダイム」へと変わりました。\n脆弱な単一プロンプトや「差し替え可能なモデル」の時代は終わりです。これからは、モジュラー型プロンプト設計と意図的なモデル選択が、プロダクトの強靭性を生みます。\nモデルが進化し続ける以上、LLMスタックやプロンプトも画一的であってはいけません。\nそれは「生きたシステム」として扱うべきです。調整し、テストし、確認し、繰り返す。\nまた、最新モデルの実運用挙動に関する詳細なベンチマークもぜひ確認してください。今後の選択に必要なデータが得られるでしょう。\nGPT-5 Codex: How it solves for GPT-5's drawbacks\nClaude Sonnet 4.5: Better performance but a paradox\nBenchmarking GPT-5: Why it’s a generational leap in reasoning\nCodeRabbitを14日間無料でお試しください。\nhttps://coderabbit.link/rk7tdeC",
      "publishedAt": "2025-10-25T00:45:11.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.982Z",
      "score": 1.0785026384190843
    },
    {
      "id": "da0ef8f119da69168d25fec79054c8cf",
      "title": "The end of one-sized-fits-all prompts: Why LLM models are no longer interchangeable",
      "url": "https://coderabbit.ai/blog/the-end-of-one-sized-fits-all-prompts-why-llm-models-are-no-longer-interchangeable",
      "content": "<p>For developers and product builders, one assumption has guided the last few years of LLM application development. To improve your product, just swap in the latest frontier large language model. Flip a single switch and your tool’s capabilities level up.</p>\n<p>But that era is over. We’re now seeing that new models like Anthropic’s Claude Sonnet 4.5 and OpenAI’s GPT-5-Codex have diverged in fundamental ways. The choice of which model to use is no longer a simple engineering decision but a critical product decision. Flip that switch today… and the very texture of your product changes. </p>\n<p>The one-size-fits-all model era is over; the model you choose now expresses something integral about what your product is and does, as well as, how it works. Whether you want it to or not.</p>\n<p>In this blog, we’ll explore three surprising takeaways from this new era: why your LLM is now a statement about your product, how models now have distinct personalities and styles, and why your prompts have to now evolve from monolithic instructions to adaptive systems. </p>\n<h2 id=\"heading-takeaway-1-llm-choice-is-now-a-statement-about-your-product\">Takeaway 1: LLM choice is now a statement about your product</h2>\n<p>Choosing a model is no longer a straightforward decision where the main consequence of your choice is having to implement a new API. It is now a product decision about the user experience you want to create, the failure modes you can tolerate, the economics you want to optimize for, and the metrics you want to excel in. </p>\n<p>Models have developed distinct “personalities,” ways of reasoning, and instincts that directly shape how your product feels and behaves that go beyond just whether its output is technically right or wrong. Choose a different model and everything from what your tool is capable of to how it communicates with your users is significantly different. </p>\n<p>So, in a world where traditional benchmarks that primarily or exclusively measure quantitative aspects of a model’s performance are no longer enough, what can you turn to for the data you need to chart your product’s direction? You could survey your team or your users or conduct focus groups but that could lack objectivity if you don’t do it in a rigorous manner. </p>\n<p>To make this choice objective for our team, we focused on creating an internal <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">North Star metrics matrix</a> at CodeRabbit. Our metrics don’t just look at raw performance or accuracy. We also take into account readability, verbosity, signal-to-noise ratios, and more.</p>\n<p>These kinds of metrics shift the focus from raw performance accuracy or leaderboard performance to what matters to our product and to our users. For example, a flood of low-impact suggestions, even if technically correct, burns user attention and consumes tokens. A theoretically “smarter” model can easily create a worse product experience if the output doesn’t align with your users’ workflow.</p>\n<p>I would strongly recommend creating your own North Star metrics to better gauge whether a new model meets your products’ and users’ needs. These shouldn’t be static metrics but should be informed by user feedback and user behavior in your product and evolve over time. Your goal is to find the right list of criteria to measure that predict your users preferences. </p>\n<p>What you’ll find is that the right model is the one whose instincts match the designed product behavior and your users’ needs, not the one at the top of any external leaderboard. </p>\n<h2 id=\"heading-takeaway-2-frontier-models-have-divergent-personalities\">Takeaway 2: Frontier models have divergent ‘personalities’</h2>\n<p>Models are (now more than ever) “<strong>grown, not built,</strong>” and as a result, the latest generation has developed distinct instincts and behaviors. Different post-training cookbooks have fundamentally changed the direction of each model class. A prompt that works perfectly for one model will not work the same in another. Their fundamental approaches to the same task have diverged.</p>\n<p>One powerful analogy that drives this point home is to think of the models as different professional archetypes. Sonnet 4.5 is like a meticulous accountant turned developer, meanwhile GPT-5-Codex is an upright ethical coder, GPT-5 is a bug-hunting detailed developer, and Sonnet 4 was a hyper-active new grad. The GPT-5 model class would make logical jumps further out in the solution space compared to the Claude model class, which tends to stay near the prompts itself. Which model is right for your use case and product, depends entirely on what you are wanting your product to achieve. </p>\n<p>At CodeRabbit, we take a methodical approach to model evaluation and characterization. We then use this data to improve how we prompt and deploy models, ensuring we are always using the right model for each use case within our product. To give you an example of how we look at the different models, let’s compare Sonnet 4.5 and GPT-5-Codex. Based on extensive internal use and evals, we characterized Sonnet 4.5 as a “<strong>high-recall point-fixer,</strong>” aiming for comprehensive coverage. In contrast, GPT-5-Codex acts as a “<strong>patch generator,</strong>” preferring surgical, local changes. </p>\n<p>These qualitative differences translate into hard, operational differences.  </p>\n<table><tbody><tr><td><p>Dimension</p></td><td><p>Claude Sonnet 4.5</p></td><td><p>GPT-5-Codex</p></td></tr><tr><td><p>Default Word Choice</p></td><td><p>“Critical,” “Add,” “Remove,” “Consider”</p></td><td><p>“Fix,” “Guard,” “Prevent,” “Restore,” “Drop”</p></td></tr><tr><td><p>Example-Efficiency </p></td><td><p>Remembers imperatives; benefits from explicit rules</p></td><td><p>Needs fewer examples; follows the formatting on longer context without additional prompting</p></td></tr><tr><td><p>Thinking Style</p></td><td><p>More cautious, catches more bugs but not as many of the critical one</p></td><td><p>Variable or elastic, less depth when not needed without need to reiterate the rules. Catches more of the hard-to-find bugs</p></td></tr><tr><td><p>Behavioral Tendencies</p></td><td><p>Wider spray of point-fixes, more commentary and hedging, inquisitive, more human-like review, finds more critical and non-critical issues</p></td><td><p>Verbose research-style rationales, notes on second-order effects to code, compact and balanced towards a code reviewer</p></td></tr><tr><td><p>Review Comment Structure</p></td><td><p>What’s wrong, why it’s wrong, concrete fix with code chunk</p></td><td><p>What to do, why do it, concrete fix with effects and code chunk</p></td></tr><tr><td><p>Context Awareness</p></td><td><p>Aware of its own context window, tracks token budget, persists/compresses based on headroom</p></td><td><p>Lacks explicit context window awareness (like cooking without a clock)</p></td></tr><tr><td><p>Verbosity</p></td><td><p>Higher, easier to read, double the word count</p></td><td><p>Lower, harder to read, information-dense</p></td></tr></tbody></table>\n\n<h2 id=\"heading-takeaway-3-end-of-an-era-prompts-are-no-longer-monoliths\">Takeaway 3: End of an era. Prompts are no longer monoliths</h2>\n<p>Because the fundamental behaviors of models have diverged, a prompt written for one model will not work “as is” on another anymore. For example, a directive-heavy prompt designed for Claude can feel over-constrained on GPT-5-Codex, and a prompt optimized for Codex to explore deep reasoning behavior will likely underperform on Claude. That means that the era of the monolithic, one-size-fits-all prompt is over. </p>\n<p>So, what does that mean for engineering teams who want to switch between models or adopt the newest models as they’re released? It means even more prompt engineering! But before you groan at the thought — there are some hacks to make this easier. </p>\n<h3 id=\"heading-the-rise-of-prompt-subunits\">The rise of prompt subunits</h3>\n<p>The first practical solution we’ve found at CodeRabbit is to introduce “<strong>prompt subunits.</strong>” This architecture consists of a model-agnostic core prompt that defines the core tasks and general instructions. This is then layered on top of smaller, model-specific prompt subunits that handle style, formatting, and examples – and which can be customized to individual models. </p>\n<p>When it comes to Codex and Sonnet 4.5, the implementation details for these subunits are likely to be starkly different. We’ve found a few tricks from our prompt testing with both models that we would like to share:</p>\n<ul>\n<li><p><strong>Claude:</strong> Use strong language like \"DO\" and \"DO NOT.\" Anthropic models pay attention to the latest information in a system prompt and are excellent at following output format specifications, even in long contexts. They prefer being told explicitly what to do.</p>\n</li>\n<li><p><strong>GPT-5:</strong> Use general instructions that are clearly aligned. OpenAI models’ attention decreases from top to bottom in a system prompt. These models may forget output format instructions in long contexts. They prefer generic guidance and tend to \"think on guidance,\" demonstrating a deeper reasoning process.</p>\n</li>\n</ul>\n<h3 id=\"heading-user-feedback-and-evals\">User feedback and evals</h3>\n<p>The second solution is to implement <strong>continuous updates driven by user feedback and internal evaluations.</strong> The best practice for optimizing an AI code-review bot or for that matter any LLM applications isn’t using an external benchmark; it’s checking to see if users accept the output. </p>\n<p>Evals are more important than ever but have to be designed more tightly around acceptability by users instead of raw performance since one model might be technically correct significantly more than another model but might drown the user in nitpicky and verbose comments, diluting its value to users. By measuring the metrics that matter ~ acceptance rate, signal-to-noise ratio, p95 latency, cost, among others - and tuning prompts in small steps, the system will remain aligned with user expectations and product goals. The last thing you want is great quantitative results on benchmarks and tests but low user acceptance. </p>\n<h2 id=\"heading-conclusion\">Conclusion</h2>\n<p>This shift from one-size-fits-all prompt engineering to a new model specific paradigm is critical. The days of brittle, monolithic prompts and plug-and-play model swaps are over. Instead, modular prompting, paired with deliberate model choice, give your product resilience. </p>\n<p>The ground will keep shifting as models evolve so your LLM stack and prompts shouldn’t be static. Treat it like a living system. Tune, test, listen, repeat. </p>\n<p>Also, be sure to check out our published detailed benchmarks on how the latest models behave in production. That gives you more data on what to expect from them. </p>\n<ul>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks\">GPT-5 Codex: How it solves for GPT-5's drawbacks</a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/claude-sonnet-45-better-performance-but-a-paradox\">Claude Sonnet 4.5: Better performance but a paradox</a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">Benchmarking GPT-5: Why it’s a generational leap in reasoning</a></p>\n</li>\n</ul>\n<p><strong><em>Try CodeRabbit with a 1</em></strong><a target=\"_blank\" href=\"https://coderabbit.link/rk7tdeC\"><strong><em>4-day free trial.</em></strong></a></p>\n",
      "summary": "For developers and product builders, one assumption has guided the last few years of LLM application development. To improve your product, just swap in the latest frontier large language model. Flip a single switch and your tool’s capabilities level up.\nBut that era is over. We’re now seeing that new models like Anthropic’s Claude Sonnet 4.5 and OpenAI’s GPT-5-Codex have diverged in fundamental ways. The choice of which model to use is no longer a simple engineering decision but a critical product decision. Flip that switch today… and the very texture of your product changes. \nThe one-size-fits-all model era is over; the model you choose now expresses something integral about what your product is and does, as well as, how it works. Whether you want it to or not.\nIn this blog, we’ll explore three surprising takeaways from this new era: why your LLM is now a statement about your product, how models now have distinct personalities and styles, and why your prompts have to now evolve from monolithic instructions to adaptive systems. \nTakeaway 1: LLM choice is now a statement about your product\nChoosing a model is no longer a straightforward decision where the main consequence of your choice is having to implement a new API. It is now a product decision about the user experience you want to create, the failure modes you can tolerate, the economics you want to optimize for, and the metrics you want to excel in. \nModels have developed distinct “personalities,” ways of reasoning, and instincts that directly shape how your product feels and behaves that go beyond just whether its output is technically right or wrong. Choose a different model and everything from what your tool is capable of to how it communicates with your users is significantly different. \nSo, in a world where traditional benchmarks that primarily or exclusively measure quantitative aspects of a model’s performance are no longer enough, what can you turn to for the data you need to chart your product’s direction? You could survey your team or your users or conduct focus groups but that could lack objectivity if you don’t do it in a rigorous manner. \nTo make this choice objective for our team, we focused on creating an internal North Star metrics matrix at CodeRabbit. Our metrics don’t just look at raw performance or accuracy. We also take into account readability, verbosity, signal-to-noise ratios, and more.\nThese kinds of metrics shift the focus from raw performance accuracy or leaderboard performance to what matters to our product and to our users. For example, a flood of low-impact suggestions, even if technically correct, burns user attention and consumes tokens. A theoretically “smarter” model can easily create a worse product experience if the output doesn’t align with your users’ workflow.\nI would strongly recommend creating your own North Star metrics to better gauge whether a new model meets your products’ and users’ needs. These shouldn’t be static metrics but should be informed by user feedback and user behavior in your product and evolve over time. Your goal is to find the right list of criteria to measure that predict your users preferences. \nWhat you’ll find is that the right model is the one whose instincts match the designed product behavior and your users’ needs, not the one at the top of any external leaderboard. \nTakeaway 2: Frontier models have divergent ‘personalities’\nModels are (now more than ever) “grown, not built,” and as a result, the latest generation has developed distinct instincts and behaviors. Different post-training cookbooks have fundamentally changed the direction of each model class. A prompt that works perfectly for one model will not work the same in another. Their fundamental approaches to the same task have diverged.\nOne powerful analogy that drives this point home is to think of the models as different professional archetypes. Sonnet 4.5 is like a meticulous accountant turned developer, meanwhile GPT-5-Codex is an upright ethical coder, GPT-5 is a bug-hunting detailed developer, and Sonnet 4 was a hyper-active new grad. The GPT-5 model class would make logical jumps further out in the solution space compared to the Claude model class, which tends to stay near the prompts itself. Which model is right for your use case and product, depends entirely on what you are wanting your product to achieve. \nAt CodeRabbit, we take a methodical approach to model evaluation and characterization. We then use this data to improve how we prompt and deploy models, ensuring we are always using the right model for each use case within our product. To give you an example of how we look at the different models, let’s compare Sonnet 4.5 and GPT-5-Codex. Based on extensive internal use and evals, we characterized Sonnet 4.5 as a “high-recall point-fixer,” aiming for comprehensive coverage. In contrast, GPT-5-Codex acts as a “patch generator,” preferring surgical, local changes. \nThese qualitative differences translate into hard, operational differences.  \n\n\nDimension\n\nClaude Sonnet 4.5\n\nGPT-5-Codex\n\n\nDefault Word Choice\n\n“Critical,” “Add,” “Remove,” “Consider”\n\n“Fix,” “Guard,” “Prevent,” “Restore,” “Drop”\n\n\nExample-Efficiency \n\nRemembers imperatives; benefits from explicit rules\n\nNeeds fewer examples; follows the formatting on longer context without additional prompting\n\n\nThinking Style\n\nMore cautious, catches more bugs but not as many of the critical one\n\nVariable or elastic, less depth when not needed without need to reiterate the rules. Catches more of the hard-to-find bugs\n\n\nBehavioral Tendencies\n\nWider spray of point-fixes, more commentary and hedging, inquisitive, more human-like review, finds more critical and non-critical issues\n\nVerbose research-style rationales, notes on second-order effects to code, compact and balanced towards a code reviewer\n\n\nReview Comment Structure\n\nWhat’s wrong, why it’s wrong, concrete fix with code chunk\n\nWhat to do, why do it, concrete fix with effects and code chunk\n\n\nContext Awareness\n\nAware of its own context window, tracks token budget, persists/compresses based on headroom\n\nLacks explicit context window awareness (like cooking without a clock)\n\n\nVerbosity\n\nHigher, easier to read, double the word count\n\nLower, harder to read, information-dense\n\n\nTakeaway 3: End of an era. Prompts are no longer monoliths\nBecause the fundamental behaviors of models have diverged, a prompt written for one model will not work “as is” on another anymore. For example, a directive-heavy prompt designed for Claude can feel over-constrained on GPT-5-Codex, and a prompt optimized for Codex to explore deep reasoning behavior will likely underperform on Claude. That means that the era of the monolithic, one-size-fits-all prompt is over. \nSo, what does that mean for engineering teams who want to switch between models or adopt the newest models as they’re released? It means even more prompt engineering! But before you groan at the thought — there are some hacks to make this easier. \nThe rise of prompt subunits\nThe first practical solution we’ve found at CodeRabbit is to introduce “prompt subunits.” This architecture consists of a model-agnostic core prompt that defines the core tasks and general instructions. This is then layered on top of smaller, model-specific prompt subunits that handle style, formatting, and examples – and which can be customized to individual models. \nWhen it comes to Codex and Sonnet 4.5, the implementation details for these subunits are likely to be starkly different. We’ve found a few tricks from our prompt testing with both models that we would like to share:\nClaude: Use strong language like \"DO\" and \"DO NOT.\" Anthropic models pay attention to the latest information in a system prompt and are excellent at following output format specifications, even in long contexts. They prefer being told explicitly what to do.\nGPT-5: Use general instructions that are clearly aligned. OpenAI models’ attention decreases from top to bottom in a system prompt. These models may forget output format instructions in long contexts. They prefer generic guidance and tend to \"think on guidance,\" demonstrating a deeper reasoning process.\nUser feedback and evals\nThe second solution is to implement continuous updates driven by user feedback and internal evaluations. The best practice for optimizing an AI code-review bot or for that matter any LLM applications isn’t using an external benchmark; it’s checking to see if users accept the output. \nEvals are more important than ever but have to be designed more tightly around acceptability by users instead of raw performance since one model might be technically correct significantly more than another model but might drown the user in nitpicky and verbose comments, diluting its value to users. By measuring the metrics that matter ~ acceptance rate, signal-to-noise ratio, p95 latency, cost, among others - and tuning prompts in small steps, the system will remain aligned with user expectations and product goals. The last thing you want is great quantitative results on benchmarks and tests but low user acceptance. \nConclusion\nThis shift from one-size-fits-all prompt engineering to a new model specific paradigm is critical. The days of brittle, monolithic prompts and plug-and-play model swaps are over. Instead, modular prompting, paired with deliberate model choice, give your product resilience. \nThe ground will keep shifting as models evolve so your LLM stack and prompts shouldn’t be static. Treat it like a living system. Tune, test, listen, repeat. \nAlso, be sure to check out our published detailed benchmarks on how the latest models behave in production. That gives you more data on what to expect from them. \nGPT-5 Codex: How it solves for GPT-5's drawbacks\nClaude Sonnet 4.5: Better performance but a paradox\nBenchmarking GPT-5: Why it’s a generational leap in reasoning\nTry CodeRabbit with a 14-day free trial.",
      "publishedAt": "2025-10-24T05:38:20.000Z",
      "author": "Nehal Gajraj",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "testing",
        "observability"
      ],
      "ingestedAt": "2025-11-23T17:37:39.982Z",
      "score": 1.9245169460806586
    },
    {
      "id": "965313c874380d87f98fdf7deb1a0dea",
      "title": "先日6,000万ドルを調達したので…おもしろ動画を作りました",
      "url": "https://coderabbit.ai/blog/we-raised-60-million-last-week-so-we-made-a-funny-video-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/we-raised-60-million-last-week-so-we-made-a-funny-video\">We raised $60M last week… so we made a funny film</a>の意訳です。</p>\n<p>先日、CodeRabbitは<strong>シリーズBで6,000万ドルの資金調達</strong>を発表しました。<br />そのお祝いに、開発者向けソフトウェア企業として当然のことをやりました──おもしろ動画を作ったのです。</p>\n<p>もちろん、全額を動画制作に使ったわけではありません。<br />ただ、AIが生成した大量のPRに追われる開発チームなら誰もが共感できる、ちょっと馬鹿げた（でも楽しい）企画で祝おうと決めました。</p>\n<h2 id=\"heading-ai\"><strong>紹介します… “AIコーディングエージェントの暴走：短編映画”</strong></h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/glfB3KLQR7E?feature=shared\">https://youtu.be/glfB3KLQR7E?feature=shared</a></div>\n<p> </p>\n<p>「AIによる開発速度の向上」が、いつの間にかレビューの滞留地獄に変わってしまった──<br />そんな現実を描いた、モキュメンタリー×シットコム風の短編です。</p>\n<ul>\n<li><p>レビュアーは1人</p>\n</li>\n<li><p>通知は何十件も</p>\n</li>\n<li><p>未レビューPRは84件</p>\n</li>\n<li><p>そして、ひたすらフィードバックを求める同僚ブラッド</p>\n</li>\n</ul>\n<h2 id=\"heading-kirjgq3jg6pjgrnjg4jntlnku4sqkg\"><strong>キャスト紹介</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758059242156/ae3ad14c-f354-4802-95e1-1f7eb67f0c85.png\" alt class=\"image--center mx-auto\" /></p>\n<p>主人公の疲弊したレビュアー役には、人気の開発者教育者（そしてインフルエンサー）<a target=\"_blank\" href=\"https://x.com/@aarondfrancis\"><strong>Aaron Francis</strong></a>を起用。<br />彼は「機能をもっと早くリリースしたい」と思っていたのに、今ではキッチンにも行けず、朝8時に家を出ようとしても、ブラッドがPRの話をしてくる始末です。</p>\n<p>そして、そのブラッドを完璧に演じたのが<a target=\"_blank\" href=\"https://www.instagram.com/4ustinvon/?hl=en\"><strong>Austin von Johnson</strong></a>。<br />彼はAI生成PRを驚くべきスピードで量産できる開発者ですが、どんな状況でもレビューを<strong>待てない</strong>タイプ。<br />彼のストーキング、付箋メモ攻撃、フーディ姿でのPR奇襲……すべてが見事に「やりきって」いました。</p>\n<h2 id=\"heading-kirnrjhjgytjga7oo4jgavjgyljgovjgihlrppmpvjga7oqrlpoywqkg\"><strong>笑いの裏にある、実際の課題</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758059125767/fc80309a-a591-4b4d-bd9b-6a1b53330edb.png\" alt class=\"image--center mx-auto\" /></p>\n<p>この短編は笑える内容ですが、そこに描かれた課題は現実のものです。</p>\n<ul>\n<li><p><strong>AIコーディングツールが、チームのレビュー速度を超える速さでコードを生成する</strong></p>\n</li>\n<li><p><strong>レビュー待ちPRが雪だるま式に増え</strong>、生産性が低下する</p>\n</li>\n<li><p><strong>シニアエンジニアがレビュー地獄に埋もれる</strong></p>\n</li>\n<li><p>レビュー品質がばらつき、リスクが増加する</p>\n</li>\n<li><p>そしていつの間にか、「開発速度の向上」という約束が悪夢に変わる</p>\n</li>\n</ul>\n<h2 id=\"heading-coderabbit\"><strong>CodeRabbitが解決すること</strong></h2>\n<p>CodeRabbitは<strong>レビューの滞留を解消するため</strong>に存在します。<br />私たちのAIコードレビューは、要件・テスト・CI・過去のdiff・所有者情報など、数十の文脈情報を参照して、見逃されがちなバグを検出します。<br />レビューアーの負担を軽減し、PRをより早く、安全にマージできるようにします。──もちろん、チームメイトを「ブラッド」にしないためにも。</p>\n<p>素早くリリースし、賢くレビューし、心の平穏を保ちましょう。<br />そして、ブラッドをもう一人増やさないように。</p>\n<p>👉 <a target=\"_blank\" href=\"https://youtu.be/glfB3KLQR7E?feature=shared\">こちらから <strong>「AIコーディングエージェントの暴走：短編映画」</strong>をご覧ください。</a><br />もしあなたの職場にも「PRまだ？」と追いかけてくるブラッドがいるなら、この動画をぜひ送ってあげてください。</p>\n",
      "summary": "We raised $60M last week… so we made a funny filmの意訳です。\n先日、CodeRabbitはシリーズBで6,000万ドルの資金調達を発表しました。\nそのお祝いに、開発者向けソフトウェア企業として当然のことをやりました──おもしろ動画を作ったのです。\nもちろん、全額を動画制作に使ったわけではありません。\nただ、AIが生成した大量のPRに追われる開発チームなら誰もが共感できる、ちょっと馬鹿げた（でも楽しい）企画で祝おうと決めました。\n紹介します… “AIコーディングエージェントの暴走：短編映画”\n\n\n\nhttps://youtu.be/glfB3KLQR7E?feature=shared\n \n「AIによる開発速度の向上」が、いつの間にかレビューの滞留地獄に変わってしまった──\nそんな現実を描いた、モキュメンタリー×シットコム風の短編です。\nレビュアーは1人\n通知は何十件も\n未レビューPRは84件\nそして、ひたすらフィードバックを求める同僚ブラッド\nキャスト紹介\n\n主人公の疲弊したレビュアー役には、人気の開発者教育者（そしてインフルエンサー）Aaron Francisを起用。\n彼は「機能をもっと早くリリースしたい」と思っていたのに、今ではキッチンにも行けず、朝8時に家を出ようとしても、ブラッドがPRの話をしてくる始末です。\nそして、そのブラッドを完璧に演じたのがAustin von Johnson。\n彼はAI生成PRを驚くべきスピードで量産できる開発者ですが、どんな状況でもレビューを待てないタイプ。\n彼のストーキング、付箋メモ攻撃、フーディ姿でのPR奇襲……すべてが見事に「やりきって」いました。\n笑いの裏にある、実際の課題\n\nこの短編は笑える内容ですが、そこに描かれた課題は現実のものです。\nAIコーディングツールが、チームのレビュー速度を超える速さでコードを生成する\nレビュー待ちPRが雪だるま式に増え、生産性が低下する\nシニアエンジニアがレビュー地獄に埋もれる\nレビュー品質がばらつき、リスクが増加する\nそしていつの間にか、「開発速度の向上」という約束が悪夢に変わる\nCodeRabbitが解決すること\nCodeRabbitはレビューの滞留を解消するために存在します。\n私たちのAIコードレビューは、要件・テスト・CI・過去のdiff・所有者情報など、数十の文脈情報を参照して、見逃されがちなバグを検出します。\nレビューアーの負担を軽減し、PRをより早く、安全にマージできるようにします。──もちろん、チームメイトを「ブラッド」にしないためにも。\n素早くリリースし、賢くレビューし、心の平穏を保ちましょう。\nそして、ブラッドをもう一人増やさないように。\n👉 こちらから 「AIコーディングエージェントの暴走：短編映画」をご覧ください。\nもしあなたの職場にも「PRまだ？」と追いかけてくるブラッドがいるなら、この動画をぜひ送ってあげてください。",
      "publishedAt": "2025-10-10T07:35:08.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.982Z",
      "score": 0.29321928261280616
    },
    {
      "id": "0419cf715ed8a478726ab051f82779fc",
      "title": "Claude Sonnet 4.5: パフォーマンス向上、でもパラドックスあり",
      "url": "https://coderabbit.ai/blog/claude-sonnet-45-better-performance-but-a-paradox-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/claude-sonnet-45-better-performance-but-a-paradox\">Claude Sonnet 4.5: Better performance but a paradox</a>の意訳です。</p>\n<p>Sonnet 4.5はAnthropicの最新Claudeモデルであり、私たちのコードレビュー・ベンチマークでは一見パラドックスのように感じられます。より高性能で、より慎重でありながら、時にもどかしい。Sonnet 4では見逃したバグを見つけ、カバレッジではOpus 4.1に近づき、さらに想定外の重大な問題をいくつか浮かび上がらせることもあります。</p>\n<p>しかし一方で、自己防衛的に振る舞い、自らを疑い、時に決断的なレビュアーというより思慮深い同僚のように見えることもありました。データでは確かな進歩が見られます。Sonnet 4ではコメントのうち、重要と判断されたものが35.3%だったのに対し、Sonnet 4.5では41.5%でした。しかし、そのコメントの調子や文体は、「AIレビュアーに何を求めるのか」というより深い問いを投げかけています。</p>\n<p>そして決定的なのは価格です。Sonnet 4.5はOpusレベルの性能に近づきながら、価格は変わらず維持されています。つまり、大規模なコードレビューを行うチームにとって、実用的な最適点に位置しているのです。</p>\n<p>Sonnet 4.5は思考を声に出しているかのようで、確かな修正を出す一方、曖昧な「条件付き」警告のようなコメントを出すこともあり、それが一部の開発者にとっては理解を難しくしているかもしれません。それでは、ベンチマークの詳細を見ていきましょう。</p>\n<h2 id=\"heading-kirjg5njg7pjg4hjg57jg7zjgqvvjroqzxkvqhjga7oprpngrkqkg\"><strong>ベンチマーク：評価の観点</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759501602825/c1640c62-ae57-42a8-a782-f22317d037e7.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Sonnet 4.5、Sonnet 4、Opus 4.1の3つを対象に、25件の難易度の高い実際のプルリクエストで評価しました。これらには既知の重大なバグが含まれており（並行性やメモリ順序、非同期レースコンディション、APIの誤使用など）、モデルがその重大な問題に直接コメントを出せた場合、そのPRは「合格」としました。</p>\n<p>評価指標は、カバレッジ（S@25）、精度（コメントの合格率）、そしてシグナル対ノイズ比です。シグナル対ノイズ比については、<strong>重要なコメント（Important comments）</strong> に注目しました。これらは最も価値のあるコメントであり、以下を含みます。</p>\n<ul>\n<li><p><strong>PASSコメント</strong>：PR内の既知の重大バグを正しく指摘・修正したもの</p>\n</li>\n<li><p><strong>その他の重要コメント</strong>：追跡対象ではないが、別の重大または深刻なバグを的確に指摘したもの</p>\n</li>\n</ul>\n<h2 id=\"heading-sonnet-45opus-41\"><strong>スコアボード：Sonnet 4.5はOpus 4.1に性能で迫る</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759501721229/c01cc54e-7647-4f84-98a4-f933078d6dd3.png\" alt class=\"image--center mx-auto\" /></p>\n<p>結果は以下の通りです。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502044781/191c79e1-971f-4209-b0b0-c0fa368fa0b9.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p><strong>カバレッジ:</strong> Sonnet 4.5はSonnet 4とOpus 4.1の間の差を大きく縮め、Sonnet 4を大きく上回りました。</p>\n</li>\n<li><p><strong>精度:</strong> Opus 4.1は依然として最も正確で信頼性の高い実行可能なコメントを生成しました。高価格モデルであるため当然の結果です。</p>\n</li>\n<li><p><strong>重要コメント率（重大な問題を指摘したコメントの割合）:</strong> より厳格な基準で測定した場合、Sonnet 4.5の重要コメント率は約41%。つまりコメントのうち4割が、主要なバグを解決するか、別の重大な問題を指摘していたことになります。Opus 4.1は50%、Sonnet 4は約35%でした。</p>\n</li>\n</ul>\n<h2 id=\"heading-sonnet-45\"><strong>文体とトーン：Sonnet 4.5は「慎重さ」にフォーカス</strong></h2>\n<p>Sonnet 4.5のコメントはコードを修正しますが、Opus 4.1ほど自信に満ちたトーンではありません。ただし、Sonnet 4よりは明確です。</p>\n<p><strong>修正パッチの提示率:</strong></p>\n<ul>\n<li><p>Sonnet 4.5の実行可能コメントのうち87%はコードブロックやdiffパッチを含み、Sonnet 4（90%）、Opus 4.1（91%）とほぼ同水準です。</p>\n</li>\n<li><p>違いは文体にあります。Opusのdiffは「外科的修正」のように明確ですが、Sonnet 4.5は探索的な文章を添える傾向があります。修正を「提案する」「検討する」といった表現が多く、断定的ではありません。</p>\n</li>\n</ul>\n<p><strong>慎重な言い回し（Hedging language）:</strong></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502294784/a66c085f-04ce-4806-b5c5-e22dfa47e2a1.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p>Sonnet 4.5では実行可能コメントの**34%**において、「might」「could」「possibly」といった慎重な表現が見られます。例：</p>\n<ul>\n<li><p>“<strong>不要なアロケーション: cacheは使用されていません。</strong> コンストラクタで4KBのメモリを確保していますが、使われていません。… <strong>cache_bufferの削除を検討してください。</strong>”</p>\n</li>\n<li><p>“<strong>空のtry/exceptブロックを削除してください。</strong> …おそらくプレースホルダーです”</p>\n</li>\n</ul>\n</li>\n<li><p>Opus 4.1は約28%、Sonnet 4は約26%とやや低め。</p>\n</li>\n<li><p>この慎重さにより、「問いかける」ようなトーンが生まれます。Sonnet 4.5はしばしば一緒に考えているような雰囲気を持ち、明確な判定を下すというより「共に推論している」ように感じられます。</p>\n</li>\n</ul>\n<p><strong>自信のある言語（Confident language）:</strong></p>\n<ul>\n<li><p>ただし、Sonnet 4.5は慎重さを補うように、高い確信を示すコメントも**39%**含んでいます。Sonnet 4（18%）、Opus 4.1（23%）よりも高い割合です。例：</p>\n<ul>\n<li><p>“<strong>重大: self.プレフィックスが欠落しており、すべてのAPIメソッドが動作しません。</strong> このままでは全てのメソッドがAttributeErrorを発生させます。”</p>\n</li>\n<li><p>“<strong>整数オーバーフローの可能性:</strong> optimization_cycle_countが無制限にインクリメントされ続けます。これは約414日稼働後に<strong>必ず</strong>オーバーフローします。”</p>\n</li>\n</ul>\n</li>\n<li><p>つまり、慎重さと確信の間で大きく揺れ動くのです。</p>\n</li>\n</ul>\n<p><strong>シグナル対ノイズ比:</strong></p>\n<ul>\n<li><p>Sonnet 4.5はSonnet 4より精度が向上しましたが、依然としてOpusよりも「軽微な」的外れコメントが多めでした。</p>\n</li>\n<li><p>ただし、重要コメント（PASSコメント＋少数の高確信度コメント）に限定すると**41.5%**を達成。Opus 4.1は依然として約50%で最高水準です。</p>\n</li>\n</ul>\n<h2 id=\"heading-sonnet-45-1\"><strong>Sonnet 4.5が得意な領域</strong></h2>\n<p>評価したPR群では、Sonnet 4.5が明確に、特に優れていた領域が見られました。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502262665/e0099bd2-4983-4708-8563-6ee3ba923c7e.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p><strong>並行性バグ検出:</strong> C++のatomic操作やcondvarの誤用を的確に特定し、実行可能なdiffを生成</p>\n</li>\n<li><p><strong>整合性チェック:</strong> サービス間での分散状態の不整合を確実に検出</p>\n</li>\n<li><p><strong>追加バグの発見:</strong> 評価対象外のCritical問題も検出しましたが、より厳密な基準では件数はやや減少</p>\n</li>\n</ul>\n<p>AnthropicはSonnet 4.5を「ハイブリッド推論」「長期的計画立案」モデルとして打ち出しています。実際、コード内の副経路を追跡し、未追跡の実際の問題を発見する傾向が見られます。</p>\n<h2 id=\"heading-sonnet-45-2\"><strong>Sonnet 4.5: 価格と性能のバランスが最適</strong></h2>\n<p>Sonnet 4.5の最大の強みの一つは、価格対性能比にあります。Opus 4.1は依然としてAnthropicの最上位モデルですが、その分コストも高額です。</p>\n<p>Sonnet 4.5はカバレッジと重大バグ検出能力の差を縮めつつ、はるかにコスト効率が良いです。多くのチームにとって、Opusレベルの結果を低コストで得られるこのバランスこそが、最も実用的な選択肢となる理由です。</p>\n<h2 id=\"heading-sonnet-45-3\"><strong>Sonnet 4.5の弱点</strong></h2>\n<p>ただし、Sonnet 4.5を使用する際はその弱点を理解しておく必要があります。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502337031/b494ee79-eb77-4342-809a-929a41db1dd1.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p><strong>デッドロック検出:</strong> Sonnet 4やOpusと同様、複雑なロック順序の追跡はまだ苦手です</p>\n</li>\n<li><p><strong>冗長さと慎重さ:</strong> コメントが長く、留保的または曖昧なことがあります。以前の研究で評価したGPT-5 Codexは「パッチのように読める」ほど明快なコメントを書く傾向がありました。例として、GPT-5 Codexのコメントを以下に示します。</p>\n<ul>\n<li><p><strong>ロック順序 / デッドロック:</strong> 「ロック取得を一貫した階層順に並べ替えてください。これにより循環待ちデッドロックを防げます」</p>\n</li>\n<li><p><strong>正規表現の壊滅的バックトラッキング:</strong> 「ネストされた量指定子を削除してバックトラッキングを回避します」</p>\n</li>\n</ul>\n</li>\n<li><p><strong>精度のギャップ:</strong> コメント単位の精度35%、重要コメント率41.5%はSonnet 4より良いものの、Opus 4.1にはまだ届きません。</p>\n</li>\n</ul>\n<h2 id=\"heading-sonnet-45-4\"><strong>Sonnet 4.5の総評</strong></h2>\n<p>Sonnet 4.5は「赤ペン先生」ではなく、そばで考えてくれる同僚のようです。可能性のある問題を指摘し、ほとんどの場合は正しく、時に慎重すぎる時もあります。ときには、思いがけない箇所にまで目を向けてくれます。</p>\n<p>このスタイルはレビューにおいて諸刃の剣です。一方では、開発者は追加の重大問題を指摘してくれることを評価するでしょう。もう一方では、「このバグを確実に見つけてほしい」という場合、Opus 4.1のほうが鋭いです。</p>\n<h2 id=\"heading-kirnt4mi6wqkg\"><strong>総括</strong></h2>\n<p>AnthropicはSonnet 4.5を「エージェント的推論とコンピュータ利用」に向けたステップと位置付けています。コードレビューでは、その推論力がより豊かで慎重かつ広範なコメントとして現れます。</p>\n<p>チームにとっての選択肢はこうです。</p>\n<ul>\n<li><p>明確でパッチのようなフィードバックを重視するなら、Opus 4.1（またはGPT-5 Codex）が依然として基準として優れています。</p>\n</li>\n<li><p>トラッキング対象外の隠れた重大問題まで発見したいなら、Sonnet 4.5が有力です。</p>\n</li>\n<li><p>コスト効率を重視するなら、Sonnet 4.5が最も賢い選択肢です。Opus並みの精度を低価格で実現します。</p>\n</li>\n</ul>\n<p>いずれにせよ、Sonnet 4.5はレビュー体験の質感を変えます。より人間的で、常に明快とは限らないものの、より探究的で、より慎重で、時にあなたが見逃していた「正解」に辿り着くこともあるのです。</p>\n",
      "summary": "Claude Sonnet 4.5: Better performance but a paradoxの意訳です。\nSonnet 4.5はAnthropicの最新Claudeモデルであり、私たちのコードレビュー・ベンチマークでは一見パラドックスのように感じられます。より高性能で、より慎重でありながら、時にもどかしい。Sonnet 4では見逃したバグを見つけ、カバレッジではOpus 4.1に近づき、さらに想定外の重大な問題をいくつか浮かび上がらせることもあります。\nしかし一方で、自己防衛的に振る舞い、自らを疑い、時に決断的なレビュアーというより思慮深い同僚のように見えることもありました。データでは確かな進歩が見られます。Sonnet 4ではコメントのうち、重要と判断されたものが35.3%だったのに対し、Sonnet 4.5では41.5%でした。しかし、そのコメントの調子や文体は、「AIレビュアーに何を求めるのか」というより深い問いを投げかけています。\nそして決定的なのは価格です。Sonnet 4.5はOpusレベルの性能に近づきながら、価格は変わらず維持されています。つまり、大規模なコードレビューを行うチームにとって、実用的な最適点に位置しているのです。\nSonnet 4.5は思考を声に出しているかのようで、確かな修正を出す一方、曖昧な「条件付き」警告のようなコメントを出すこともあり、それが一部の開発者にとっては理解を難しくしているかもしれません。それでは、ベンチマークの詳細を見ていきましょう。\nベンチマーク：評価の観点\n\nSonnet 4.5、Sonnet 4、Opus 4.1の3つを対象に、25件の難易度の高い実際のプルリクエストで評価しました。これらには既知の重大なバグが含まれており（並行性やメモリ順序、非同期レースコンディション、APIの誤使用など）、モデルがその重大な問題に直接コメントを出せた場合、そのPRは「合格」としました。\n評価指標は、カバレッジ（S@25）、精度（コメントの合格率）、そしてシグナル対ノイズ比です。シグナル対ノイズ比については、重要なコメント（Important comments） に注目しました。これらは最も価値のあるコメントであり、以下を含みます。\nPASSコメント：PR内の既知の重大バグを正しく指摘・修正したもの\nその他の重要コメント：追跡対象ではないが、別の重大または深刻なバグを的確に指摘したもの\nスコアボード：Sonnet 4.5はOpus 4.1に性能で迫る\n\n結果は以下の通りです。\n\nカバレッジ: Sonnet 4.5はSonnet 4とOpus 4.1の間の差を大きく縮め、Sonnet 4を大きく上回りました。\n精度: Opus 4.1は依然として最も正確で信頼性の高い実行可能なコメントを生成しました。高価格モデルであるため当然の結果です。\n重要コメント率（重大な問題を指摘したコメントの割合）: より厳格な基準で測定した場合、Sonnet 4.5の重要コメント率は約41%。つまりコメントのうち4割が、主要なバグを解決するか、別の重大な問題を指摘していたことになります。Opus 4.1は50%、Sonnet 4は約35%でした。\n文体とトーン：Sonnet 4.5は「慎重さ」にフォーカス\nSonnet 4.5のコメントはコードを修正しますが、Opus 4.1ほど自信に満ちたトーンではありません。ただし、Sonnet 4よりは明確です。\n修正パッチの提示率:\nSonnet 4.5の実行可能コメントのうち87%はコードブロックやdiffパッチを含み、Sonnet 4（90%）、Opus 4.1（91%）とほぼ同水準です。\n違いは文体にあります。Opusのdiffは「外科的修正」のように明確ですが、Sonnet 4.5は探索的な文章を添える傾向があります。修正を「提案する」「検討する」といった表現が多く、断定的ではありません。\n慎重な言い回し（Hedging language）:\n\nSonnet 4.5では実行可能コメントの**34%**において、「might」「could」「possibly」といった慎重な表現が見られます。例：\n“不要なアロケーション: cacheは使用されていません。 コンストラクタで4KBのメモリを確保していますが、使われていません。… cache_bufferの削除を検討してください。”\n“空のtry/exceptブロックを削除してください。 …おそらくプレースホルダーです”\nOpus 4.1は約28%、Sonnet 4は約26%とやや低め。\nこの慎重さにより、「問いかける」ようなトーンが生まれます。Sonnet 4.5はしばしば一緒に考えているような雰囲気を持ち、明確な判定を下すというより「共に推論している」ように感じられます。\n自信のある言語（Confident language）:\nただし、Sonnet 4.5は慎重さを補うように、高い確信を示すコメントも**39%**含んでいます。Sonnet 4（18%）、Opus 4.1（23%）よりも高い割合です。例：\n“重大: self.プレフィックスが欠落しており、すべてのAPIメソッドが動作しません。 このままでは全てのメソッドがAttributeErrorを発生させます。”\n“整数オーバーフローの可能性: optimization_cycle_countが無制限にインクリメントされ続けます。これは約414日稼働後に必ずオーバーフローします。”\nつまり、慎重さと確信の間で大きく揺れ動くのです。\nシグナル対ノイズ比:\nSonnet 4.5はSonnet 4より精度が向上しましたが、依然としてOpusよりも「軽微な」的外れコメントが多めでした。\nただし、重要コメント（PASSコメント＋少数の高確信度コメント）に限定すると**41.5%**を達成。Opus 4.1は依然として約50%で最高水準です。\nSonnet 4.5が得意な領域\n評価したPR群では、Sonnet 4.5が明確に、特に優れていた領域が見られました。\n\n並行性バグ検出: C++のatomic操作やcondvarの誤用を的確に特定し、実行可能なdiffを生成\n整合性チェック: サービス間での分散状態の不整合を確実に検出\n追加バグの発見: 評価対象外のCritical問題も検出しましたが、より厳密な基準では件数はやや減少\nAnthropicはSonnet 4.5を「ハイブリッド推論」「長期的計画立案」モデルとして打ち出しています。実際、コード内の副経路を追跡し、未追跡の実際の問題を発見する傾向が見られます。\nSonnet 4.5: 価格と性能のバランスが最適\nSonnet 4.5の最大の強みの一つは、価格対性能比にあります。Opus 4.1は依然としてAnthropicの最上位モデルですが、その分コストも高額です。\nSonnet 4.5はカバレッジと重大バグ検出能力の差を縮めつつ、はるかにコスト効率が良いです。多くのチームにとって、Opusレベルの結果を低コストで得られるこのバランスこそが、最も実用的な選択肢となる理由です。\nSonnet 4.5の弱点\nただし、Sonnet 4.5を使用する際はその弱点を理解しておく必要があります。\n\nデッドロック検出: Sonnet 4やOpusと同様、複雑なロック順序の追跡はまだ苦手です\n冗長さと慎重さ: コメントが長く、留保的または曖昧なことがあります。以前の研究で評価したGPT-5 Codexは「パッチのように読める」ほど明快なコメントを書く傾向がありました。例として、GPT-5 Codexのコメントを以下に示します。\nロック順序 / デッドロック: 「ロック取得を一貫した階層順に並べ替えてください。これにより循環待ちデッドロックを防げます」\n正規表現の壊滅的バックトラッキング: 「ネストされた量指定子を削除してバックトラッキングを回避します」\n精度のギャップ: コメント単位の精度35%、重要コメント率41.5%はSonnet 4より良いものの、Opus 4.1にはまだ届きません。\nSonnet 4.5の総評\nSonnet 4.5は「赤ペン先生」ではなく、そばで考えてくれる同僚のようです。可能性のある問題を指摘し、ほとんどの場合は正しく、時に慎重すぎる時もあります。ときには、思いがけない箇所にまで目を向けてくれます。\nこのスタイルはレビューにおいて諸刃の剣です。一方では、開発者は追加の重大問題を指摘してくれることを評価するでしょう。もう一方では、「このバグを確実に見つけてほしい」という場合、Opus 4.1のほうが鋭いです。\n総括\nAnthropicはSonnet 4.5を「エージェント的推論とコンピュータ利用」に向けたステップと位置付けています。コードレビューでは、その推論力がより豊かで慎重かつ広範なコメントとして現れます。\nチームにとっての選択肢はこうです。\n明確でパッチのようなフィードバックを重視するなら、Opus 4.1（またはGPT-5 Codex）が依然として基準として優れています。\nトラッキング対象外の隠れた重大問題まで発見したいなら、Sonnet 4.5が有力です。\nコスト効率を重視するなら、Sonnet 4.5が最も賢い選択肢です。Opus並みの精度を低価格で実現します。\nいずれにせよ、Sonnet 4.5はレビュー体験の質感を変えます。より人間的で、常に明快とは限らないものの、より探究的で、より慎重で、時にあなたが見逃していた「正解」に辿り着くこともあるのです。",
      "publishedAt": "2025-10-10T07:28:00.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.982Z",
      "score": 0.3349891993534594
    },
    {
      "id": "a32e3ff8a0ed4196c0e21bb3980a4e2f",
      "title": "Claude Sonnet 4.5:  Better performance but a paradox",
      "url": "https://coderabbit.ai/blog/claude-sonnet-45-better-performance-but-a-paradox",
      "content": "<p>Sonnet 4.5 is Anthropic’s newest Claude model and in our code review benchmark, it feels like a paradox: more capable, more cautious, and at times more frustrating. It catches bugs Sonnet 4 missed, edges closer to Opus 4.1 in coverage, and even surfaces a handful of unexpected critical issues off the beaten path.</p>\n<p>Yet, it hedges, it questions itself, and it sometimes sounds more like a thoughtful colleague than a decisive reviewer. The data shows real progress:41.5% of its comments were Important in Sonnet 4.5 vs only 35.3% in Sonnet 4.But the tone and texture of those comments raise deeper questions about what we want in an AI reviewer.</p>\n<p>And then there’s the kicker: Sonnet 4.5 gets you close to Opus-level performance at a fraction of the price, making it a pragmatic sweet spot for teams reviewing code at scale.</p>\n<p>Sonnet 4.5 thinks aloud and still delivers decisive fixes but some of its comments are framed as vague “conditional” warnings that could make its comments harder for some to parse..  Let’s dive into our benchmark.</p>\n<h2 id=\"heading-benchmark-what-we-looked-for\"><strong>Benchmark: What we looked for</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759501602825/c1640c62-ae57-42a8-a782-f22317d037e7.png\" alt class=\"image--center mx-auto\" /></p>\n<p>We evaluated Sonnet 4.5, Sonnet 4, and Opus 4.1 across 25 difficult real-world pull requests containing known critical bugs (ranging from concurrency and memory ordering to async race conditions and API misuse). A model “Passed’ a PR if it produced at least one comment directly on the critical issue.</p>\n<p>We measured coverage (S@25), precision (comment PASS rate), and signal-to-noise ratio. For signal-to-noise we focus on <strong>Important comments</strong> (these are the comments that matter most). They include:</p>\n<ul>\n<li><p><strong>PASS comments</strong> that correctly addressed the known critical bug in the PR.</p>\n</li>\n<li><p><strong>Other important comments</strong> that did not solve the tracked issue, but still flagged a truly Critical or Major bug elsewhere.</p>\n</li>\n</ul>\n<h2 id=\"heading-scoreboard-sonnet-45-gets-closer-to-opus-41-in-performance\"><strong>Scoreboard - Sonnet 4.5 gets closer to Opus 4.1 in performance</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759501721229/c01cc54e-7647-4f84-98a4-f933078d6dd3.png\" alt class=\"image--center mx-auto\" /></p>\n<p>The results were mixed:</p>\n<ul>\n<li><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502044781/191c79e1-971f-4209-b0b0-c0fa368fa0b9.png\" alt class=\"image--center mx-auto\" /></p>\n<p>  <strong>Coverage:</strong> Sonnet 4.5 closes much of the gap between Sonnet 4 and Opus 4.1 and lands far ahead of Sonnet 4.</p>\n</li>\n<li><p><strong>Precision:</strong> Opus 4.1 still produces the cleanest, most reliable actionable comments but that is to be expected given that it’s a more expensive model.</p>\n</li>\n<li><p><strong>Imp</strong><a target=\"_blank\" href=\"http://i.ei\"><strong>or</strong></a><strong>tant share (</strong><a target=\"_blank\" href=\"http://i.ei\"><strong>i.</strong></a><strong>e. percenta</strong><a target=\"_blank\" href=\"http://i.ei\"><strong>ge</strong></a> <strong>of comments flagging a significant issue):</strong> With stricter criteria, Sonnet 4.5 lands at just over 41% Importan<a target=\"_blank\" href=\"http://i.ei\">t</a> share. That means about 4 in 10 of its comments either solved the key bug or flagged another truly significant issue. Opus 4.1 leads here at 50%, with So<a target=\"_blank\" href=\"http://i.ei\">nn</a>et 4 at ~35%.</p>\n</li>\n</ul>\n<h2 id=\"heading-style-and-tone-sonnet-45-is-focused-on-hedging\"><strong>Style and tone: Sonnet 4.5 is focused on hedging</strong></h2>\n<p>Sonnet 4.5’s comments patch the code but do so in a less confident tone than Opus 4.1 does but is still more confident than Sonnet 4.</p>\n<p><strong>Patches present:</strong></p>\n<ul>\n<li><p>87% of Sonnet 4.5’s actionable comments included a code block or diff patch, similar to Sonnet 4 (90%) and Opus 4.1 (91%).</p>\n</li>\n<li><p>The difference is in style: Opus’s diffs read like surgical fixes, while Sonnet 4.5 often couches them in exploratory text. It “suggests” or “considers” changes rather than asserting them.</p>\n</li>\n</ul>\n<p><strong>Hedging language:</strong></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502294784/a66c085f-04ce-4806-b5c5-e22dfa47e2a1.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p>Sonnet 4.5 hedges in <strong>34%</strong> of actionable comments—words like <em>might</em>, <em>could</em>, <em>possibly</em>. For example:</p>\n<ul>\n<li><p>“<strong>Unnecessary allocation: cache is never used.</strong> The constructor allocates 4KB of memory that is never utilized … <strong>Consider removing</strong> the cache_buffer.”</p>\n</li>\n<li><p>“<strong>Remove the empty try/except block.</strong> … likely a placeholder”</p>\n</li>\n</ul>\n</li>\n<li><p>Opus 4.1 is steady at ~28%. Sonnet 4 sits slightly lower at ~26%.</p>\n</li>\n<li><p>This hedging creates an “interrogative” tone: Sonnet 4.5 sometimes feels like it’s thinking out loud with you, rather than delivering verdicts.</p>\n</li>\n</ul>\n<p><strong>Confident language:</strong></p>\n<ul>\n<li><p>Sonnet 4.5 balances that hedging with higher confidence markers (<strong>39%</strong>) than Sonnet 4 (18%) or Opus 4.1 (23%). For example:</p>\n<ul>\n<li><p>“<strong>Critical: Missing self. prefix breaks all API methods.</strong> All subsequent methods will raise AttributeError until this is corrected.”</p>\n</li>\n<li><p>“<strong>Potential integer overflow.</strong> optimization_cycle_count increments unbounded … this <strong>will</strong> overflow after ~414 days of runtime.”</p>\n</li>\n</ul>\n</li>\n<li><p>In other words, it swings between caution and certainty more dramatically.</p>\n</li>\n</ul>\n<p><strong>Signal-to-noise:</strong></p>\n<ul>\n<li>Sonnet 4.5 improved precision over Sonnet 4, but still produced more “minor” off-target notes than Opus.</li>\n</ul>\n<p>However, when you count its true Important comments—PASS comments plus a small number of high-confidence off-EP issues—it lands at <strong>41.5% Important share</strong>. Opus 4.1 is still the gold standard Anthropic model at ~50%.</p>\n<h2 id=\"heading-what-sonnet-45-is-good-at\"><strong>What Sonnet 4.5 is good at</strong></h2>\n<p>Across the PRs we tested Sonnet 4.5 with, we saw some clear areas where it stood out.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502262665/e0099bd2-4983-4708-8563-6ee3ba923c7e.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p><strong>Concurrency bug-finding:</strong> Sonnet 4.5 nailed <strong>C++ atomics and condvar misuses</strong> with clean, actionable diffs.</p>\n</li>\n<li><p><strong>Consistency checks:</strong> It reliably flagged distributed state mismatches across services.</p>\n</li>\n<li><p><strong>Extra bug surfacing:</strong> It did identify additional Critical issues not originally under evaluation, though fewer than initially expected under a stricter rubric.</p>\n</li>\n</ul>\n<p>As Anthropic markets Sonnet 4.5, they emphasize “hybrid reasoning” and “long horizon” planning. In practice, that shows up as more willingness to chase down side-paths in the code and note real but untracked issues.</p>\n<h2 id=\"heading-sonnet-45-hits-a-price-vs-performance-sweet-spot\"><strong>Sonnet 4.5: Hits a price vs. performance sweet spot</strong></h2>\n<p>One of the biggest advantages of Sonnet 4.5 is its price-to-performance ratio. While Opus 4.1 remains Anthropic's flagship model in raw capability, it also comes at a significantly higher cost.</p>\n<p>Sonnet 4.5 narrows the gap in coverage and important bug-finding while staying far more cost-efficient to run. For many teams, that balance of having close to Opus-level results at a fraction of the price is what makes Sonnet 4.5 the most pragmatic choice.</p>\n<h2 id=\"heading-sonnet-45-weaknesses\"><strong>Sonnet 4.5 weaknesses</strong></h2>\n<p>But if using Sonnet 4.5, it’s critical to be aware of its weaknesses. These include:</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502337031/b494ee79-eb77-4342-809a-929a41db1dd1.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p><strong>Deadlock coverage:</strong> Like Sonnet 4 and even Opus, it still struggles to trace complex lock ordering.</p>\n</li>\n<li><p><strong>Verbosity and hedging:</strong> Many comments run long, caveated, or uncertain. Compare this to GPT-5 Codex, which in our earlier work wrote comments that “read like patches” with crisp directness. For example, with GPT-5 Codex:</p>\n<ul>\n<li><p><strong>Lock ordering / deadlock:</strong> Reorder the lock acquisitions to follow a consistent hierarchy. This prevents circular wait deadlocks.”</p>\n</li>\n<li><p><strong>Regex catastrophic backtracking:</strong> “Remove the nested quantifier to avoid catastrophic backtracking.”</p>\n</li>\n</ul>\n</li>\n<li><p><strong>Precision gap:</strong> At 35% comment-level precision and 41.5% important share percentage, it’s better than Sonnet 4 but well short of Opus 4.1.</p>\n</li>\n</ul>\n<h2 id=\"heading-sonnet-45-verdict\"><strong>Sonnet 4.5 verdict</strong></h2>\n<p>Sonnet 4.5 feels less like a teacher writing in red pen and more like a thoughtful colleague at your side: pointing out possible issues, often right, occasionally over-hedged, and sometimes spotting things you didn’t know were there.</p>\n<p>That style is a double-edged sword in review. On one hand, developers may appreciate the extra critical issues it flags. On the other, when the task is “please catch this bug,” Opus 4.1 is still sharper.</p>\n<h2 id=\"heading-closing-thoughts\"><strong>Closing thoughts</strong></h2>\n<p>Anthropic positioned Sonnet 4.5 as a step toward agentic reasoning and computer use. In code review, that reasoning shows up in richer, more cautious, and more wide-ranging comments.</p>\n<p>For teams:</p>\n<ul>\n<li><p>If you value decisive, patch-like feedback, Opus 4.1 (or GPT-5 Codex) still sets the bar.</p>\n</li>\n<li><p>If you want a reviewer that finds critical issues anywhere they lurk, even beyond the tracked bug, Sonnet 4.5 has surprising upside.</p>\n</li>\n<li><p>And if you care about pragmatic price-to-performance, Sonnet 4.5 may be the smartest choice: close to Opus’s accuracy at a fraction of the cost.</p>\n</li>\n</ul>\n<p>Either way, Sonnet 4.5 changes the texture of reviews. It feels more human—not always cleaner, but more inquisitive, more hedged, sometimes more <em>right</em> in the places you weren’t looking.</p>\n",
      "summary": "Sonnet 4.5 is Anthropic’s newest Claude model and in our code review benchmark, it feels like a paradox: more capable, more cautious, and at times more frustrating. It catches bugs Sonnet 4 missed, edges closer to Opus 4.1 in coverage, and even surfaces a handful of unexpected critical issues off the beaten path.\nYet, it hedges, it questions itself, and it sometimes sounds more like a thoughtful colleague than a decisive reviewer. The data shows real progress:41.5% of its comments were Important in Sonnet 4.5 vs only 35.3% in Sonnet 4.But the tone and texture of those comments raise deeper questions about what we want in an AI reviewer.\nAnd then there’s the kicker: Sonnet 4.5 gets you close to Opus-level performance at a fraction of the price, making it a pragmatic sweet spot for teams reviewing code at scale.\nSonnet 4.5 thinks aloud and still delivers decisive fixes but some of its comments are framed as vague “conditional” warnings that could make its comments harder for some to parse..  Let’s dive into our benchmark.\nBenchmark: What we looked for\n\nWe evaluated Sonnet 4.5, Sonnet 4, and Opus 4.1 across 25 difficult real-world pull requests containing known critical bugs (ranging from concurrency and memory ordering to async race conditions and API misuse). A model “Passed’ a PR if it produced at least one comment directly on the critical issue.\nWe measured coverage (S@25), precision (comment PASS rate), and signal-to-noise ratio. For signal-to-noise we focus on Important comments (these are the comments that matter most). They include:\nPASS comments that correctly addressed the known critical bug in the PR.\nOther important comments that did not solve the tracked issue, but still flagged a truly Critical or Major bug elsewhere.\nScoreboard - Sonnet 4.5 gets closer to Opus 4.1 in performance\n\nThe results were mixed:\n\n  Coverage: Sonnet 4.5 closes much of the gap between Sonnet 4 and Opus 4.1 and lands far ahead of Sonnet 4.\nPrecision: Opus 4.1 still produces the cleanest, most reliable actionable comments but that is to be expected given that it’s a more expensive model.\nImportant share (i.e. percentage of comments flagging a significant issue): With stricter criteria, Sonnet 4.5 lands at just over 41% Important share. That means about 4 in 10 of its comments either solved the key bug or flagged another truly significant issue. Opus 4.1 leads here at 50%, with Sonnet 4 at ~35%.\nStyle and tone: Sonnet 4.5 is focused on hedging\nSonnet 4.5’s comments patch the code but do so in a less confident tone than Opus 4.1 does but is still more confident than Sonnet 4.\nPatches present:\n87% of Sonnet 4.5’s actionable comments included a code block or diff patch, similar to Sonnet 4 (90%) and Opus 4.1 (91%).\nThe difference is in style: Opus’s diffs read like surgical fixes, while Sonnet 4.5 often couches them in exploratory text. It “suggests” or “considers” changes rather than asserting them.\nHedging language:\n\nSonnet 4.5 hedges in 34% of actionable comments—words like might, could, possibly. For example:\n“Unnecessary allocation: cache is never used. The constructor allocates 4KB of memory that is never utilized … Consider removing the cache_buffer.”\n“Remove the empty try/except block. … likely a placeholder”\nOpus 4.1 is steady at ~28%. Sonnet 4 sits slightly lower at ~26%.\nThis hedging creates an “interrogative” tone: Sonnet 4.5 sometimes feels like it’s thinking out loud with you, rather than delivering verdicts.\nConfident language:\nSonnet 4.5 balances that hedging with higher confidence markers (39%) than Sonnet 4 (18%) or Opus 4.1 (23%). For example:\n“Critical: Missing self. prefix breaks all API methods. All subsequent methods will raise AttributeError until this is corrected.”\n“Potential integer overflow. optimization_cycle_count increments unbounded … this will overflow after ~414 days of runtime.”\nIn other words, it swings between caution and certainty more dramatically.\nSignal-to-noise:\nSonnet 4.5 improved precision over Sonnet 4, but still produced more “minor” off-target notes than Opus.\nHowever, when you count its true Important comments—PASS comments plus a small number of high-confidence off-EP issues—it lands at 41.5% Important share. Opus 4.1 is still the gold standard Anthropic model at ~50%.\nWhat Sonnet 4.5 is good at\nAcross the PRs we tested Sonnet 4.5 with, we saw some clear areas where it stood out.\n\nConcurrency bug-finding: Sonnet 4.5 nailed C++ atomics and condvar misuses with clean, actionable diffs.\nConsistency checks: It reliably flagged distributed state mismatches across services.\nExtra bug surfacing: It did identify additional Critical issues not originally under evaluation, though fewer than initially expected under a stricter rubric.\nAs Anthropic markets Sonnet 4.5, they emphasize “hybrid reasoning” and “long horizon” planning. In practice, that shows up as more willingness to chase down side-paths in the code and note real but untracked issues.\nSonnet 4.5: Hits a price vs. performance sweet spot\nOne of the biggest advantages of Sonnet 4.5 is its price-to-performance ratio. While Opus 4.1 remains Anthropic's flagship model in raw capability, it also comes at a significantly higher cost.\nSonnet 4.5 narrows the gap in coverage and important bug-finding while staying far more cost-efficient to run. For many teams, that balance of having close to Opus-level results at a fraction of the price is what makes Sonnet 4.5 the most pragmatic choice.\nSonnet 4.5 weaknesses\nBut if using Sonnet 4.5, it’s critical to be aware of its weaknesses. These include:\n\nDeadlock coverage: Like Sonnet 4 and even Opus, it still struggles to trace complex lock ordering.\nVerbosity and hedging: Many comments run long, caveated, or uncertain. Compare this to GPT-5 Codex, which in our earlier work wrote comments that “read like patches” with crisp directness. For example, with GPT-5 Codex:\nLock ordering / deadlock: Reorder the lock acquisitions to follow a consistent hierarchy. This prevents circular wait deadlocks.”\nRegex catastrophic backtracking: “Remove the nested quantifier to avoid catastrophic backtracking.”\nPrecision gap: At 35% comment-level precision and 41.5% important share percentage, it’s better than Sonnet 4 but well short of Opus 4.1.\nSonnet 4.5 verdict\nSonnet 4.5 feels less like a teacher writing in red pen and more like a thoughtful colleague at your side: pointing out possible issues, often right, occasionally over-hedged, and sometimes spotting things you didn’t know were there.\nThat style is a double-edged sword in review. On one hand, developers may appreciate the extra critical issues it flags. On the other, when the task is “please catch this bug,” Opus 4.1 is still sharper.\nClosing thoughts\nAnthropic positioned Sonnet 4.5 as a step toward agentic reasoning and computer use. In code review, that reasoning shows up in richer, more cautious, and more wide-ranging comments.\nFor teams:\nIf you value decisive, patch-like feedback, Opus 4.1 (or GPT-5 Codex) still sets the bar.\nIf you want a reviewer that finds critical issues anywhere they lurk, even beyond the tracked bug, Sonnet 4.5 has surprising upside.\nAnd if you care about pragmatic price-to-performance, Sonnet 4.5 may be the smartest choice: close to Opus’s accuracy at a fraction of the cost.\nEither way, Sonnet 4.5 changes the texture of reviews. It feels more human—not always cleaner, but more inquisitive, more hedged, sometimes more right in the places you weren’t looking.",
      "publishedAt": "2025-10-03T14:51:00.000Z",
      "author": "David Loker",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "funding_mna",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.982Z",
      "score": 0.3504859572213951
    },
    {
      "id": "51dab5d566315180ec1b4a0c6d173cc9",
      "title": "Aiを使ってci/cdパイプラインで静的解析を実行する方法",
      "url": "https://coderabbit.ai/blog/how-to-run-static-analysis-on-your-ci-cd-pipelines-using-ai-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/how-to-run-static-analysis-on-your-ci-cd-pipelines-using-ai\">How To Run Static Analysis On Your CI/CD Pipelines Using AI</a>の意訳です。</p>\n<p><em>「セットアップ時の思いがけない誤設定によりデータフィールドが空になり、その結果システムがアカウントを自動削除しました。」</em> —— これは、<a target=\"_blank\" href=\"https://www.itmedia.co.jp/news/articles/2405/28/news095.html\">Googleが年金基金のアカウント全体を誤って削除した件についての説明</a>です。</p>\n<p>このようなインシデントは、現代のソフトウェアシステムにおける正確な設定の重要性を浮き彫りにします。ちょっとした誤設定が、特にCI/CDパイプラインにおいて壊滅的な結果を招くことがあります。</p>\n<p>設定の正確性を担保し、コードレビューの複雑さを管理することは、DevOpsエンジニアにとって大きな負担になりえます。チームはしばしば機能開発を優先し、設定レビューは後回しになりがちです。その結果、見過ごされた誤設定が本番障害やダウンタイムを引き起こす可能性があります。</p>\n<p>CodeRabbitは、AI駆動の分析とリアルタイムフィードバックによってコードレビューを自動化し、この問題の解決を支援します。他のツールのように複雑なセットアップを必要とせず、CodeRabbitはパイプラインにシームレスに統合され、構成ファイルに対する静的チェックの正確性と効率性を確保します。</p>\n<p>本記事では、CodeRabbitがCI/CDパイプラインでの静的チェックにどのように役立ち、エンドツーエンドのデプロイプロセス全体で設定品質を保証し、効率を向上させるかを解説します。</p>\n<h2 id=\"heading-cicd\">なぜCI/CDパイプラインに静的チェックが不可欠なのか</h2>\n<p>構成ファイルは、インフラやアプリケーションのデプロイを制御するCI/CDパイプラインの要です。これらのファイルのエラーは大きな障害や事業中断リスクにつながるため、早期の検証が不可欠です。静的チェックは、セキュリティ脆弱性、コード品質問題、運用上の混乱を緩和する上で重要な役割を果たします。</p>\n<p>以下は、仮想環境のセットアップ、依存関係のインストール、Lintコマンドの実行を行うCircleCIのワークフロー構成ファイルの例です。</p>\n<pre><code class=\"lang-yaml\"><span class=\"hljs-attr\">jobs:</span>\n <span class=\"hljs-attr\">lint:</span>\n   <span class=\"hljs-attr\">docker:</span>\n     <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">image:</span> <span class=\"hljs-string\">circleci/python:3.9</span>\n   <span class=\"hljs-attr\">steps:</span>\n     <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">checkout</span>\n     <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">run:</span>\n         <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Install</span> <span class=\"hljs-string\">Dependencies</span>\n         <span class=\"hljs-attr\">command:</span> <span class=\"hljs-string\">|\n          python -m venv venv\n          . venv/bin/activate\n          pip install flake8\n</span>     <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">run:</span>\n         <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Run</span> <span class=\"hljs-string\">Linting</span>\n         <span class=\"hljs-attr\">command:</span> <span class=\"hljs-string\">|\n          . venv/bin/activate\n          flake8 .</span>\n</code></pre>\n<p>上記の構成で静的チェックが行われなければ、認識されない構文や無効な設定といった問題が漏れ、後工程でビルドが失敗する恐れがあります。例えば、依存関係の不足や不適切に整形されたコードは、デプロイパイプラインを破綻させる実行時エラーを招いたり、本番で原因追跡が難しいバグを持ち込む可能性があります。</p>\n<p>総じて、静的チェックは以下を実現します。</p>\n<ul>\n<li><p><strong>早期のエラー検出</strong>: 実行前に構文エラーや誤設定を検出し、実行時障害の可能性を減らします</p>\n</li>\n<li><p><strong>コーディング標準の遵守</strong>: スタイルガイドやベストプラクティスをコードや構成ファイル全体に適用し、品質の一貫性を確保して変更の保守・レビューを容易にします</p>\n</li>\n<li><p><strong>コード品質の向上</strong>: テストの成功や一定以上のカバレッジなど、デプロイ前に満たすべき基準を静的チェックで担保し、全体的な品質を高めます</p>\n</li>\n</ul>\n<h2 id=\"heading-coderabbit\">CodeRabbitを使った静的チェック</h2>\n<p>CodeRabbitはCI/CDワークフローに統合され、一般的な誤設定を特定することで優位性を発揮します。この能力はデプロイプロセスの整合性を維持し、エンドユーザーに影響しうる中断を防ぐ上で重要です。</p>\n<p>さらに、追加の設定を必要とせずに静的解析やLintを自動実行できるという独自の利点があります。DevOpsチームにとって、この機能はセットアップ工程を簡素化し、複雑な設定ではなく開発に集中できるようにします。</p>\n<ul>\n<li><p>既存のCI/CDパイプラインに影響を与えずに統合され、追加設定なしでLintと静的解析を自動実行します。</p>\n</li>\n<li><p>GitHub、CircleCI、GitLabなどの主要プラットフォーム上の多様なツールと統合し、<a target=\"_blank\" href=\"https://docs.coderabbit.ai/tools/actionlint\">Actionlint</a>、<a target=\"_blank\" href=\"https://docs.coderabbit.ai/tools/yamllint\">Yamllint</a>、<a target=\"_blank\" href=\"https://docs.coderabbit.ai/tools/shellcheck\">ShellCheck</a>、<a target=\"_blank\" href=\"https://docs.coderabbit.ai/tools/circleci\">CircleCI</a>パイプラインなどのチェックを実行します。これによりセットアップが簡素化され、追加の手作業なしに素早く結果を得られます。</p>\n</li>\n<li><p>JenkinsやGitHub Actionsのようなツールでは、CodeRabbitはビルドやコミットごとに継続的に静的解析を行い、誤設定を早期に検出してワークフローの信頼性を高めます。</p>\n</li>\n</ul>\n<p>次のセクションでは、実際のCodeRabbitの動作を見ていきます。</p>\n<h2 id=\"heading-coderabbitgithub-actionsactionlint\">CodeRabbitとGitHub ActionsのActionlintで誤設定を検出する</h2>\n<p>CodeRabbitの機能を示すため、GitHub Actionsワークフローをプロジェクトに統合し、CI/CDパイプラインを自動化する方法を見ていきます。リポジトリには潜在的なエラーを含む構成ファイルがあり、CodeRabbitがそれを検出して報告します。</p>\n<p>以下は、作成したワークフロー内のタスクシーケンス図です。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730095735116/4c75a2d8-b953-4b43-ad37-42d5fdcdf188.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>プルリクエストを送信すると、CodeRabbitがファイルをレビューし、潜在的な誤設定を自動的に検出します。リポジトリの準備ができたら、<a target=\"_blank\" href=\"https://coderabbit.ai/blog/how-to-integrate-ai-code-review-into-your-devops-pipeline\">CodeRabbitと統合して自動コードレビューをセットアップ</a>し、以下の主要セクションからなる、包括的で構造化されたレポートを生成します。</p>\n<ul>\n<li><strong>Summary（概要）</strong> – コードや構成で検出された主要な変更点の簡潔なサマリー。注意が必要な領域を素早く把握できます。</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730095840397/69ebd9cc-bc73-4dc5-a6df-e28e1207a3e1.png?auto=compress,format&amp;format=webp\" alt /></p>\n<ul>\n<li><strong>Walkthrough（詳細解説）</strong> – 対象ファイルの詳細なステップバイステップ分析。具体的な問題点、設定、推奨事項をガイドします。</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730095921727/ac69901a-0730-4a94-a5a6-90fa6929fb45.png?auto=compress,format&amp;format=webp\" alt /></p>\n<ul>\n<li><strong>Table of Changes（変更一覧）</strong> – 各ファイルの変更点と要約の一覧。必要な対応の優先度付けを素早く行えます。</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730095947279/0f6d9a21-cae6-4536-ab8c-83f29d365a49.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>これらのセクションは、構成ファイルの調整や<a target=\"_blank\" href=\"https://app.coderabbit.ai/\">CodeRabbitダッシュボード</a>の利用でカスタマイズできます。詳しくは<a target=\"_blank\" href=\"https://docs.coderabbit.ai/configure-coderabbit\">CodeRabbit設定ガイド</a>をご覧ください。</p>\n<p>以下は、CodeRabbitのレビューを通じて詳細な洞察と提案が得られたサンプルのworkflow.yaml構成です。</p>\n<pre><code class=\"lang-yaml\"><span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">development</span> <span class=\"hljs-string\">task</span>\n\n<span class=\"hljs-attr\">on:</span>\n  <span class=\"hljs-attr\">push:</span>\n    <span class=\"hljs-attr\">branches:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">main</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">develop</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">staging</span>\n  <span class=\"hljs-attr\">pull_request:</span>\n    <span class=\"hljs-attr\">branches:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">main</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">develop</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">staging</span>\n\n<span class=\"hljs-attr\">jobs:</span>\n  <span class=\"hljs-attr\">lint:</span>\n    <span class=\"hljs-attr\">runs-on:</span> <span class=\"hljs-string\">ubuntu-latest</span>\n    <span class=\"hljs-attr\">steps:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Checkout</span> <span class=\"hljs-string\">code</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/checkout@v3</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Lint</span> <span class=\"hljs-string\">workflow</span> <span class=\"hljs-string\">YAML</span> <span class=\"hljs-string\">files</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">rhysd/actionlint@v1</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Setup</span> <span class=\"hljs-string\">Node.js</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/setup-node@v3</span>\n        <span class=\"hljs-attr\">with:</span>\n          <span class=\"hljs-attr\">node-version:</span> <span class=\"hljs-string\">'18'</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Install</span> <span class=\"hljs-string\">dependencies</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">npm</span> <span class=\"hljs-string\">install</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Lint</span> <span class=\"hljs-string\">JavaScript</span> <span class=\"hljs-string\">code</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">npm</span> <span class=\"hljs-string\">run</span> <span class=\"hljs-string\">lint</span>\n\n  <span class=\"hljs-attr\">build:</span>\n    <span class=\"hljs-attr\">runs-on:</span> <span class=\"hljs-string\">ubuntu-latest</span>\n    <span class=\"hljs-attr\">needs:</span> <span class=\"hljs-string\">lint</span>\n    <span class=\"hljs-attr\">steps:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Checkout</span> <span class=\"hljs-string\">code</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/checkout@v3</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Setup</span> <span class=\"hljs-string\">Node.js</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/setup-node@v3</span>\n        <span class=\"hljs-attr\">with:</span>\n          <span class=\"hljs-attr\">node-version:</span> <span class=\"hljs-string\">'18'</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Install</span> <span class=\"hljs-string\">dependencies</span> <span class=\"hljs-string\">and</span> <span class=\"hljs-string\">cache</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/cache@v3</span>\n        <span class=\"hljs-attr\">with:</span>\n          <span class=\"hljs-attr\">path:</span> <span class=\"hljs-string\">~/.npm</span>\n          <span class=\"hljs-attr\">key:</span> <span class=\"hljs-string\">${{</span> <span class=\"hljs-string\">runner.os</span> <span class=\"hljs-string\">}}-node-${{</span> <span class=\"hljs-string\">hashFiles('package-lock.json')</span> <span class=\"hljs-string\">}}</span>\n          <span class=\"hljs-attr\">restore-keys:</span> <span class=\"hljs-string\">|\n            ${{ runner.os }}-node-\n</span>        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">npm</span> <span class=\"hljs-string\">install</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Run</span> <span class=\"hljs-string\">tests</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">npm</span> <span class=\"hljs-string\">test</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Check</span> <span class=\"hljs-string\">for</span> <span class=\"hljs-string\">vulnerabilities</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">npm</span> <span class=\"hljs-string\">audit</span> <span class=\"hljs-string\">--production</span>\n\n  <span class=\"hljs-attr\">terraform:</span>\n    <span class=\"hljs-attr\">runs-on:</span> <span class=\"hljs-string\">ubuntu-latest</span>\n    <span class=\"hljs-attr\">needs:</span> <span class=\"hljs-string\">build</span>\n    <span class=\"hljs-attr\">steps:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Checkout</span> <span class=\"hljs-string\">code</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/checkout@v3</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Setup</span> <span class=\"hljs-string\">Terraform</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">hashicorp/setup-terraform@v2</span>\n        <span class=\"hljs-attr\">with:</span>\n          <span class=\"hljs-attr\">terraform_version:</span> <span class=\"hljs-number\">1.5</span><span class=\"hljs-number\">.0</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Terraform</span> <span class=\"hljs-string\">init</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">terraform</span> <span class=\"hljs-string\">init</span>\n        <span class=\"hljs-attr\">working-directory:</span> <span class=\"hljs-string\">infrastructure/</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Terraform</span> <span class=\"hljs-string\">plan</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">terraform</span> <span class=\"hljs-string\">plan</span>\n        <span class=\"hljs-attr\">working-directory:</span> <span class=\"hljs-string\">infrastructure/</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Terraform</span> <span class=\"hljs-string\">apply</span> <span class=\"hljs-string\">(development)</span>\n        <span class=\"hljs-attr\">if:</span> <span class=\"hljs-string\">github.ref</span> <span class=\"hljs-string\">==</span> <span class=\"hljs-string\">'refs/heads/develop'</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">terraform</span> <span class=\"hljs-string\">apply</span> <span class=\"hljs-string\">-auto-approve</span>\n        <span class=\"hljs-attr\">working-directory:</span> <span class=\"hljs-string\">infrastructure/</span>\n        <span class=\"hljs-attr\">env:</span>\n          <span class=\"hljs-attr\">AWS_ACCESS_KEY_ID:</span> <span class=\"hljs-string\">${{</span> <span class=\"hljs-string\">secrets.AWS_ACCESS_KEY_ID</span> <span class=\"hljs-string\">}}</span>\n          <span class=\"hljs-attr\">AWS_SECRET_ACCES_KEY:</span> <span class=\"hljs-string\">${{</span> <span class=\"hljs-string\">secrets.AWS_SECRET_ACCES_KEY</span> <span class=\"hljs-string\">}}</span>\n\n  <span class=\"hljs-attr\">docker:</span>\n    <span class=\"hljs-attr\">runs-on:</span> <span class=\"hljs-string\">ubuntu-latest</span>\n    <span class=\"hljs-attr\">needs:</span> <span class=\"hljs-string\">terraform</span>\n    <span class=\"hljs-attr\">steps:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Checkout</span> <span class=\"hljs-string\">code</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/checkout@v3</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Login</span> <span class=\"hljs-string\">to</span> <span class=\"hljs-string\">AWS</span> <span class=\"hljs-string\">ECR</span>\n        <span class=\"hljs-attr\">id:</span> <span class=\"hljs-string\">login-ecr</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">aws-actions/amazon-ecr-login@v1</span>\n        <span class=\"hljs-attr\">with:</span>\n          <span class=\"hljs-attr\">region:</span> <span class=\"hljs-string\">us-east-1</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Build</span> <span class=\"hljs-string\">and</span> <span class=\"hljs-string\">tag</span> <span class=\"hljs-string\">Docker</span> <span class=\"hljs-string\">image</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n          IMAGE_TAG=${{ github.sha }}\n          docker build -t ${{ secrets.ECR_REGISTRY }}/my-app:latest .\n          echo \"IMAGE_TAG=$IMAGE_TAG\" &gt;&gt; $GITHUB_ENV\n</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Push</span> <span class=\"hljs-string\">Docker</span> <span class=\"hljs-string\">image</span> <span class=\"hljs-string\">to</span> <span class=\"hljs-string\">AWS</span> <span class=\"hljs-string\">ECR</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n          IMAGE_TAG=${{ env.IMAGE_TAG }}\n          docker push ${{ secrets.ECR_REGISTRY }}/my-app:$IMAGE_TAG\n</span>\n  <span class=\"hljs-attr\">deploy:</span>\n    <span class=\"hljs-attr\">runs-on:</span> <span class=\"hljs-string\">ubuntu-latest</span>\n    <span class=\"hljs-attr\">needs:</span> <span class=\"hljs-string\">docker</span>\n    <span class=\"hljs-attr\">environment:</span> <span class=\"hljs-string\">production</span>\n    <span class=\"hljs-attr\">steps:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Deploy</span> <span class=\"hljs-string\">to</span> <span class=\"hljs-string\">Development</span>\n        <span class=\"hljs-attr\">if:</span> <span class=\"hljs-string\">github.ref</span> <span class=\"hljs-string\">==</span> <span class=\"hljs-string\">'refs/heads/develop'</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n          echo \"Deploying to development environment\"\n          # Your deployment script here\n</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Deploy</span> <span class=\"hljs-string\">to</span> <span class=\"hljs-string\">Staging</span>\n        <span class=\"hljs-attr\">if:</span> <span class=\"hljs-string\">github.ref</span> <span class=\"hljs-string\">==</span> <span class=\"hljs-string\">'refs/heads/staging'</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n          echo \"Deploying to staging environment\"\n          # Your deployment script here\n</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Manual</span> <span class=\"hljs-string\">Approval</span> <span class=\"hljs-string\">for</span> <span class=\"hljs-string\">Production</span>\n        <span class=\"hljs-attr\">if:</span> <span class=\"hljs-string\">github.ref</span> <span class=\"hljs-string\">==</span> <span class=\"hljs-string\">'refs/head/main'</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">hmarr/auto-approve-action@v2</span>\n        <span class=\"hljs-attr\">with:</span>\n          <span class=\"hljs-attr\">github-token:</span> <span class=\"hljs-string\">${{</span> <span class=\"hljs-string\">secrets.GITHUB_TOKEN</span> <span class=\"hljs-string\">}}</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Deploy</span> <span class=\"hljs-string\">to</span> <span class=\"hljs-string\">Production</span>\n        <span class=\"hljs-attr\">if:</span> <span class=\"hljs-string\">github.ref</span> <span class=\"hljs-string\">==</span> <span class=\"hljs-string\">'refs/heads/main'</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n          echo \"Deploying to production environment\"\n          # Your deployment script here</span>\n</code></pre>\n<p>コードレビューに入る前に、このワークフローが何を行っているかを高レベルで整理します。</p>\n<ul>\n<li><p>main、develop、stagingブランチへのpushおよびプルリクエストでCI/CDパイプラインをトリガーし、継続的インテグレーションを実現します。</p>\n</li>\n<li><p>YAML構成の構文チェックや、アプリケーションに必要な依存関係のインストールを含むLintワークフローを実行し、コード品質を担保します。</p>\n</li>\n<li><p>アプリケーションに必要なクラウドインフラのプロビジョニングと管理のためにTerraformをセットアップします。</p>\n</li>\n<li><p>アプリケーションの機能を検証するテストを実行し、脆弱性チェックを行ってコードの安全性と安定性を確保します。</p>\n</li>\n<li><p>デプロイに備えてアプリケーションのDockerイメージをビルド・タグ付けします。</p>\n</li>\n<li><p>DockerイメージをAWS Elastic Container Registry（ECR）にプッシュし、デプロイのためのアクセスを容易にします。</p>\n</li>\n<li><p>ブランチに応じてアプリケーションを開発・ステージング・本番の各環境にデプロイし、本番デプロイにはコントロールと監視のための手動承認ステップを含めます。</p>\n</li>\n</ul>\n<p>workflow.yamlの構成と各コンポーネントを確認したので、まずSummaryから各セクションを見ていきます。</p>\n<h3 id=\"heading-summary\">Summary</h3>\n<p>Summaryはレビューの第一歩として、最新コミットで導入された変更点の明確で簡潔な概要を提供します。新機能、スタイル調整、構成変更、プルリクエストで挙げられたその他の関連修正など、重点ポイントを素早く把握できます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730096176817/6d47a2f7-aa15-42ef-b9b9-9e2e265fd26f.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>このスニペットは、パフォーマンス向上のための非デバッグモードでの実行、コードのLint・ビルド・デプロイを合理化する自動CI/CDパイプラインの実装など、重要な保守作業を強調しています。</p>\n<p>Summaryは、最新コミットで加えられた主要な変更点と改善点の理解に役立ちます。</p>\n<p>Summaryで要点を把握したら、次はWalkthroughセクションで具体的な変更点の詳細を見ていきます。</p>\n<h3 id=\"heading-walkthrough\">Walkthrough</h3>\n<p>このセクションでは、最新コミットで各ファイルに加えられた具体的な変更を包括的に概観します。各ファイルの変更がプロジェクト全体の機能性やユーザー体験の向上にどう寄与するかを明確にします。</p>\n<p>Changes Tableは、最新コミットにおける各ファイルの変更点を簡潔にまとめ、コードベースのどこが変更されたかを素早く特定できるようにします。</p>\n<p>各行には変更されたファイルと、Change Summary列に詳細な変更説明が含まれます。CSSファイルのスタイル更新、アプリケーションロジックの機能調整、CI/CDパイプライン構成の改善などが含まれます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730096224642/c714ccfb-0c73-4987-a314-f56c9b6cd8ed.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>情報を構造化して提示することで、変更の影響を理解しやすくし、開発者がプロジェクトへの影響を素早く把握できるようにします。</p>\n<p>全体として、コラボレーションにおける重要なリファレンスとして機能し、さらなる議論やレビューが必要な箇所にチームメンバーが集中できるよう支援しつつ、コードベースの変遷を追跡します。ちょっとした遊び心として、エラーに関するポエムも生成します。</p>\n<h3 id=\"heading-code-review\">Code Review</h3>\n<p>以下のセクションでは、構成ファイルを詳細に検査し、改善余地のある領域を特定します。キャッシュ戦略の改善からデプロイプロセスの最適化まで、GitHub Actionsワークフロー全体の効率と堅牢性を高めるための提案が、コードに対して具体的に提示されます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730096263866/b6fd24d9-e011-42ff-a2af-4025588aab4a.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>レビュー詳細、使用された構成、レビューのプロファイル、処理対象ファイル、使用された追加コンテキストなどに関する<strong>実行可能なコメント</strong>の詳細と概要が提供されます。ワンクリックでコミットできる提案が含まれる場合もあります。</p>\n<p>ここから、CodeRabbitがワークフロー各部に対して提案したレビューコメントを見ていきます。CodeRabbitは構成ファイルがGitHub Actionsのワークフローであることを自動認識し、<code>actionlint</code>で徹底的に解析します。レビューの過程で、パフォーマンス最適化に関する有益な洞察と提案が示されます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730096291950/a7c0ff57-e67c-4184-990c-428289bfc67b.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>lintジョブでは、<code>actions/cache@v3</code>を用いたnpm依存関係のキャッシュ機会を検出しました。Lint実行前にキャッシュステップを追加する提案により、以降の実行時間を短縮できます。このプロアクティブなフィードバックは、手動介入なしにワークフローを効率化し、より最適化されたCI/CDパイプラインを実現します。</p>\n<p>指摘の通り、キャッシュステップの構造に誤りがあります。runコマンド（npm install）がcacheアクションのusesブロック内に置かれており、正しく実行されない可能性があります。</p>\n<p>これを解決するため、キャッシュとインストールのステップを分離することを提案しています。修正案ではキャッシュ処理を独立したブロックに移し、次のステップで<code>npm ci</code>を使用して依存関係をクリーンかつ高速にインストールするようにしています。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730096332588/972bd50d-aa52-454d-9638-bacf814759e3.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>Terraformセクションでは、Terraformバージョンに変数を使う点での潜在的問題を検出しました。加えて、特に<code>plan</code>や<code>apply</code>でAWS認証情報に関する問題が生じうること、<code>AWS_SECRET_ACCESS_KEY</code>のタイポなど、わずかなミスでもパイプラインの実行失敗につながる可能性を指摘しています。</p>\n<p>これらに対し、タイポ修正、Terraformバージョンの更新容易化、すべてのTerraformコマンドでAWSクレデンシャルが利用可能になるような構成変更が提案されました。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730096375171/65fd86fc-bdda-4fe1-9478-581a24eb94e2.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>dockerジョブでは、Dockerイメージに<code>latest</code>タグを使用している点でのセキュリティリスクを検出しました。<code>latest</code>のみの運用はバージョニングやロールバックに問題を生じうるため、<code>latest</code>と特定バージョンタグ（例: git SHA）の併用を提案し、追跡性とロールバック容易性を高めます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730096418163/5cf101a1-e681-4aac-b508-96535af67810.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>deployジョブでも複数の潜在的問題を検出しました。手動承認ステップに自動承認アクションを用いており、本来の目的に反しています。さらに、本番デプロイのステップに構文エラーがあり、実際のデプロイスクリプトが欠落しているため、プロセスが不完全です。これらの問題に対する修正案が提示されています。</p>\n<p>CodeRabbitのAI駆動分析により、構成ファイルの問題が迅速に特定・強調され、修正提案が示されることが分かりました。</p>\n<h2 id=\"heading-cicdcoderabbit\">CI/CDパイプラインでCodeRabbitを使う利点</h2>\n<p>コードレビューを自動化し精密なフィードバックを提供することで、CodeRabbitはコード品質を高め、CI/CDパイプラインに潜む問題や脆弱性を早期に捕捉します。その結果、スムーズなデプロイとエラー削減につながります。</p>\n<p>CI/CDでCodeRabbitを使うことで得られる主な利点を見ていきましょう。</p>\n<h3 id=\"heading-6zal55m66icf55sf55sj5ocn44gu5zcr5lik\">開発者生産性の向上</h3>\n<p>自動化された静的チェックワークフローにより、CodeRabbitは手動レビューの必要性を減らし、DevOpsエンジニアが設定修正ではなく、インフラやデプロイプロセスの最適化といった戦略的タスクに集中できるようにします。即時のフィードバックループにより、コミットごとに問題を素早く検出・対処でき、迅速な開発ペースを維持できます。</p>\n<h3 id=\"heading-44kz44o844oj5zob6loq44gu5ps55zae\">コード品質の改善</h3>\n<p>CodeRabbitはベストプラクティスに照らして構成ファイルを自動検証し、設定の一貫性を強制して早期にエラーを捕捉します。プラットフォームは過去のレビューから学習し、反復的なアラートを賢く抑制、最も重要な問題に集中できるようにします。さらに、ワンクリックの提案を提供し、構成ファイルに素早く取り込めます。</p>\n<h3 id=\"heading-44k744kt44ol44oq44og44kj\">セキュリティ</h3>\n<p>CodeRabbitは、誤設定されたアクセス制御や不適切な設定などのセキュリティ脆弱性を早期に検出し、侵害の可能性を低減します。静的チェックをCI/CDプロセスに統合することで、構成ミスによるデプロイ失敗を防ぎ、より安定で信頼性の高いソフトウェアデリバリーパイプラインを実現します。</p>\n<h2 id=\"heading-44g44go44kb\">まとめ</h2>\n<p>本記事では、誤設定が遅延やセキュリティ脆弱性、さらにはデプロイ失敗につながること、そしてアプリケーションコードと同等に厳密にテストする重要性を見てきました。</p>\n<p>従来の手法が構成ファイルのテストの重要性を見落としがちなのに対し、<a target=\"_blank\" href=\"https://coderabbit.ai\">CodeRabbit</a>はCI/CDパイプラインのレビューをコード/構成レビューの自動化、重大なエラーの検出、全体的な品質向上によって支援します。手動レビュー時間を大幅に削減し、DevOpsチームが戦略的タスクに集中してデプロイサイクルを加速できるようにします。</p>\n<p>AIコードレビューの効果をワークフローで体験してみてください——<a target=\"_blank\" href=\"https://coderabbit.ai/\">今すぐCodeRabbitの無料トライアルを始めましょう</a>。</p>\n",
      "summary": "How To Run Static Analysis On Your CI/CD Pipelines Using AIの意訳です。\n「セットアップ時の思いがけない誤設定によりデータフィールドが空になり、その結果システムがアカウントを自動削除しました。」 —— これは、Googleが年金基金のアカウント全体を誤って削除した件についての説明です。\nこのようなインシデントは、現代のソフトウェアシステムにおける正確な設定の重要性を浮き彫りにします。ちょっとした誤設定が、特にCI/CDパイプラインにおいて壊滅的な結果を招くことがあります。\n設定の正確性を担保し、コードレビューの複雑さを管理することは、DevOpsエンジニアにとって大きな負担になりえます。チームはしばしば機能開発を優先し、設定レビューは後回しになりがちです。その結果、見過ごされた誤設定が本番障害やダウンタイムを引き起こす可能性があります。\nCodeRabbitは、AI駆動の分析とリアルタイムフィードバックによってコードレビューを自動化し、この問題の解決を支援します。他のツールのように複雑なセットアップを必要とせず、CodeRabbitはパイプラインにシームレスに統合され、構成ファイルに対する静的チェックの正確性と効率性を確保します。\n本記事では、CodeRabbitがCI/CDパイプラインでの静的チェックにどのように役立ち、エンドツーエンドのデプロイプロセス全体で設定品質を保証し、効率を向上させるかを解説します。\nなぜCI/CDパイプラインに静的チェックが不可欠なのか\n構成ファイルは、インフラやアプリケーションのデプロイを制御するCI/CDパイプラインの要です。これらのファイルのエラーは大きな障害や事業中断リスクにつながるため、早期の検証が不可欠です。静的チェックは、セキュリティ脆弱性、コード品質問題、運用上の混乱を緩和する上で重要な役割を果たします。\n以下は、仮想環境のセットアップ、依存関係のインストール、Lintコマンドの実行を行うCircleCIのワークフロー構成ファイルの例です。\njobs:\n lint:\n   docker:\n     - image: circleci/python:3.9\n   steps:\n     - checkout\n     - run:\n         name: Install Dependencies\n         command: |\n          python -m venv venv\n          . venv/bin/activate\n          pip install flake8\n     - run:\n         name: Run Linting\n         command: |\n          . venv/bin/activate\n          flake8 .\n\n上記の構成で静的チェックが行われなければ、認識されない構文や無効な設定といった問題が漏れ、後工程でビルドが失敗する恐れがあります。例えば、依存関係の不足や不適切に整形されたコードは、デプロイパイプラインを破綻させる実行時エラーを招いたり、本番で原因追跡が難しいバグを持ち込む可能性があります。\n総じて、静的チェックは以下を実現します。\n早期のエラー検出: 実行前に構文エラーや誤設定を検出し、実行時障害の可能性を減らします\nコーディング標準の遵守: スタイルガイドやベストプラクティスをコードや構成ファイル全体に適用し、品質の一貫性を確保して変更の保守・レビューを容易にします\nコード品質の向上: テストの成功や一定以上のカバレッジなど、デプロイ前に満たすべき基準を静的チェックで担保し、全体的な品質を高めます\nCodeRabbitを使った静的チェック\nCodeRabbitはCI/CDワークフローに統合され、一般的な誤設定を特定することで優位性を発揮します。この能力はデプロイプロセスの整合性を維持し、エンドユーザーに影響しうる中断を防ぐ上で重要です。\nさらに、追加の設定を必要とせずに静的解析やLintを自動実行できるという独自の利点があります。DevOpsチームにとって、この機能はセットアップ工程を簡素化し、複雑な設定ではなく開発に集中できるようにします。\n既存のCI/CDパイプラインに影響を与えずに統合され、追加設定なしでLintと静的解析を自動実行します。\nGitHub、CircleCI、GitLabなどの主要プラットフォーム上の多様なツールと統合し、Actionlint、Yamllint、ShellCheck、CircleCIパイプラインなどのチェックを実行します。これによりセットアップが簡素化され、追加の手作業なしに素早く結果を得られます。\nJenkinsやGitHub Actionsのようなツールでは、CodeRabbitはビルドやコミットごとに継続的に静的解析を行い、誤設定を早期に検出してワークフローの信頼性を高めます。\n次のセクションでは、実際のCodeRabbitの動作を見ていきます。\nCodeRabbitとGitHub ActionsのActionlintで誤設定を検出する\nCodeRabbitの機能を示すため、GitHub Actionsワークフローをプロジェクトに統合し、CI/CDパイプラインを自動化する方法を見ていきます。リポジトリには潜在的なエラーを含む構成ファイルがあり、CodeRabbitがそれを検出して報告します。\n以下は、作成したワークフロー内のタスクシーケンス図です。\n\nプルリクエストを送信すると、CodeRabbitがファイルをレビューし、潜在的な誤設定を自動的に検出します。リポジトリの準備ができたら、CodeRabbitと統合して自動コードレビューをセットアップし、以下の主要セクションからなる、包括的で構造化されたレポートを生成します。\nSummary（概要） – コードや構成で検出された主要な変更点の簡潔なサマリー。注意が必要な領域を素早く把握できます。\n\nWalkthrough（詳細解説） – 対象ファイルの詳細なステップバイステップ分析。具体的な問題点、設定、推奨事項をガイドします。\n\nTable of Changes（変更一覧） – 各ファイルの変更点と要約の一覧。必要な対応の優先度付けを素早く行えます。\n\nこれらのセクションは、構成ファイルの調整やCodeRabbitダッシュボードの利用でカスタマイズできます。詳しくはCodeRabbit設定ガイドをご覧ください。\n以下は、CodeRabbitのレビューを通じて詳細な洞察と提案が得られたサンプルのworkflow.yaml構成です。\nname: development task\n\non:\n  push:\n    branches:\n      - main\n      - develop\n      - staging\n  pull_request:\n    branches:\n      - main\n      - develop\n      - staging\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Lint workflow YAML files\n        uses: rhysd/actionlint@v1\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: '18'\n\n      - name: Install dependencies\n        run: npm install\n\n      - name: Lint JavaScript code\n        run: npm run lint\n\n  build:\n    runs-on: ubuntu-latest\n    needs: lint\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: '18'\n\n      - name: Install dependencies and cache\n        uses: actions/cache@v3\n        with:\n          path: ~/.npm\n          key: ${{ runner.os }}-node-${{ hashFiles('package-lock.json') }}\n          restore-keys: |\n            ${{ runner.os }}-node-\n        run: npm install\n\n      - name: Run tests\n        run: npm test\n\n      - name: Check for vulnerabilities\n        run: npm audit --production\n\n  terraform:\n    runs-on: ubuntu-latest\n    needs: build\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v2\n        with:\n          terraform_version: 1.5.0\n\n      - name: Terraform init\n        run: terraform init\n        working-directory: infrastructure/\n\n      - name: Terraform plan\n        run: terraform plan\n        working-directory: infrastructure/\n\n      - name: Terraform apply (development)\n        if: github.ref == 'refs/heads/develop'\n        run: terraform apply -auto-approve\n        working-directory: infrastructure/\n        env:\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCES_KEY: ${{ secrets.AWS_SECRET_ACCES_KEY }}\n\n  docker:\n    runs-on: ubuntu-latest\n    needs: terraform\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Login to AWS ECR\n        id: login-ecr\n        uses: aws-actions/amazon-ecr-login@v1\n        with:\n          region: us-east-1\n\n      - name: Build and tag Docker image\n        run: |\n          IMAGE_TAG=${{ github.sha }}\n          docker build -t ${{ secrets.ECR_REGISTRY }}/my-app:latest .\n          echo \"IMAGE_TAG=$IMAGE_TAG\" >> $GITHUB_ENV\n\n      - name: Push Docker image to AWS ECR\n        run: |\n          IMAGE_TAG=${{ env.IMAGE_TAG }}\n          docker push ${{ secrets.ECR_REGISTRY }}/my-app:$IMAGE_TAG\n\n  deploy:\n    runs-on: ubuntu-latest\n    needs: docker\n    environment: production\n    steps:\n      - name: Deploy to Development\n        if: github.ref == 'refs/heads/develop'\n        run: |\n          echo \"Deploying to development environment\"\n          # Your deployment script here\n\n      - name: Deploy to Staging\n        if: github.ref == 'refs/heads/staging'\n        run: |\n          echo \"Deploying to staging environment\"\n          # Your deployment script here\n\n      - name: Manual Approval for Production\n        if: github.ref == 'refs/head/main'\n        uses: hmarr/auto-approve-action@v2\n        with:\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Deploy to Production\n        if: github.ref == 'refs/heads/main'\n          run: |\n          echo \"Deploying to production environment\"\n          # Your deployment script here\n\nコードレビューに入る前に、このワークフローが何を行っているかを高レベルで整理します。\nmain、develop、stagingブランチへのpushおよびプルリクエストでCI/CDパイプラインをトリガーし、継続的インテグレーションを実現します。\nYAML構成の構文チェックや、アプリケーションに必要な依存関係のインストールを含むLintワークフローを実行し、コード品質を担保します。\nアプリケーションに必要なクラウドインフラのプロビジョニングと管理のためにTerraformをセットアップします。\nアプリケーションの機能を検証するテストを実行し、脆弱性チェックを行ってコードの安全性と安定性を確保します。\nデプロイに備えてアプリケーションのDockerイメージをビルド・タグ付けします。\nDockerイメージをAWS Elastic Container Registry（ECR）にプッシュし、デプロイのためのアクセスを容易にします。\nブランチに応じてアプリケーションを開発・ステージング・本番の各環境にデプロイし、本番デプロイにはコントロールと監視のための手動承認ステップを含めます。\nworkflow.yamlの構成と各コンポーネントを確認したので、まずSummaryから各セクションを見ていきます。\nSummary\nSummaryはレビューの第一歩として、最新コミットで導入された変更点の明確で簡潔な概要を提供します。新機能、スタイル調整、構成変更、プルリクエストで挙げられたその他の関連修正など、重点ポイントを素早く把握できます。\n\nこのスニペットは、パフォーマンス向上のための非デバッグモードでの実行、コードのLint・ビルド・デプロイを合理化する自動CI/CDパイプラインの実装など、重要な保守作業を強調しています。\nSummaryは、最新コミットで加えられた主要な変更点と改善点の理解に役立ちます。\nSummaryで要点を把握したら、次はWalkthroughセクションで具体的な変更点の詳細を見ていきます。\nWalkthrough\nこのセクションでは、最新コミットで各ファイルに加えられた具体的な変更を包括的に概観します。各ファイルの変更がプロジェクト全体の機能性やユーザー体験の向上にどう寄与するかを明確にします。\nChanges Tableは、最新コミットにおける各ファイルの変更点を簡潔にまとめ、コードベースのどこが変更されたかを素早く特定できるようにします。\n各行には変更されたファイルと、Change Summary列に詳細な変更説明が含まれます。CSSファイルのスタイル更新、アプリケーションロジックの機能調整、CI/CDパイプライン構成の改善などが含まれます。\n\n情報を構造化して提示することで、変更の影響を理解しやすくし、開発者がプロジェクトへの影響を素早く把握できるようにします。\n全体として、コラボレーションにおける重要なリファレンスとして機能し、さらなる議論やレビューが必要な箇所にチームメンバーが集中できるよう支援しつつ、コードベースの変遷を追跡します。ちょっとした遊び心として、エラーに関するポエムも生成します。\nCode Review\n以下のセクションでは、構成ファイルを詳細に検査し、改善余地のある領域を特定します。キャッシュ戦略の改善からデプロイプロセスの最適化まで、GitHub Actionsワークフロー全体の効率と堅牢性を高めるための提案が、コードに対して具体的に提示されます。\n\nレビュー詳細、使用された構成、レビューのプロファイル、処理対象ファイル、使用された追加コンテキストなどに関する実行可能なコメントの詳細と概要が提供されます。ワンクリックでコミットできる提案が含まれる場合もあります。\nここから、CodeRabbitがワークフロー各部に対して提案したレビューコメントを見ていきます。CodeRabbitは構成ファイルがGitHub Actionsのワークフローであることを自動認識し、actionlintで徹底的に解析します。レビューの過程で、パフォーマンス最適化に関する有益な洞察と提案が示されます。\n\nlintジョブでは、actions/cache@v3を用いたnpm依存関係のキャッシュ機会を検出しました。Lint実行前にキャッシュステップを追加する提案により、以降の実行時間を短縮できます。このプロアクティブなフィードバックは、手動介入なしにワークフローを効率化し、より最適化されたCI/CDパイプラインを実現します。\n指摘の通り、キャッシュステップの構造に誤りがあります。runコマンド（npm install）がcacheアクションのusesブロック内に置かれており、正しく実行されない可能性があります。\nこれを解決するため、キャッシュとインストールのステップを分離することを提案しています。修正案ではキャッシュ処理を独立したブロックに移し、次のステップでnpm ciを使用して依存関係をクリーンかつ高速にインストールするようにしています。\n\nTerraformセクションでは、Terraformバージョンに変数を使う点での潜在的問題を検出しました。加えて、特にplanやapplyでAWS認証情報に関する問題が生じうること、AWS_SECRET_ACCESS_KEYのタイポなど、わずかなミスでもパイプラインの実行失敗につながる可能性を指摘しています。\nこれらに対し、タイポ修正、Terraformバージョンの更新容易化、すべてのTerraformコマンドでAWSクレデンシャルが利用可能になるような構成変更が提案されました。\n\ndockerジョブでは、Dockerイメージにlatestタグを使用している点でのセキュリティリスクを検出しました。latestのみの運用はバージョニングやロールバックに問題を生じうるため、latestと特定バージョンタグ（例: git SHA）の併用を提案し、追跡性とロールバック容易性を高めます。\n\ndeployジョブでも複数の潜在的問題を検出しました。手動承認ステップに自動承認アクションを用いており、本来の目的に反しています。さらに、本番デプロイのステップに構文エラーがあり、実際のデプロイスクリプトが欠落しているため、プロセスが不完全です。これらの問題に対する修正案が提示されています。\nCodeRabbitのAI駆動分析により、構成ファイルの問題が迅速に特定・強調され、修正提案が示されることが分かりました。\nCI/CDパイプラインでCodeRabbitを使う利点\nコードレビューを自動化し精密なフィードバックを提供することで、CodeRabbitはコード品質を高め、CI/CDパイプラインに潜む問題や脆弱性を早期に捕捉します。その結果、スムーズなデプロイとエラー削減につながります。\nCI/CDでCodeRabbitを使うことで得られる主な利点を見ていきましょう。\n開発者生産性の向上\n自動化された静的チェックワークフローにより、CodeRabbitは手動レビューの必要性を減らし、DevOpsエンジニアが設定修正ではなく、インフラやデプロイプロセスの最適化といった戦略的タスクに集中できるようにします。即時のフィードバックループにより、コミットごとに問題を素早く検出・対処でき、迅速な開発ペースを維持できます。\nコード品質の改善\nCodeRabbitはベストプラクティスに照らして構成ファイルを自動検証し、設定の一貫性を強制して早期にエラーを捕捉します。プラットフォームは過去のレビューから学習し、反復的なアラートを賢く抑制、最も重要な問題に集中できるようにします。さらに、ワンクリックの提案を提供し、構成ファイルに素早く取り込めます。\nセキュリティ\nCodeRabbitは、誤設定されたアクセス制御や不適切な設定などのセキュリティ脆弱性を早期に検出し、侵害の可能性を低減します。静的チェックをCI/CDプロセスに統合することで、構成ミスによるデプロイ失敗を防ぎ、より安定で信頼性の高いソフトウェアデリバリーパイプラインを実現します。\nまとめ\n本記事では、誤設定が遅延やセキュリティ脆弱性、さらにはデプロイ失敗につながること、そしてアプリケーションコードと同等に厳密にテストする重要性を見てきました。\n従来の手法が構成ファイルのテストの重要性を見落としがちなのに対し、CodeRabbitはCI/CDパイプラインのレビューをコード/構成レビューの自動化、重大なエラーの検出、全体的な品質向上によって支援します。手動レビュー時間を大幅に削減し、DevOpsチームが戦略的タスクに集中してデプロイサイクルを加速できるようにします。\nAIコードレビューの効果をワークフローで体験してみてください——今すぐCodeRabbitの無料トライアルを始めましょう。",
      "publishedAt": "2025-10-02T06:11:57.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "thought_leadership",
      "tags": [
        "code_review",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.983Z",
      "score": 0.20024107560775445
    },
    {
      "id": "891e9f13dfd4ae0d07a1fa623cdd9730",
      "title": "GPT-5 Codex: GPT-5の欠点をどう解消するか",
      "url": "https://coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks\">GPT-5 Codex: How it solves for GPT-5's drawbacks</a>の意訳です。</p>\n<p>CodeRabbitのコードレビューは、開発者がバグを修正しコードをデリバリーするのを支援します。私たちは最近、<a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning-ja\">GPT-5のベンチマーク</a>について記事を書き、AIコードレビューという私たちのユースケースにおいて、このモデルが推論面で世代的な飛躍を遂げているという見解を述べました。より広いユーザーベースに展開する中で、S/N値（シグナル/ノイズ値。以下SNR）が低下し、レビューが過度に細かすぎるという印象を持たれることが分かりました。</p>\n<p>GPT-5 Codexのリリースと、私たちが実施した製品変更（重大度タグ付け、より厳格なリファクタ提案のゲーティング、フィルタリング改善）により、難しいバグを見つける能力を犠牲にすることなく、SNRを取り戻すことができました。</p>\n<p>刷新した「Hard 25」PRセットにおいて、GPT-5 CodexはGPT-5と比べてコメントあたりの精度が約35%向上し、エラーパターンレベルの不具合カバレッジは本質的に同等のまま、コメント量を約3分の1削減しました。さらにGPT-5 Codexモデルの低レイテンシと組み合わせることで、体感はより軽快、かつフォーカスされたものになります。</p>\n<h2 id=\"heading-kirkvzxjgplvvijjgarjgzzvvinmukzlrprjgzfjgzjgysqkg\"><strong>何を（なぜ）測定したか</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759249757952/184a4486-2f03-4293-8b39-82ca88ed9a32.png\" alt class=\"image--center mx-auto\" /></p>\n<p>GPT-5 Codexのテストでは、OSSのPRからなる新しい「Hard 25」スイート（<a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning-ja\">以前の記事</a>よりやや難度高め）を実行しました。これは私たちのデータセットに含まれる中でも特に難しい25本のプルリクエストです。現実世界のバグを表したもので、対象は以下の通りです。</p>\n<ul>\n<li><p>並行性の問題（例: TOCTOUレース、誤った同期化）</p>\n</li>\n<li><p>オブジェクト指向設計の欠陥（例: 仮想呼び出しの落とし穴、参照カウントメモリモデルの破綻）</p>\n</li>\n<li><p>パフォーマンス上の危険（例: 無制御なキャッシュ成長、タイトループによるスタール）</p>\n</li>\n<li><p>言語特有の落とし穴（例: TypeScriptの誤用、C++のメモリ順序の微妙さ）</p>\n</li>\n</ul>\n<p>評価したモデルは以下の通りです。</p>\n<ul>\n<li><p><strong>GPT-5 Codex</strong></p>\n</li>\n<li><p><strong>GPT-5</strong></p>\n</li>\n<li><p><strong>Claude</strong>（Sonnet 4 および Opus-4.1）</p>\n</li>\n</ul>\n<h2 id=\"heading-5l2v44ks6kmv5l6h44gx44gf44gl\">何を評価したか</h2>\n<p>各モデルには、以下の観点でスコアを与えました:</p>\n<ul>\n<li><p><strong>EP（Error Pattern / エラーパターン）</strong><br />  PRに潜む特定の根本欠陥（例: 条件変数でのlost wakeup、ロック順序の不整合、ブール条件が錯綜する中に隠れたロジックバグ）。</p>\n</li>\n<li><p><strong>EP PASS/FAIL（PR単位）</strong><br />  そのPRのEPを直接修正、または信頼できる形で表面化させるコメントを少なくとも1つ残せばPASS。コメントがゼロならそのPRはFAIL。</p>\n</li>\n<li><p><strong>コメントPASS/FAIL（コメント単位）</strong><br />  EPを直接修正、または信頼できる形で表面化させればPASS、そうでなければFAIL。</p>\n</li>\n<li><p><strong>コメントあたり精度（Per comment precision）</strong><br />  PASSコメント ÷ 全コメント。今回のデータセットにおける実務上のSNR。</p>\n</li>\n<li><p><strong>Important share（重要コメント比率）</strong><br />  すべてのPASSはImportant扱い。EPを解決しないが、重大なバグ（use-after-free、二重解放、lost wakeup、メモリリーク、null参照、パストラバーサル、破滅的な正規表現など）を正しく指摘するコメントもImportant。それ以外はMinor。</p>\n</li>\n</ul>\n<h2 id=\"heading-codexsnr\"><strong>スコアボード - CodexはSNRを改善</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759197629508/de48651d-069a-4530-b070-de6107b57583.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>要点:</strong> Codexは、GPT-5とほぼ同じEPを見つけつつ、より少ない・締まったコメントで行うため、SNRが向上します。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759238036950/7c99e827-daba-4205-8f70-0d5a9413b74b.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>意味するところ:</strong> Codexは25本中20本のPRをカバー（残り5本は未カバーのFAIL）。総コメント数は少ないにもかかわらず、EPのPASS数はやや上回り（16 対 15）、重要（Important）コメントは大幅に増加。コメントの半分以上が、そのPRで想定していた問題へのダイレクト、または別の重大バグの指摘でした。GPT-5とClaudeは精度・重要比率ともに約40%で、後塵を拝しました。</p>\n<p><strong>結論: 同等のEPカバレッジで、ノイズは減少</strong><br />CodexはGPT-5のバグ発見力を維持したまま、コメント量を約32%削減（54 対 79）し、コメントあたり精度を約35%向上（46.3% 対 34.2%）。ClaudeはカバレッジはGPT-5に近いものの、より冗長で精度は低めでした。</p>\n<h2 id=\"heading-codex\"><strong>スタイルと構造（Codexがパッチのように読める理由）</strong></h2>\n<p>Codexの返信は一貫してアクション優先（ほぼ常にdiff付き）で、曖昧表現が少ない。これは「すぐパッチに反映できる提案」を望むレビュアーの期待に合致します。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759237978515/bf5eab59-e642-486d-bbb7-8adfd02fc00e.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-codex-1\"><strong>Codexが得意とするバグの種類</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759197684585/6b90b664-dbf0-46d0-ae5e-cf988083ab8b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>スイート全体では、どのモデルも並行性・同期の問題に強みを見せましたが、Codexは特に以下で際立ちました。</p>\n<ol>\n<li><p><strong>条件変数の誤用とlost wakeup</strong><br /> ロック下でのwait、ループ内での述語チェックといった標準パターンを提案し、具体的なdiffを提示。</p>\n</li>\n<li><p><strong>ロック順序とデッドロック</strong><br /> 取得順の不整合を指摘し、ロック階層の導入やクリティカルセクション外への処理移動を提案（いずれも実行可能な編集付き）。</p>\n</li>\n<li><p><strong>APIやパフォーマンスの微妙な罠</strong><br /> 破滅的な正規表現のバックトラッキングやメモリモデルの順序問題などを的確に特定し、パッチを提示。</p>\n</li>\n</ol>\n<h2 id=\"heading-gpt-5\"><strong>なぜGPT-5は騒がしく感じられたのか、そしてどう解決したか</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759251737131/d79ed373-5213-4cd0-a168-64441227b7b9.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>観測:</strong> SonnetやOpusからGPT-5に移行した際、レビューあたりの総コメント数はほぼ倍増しました。一方でハルシネーションは1%未満、ネガティブトーンも1%未満まで低下したにもかかわらず、受け入れ率（有益と判断されたコメントの比率）は、GPT-5導入前のベースラインに比べて大きく低下しました。</p>\n<p><strong>Codexでの変化:</strong> GPT-5 Codexと私たちの製品変更の併用により、受け入れ率は以前の水準まで回復。一方で総コメント量は「GPT-5導入前」より依然多いままです。要するに、「有益さ」は取り戻しつつ、GPT-5並みに実問題を見つけ続けられるようになりました。</p>\n<p>この改善には2つの製品変更が寄与しました。</p>\n<ol>\n<li><p><strong>重大度とレビュータイプのタグを前面に</strong></p>\n<ul>\n<li><p><strong>レビュータイプ:</strong> ユーザーが読みたいコメントの種類を自己選択できるよう、⚠️ Potential issue、🛠️ Refactor suggestion、🧹 Nitpick（<em>Assertive</em>モードにしない限り非表示）を用意。</p>\n</li>\n<li><p><strong>重大度:</strong> コメントに重大度タグを付け、優先度を明確化。タグは🔴 Critical、🟠 Major、🟡 Minor、🔵 Trivial、⚪ Info。</p>\n</li>\n<li><p>バグ（Critical/Major/Minor）は常に表示。その他は常にではありません。リファクタはモデルが「本質的」と判定した場合のみ表示。すべて見たいユーザーは<em>Assertive</em>に切替可能。</p>\n</li>\n</ul>\n</li>\n<li><p><strong>より厳格なフィルタリングと集約</strong></p>\n<ul>\n<li>重複メモを折りたたみ、「あると嬉しい」レベルの提案は明確なROIがない限り除外。結果として、コメントは少数精鋭化し、ノイズで見落とすリスクが減少。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"heading-amp-codex\"><strong>レイテンシ: 速さは正義 &amp; Codexは速い</strong></h2>\n<p>5分のレビューは許容範囲ですが、30分は許容できません。GPT-5の「常に深く考える」スタイルは、ファーストトークンまでの時間と全体のレビュー時間を大幅に増やしました。私たちは最近いくつかのパイプライン最適化を行い、さらにCodexがGPT-5由来のレイテンシを低減できるようになりました。</p>\n<p>Codexの可変（弾力的）な思考は、不要な場面では深掘りを減らし、実運用でTTFT（最初の出力までの時間）とE2Eレビュー時間を短縮しています。総じて、レビューは速くなり、フィードバックは早く、ヒューマン・イン・ザ・ループの流れが改善されます。</p>\n<h2 id=\"heading-coderabbit\"><strong>CodeRabbitユーザーが期待できること</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759197710045/2f32eef6-3227-450f-ac4b-485f89126197.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Codex導入後、AIコードレビューはどう変わるでしょうか？</p>\n<ol>\n<li><p><strong>生のバグ検出力は同等</strong></p>\n<ul>\n<li>刷新したHard 25で、CodexのEPレベルPASSは64%、GPT-5は60%（<a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning-ja\">以前のPRセット</a>ではGPT-5が77.3%）。GPT-5がもたらした重要な勝ち筋を失っていません。</li>\n</ul>\n</li>\n<li><p><strong>コメントは少なく、しかし強く</strong></p>\n<ul>\n<li>総コメント数はGPT-5比で約32%減、SNR（コメントあたり精度）は約35%向上。文章よりパッチが増えます。</li>\n</ul>\n</li>\n<li><p><strong>重大度タグでレビューに集中</strong></p>\n<ul>\n<li>新しい重大度タグにより、Critical/Majorがトップに浮上。リファクタはゲート制御、ニットピックはオプトイン。コメントの走査に費やす時間が減り、修正に時間を割けます。</li>\n</ul>\n</li>\n<li><p><strong>フィードバックループの高速化</strong></p>\n<ul>\n<li>Codexの軽量な推論とパイプライン改善で、最初の有益なコメントまでの時間が短縮。体感で分かります。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"heading-kirlrprph4nmotku5jpjllvvijjg4fjg7zjgrlpb3jgy3jga7jgyljgarjgzjgbjvvikqkg\"><strong>定量的付録（データ好きのあなたへ）</strong></h2>\n<p>以下は興味深かった追加統計を紹介します。</p>\n<ul>\n<li><p><strong>コメントあたり精度（SNR）の向上:</strong> Codex 46.3% 対 GPT-5 34.2% — 相対で約+35%。</p>\n</li>\n<li><p><strong>コメント量の差:</strong> Codex 54 対 GPT-5 79 — 約32%減、EPのPASSは実質同等（16 対 15）。</p>\n</li>\n<li><p><strong>スタイル:</strong> Codexは94%のコメントでdiffを含み、このセットではClaudeやGPT-5より曖昧表現が少ない。</p>\n</li>\n<li><p><strong>実環境での受け入れ:</strong> GPT-5ロールアウト中は受け入れ率が大きく低下。Codexと製品変更の併用で約20–25%相対上昇し、導入前水準に回復。かつ、GPT-5導入前より受け入れコメント数は多いまま。</p>\n</li>\n</ul>\n<h2 id=\"heading-codex-2\"><strong>Codexがまだ弱い点（と取り組み）</strong></h2>\n<p>改善は大きいものの、課題が残っていないわけではありません。現在、以下に取り組んでいます。</p>\n<ul>\n<li><p><strong>カバレッジの穴</strong><br />  モデルがPRにコメントを残さない場合、そのEPはハードFAIL。Codexの探索ヒューリスティクスを広げ、特定クラスの問題を見落としにくくします。</p>\n</li>\n<li><p><strong>リファクタ過剰提案（調整済みだが未完）</strong><br />  「本質的なもののみ」のゲートでノイズは抑制しましたが、特に大規模diffでコメント過多になりがちなケースの閾値をさらに引き締めます。</p>\n</li>\n<li><p><strong>ユーザー主導の優先度付け</strong><br />  GitHubのインライン順序は変更できませんが、各コメントに重大度を注記し、上から順にトリアージしやすくします。</p>\n</li>\n</ul>\n<h2 id=\"heading-codex-gpt-5\"><strong>Codex GPT-5: バグ捕捉力はそのまま、副作用は少なく</strong></h2>\n<p>私たちの指標はシンプルです: <strong>重要なバグを、素早く、ノイズに埋もれさせずに捕まえること</strong>。Codexはその実現を助けてくれます。GPT-5の噛み応えある推論力を保ちながら、SNRを回復させ、レイテンシを大幅に削りました。今後も測定・改善を継続し、より良い製品をリリースし続けます。</p>\n",
      "summary": "GPT-5 Codex: How it solves for GPT-5's drawbacksの意訳です。\nCodeRabbitのコードレビューは、開発者がバグを修正しコードをデリバリーするのを支援します。私たちは最近、GPT-5のベンチマークについて記事を書き、AIコードレビューという私たちのユースケースにおいて、このモデルが推論面で世代的な飛躍を遂げているという見解を述べました。より広いユーザーベースに展開する中で、S/N値（シグナル/ノイズ値。以下SNR）が低下し、レビューが過度に細かすぎるという印象を持たれることが分かりました。\nGPT-5 Codexのリリースと、私たちが実施した製品変更（重大度タグ付け、より厳格なリファクタ提案のゲーティング、フィルタリング改善）により、難しいバグを見つける能力を犠牲にすることなく、SNRを取り戻すことができました。\n刷新した「Hard 25」PRセットにおいて、GPT-5 CodexはGPT-5と比べてコメントあたりの精度が約35%向上し、エラーパターンレベルの不具合カバレッジは本質的に同等のまま、コメント量を約3分の1削減しました。さらにGPT-5 Codexモデルの低レイテンシと組み合わせることで、体感はより軽快、かつフォーカスされたものになります。\n何を（なぜ）測定したか\n\nGPT-5 Codexのテストでは、OSSのPRからなる新しい「Hard 25」スイート（以前の記事よりやや難度高め）を実行しました。これは私たちのデータセットに含まれる中でも特に難しい25本のプルリクエストです。現実世界のバグを表したもので、対象は以下の通りです。\n並行性の問題（例: TOCTOUレース、誤った同期化）\nオブジェクト指向設計の欠陥（例: 仮想呼び出しの落とし穴、参照カウントメモリモデルの破綻）\nパフォーマンス上の危険（例: 無制御なキャッシュ成長、タイトループによるスタール）\n言語特有の落とし穴（例: TypeScriptの誤用、C++のメモリ順序の微妙さ）\n評価したモデルは以下の通りです。\nGPT-5 Codex\nGPT-5\nClaude（Sonnet 4 および Opus-4.1）\n何を評価したか\n各モデルには、以下の観点でスコアを与えました:\nEP（Error Pattern / エラーパターン）\n  PRに潜む特定の根本欠陥（例: 条件変数でのlost wakeup、ロック順序の不整合、ブール条件が錯綜する中に隠れたロジックバグ）。\nEP PASS/FAIL（PR単位）\n  そのPRのEPを直接修正、または信頼できる形で表面化させるコメントを少なくとも1つ残せばPASS。コメントがゼロならそのPRはFAIL。\nコメントPASS/FAIL（コメント単位）\n  EPを直接修正、または信頼できる形で表面化させればPASS、そうでなければFAIL。\nコメントあたり精度（Per comment precision）\n  PASSコメント ÷ 全コメント。今回のデータセットにおける実務上のSNR。\nImportant share（重要コメント比率）\n  すべてのPASSはImportant扱い。EPを解決しないが、重大なバグ（use-after-free、二重解放、lost wakeup、メモリリーク、null参照、パストラバーサル、破滅的な正規表現など）を正しく指摘するコメントもImportant。それ以外はMinor。\nスコアボード - CodexはSNRを改善\n\n要点: Codexは、GPT-5とほぼ同じEPを見つけつつ、より少ない・締まったコメントで行うため、SNRが向上します。\n\n意味するところ: Codexは25本中20本のPRをカバー（残り5本は未カバーのFAIL）。総コメント数は少ないにもかかわらず、EPのPASS数はやや上回り（16 対 15）、重要（Important）コメントは大幅に増加。コメントの半分以上が、そのPRで想定していた問題へのダイレクト、または別の重大バグの指摘でした。GPT-5とClaudeは精度・重要比率ともに約40%で、後塵を拝しました。\n結論: 同等のEPカバレッジで、ノイズは減少\nCodexはGPT-5のバグ発見力を維持したまま、コメント量を約32%削減（54 対 79）し、コメントあたり精度を約35%向上（46.3% 対 34.2%）。ClaudeはカバレッジはGPT-5に近いものの、より冗長で精度は低めでした。\nスタイルと構造（Codexがパッチのように読める理由）\nCodexの返信は一貫してアクション優先（ほぼ常にdiff付き）で、曖昧表現が少ない。これは「すぐパッチに反映できる提案」を望むレビュアーの期待に合致します。\n\nCodexが得意とするバグの種類\n\nスイート全体では、どのモデルも並行性・同期の問題に強みを見せましたが、Codexは特に以下で際立ちました。\n条件変数の誤用とlost wakeup\n ロック下でのwait、ループ内での述語チェックといった標準パターンを提案し、具体的なdiffを提示。\nロック順序とデッドロック\n 取得順の不整合を指摘し、ロック階層の導入やクリティカルセクション外への処理移動を提案（いずれも実行可能な編集付き）。\nAPIやパフォーマンスの微妙な罠\n 破滅的な正規表現のバックトラッキングやメモリモデルの順序問題などを的確に特定し、パッチを提示。\nなぜGPT-5は騒がしく感じられたのか、そしてどう解決したか\n\n観測: SonnetやOpusからGPT-5に移行した際、レビューあたりの総コメント数はほぼ倍増しました。一方でハルシネーションは1%未満、ネガティブトーンも1%未満まで低下したにもかかわらず、受け入れ率（有益と判断されたコメントの比率）は、GPT-5導入前のベースラインに比べて大きく低下しました。\nCodexでの変化: GPT-5 Codexと私たちの製品変更の併用により、受け入れ率は以前の水準まで回復。一方で総コメント量は「GPT-5導入前」より依然多いままです。要するに、「有益さ」は取り戻しつつ、GPT-5並みに実問題を見つけ続けられるようになりました。\nこの改善には2つの製品変更が寄与しました。\n重大度とレビュータイプのタグを前面に\nレビュータイプ: ユーザーが読みたいコメントの種類を自己選択できるよう、⚠️ Potential issue、🛠️ Refactor suggestion、🧹 Nitpick（Assertiveモードにしない限り非表示）を用意。\n重大度: コメントに重大度タグを付け、優先度を明確化。タグは🔴 Critical、🟠 Major、🟡 Minor、🔵 Trivial、⚪ Info。\nバグ（Critical/Major/Minor）は常に表示。その他は常にではありません。リファクタはモデルが「本質的」と判定した場合のみ表示。すべて見たいユーザーはAssertiveに切替可能。\nより厳格なフィルタリングと集約\n重複メモを折りたたみ、「あると嬉しい」レベルの提案は明確なROIがない限り除外。結果として、コメントは少数精鋭化し、ノイズで見落とすリスクが減少。\nレイテンシ: 速さは正義 & Codexは速い\n5分のレビューは許容範囲ですが、30分は許容できません。GPT-5の「常に深く考える」スタイルは、ファーストトークンまでの時間と全体のレビュー時間を大幅に増やしました。私たちは最近いくつかのパイプライン最適化を行い、さらにCodexがGPT-5由来のレイテンシを低減できるようになりました。\nCodexの可変（弾力的）な思考は、不要な場面では深掘りを減らし、実運用でTTFT（最初の出力までの時間）とE2Eレビュー時間を短縮しています。総じて、レビューは速くなり、フィードバックは早く、ヒューマン・イン・ザ・ループの流れが改善されます。\nCodeRabbitユーザーが期待できること\n\nCodex導入後、AIコードレビューはどう変わるでしょうか？\n生のバグ検出力は同等\n刷新したHard 25で、CodexのEPレベルPASSは64%、GPT-5は60%（以前のPRセットではGPT-5が77.3%）。GPT-5がもたらした重要な勝ち筋を失っていません。\nコメントは少なく、しかし強く\n総コメント数はGPT-5比で約32%減、SNR（コメントあたり精度）は約35%向上。文章よりパッチが増えます。\n重大度タグでレビューに集中\n新しい重大度タグにより、Critical/Majorがトップに浮上。リファクタはゲート制御、ニットピックはオプトイン。コメントの走査に費やす時間が減り、修正に時間を割けます。\nフィードバックループの高速化\nCodexの軽量な推論とパイプライン改善で、最初の有益なコメントまでの時間が短縮。体感で分かります。\n定量的付録（データ好きのあなたへ）\n以下は興味深かった追加統計を紹介します。\nコメントあたり精度（SNR）の向上: Codex 46.3% 対 GPT-5 34.2% — 相対で約+35%。\nコメント量の差: Codex 54 対 GPT-5 79 — 約32%減、EPのPASSは実質同等（16 対 15）。\nスタイル: Codexは94%のコメントでdiffを含み、このセットではClaudeやGPT-5より曖昧表現が少ない。\n実環境での受け入れ: GPT-5ロールアウト中は受け入れ率が大きく低下。Codexと製品変更の併用で約20–25%相対上昇し、導入前水準に回復。かつ、GPT-5導入前より受け入れコメント数は多いまま。\nCodexがまだ弱い点（と取り組み）\n改善は大きいものの、課題が残っていないわけではありません。現在、以下に取り組んでいます。\nカバレッジの穴\n  モデルがPRにコメントを残さない場合、そのEPはハードFAIL。Codexの探索ヒューリスティクスを広げ、特定クラスの問題を見落としにくくします。\nリファクタ過剰提案（調整済みだが未完）\n  「本質的なもののみ」のゲートでノイズは抑制しましたが、特に大規模diffでコメント過多になりがちなケースの閾値をさらに引き締めます。\nユーザー主導の優先度付け\n  GitHubのインライン順序は変更できませんが、各コメントに重大度を注記し、上から順にトリアージしやすくします。\nCodex GPT-5: バグ捕捉力はそのまま、副作用は少なく\n私たちの指標はシンプルです: 重要なバグを、素早く、ノイズに埋もれさせずに捕まえること。Codexはその実現を助けてくれます。GPT-5の噛み応えある推論力を保ちながら、SNRを回復させ、レイテンシを大幅に削りました。今後も測定・改善を継続し、より良い製品をリリースし続けます。",
      "publishedAt": "2025-10-01T13:09:23.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.983Z",
      "score": 0.1567486318665634
    },
    {
      "id": "5a37a06bed9990e5849019396918b701",
      "title": "CodeRabbitのMCP連携 = コンテキストとコードレビュー",
      "url": "https://coderabbit.ai/blog/coderabbits-mcp-server-integration-code-reviews-that-see-the-whole-picture-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/coderabbits-mcp-server-integration-code-reviews-that-see-the-whole-picture\">CodeRabbit MCP server integration: Code reviews with more context</a>の意訳です。</p>\n<p>すべての開発チームは、孤立した状態で行うコードレビューのつらさを知っています。AIツール（あるいはチームメイト）であっても、文法やスタイル、パターンにコメントはできます。しかしビジネス要件、デプロイ依存関係、組織的な知識がなければ、全体像の半分を推測に基づいている状態です。</p>\n<p>CodeRabbitは現在、Linear、Jira、Circle CIといったいくつかのネイティブ統合を提供しており、これらのツールがコードレビューにもたらす価値を確認してきました。だからこそ今回、<strong>CodeRabbitのMCPサーバー統合のGAリリース</strong> を発表できることがとても嬉しいです。これにより、さらに多くのコンテキストをレビューに取り込めるようになります。</p>\n<p>本リリースで、CodeRabbitはConfluenceにあるビジネス要件からCI/CDパイプラインのシステム依存関係、さらには社内MCPサーバーのデータまで、開発エコシステム全体からコンテキストをオーケストレーションできる初のAIコードレビュープラットフォームとなりました。つまり、コードが「何を達成しようとしているのか」を本当に理解するレビューが可能になるのです。</p>\n<p><a target=\"_blank\" href=\"https://app.coderabbit.ai/login?free-trial\"><strong>14日間の無料トライアルを開始 →</strong></a> <em>約10分で、チーム標準に基づいたコンテキスト対応のレビューを実現。</em></p>\n<h2 id=\"heading-aimcp\"><strong>なぜAIコードレビューにMCPが必要なのか？</strong></h2>\n<p>開発チームは数多くのツールを使って作業しています。</p>\n<ul>\n<li><p>要件はLinearにある</p>\n</li>\n<li><p>設計仕様はFigmaにある</p>\n</li>\n<li><p>アーキテクチャの決定はConfluenceに記録される</p>\n</li>\n<li><p>セキュリティ基準は監査ごとに社内Wikiで更新される</p>\n</li>\n</ul>\n<p>AIコードレビューツールは基本的なコンテキスト、つまりコードベース、コーディング規約、いくつかの統合から始めます。構文を解析し、パターンを確認し、改善を提案します。しかし「そのコードがチームにとって本当に機能するかどうか」を左右するコンテキストは欠けています。</p>\n<p>MCPクライアントとしてのCodeRabbitは、組織コンテキストのコンパイラの役割を果たします。Wiki、チケット、デプロイパターンといった高レベルの入力を正確で実用的なコードレビューインサイトへと変換します。冗長な統合や脆いハックに頼ることなく、MCPはCodeRabbitのようなクライアントがLinearチケット、Confluenceドキュメント、Datadogメトリクス、Slackのディスカッションといった場所から必要なデータだけを取り込めるようにします。</p>\n<h2 id=\"heading-5a6f6zqb44gr44gv44gp44gg5yuv44gp44gu44gl4ocm\">実際にはどう動くのか…</h2>\n<p>CodeRabbitはレビューを開始する前に接続済みMCPサーバーを検索します。たとえばデータベーススキーマの変更はデータアーキテクチャ文書と照合され、APIエンドポイントの実装は社内Wikiに記録されたサービス設計パターンと突き合わせられます。</p>\n<p><strong>例: CodeRabbitによるコード整合性の確認</strong></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758126014841/53d19c71-2051-410c-a109-1e056cc0094d.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-44gp44gu44oe44o844or44gl44kj44gn44kc6yen6kab44gq44kz44oz44og44kt44k544oi44ks5yw44kk6l6844ka\">どのツールからでも重要なコンテキストを取り込む</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125357433/fa378ff4-743d-4841-a3fa-e933bcf4fe7c.png\" alt class=\"image--center mx-auto\" /></p>\n<p>従来のコードレビューツールは特定の統合を前提としています。CodeRabbitのMCP統合は、MCPサーバーを持つあらゆるシステムで動作します。独自の社内ツール、ニッチなSaaSプラットフォーム、カスタムドキュメントシステム。MCPサーバーがあれば、CodeRabbitはどこにでも接続できます。</p>\n<p>CodeRabbitをMCPクライアントとして利用すると、3種類の異なるコンテキストからレビューの深みを得られます。</p>\n<h3 id=\"heading-kirmiodoozpnmotjgrpjg7pjg4bjgq3jgrnjg4gqkg\"><strong>技術的コンテキスト</strong></h3>\n<ul>\n<li><p>依存関係、パフォーマンスデータ、静的解析、テストカバレッジなど</p>\n</li>\n<li><p><strong>ネイティブ統合:</strong> GitHub Actions、GitLab CI、Bitbucket Pipelines</p>\n</li>\n<li><p><strong>MCPサーバー:</strong> Datadog、New Relic、SonarQube、Snyk、Grafana</p>\n</li>\n</ul>\n<p>レビューコメント例は以下の通りです。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125833426/facf17b5-32c7-47ce-b6cd-8170f9837bf6.png\" alt class=\"image--center mx-auto\" /></p>\n<h3 id=\"heading-kirjg5pjgrjjg43jgrnjgrpjg7pjg4bjgq3jgrnjg4gqkg\"><strong>ビジネスコンテキスト</strong></h3>\n<ul>\n<li><p>要件、ユーザーストーリー、受け入れ基準など</p>\n</li>\n<li><p><strong>ネイティブ統合:</strong> Linear、Jira、GitHub Issues、GitLab Issues</p>\n</li>\n<li><p><strong>MCPサーバー:</strong> Confluence、Notion</p>\n</li>\n</ul>\n<p>レビューコメント例は以下の通りです。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125848608/26c21700-1f57-42af-9993-cc372c629e6f.png\" alt class=\"image--center mx-auto\" /></p>\n<h3 id=\"heading-kirntytnuztnmotjgrpjg7pjg4bjgq3jgrnjg4gqkg\"><strong>組織的コンテキスト</strong></h3>\n<ul>\n<li><p>過去の意思決定、慣例、会議メモ、組織的知識など</p>\n</li>\n<li><p><strong>ネイティブ統合:</strong> PR履歴、チーム慣習</p>\n</li>\n<li><p><strong>MCPサーバー:</strong> Slack、Microsoft Teams、Stack Overflow for Teams、PagerDuty</p>\n</li>\n</ul>\n<p>レビューコメント例は以下の通りです。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125858492/5bc5b006-b61c-4b9c-9642-5246978db056.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-mcp\"><strong>MCP統合を始めるには</strong></h2>\n<p>CodeRabbitのMCPクライアントは最小限の設定で導入できます。ほとんどの開発チームは10分以内に最初のMCPサーバーを接続できます。</p>\n<p><strong>MCPサーバー対応の人気開発ツール:</strong></p>\n<ul>\n<li><p><strong>Linear</strong>（ネイティブMCPサポート、5分）</p>\n</li>\n<li><p><strong>Notion</strong>（MCPサーバーあり、10分）</p>\n</li>\n<li><p><strong>Confluence</strong>（コミュニティ製MCPサーバー、15分）</p>\n</li>\n<li><p><strong>Figma</strong>（MCPプラグインあり、10分）</p>\n</li>\n</ul>\n<p>コード変更がどの開発システムを参照すべきかを定義します。データベース変更はアーキテクチャ文書を、認証変更はセキュリティ文書を確認する、という具合です。</p>\n<p>MCPサーバーを追加するのは簡単です。</p>\n<ol>\n<li><p>CodeRabbitダッシュボードで「integrations」に進み、必要ならMCP Serversタブに切り替えます  </p>\n</li>\n<li><p>あらかじめ用意されたMCPサーバーオプションをクリックするか、「New MCP Server」ボタンから他のMCPサーバーを追加できます  </p>\n</li>\n<li><p>リストにないMCPサーバーについては、必要な認証情報を入力します</p>\n</li>\n<li><p>MCP情報をどのように利用するかの使用ガイダンスを確認します  </p>\n</li>\n<li><p>接続が完了すると、利用可能な呼び出し一覧が表示され、カーソルを合わせると詳細を確認できます  </p>\n</li>\n<li><p>各呼び出しをクリックしてアクセスを有効化/無効化することも可能です  </p>\n</li>\n</ol>\n<h2 id=\"heading-kirjgyljgonjgobjgovjgrpjg7pjg4bjgq3jgrnjg4jjgpllj5bjgorovrzjgodjg6zjg5pjg6xjg7zln7rnm6qqkg\"><strong>あらゆるコンテキストを取り込むレビュー基盤</strong></h2>\n<p>CodeRabbitは50以上の統合に標準対応しています。MCPを利用すれば、カスタムサーバーや社内ツールにも拡張できます。まずはLinear、Confluence、Datadog、Slackといった既存システムから始め、必要に応じて追加していけます。</p>\n<h3 id=\"heading-kirmrkhjga7jgrnjg4bjg4pjg5c6kio\"><strong>次のステップ:</strong></h3>\n<ol>\n<li><p><a target=\"_blank\" href=\"https://app.coderabbit.ai/login?free-trial\"><strong>14日間の無料トライアルを開始</strong></a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://app.coderabbit.ai/integrations\"><strong>MCPサーバーディレクトリを表示</strong></a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://docs.coderabbit.ai/context-enrichment/mcp-server-integrations\"><strong>MCPドキュメントを見る</strong></a></p>\n</li>\n</ol>\n",
      "summary": "CodeRabbit MCP server integration: Code reviews with more contextの意訳です。\nすべての開発チームは、孤立した状態で行うコードレビューのつらさを知っています。AIツール（あるいはチームメイト）であっても、文法やスタイル、パターンにコメントはできます。しかしビジネス要件、デプロイ依存関係、組織的な知識がなければ、全体像の半分を推測に基づいている状態です。\nCodeRabbitは現在、Linear、Jira、Circle CIといったいくつかのネイティブ統合を提供しており、これらのツールがコードレビューにもたらす価値を確認してきました。だからこそ今回、CodeRabbitのMCPサーバー統合のGAリリース を発表できることがとても嬉しいです。これにより、さらに多くのコンテキストをレビューに取り込めるようになります。\n本リリースで、CodeRabbitはConfluenceにあるビジネス要件からCI/CDパイプラインのシステム依存関係、さらには社内MCPサーバーのデータまで、開発エコシステム全体からコンテキストをオーケストレーションできる初のAIコードレビュープラットフォームとなりました。つまり、コードが「何を達成しようとしているのか」を本当に理解するレビューが可能になるのです。\n14日間の無料トライアルを開始 → 約10分で、チーム標準に基づいたコンテキスト対応のレビューを実現。\nなぜAIコードレビューにMCPが必要なのか？\n開発チームは数多くのツールを使って作業しています。\n要件はLinearにある\n設計仕様はFigmaにある\nアーキテクチャの決定はConfluenceに記録される\nセキュリティ基準は監査ごとに社内Wikiで更新される\nAIコードレビューツールは基本的なコンテキスト、つまりコードベース、コーディング規約、いくつかの統合から始めます。構文を解析し、パターンを確認し、改善を提案します。しかし「そのコードがチームにとって本当に機能するかどうか」を左右するコンテキストは欠けています。\nMCPクライアントとしてのCodeRabbitは、組織コンテキストのコンパイラの役割を果たします。Wiki、チケット、デプロイパターンといった高レベルの入力を正確で実用的なコードレビューインサイトへと変換します。冗長な統合や脆いハックに頼ることなく、MCPはCodeRabbitのようなクライアントがLinearチケット、Confluenceドキュメント、Datadogメトリクス、Slackのディスカッションといった場所から必要なデータだけを取り込めるようにします。\n実際にはどう動くのか…\nCodeRabbitはレビューを開始する前に接続済みMCPサーバーを検索します。たとえばデータベーススキーマの変更はデータアーキテクチャ文書と照合され、APIエンドポイントの実装は社内Wikiに記録されたサービス設計パターンと突き合わせられます。\n例: CodeRabbitによるコード整合性の確認\n\nどのツールからでも重要なコンテキストを取り込む\n\n従来のコードレビューツールは特定の統合を前提としています。CodeRabbitのMCP統合は、MCPサーバーを持つあらゆるシステムで動作します。独自の社内ツール、ニッチなSaaSプラットフォーム、カスタムドキュメントシステム。MCPサーバーがあれば、CodeRabbitはどこにでも接続できます。\nCodeRabbitをMCPクライアントとして利用すると、3種類の異なるコンテキストからレビューの深みを得られます。\n技術的コンテキスト\n依存関係、パフォーマンスデータ、静的解析、テストカバレッジなど\nネイティブ統合: GitHub Actions、GitLab CI、Bitbucket Pipelines\nMCPサーバー: Datadog、New Relic、SonarQube、Snyk、Grafana\nレビューコメント例は以下の通りです。\n\nビジネスコンテキスト\n要件、ユーザーストーリー、受け入れ基準など\nネイティブ統合: Linear、Jira、GitHub Issues、GitLab Issues\nMCPサーバー: Confluence、Notion\nレビューコメント例は以下の通りです。\n\n組織的コンテキスト\n過去の意思決定、慣例、会議メモ、組織的知識など\nネイティブ統合: PR履歴、チーム慣習\nMCPサーバー: Slack、Microsoft Teams、Stack Overflow for Teams、PagerDuty\nレビューコメント例は以下の通りです。\n\nMCP統合を始めるには\nCodeRabbitのMCPクライアントは最小限の設定で導入できます。ほとんどの開発チームは10分以内に最初のMCPサーバーを接続できます。\nMCPサーバー対応の人気開発ツール:\nLinear（ネイティブMCPサポート、5分）\nNotion（MCPサーバーあり、10分）\nConfluence（コミュニティ製MCPサーバー、15分）\nFigma（MCPプラグインあり、10分）\nコード変更がどの開発システムを参照すべきかを定義します。データベース変更はアーキテクチャ文書を、認証変更はセキュリティ文書を確認する、という具合です。\nMCPサーバーを追加するのは簡単です。\nCodeRabbitダッシュボードで「integrations」に進み、必要ならMCP Serversタブに切り替えます  \nあらかじめ用意されたMCPサーバーオプションをクリックするか、「New MCP Server」ボタンから他のMCPサーバーを追加できます  \nリストにないMCPサーバーについては、必要な認証情報を入力します\nMCP情報をどのように利用するかの使用ガイダンスを確認します  \n接続が完了すると、利用可能な呼び出し一覧が表示され、カーソルを合わせると詳細を確認できます  \n各呼び出しをクリックしてアクセスを有効化/無効化することも可能です  \nあらゆるコンテキストを取り込むレビュー基盤\nCodeRabbitは50以上の統合に標準対応しています。MCPを利用すれば、カスタムサーバーや社内ツールにも拡張できます。まずはLinear、Confluence、Datadog、Slackといった既存システムから始め、必要に応じて追加していけます。\n次のステップ:\n14日間の無料トライアルを開始\nMCPサーバーディレクトリを表示\nMCPドキュメントを見る",
      "publishedAt": "2025-10-01T12:45:00.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.983Z",
      "score": 0.31311832090551
    },
    {
      "id": "6f91c11c2f83634699465b2d81cf8f8a",
      "title": "GPT-5 Codex: How it solves for GPT-5's drawbacks",
      "url": "https://coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks",
      "content": "<p>CodeRabbit’s code reviews help developers fix bugs and ship code. We recently wrote about <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">benchmarking GPT-5</a> and opined that the model was a generational leap in reasoning for our use case of AI code reviews. As we rolled out to our wider user base, we observed that the signal to noise ratio (SNR) dipped, and users felt the reviews were too pedantic.</p>\n<p>The release of GPT‑5 Codex, plus the product changes we made (severity tagging, stricter refactor gating, better filtering), brings our signal to noise ratio back without sacrificing the ability to find the hard bugs.</p>\n<p>On our refreshed hard 25 PR set, GPT-5 Codex delivers about 35% higher per comment precision than GPT‑5, maintains essentially the same error pattern- level bug coverage, and cuts roughly a third of the comment volume. Combine that with the lower latency of the GPT-5 Codex model and the experience feels snappier and more focused.</p>\n<h2 id=\"heading-what-we-measured-and-why\"><strong>What we measured (and why)</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759249757952/184a4486-2f03-4293-8b39-82ca88ed9a32.png\" alt class=\"image--center mx-auto\" /></p>\n<p>When testing GPT-5 Codex, we ran a fresh “hard 25” suite of OSS PRs (slightly tougher than <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">the previous post</a>). These are 25 of the most difficult pull requests from our dataset. These PRs represent real-world bugs that span:</p>\n<ul>\n<li><p>Concurrency issues (e.g. TOCTOU races, incorrect synchronization)</p>\n</li>\n<li><p>Object-oriented design flaws (e.g. virtual call pitfalls, refcount memory model violations)</p>\n</li>\n<li><p>Performance hazards (e.g. runaway cache growth, tight loop stalls)</p>\n</li>\n<li><p>Language-specific footguns (e.g. TypeScript misuses, C++ memory order subtleties)</p>\n</li>\n</ul>\n<p>We evaluated the following models: :</p>\n<ul>\n<li><p><strong>GPT‑5 Codex</strong></p>\n</li>\n<li><p><strong>GPT‑5</strong></p>\n</li>\n<li><p><strong>Claude</strong> (Sonnet 4 and Opus‑4.1)</p>\n</li>\n</ul>\n<h2 id=\"heading-what-we-looked-for\">What we looked for</h2>\n<p>We gave each of the models a score based on how they performed on these factors:</p>\n<ul>\n<li><p><strong>EP (Error Pattern).</strong> The specific underlying defect seeded in a PR (e.g., lost wakeup on a condition variable, inconsistent lock order, logic bug hidden in boolean soup).</p>\n</li>\n<li><p><strong>EP PASS/FAIL (per PR).</strong> PASS if the model left at least one comment that directly fixes or credibly surfaces that PR’s EP. If it left no comment on that PR, it is counted as FAIL for that PR.</p>\n</li>\n<li><p><strong>Comment PASS/FAIL (per comment).</strong> PASS if the comment directly fixes or credibly surfaces the EP, otherwise FAIL.</p>\n</li>\n<li><p><strong>Per comment precision.</strong> PASS comments ÷ all comments. This is our operational SNR for this dataset.</p>\n</li>\n<li><p><strong>Important share.</strong> Every PASS is Important. Comments that do not solve the EP but still flag a genuine critical or major bug (like a use after free, double free, lost wakeup, memory leak, null deref, path traversal, catastrophic regex) are also Important. Everything else is Minor.</p>\n</li>\n</ul>\n<h2 id=\"heading-scoreboard-codex-improves-signal-to-noise\"><strong>Scoreboard - Codex improves signal-to-noise</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759197629508/de48651d-069a-4530-b070-de6107b57583.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>Takeaway:</strong> Codex finds essentially the same EPs as GPT‑5 but does it with fewer, tighter comments, so the signal-to-noise ratio is improved.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759238036950/7c99e827-daba-4205-8f70-0d5a9413b74b.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>What this means:</strong> Codex covered 20 of the 25 PRs (the other 5 count as uncovered fails). Despite fewer comments overall, Codex passed slightly more EPs (16 vs. 15) and landed far more Important comments. Over half its comments are either direct hits on the issue we were representing in that PR or flag other  EP critical bugs. GPT‑5 and Claude trailed in precision and Importance share at about 40%.</p>\n<p><strong>The verdict: Same EP coverage, less noise:</strong> Codex retains GPT-5’s bug finding power but trims the chatter with about 32% fewer comments than GPT‑5 (54 vs. 79) and about 35% higher per comment precision (46.3% vs. 34.2%). Claude looks similar to GPT‑5 on coverage but is chattier, with lower precision.</p>\n<h2 id=\"heading-style-and-structure-why-codex-reads-like-a-patch\"><strong>Style and structure (why Codex reads like a patch)</strong></h2>\n<p>Codex replies are consistently action forward (diffs almost always included) and low hedge. That lines up with what reviewers want: suggestions that translate directly into a patch.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759237978515/bf5eab59-e642-486d-bbb7-8adfd02fc00e.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-the-kinds-of-bugs-codex-is-good-at\"><strong>The kinds of bugs Codex is good at</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759197684585/6b90b664-dbf0-46d0-ae5e-cf988083ab8b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Across the suite, all models did well on concurrency and synchronization, but Codex stood out for:</p>\n<ol>\n<li><p><strong>Condition variable misuse and lost wakeups.</strong> Codex proposes the canonical patterns (wait under lock, check predicate in a loop) and supplies concrete diffs.</p>\n</li>\n<li><p><strong>Lock ordering and deadlocks.</strong> It calls out inconsistent acquisition order and suggests a lock hierarchy or moving work outside critical sections, again with actionable edits.</p>\n</li>\n<li><p><strong>Subtle API and performance traps.</strong> Examples include catastrophic regex backtracking and memory model orderings. Codex pinpoints and patches them cleanly.</p>\n</li>\n</ol>\n<h2 id=\"heading-why-gpt5-felt-noisier-and-how-we-fixed-that\"><strong>Why GPT‑5 felt noisier, and how we fixed that</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759251737131/d79ed373-5213-4cd0-a168-64441227b7b9.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>What we saw:</strong> When we moved from Sonnet and Opus to GPT‑5 our total comments per review nearly doubled. Even though hallucinations fell to under 1% and negative tone fell to under 1%, the acceptance rate (share of comments judged helpful) declined significantly relative to its baseline prior to the adoption of GPT-5.</p>\n<p><strong>What changed with Codex:</strong> With GPT‑5 Codex plus some product changes we’ve implemented, our acceptance climbed back to prior levels while overall comment volume stayed higher than the pre-GPT‑5 era. Put simply: our tool is now back to its prior helpfulness level, while still finding as many real issues as GPT-5.</p>\n<p>Two product levers helped with this:</p>\n<ol>\n<li><p><strong>We created severity and review type tags, front and center</strong></p>\n<ul>\n<li><p><strong>Review Types:</strong> We created review types to allow users to self-select what kinds of comments they wanted to read including: ⚠️ Potential issue, 🛠️ Refactor suggestion, 🧹 Nitpick (nitpicks are hidden unless you opt into <em>Assertive</em> mode)</p>\n</li>\n<li><p><strong>Severity:</strong> We now tag comments by severity to better signal which matter more than others. Our tags are: 🔴 Critical, 🟠 Major, 🟡 Minor, 🔵 Trivial, ⚪ Info</p>\n</li>\n<li><p>We always show bugs (Critical, Major, Minor) but don’t always show other types of comments. Refactors show only if the model marks them as essential. Users who want everything can still switch to <em>Assertive</em> mode.</p>\n</li>\n</ul>\n</li>\n<li><p><strong>We implemented stricter filtering and aggregation</strong></p>\n<ul>\n<li>We collapse duplicative notes and filter out “nice to have” suggestions unless they have clear ROI for the user. The result: fewer, denser comments, and fewer reasons to tune out.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"heading-latency-fast-matters-amp-codex-is-faster\"><strong>Latency: Fast matters &amp; Codex is faster</strong></h2>\n<p>A five minute review is fine. Thirty minutes is not. GPT‑5’s “always think hard” style significantly increased time to first token and overall review time. But we shipped several pipeline optimizations recently and Codex helps further reduce the latency that GPT-5 introduced.</p>\n<p>Codex’s variable or elastic thinking uses less depth when it is not needed, improving time to first output and end-to-end review time in practice. Net: faster reviews, earlier feedback, better flow for the human in the loop.</p>\n<h2 id=\"heading-what-a-coderabbit-user-should-expect\"><strong>What a CodeRabbit user should expect</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759197710045/2f32eef6-3227-450f-ac4b-485f89126197.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Now that Codex is implemented, how will that change your AI code reviews?</p>\n<ol>\n<li><p><strong>The same raw bug finding power</strong></p>\n<ul>\n<li>On the refreshed hard 25, Codex passed 64% at the EP level vs. 60% for GPT‑5 (our <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">previous set of PRs</a> had GPT-5 passing 77.3%). No loss of the important wins GPT-5 helped with.</li>\n</ul>\n</li>\n<li><p><strong>Fewer but stronger comments</strong></p>\n<ul>\n<li>About 32% fewer total comments than GPT‑5, with about 35% higher SNR (per comment precision). More patches, less prose.</li>\n</ul>\n</li>\n<li><p><strong>Severity tags to focus your review</strong></p>\n<ul>\n<li>Critical and Major issues float to the top with our new severity tags. Refactors are gated. Nitpicks are opt-in. You will spend less time scanning comments and more time fixing.</li>\n</ul>\n</li>\n<li><p><strong>A faster feedback loop</strong></p>\n<ul>\n<li>Codex’s leaner reasoning plus pipeline improvements bring time to first helpful comment  down. You will feel it.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"heading-quantitative-appendix-for-the-curious\"><strong>Quantitative appendix (for the curious)</strong></h2>\n<p>We know you love data! Here’s some other stats we found interesting:</p>\n<ul>\n<li><p><strong>Per comment precision (SNR) uplift:</strong> Codex 46.3% vs. GPT‑5 34.2% — about +35% relative.</p>\n</li>\n<li><p><strong>Comment volume delta:</strong> Codex 54 vs. GPT‑5 79 — 32% fewer comments, with EP passes essentially unchanged (16 vs. 15).</p>\n</li>\n<li><p><strong>Style:</strong> Codex includes diffs in 94% of comments and uses hedging far less than Claude and GPT‑5 on this set.</p>\n</li>\n<li><p><strong>Acceptance (real world):</strong> During GPT‑5 rollout, acceptance dropped significantly. With Codex plus changes, it rose by about 20–25% relative and returned to prior levels while still delivering more accepted comments than pre GPT‑5.</p>\n</li>\n</ul>\n<h2 id=\"heading-where-codex-still-needs-work-and-what-we-are-doing\"><strong>Where Codex still needs work (and what we are doing)</strong></h2>\n<p>These improvements are great but that doesn’t mean that there aren’t still issues with Codex. Here are some that we are actively working on:</p>\n<ul>\n<li><p><strong>Coverage gaps.</strong> When a model leaves no comment on a PR, that is a hard fail for that EP. We are widening Codex’s search heuristics so it is less likely to miss entire classes of issues.</p>\n</li>\n<li><p><strong>Refactor over eagerness (tuned, not solved).</strong> The “essential only” gate curbs refactor noise, but we will keep tightening the threshold, especially on large diffs where a high number of comments would be overwhelming.</p>\n</li>\n<li><p><strong>User driven prioritization.</strong> We cannot change GitHub’s in-line ordering, but we annotate every comment with severity so you can triage from the top down without hunting.</p>\n</li>\n</ul>\n<h2 id=\"heading-codex-gpt-5-all-of-the-great-bug-catching-ability-fewer-downsides\"><strong>Codex GPT-5: All of the great bug catching ability, fewer downsides</strong></h2>\n<p>Our north star is simple: <strong>catch the bugs that matter, quickly, without making you sift through noise</strong>. Codex helps us do that. It keeps the bite of GPT‑5’s reasoning while restoring SNR and shaving latency down significantly. We will keep measuring, improving, and shipping a better product every release.</p>\n",
      "summary": "CodeRabbit’s code reviews help developers fix bugs and ship code. We recently wrote about benchmarking GPT-5 and opined that the model was a generational leap in reasoning for our use case of AI code reviews. As we rolled out to our wider user base, we observed that the signal to noise ratio (SNR) dipped, and users felt the reviews were too pedantic.\nThe release of GPT‑5 Codex, plus the product changes we made (severity tagging, stricter refactor gating, better filtering), brings our signal to noise ratio back without sacrificing the ability to find the hard bugs.\nOn our refreshed hard 25 PR set, GPT-5 Codex delivers about 35% higher per comment precision than GPT‑5, maintains essentially the same error pattern- level bug coverage, and cuts roughly a third of the comment volume. Combine that with the lower latency of the GPT-5 Codex model and the experience feels snappier and more focused.\nWhat we measured (and why)\n\nWhen testing GPT-5 Codex, we ran a fresh “hard 25” suite of OSS PRs (slightly tougher than the previous post). These are 25 of the most difficult pull requests from our dataset. These PRs represent real-world bugs that span:\nConcurrency issues (e.g. TOCTOU races, incorrect synchronization)\nObject-oriented design flaws (e.g. virtual call pitfalls, refcount memory model violations)\nPerformance hazards (e.g. runaway cache growth, tight loop stalls)\nLanguage-specific footguns (e.g. TypeScript misuses, C++ memory order subtleties)\nWe evaluated the following models: :\nGPT‑5 Codex\nGPT‑5\nClaude (Sonnet 4 and Opus‑4.1)\nWhat we looked for\nWe gave each of the models a score based on how they performed on these factors:\nEP (Error Pattern). The specific underlying defect seeded in a PR (e.g., lost wakeup on a condition variable, inconsistent lock order, logic bug hidden in boolean soup).\nEP PASS/FAIL (per PR). PASS if the model left at least one comment that directly fixes or credibly surfaces that PR’s EP. If it left no comment on that PR, it is counted as FAIL for that PR.\nComment PASS/FAIL (per comment). PASS if the comment directly fixes or credibly surfaces the EP, otherwise FAIL.\nPer comment precision. PASS comments ÷ all comments. This is our operational SNR for this dataset.\nImportant share. Every PASS is Important. Comments that do not solve the EP but still flag a genuine critical or major bug (like a use after free, double free, lost wakeup, memory leak, null deref, path traversal, catastrophic regex) are also Important. Everything else is Minor.\nScoreboard - Codex improves signal-to-noise\n\nTakeaway: Codex finds essentially the same EPs as GPT‑5 but does it with fewer, tighter comments, so the signal-to-noise ratio is improved.\n\nWhat this means: Codex covered 20 of the 25 PRs (the other 5 count as uncovered fails). Despite fewer comments overall, Codex passed slightly more EPs (16 vs. 15) and landed far more Important comments. Over half its comments are either direct hits on the issue we were representing in that PR or flag other  EP critical bugs. GPT‑5 and Claude trailed in precision and Importance share at about 40%.\nThe verdict: Same EP coverage, less noise: Codex retains GPT-5’s bug finding power but trims the chatter with about 32% fewer comments than GPT‑5 (54 vs. 79) and about 35% higher per comment precision (46.3% vs. 34.2%). Claude looks similar to GPT‑5 on coverage but is chattier, with lower precision.\nStyle and structure (why Codex reads like a patch)\nCodex replies are consistently action forward (diffs almost always included) and low hedge. That lines up with what reviewers want: suggestions that translate directly into a patch.\n\nThe kinds of bugs Codex is good at\n\nAcross the suite, all models did well on concurrency and synchronization, but Codex stood out for:\nCondition variable misuse and lost wakeups. Codex proposes the canonical patterns (wait under lock, check predicate in a loop) and supplies concrete diffs.\nLock ordering and deadlocks. It calls out inconsistent acquisition order and suggests a lock hierarchy or moving work outside critical sections, again with actionable edits.\nSubtle API and performance traps. Examples include catastrophic regex backtracking and memory model orderings. Codex pinpoints and patches them cleanly.\nWhy GPT‑5 felt noisier, and how we fixed that\n\nWhat we saw: When we moved from Sonnet and Opus to GPT‑5 our total comments per review nearly doubled. Even though hallucinations fell to under 1% and negative tone fell to under 1%, the acceptance rate (share of comments judged helpful) declined significantly relative to its baseline prior to the adoption of GPT-5.\nWhat changed with Codex: With GPT‑5 Codex plus some product changes we’ve implemented, our acceptance climbed back to prior levels while overall comment volume stayed higher than the pre-GPT‑5 era. Put simply: our tool is now back to its prior helpfulness level, while still finding as many real issues as GPT-5.\nTwo product levers helped with this:\nWe created severity and review type tags, front and center\nReview Types: We created review types to allow users to self-select what kinds of comments they wanted to read including: ⚠️ Potential issue, 🛠️ Refactor suggestion, 🧹 Nitpick (nitpicks are hidden unless you opt into Assertive mode)\nSeverity: We now tag comments by severity to better signal which matter more than others. Our tags are: 🔴 Critical, 🟠 Major, 🟡 Minor, 🔵 Trivial, ⚪ Info\nWe always show bugs (Critical, Major, Minor) but don’t always show other types of comments. Refactors show only if the model marks them as essential. Users who want everything can still switch to Assertive mode.\nWe implemented stricter filtering and aggregation\nWe collapse duplicative notes and filter out “nice to have” suggestions unless they have clear ROI for the user. The result: fewer, denser comments, and fewer reasons to tune out.\nLatency: Fast matters & Codex is faster\nA five minute review is fine. Thirty minutes is not. GPT‑5’s “always think hard” style significantly increased time to first token and overall review time. But we shipped several pipeline optimizations recently and Codex helps further reduce the latency that GPT-5 introduced.\nCodex’s variable or elastic thinking uses less depth when it is not needed, improving time to first output and end-to-end review time in practice. Net: faster reviews, earlier feedback, better flow for the human in the loop.\nWhat a CodeRabbit user should expect\n\nNow that Codex is implemented, how will that change your AI code reviews?\nThe same raw bug finding power\nOn the refreshed hard 25, Codex passed 64% at the EP level vs. 60% for GPT‑5 (our previous set of PRs had GPT-5 passing 77.3%). No loss of the important wins GPT-5 helped with.\nFewer but stronger comments\nAbout 32% fewer total comments than GPT‑5, with about 35% higher SNR (per comment precision). More patches, less prose.\nSeverity tags to focus your review\nCritical and Major issues float to the top with our new severity tags. Refactors are gated. Nitpicks are opt-in. You will spend less time scanning comments and more time fixing.\nA faster feedback loop\nCodex’s leaner reasoning plus pipeline improvements bring time to first helpful comment  down. You will feel it.\nQuantitative appendix (for the curious)\nWe know you love data! Here’s some other stats we found interesting:\nPer comment precision (SNR) uplift: Codex 46.3% vs. GPT‑5 34.2% — about +35% relative.\nComment volume delta: Codex 54 vs. GPT‑5 79 — 32% fewer comments, with EP passes essentially unchanged (16 vs. 15).\nStyle: Codex includes diffs in 94% of comments and uses hedging far less than Claude and GPT‑5 on this set.\nAcceptance (real world): During GPT‑5 rollout, acceptance dropped significantly. With Codex plus changes, it rose by about 20–25% relative and returned to prior levels while still delivering more accepted comments than pre GPT‑5.\nWhere Codex still needs work (and what we are doing)\nThese improvements are great but that doesn’t mean that there aren’t still issues with Codex. Here are some that we are actively working on:\nCoverage gaps. When a model leaves no comment on a PR, that is a hard fail for that EP. We are widening Codex’s search heuristics so it is less likely to miss entire classes of issues.\nRefactor over eagerness (tuned, not solved). The “essential only” gate curbs refactor noise, but we will keep tightening the threshold, especially on large diffs where a high number of comments would be overwhelming.\nUser driven prioritization. We cannot change GitHub’s in-line ordering, but we annotate every comment with severity so you can triage from the top down without hunting.\nCodex GPT-5: All of the great bug catching ability, fewer downsides\nOur north star is simple: catch the bugs that matter, quickly, without making you sift through noise. Codex helps us do that. It keeps the bite of GPT‑5’s reasoning while restoring SNR and shaving latency down significantly. We will keep measuring, improving, and shipping a better product every release.",
      "publishedAt": "2025-09-30T13:27:04.000Z",
      "author": "David Loker",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "funding_mna",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "testing"
      ],
      "ingestedAt": "2025-11-23T17:37:39.983Z",
      "score": 0.27127450391802244
    },
    {
      "id": "1fee642c14d1a3ef024133891cae6f3b",
      "title": "We raised $60 million last week… so we made a funny video",
      "url": "https://coderabbit.ai/blog/we-raised-60-million-last-week-so-we-made-a-funny-video",
      "content": "<p>Last week, we announced CodeRabbit’s <strong>$60 million Series B</strong>. To celebrate, we did what any responsible, developer-focused software company would do: we made a funny video.</p>\n<p>Not with <em>all</em> the money, to be clear. But we did decide to celebrate with something fun, absurd, and painfully relatable for any dev team trying to keep up with the flood of AI-generated PRs.</p>\n<h2 id=\"heading-introducing-when-ai-coding-agents-backfire-a-short-film\"><strong>Introducing… “When AI coding agents backfire: A short film”</strong></h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/glfB3KLQR7E?feature=shared\">https://youtu.be/glfB3KLQR7E?feature=shared</a></div>\n<p> </p>\n<p>It’s a short mockumentary-meets-sitcom about what really happens when “AI velocity” turns into a PR review backlog.</p>\n<ul>\n<li><p>One reviewer.</p>\n</li>\n<li><p>Dozens of notifications.</p>\n</li>\n<li><p>84 open PRs.</p>\n</li>\n<li><p>And one overly eager coworker named Brad who just wants feedback.</p>\n</li>\n</ul>\n<h2 id=\"heading-the-cast\"><strong>The cast</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758059242156/ae3ad14c-f354-4802-95e1-1f7eb67f0c85.png\" alt class=\"image--center mx-auto\" /></p>\n<p>To bring it to life, we pulled in beloved developer educator (and influencer) <a target=\"_blank\" href=\"https://x.com/@aarondfrancis\"><strong>Aaron Francis</strong></a> to star as our beleaguered reviewer. He’s the guy who just wanted to ship features faster and now can’t go to the kitchen (or even leave his house at 8 a.m.) without Brad asking about his PR.</p>\n<p>And speaking of Brad: the inimitable <a target=\"_blank\" href=\"https://www.instagram.com/4ustinvon/?hl=en\"><strong>Austin von Johnson</strong></a> plays him to perfection. Brad’s a developer who can crank out AI-generated PRs at lightning speed but cannot, under any circumstances, <em>wait patiently</em> for a review. His lurking, his post-its, his hoodie PR ambushes… let’s just say he was perfectly committed to the bit.</p>\n<h2 id=\"heading-the-very-real-problem-behind-the-joke\"><strong>The very real problem behind the joke</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758059125767/fc80309a-a591-4b4d-bd9b-6a1b53330edb.png\" alt class=\"image--center mx-auto\" /></p>\n<p>The short film is funny, but the problem it highlights is real:</p>\n<ul>\n<li><p><strong>AI coding tools crank out code faster than teams can review it</strong></p>\n</li>\n<li><p><strong>Review backlogs balloon</strong> while productivity drops</p>\n</li>\n<li><p><strong>Senior engineers get buried</strong> in endless PRs</p>\n</li>\n<li><p>Review quality is uneven, risk goes up, and you have to deal with more issues</p>\n</li>\n<li><p>And suddenly, the promise of velocity feels more like a nightmare.</p>\n</li>\n</ul>\n<h2 id=\"heading-heres-what-were-doing-about-it\"><strong>Here’s what we’re doing about it</strong></h2>\n<p>CodeRabbit exists to <strong>clear the backlog</strong>, not add to it. Our AI code reviews pull in dozens of points of context (requirements, tests, CI, past diffs, ownership) to catch bugs you’d miss, reduce reviewer fatigue, and move PRs through faster—without turning teammates into… Brad. </p>\n<p>Ship faster, review smarter, and keep your sanity. Also, avoid creating a… Brad. </p>\n<p>👉 <a target=\"_blank\" href=\"https://youtu.be/glfB3KLQR7E?feature=shared\">Watch <strong>“When AI coding agents backfire: a short film”</strong> right here.</a> And if you’ve ever been chased around the office about a PR, please, send it to your team’s Brad.</p>\n",
      "summary": "Last week, we announced CodeRabbit’s $60 million Series B. To celebrate, we did what any responsible, developer-focused software company would do: we made a funny video.\nNot with all the money, to be clear. But we did decide to celebrate with something fun, absurd, and painfully relatable for any dev team trying to keep up with the flood of AI-generated PRs.\nIntroducing… “When AI coding agents backfire: A short film”\n\n\n\nhttps://youtu.be/glfB3KLQR7E?feature=shared\n \nIt’s a short mockumentary-meets-sitcom about what really happens when “AI velocity” turns into a PR review backlog.\nOne reviewer.\nDozens of notifications.\n84 open PRs.\nAnd one overly eager coworker named Brad who just wants feedback.\nThe cast\n\nTo bring it to life, we pulled in beloved developer educator (and influencer) Aaron Francis to star as our beleaguered reviewer. He’s the guy who just wanted to ship features faster and now can’t go to the kitchen (or even leave his house at 8 a.m.) without Brad asking about his PR.\nAnd speaking of Brad: the inimitable Austin von Johnson plays him to perfection. Brad’s a developer who can crank out AI-generated PRs at lightning speed but cannot, under any circumstances, wait patiently for a review. His lurking, his post-its, his hoodie PR ambushes… let’s just say he was perfectly committed to the bit.\nThe very real problem behind the joke\n\nThe short film is funny, but the problem it highlights is real:\nAI coding tools crank out code faster than teams can review it\nReview backlogs balloon while productivity drops\nSenior engineers get buried in endless PRs\nReview quality is uneven, risk goes up, and you have to deal with more issues\nAnd suddenly, the promise of velocity feels more like a nightmare.\nHere’s what we’re doing about it\nCodeRabbit exists to clear the backlog, not add to it. Our AI code reviews pull in dozens of points of context (requirements, tests, CI, past diffs, ownership) to catch bugs you’d miss, reduce reviewer fatigue, and move PRs through faster—without turning teammates into… Brad. \nShip faster, review smarter, and keep your sanity. Also, avoid creating a… Brad. \n👉 Watch “When AI coding agents backfire: a short film” right here. And if you’ve ever been chased around the office about a PR, please, send it to your team’s Brad.",
      "publishedAt": "2025-09-23T15:41:30.000Z",
      "author": "Aravind Putrevu",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.984Z",
      "score": 0.2038611086174588
    },
    {
      "id": "d8d6518d6b0d06b7830060371468c60a",
      "title": "Mcp時代におけるコンテキスト肥大化への対応",
      "url": "https://coderabbit.ai/blog/handling-ballooning-context-in-the-mcp-era-context-engineering-on-steroids-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/handling-ballooning-context-in-the-mcp-era-context-engineering-on-steroids\">Ballooning context in the MCP era: Context engineering on steroids</a>意訳です。</p>\n<p>かつて、LLMにコンテキストを渡すには、ハックやベクターストラテジー、そしてお祈り…と、過剰に複雑なRAGパイプラインをつなぎ合わせる必要がありました。そこに登場したのが <strong>Model Context Protocol (MCP)</strong>。外部データを本番環境のモデルに提供するための、クリーンでモジュール的手法です。MCPは、実際に「何かをする」エージェントシステムを構築する人々にとって、瞬く間に標準的なプロトコルとなりました。</p>\n<p>今ではほとんどのテック企業がMCP機能を打ち出していますが、その理由は明白です。MCPはコンテキストのロジックとアプリケーションロジックを分離し、信頼性を向上させ、複雑なワークフローでのプロンプト構築の混乱を抑える役割を果たします。</p>\n<p>私たちはしばらく前からコンテキストエンジニアリングの領域に深く取り組んでおり、今回独自のMCPクライアントを立ち上げるにあたり、コードレビューにより豊かなコンテキストを注入できることに大いに期待しています。しかし正直に言えば、豊富なコンテキストにはリスクも伴います。MCP時代の隠された真実はこうです：<strong>かつて欲していたコンテキストに、今や私たちは溺れそうである</strong>。ログやトレース、diffなど \"関連\"ファイルが増え、モデルが本当に必要としているものが見えにくくなっています。</p>\n<p>役立つ入力はすぐにトークンの膨張、ノイズ、パフォーマンス劣化につながります。引用付きハルシネーション、レイテンシの急上昇、あるいはカフェインを摂りすぎたインターンが書いたような散漫なレビュー。良いコンテキストエンジニアリングとは「すべて詰め込む」ことではなく、「何を省くか」を知ることでもあります。そしてMCP以降、そのバランスを取るのはより難しく、より重要になっています。</p>\n<p>この記事では、<strong>膨張するコンテキスト問題</strong> の詳細、その副作用、そして私たちがそれにどう立ち向かっているかを解説します。MCPを用いたLLM機能を開発している方で、プロンプト形のブラックホールを作り出したくない方に役立つ内容です。</p>\n<h3 id=\"heading-mcp-amp\"><strong>MCPクライアント &amp; サーバーにおける「膨張するコンテキスト問題」</strong></h3>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123932448/7988a3c4-f1be-4033-b8e2-43aee929d878.png\" alt class=\"image--center mx-auto\" /></p>\n<p>MCPサーバーとクライアントは、モデルに膨大な情報を渡すことを容易にします：ログ、トレース、diff、設定、チケット、さらには誰も所有を覚えていないリポジトリの隅まで。すべてがモデルの手の届くところにあります。しかし、ここで重要な問いがあります：「コンテキストが多ければ多いほど良いのか？」 答えは間違いなく「NO」です。</p>\n<p>過剰なコンテキストは、試験勉強で図書館全体を読むようなもの。ノイズは増えても、知識にはなりません。コンテキストが制御されなければ、次の3つの問題がすぐに現れます：</p>\n<ul>\n<li><p><strong>トークンの膨張</strong><br />  LLMには無限のキャパシティはありません。入力ウィンドウにはコストと限界があり、念のため…と詳細情報を詰め込みすぎれば、コストは増大してスループットは低下し、不要なテキストに予算を浪費します。</p>\n</li>\n<li><p><strong>関連性の低下</strong><br />  情報が多いほど出力が良くなるわけではありません。むしろ悪化することが多いのです。無関係または冗長なスニペットがシグナルを希釈し、モデルはインサイトではなく枝葉に追われます。</p>\n</li>\n<li><p><strong>レイテンシ</strong><br />  追加されるログやdiff、スタックトレースはすべて取得・処理され、プロンプトに押し込まれます。コンテキスト構築がボトルネックとなり、レビュー速度を著しく低下させます。</p>\n</li>\n</ul>\n<p>要するに、膨張するコンテキストはMCPの優雅さを逆にリスクへと変えてしまいます。意図的なコンテキストエンジニアリングがなければ、出力を磨くどころか押しつぶしてしまうのです。</p>\n<h2 id=\"heading-44kz44oz44og44kt44k544oi44gm5a6z44gr44gq44kl44go44gn\">コンテキストが害になるとき</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123946588/f8c48c9f-db75-47bc-a349-bdb58b81258b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>実際には、以下の3つの典型的な問題が発生します：</p>\n<ul>\n<li><p><strong>コンテキスト混乱</strong><br />  モデルが無関係な詳細をシグナルと誤解してしまうケース。例えば認証ロジックを更新するPRに、無関係なテストフィクスチャが含まれていると、モデルはフィクスチャをレビューし始め、実際の変更と関係のないコメントを生成します。</p>\n</li>\n<li><p><strong>コンテキスト衝突</strong><br />  コンテキスト同士が矛盾する場合です。例えば最新のスキーママイグレーションと古いドックストリングが同時に含まれると、モデルはどちらを信じるべきか迷い、結果として全方位的で自信のないレビューを生成します。まるで決断できないレビュアーのように。</p>\n</li>\n<li><p><strong>コンテキスト汚染</strong><br />  最も厄介なのは誤った情報が混入するケースです。無用な関連ファイルや、誤ってインデックス化されたスニペットが注入されると、存在しないコードを引用するようになります。レビューでは、存在しないファイルのバグに言及し、開発者を混乱させ、時間を無駄にし、信頼を損ないます。</p>\n</li>\n</ul>\n<p>これはコードレビューに限りません。サポートボットが無関係なチケットを引っ張ってくる場合や、リサーチアシスタントが周辺論文に気を取られる場合、セキュリティエージェントがノイジーなログを証拠として扱う場合なども同様です。いずれにしても、間違ったコンテキストは「ない方がまし」なのです。</p>\n<h2 id=\"heading-mcp\">MCPサーバーでのコンテキスト過負荷を防ぐ主要パターン</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123970317/3dd0ac82-f438-4150-83d8-fae81f7dafa2.png\" alt class=\"image--center mx-auto\" /></p>\n<p>MCP時代の問題が、膨張するコンテキストだとすれば、解決策は情報の流入を止めることではありません。<strong>意図を持って選別・圧縮・提供すること</strong>です。MCPのコンテキストは、生の素材をモデルに渡す前にきちんと設計されたデータ変換プロセスを経るべきものです。私たち自身のコードレビュー用MCPクライアントでも、コンテキストを高シグナル・低ノイズに保つために以下のパターンを採用しています。</p>\n<ul>\n<li><p><strong>コンテキストの重複排除と差分化</strong><br />  冗長な入力はトークン浪費の最短ルートです。同一のスタックトレース、繰り返しのログ、変更されていないdiff部分は10回も登場する必要はありません。クライアントは重複を検出して折りたたみ、新しい部分だけを強調します。この原則は他の領域にも適用可能です：重複するサポートチケットをまとめ、繰り返しのトレースを圧縮し、差分のみを残します。</p>\n</li>\n<li><p><strong>コンテキスト要約パイプライン</strong><br />  MCP出力が依然として大きすぎる場合、LLM自体が要約して小さくすることも可能です。代償は圧縮と忠実度のトレードオフ：要約はニュアンスを失う可能性がありますが、詳細に溺れるよりはましです。実際には、重要ファイルは生のdiff、優先度の低いコンテキストは要約といったハイブリッド設計を採用します。</p>\n</li>\n<li><p><strong>コンテキストの優先順位付けと切り捨て</strong><br />  プルーニングや要約後でも、どれを最初に入れ、後に回し、容量不足時に捨てるかを決める必要があります。MCPクエリごとにトークン予算を設定することは不可欠です。そうしなければプロンプトが予測不能に膨張します。私たちは切り捨てを前提にした設計を試し、場合によっては概要を先頭に、詳細を後半に配置するなど調整しています。</p>\n</li>\n<li><p><strong>コンテキストの隔離</strong><br />  すべてのコンテキストを最初のプロンプトに含める必要はありません。サブタスクごとに専用のコンテキストスレッドを持たせるべきです。例えば、私たちのMCPクライアントではテスト失敗は専用のレビューサブスレッドに置かれ、メインのレビューコンテキストを妨げません。これにより混乱を減らし、長い対話でも明瞭さを保てます。</p>\n</li>\n<li><p><strong>継続的な改善と学習</strong><br />  コンテキストエンジニアリングは静的ではありません。モデルのフィードバックや人間による修正を取り入れ、優先順位を調整していきます。重要なのは可観測性です。モジュールごとにプロンプト入力を記録し、何が通って何が無駄かを把握します。MCPダッシュボードやトークンヒートマップのようなツールが、予算超過や不要な入力を可視化します。</p>\n</li>\n</ul>\n<h2 id=\"heading-mcp-amp-1\"><strong>MCPサーバー &amp; クライアントにおけるアンチパターン</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123985534/b1a3acb1-5c02-4d88-b3d0-883aeb0dd573.png\" alt class=\"image--center mx-auto\" /></p>\n<p>MCP時代はコンテキスト取得を容易にしました。おそらく「容易すぎる」ほどに。以下のようなアンチパターンがよく見られます：</p>\n<ul>\n<li><p><strong>ベクトル無差別投入</strong><br />  ベクトルDBは「関連」情報を見つけるのに優れていますが、それを万能の答えと見なすのは危険です。曖昧に関連するスニペットをすべて投入すると、関係のないファイルへのコメントや古いコードへの指摘で溢れるレビューになります。コンテキストの不適合はトークンの無駄だけでなく、モデルのパフォーマンスを引き下げる要因になります。</p>\n</li>\n<li><p><strong>「全部突っ込め」方式</strong><br />  すべてのログ、diff、ドックストリングをコンテキストに放り込み、あとは神に祈るやり方です。コストの増加、レイテンシの悪化、結果の予測不能を保証します。モデルは重要な部分と不要な部分を区別できないため、全方位的で散漫なレビューを生成します。矛盾が混入すれば、モデルは曖昧さを埋めるために幻覚を引き起こします。</p>\n</li>\n</ul>\n<p>要するに、コンテキストは多ければ良いというものではありません。フィルタリング、優先順位付け、設計がなければ、「情報全部」はすぐにノイズに変わり、システムを遅く、鈍く、高コストにしてしまいます。</p>\n<h2 id=\"heading-mcp-1\">私たちのMCPクライアントでのアプローチ</h2>\n<p>MCP時代において、コンテキストは「王様」です。しかし正直なところ、その王様は酔いすぎて上下も分からなくなっていることがあります。課題はもはや「コンテキストを得ること」ではなく、「それを制御すること」です。優れたコンテキストエンジニアリングには、緻密な変換パイプライン、徹底的な優先順位付け、そして改善を続ける謙虚さが必要です。これを怠れば、トークン膨張、レイテンシ、混乱したレビューを招きます。うまく実践すれば、ワークフローに沿った鋭い出力が得られます。</p>\n<p>私たちは自社のコードレビュー用MCPクライアントでこれを実感しました。初期段階では全ログ・全ファイルをそのまま渡していました。その結果は高コストで、役に立たないほど散漫なレビューです。そこで重複排除、要約、タスク専用の隔離を導入したところ、レビュー品質が飛躍的に向上しました。すべてを指摘するのではなく、本当のクロスファイルリスクに集中するようになり、トークン消費とレイテンシも低下しました。</p>\n<p>これこそが良いコンテキストエンジニアリングの成果です：情報量が多いのに散漫ではなく、本質を突いたレビュー。そしてそれこそが、私たちのMCPクライアントで実現しようとしていることです。</p>\n<p><strong><em>👉 コンテキスト設計の正しい姿を体験してみませんか？ 今すぐ</em></strong> <a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong><em>14日間の無料トライアル</em></strong></a> <strong><em>でAIコードレビューをお試しください。</em></strong></p>\n",
      "summary": "Ballooning context in the MCP era: Context engineering on steroids意訳です。\nかつて、LLMにコンテキストを渡すには、ハックやベクターストラテジー、そしてお祈り…と、過剰に複雑なRAGパイプラインをつなぎ合わせる必要がありました。そこに登場したのが Model Context Protocol (MCP)。外部データを本番環境のモデルに提供するための、クリーンでモジュール的手法です。MCPは、実際に「何かをする」エージェントシステムを構築する人々にとって、瞬く間に標準的なプロトコルとなりました。\n今ではほとんどのテック企業がMCP機能を打ち出していますが、その理由は明白です。MCPはコンテキストのロジックとアプリケーションロジックを分離し、信頼性を向上させ、複雑なワークフローでのプロンプト構築の混乱を抑える役割を果たします。\n私たちはしばらく前からコンテキストエンジニアリングの領域に深く取り組んでおり、今回独自のMCPクライアントを立ち上げるにあたり、コードレビューにより豊かなコンテキストを注入できることに大いに期待しています。しかし正直に言えば、豊富なコンテキストにはリスクも伴います。MCP時代の隠された真実はこうです：かつて欲していたコンテキストに、今や私たちは溺れそうである。ログやトレース、diffなど \"関連\"ファイルが増え、モデルが本当に必要としているものが見えにくくなっています。\n役立つ入力はすぐにトークンの膨張、ノイズ、パフォーマンス劣化につながります。引用付きハルシネーション、レイテンシの急上昇、あるいはカフェインを摂りすぎたインターンが書いたような散漫なレビュー。良いコンテキストエンジニアリングとは「すべて詰め込む」ことではなく、「何を省くか」を知ることでもあります。そしてMCP以降、そのバランスを取るのはより難しく、より重要になっています。\nこの記事では、膨張するコンテキスト問題 の詳細、その副作用、そして私たちがそれにどう立ち向かっているかを解説します。MCPを用いたLLM機能を開発している方で、プロンプト形のブラックホールを作り出したくない方に役立つ内容です。\nMCPクライアント & サーバーにおける「膨張するコンテキスト問題」\n\nMCPサーバーとクライアントは、モデルに膨大な情報を渡すことを容易にします：ログ、トレース、diff、設定、チケット、さらには誰も所有を覚えていないリポジトリの隅まで。すべてがモデルの手の届くところにあります。しかし、ここで重要な問いがあります：「コンテキストが多ければ多いほど良いのか？」 答えは間違いなく「NO」です。\n過剰なコンテキストは、試験勉強で図書館全体を読むようなもの。ノイズは増えても、知識にはなりません。コンテキストが制御されなければ、次の3つの問題がすぐに現れます：\nトークンの膨張\n  LLMには無限のキャパシティはありません。入力ウィンドウにはコストと限界があり、念のため…と詳細情報を詰め込みすぎれば、コストは増大してスループットは低下し、不要なテキストに予算を浪費します。\n関連性の低下\n  情報が多いほど出力が良くなるわけではありません。むしろ悪化することが多いのです。無関係または冗長なスニペットがシグナルを希釈し、モデルはインサイトではなく枝葉に追われます。\nレイテンシ\n  追加されるログやdiff、スタックトレースはすべて取得・処理され、プロンプトに押し込まれます。コンテキスト構築がボトルネックとなり、レビュー速度を著しく低下させます。\n要するに、膨張するコンテキストはMCPの優雅さを逆にリスクへと変えてしまいます。意図的なコンテキストエンジニアリングがなければ、出力を磨くどころか押しつぶしてしまうのです。\nコンテキストが害になるとき\n\n実際には、以下の3つの典型的な問題が発生します：\nコンテキスト混乱\n  モデルが無関係な詳細をシグナルと誤解してしまうケース。例えば認証ロジックを更新するPRに、無関係なテストフィクスチャが含まれていると、モデルはフィクスチャをレビューし始め、実際の変更と関係のないコメントを生成します。\nコンテキスト衝突\n  コンテキスト同士が矛盾する場合です。例えば最新のスキーママイグレーションと古いドックストリングが同時に含まれると、モデルはどちらを信じるべきか迷い、結果として全方位的で自信のないレビューを生成します。まるで決断できないレビュアーのように。\nコンテキスト汚染\n  最も厄介なのは誤った情報が混入するケースです。無用な関連ファイルや、誤ってインデックス化されたスニペットが注入されると、存在しないコードを引用するようになります。レビューでは、存在しないファイルのバグに言及し、開発者を混乱させ、時間を無駄にし、信頼を損ないます。\nこれはコードレビューに限りません。サポートボットが無関係なチケットを引っ張ってくる場合や、リサーチアシスタントが周辺論文に気を取られる場合、セキュリティエージェントがノイジーなログを証拠として扱う場合なども同様です。いずれにしても、間違ったコンテキストは「ない方がまし」なのです。\nMCPサーバーでのコンテキスト過負荷を防ぐ主要パターン\n\nMCP時代の問題が、膨張するコンテキストだとすれば、解決策は情報の流入を止めることではありません。意図を持って選別・圧縮・提供することです。MCPのコンテキストは、生の素材をモデルに渡す前にきちんと設計されたデータ変換プロセスを経るべきものです。私たち自身のコードレビュー用MCPクライアントでも、コンテキストを高シグナル・低ノイズに保つために以下のパターンを採用しています。\nコンテキストの重複排除と差分化\n  冗長な入力はトークン浪費の最短ルートです。同一のスタックトレース、繰り返しのログ、変更されていないdiff部分は10回も登場する必要はありません。クライアントは重複を検出して折りたたみ、新しい部分だけを強調します。この原則は他の領域にも適用可能です：重複するサポートチケットをまとめ、繰り返しのトレースを圧縮し、差分のみを残します。\nコンテキスト要約パイプライン\n  MCP出力が依然として大きすぎる場合、LLM自体が要約して小さくすることも可能です。代償は圧縮と忠実度のトレードオフ：要約はニュアンスを失う可能性がありますが、詳細に溺れるよりはましです。実際には、重要ファイルは生のdiff、優先度の低いコンテキストは要約といったハイブリッド設計を採用します。\nコンテキストの優先順位付けと切り捨て\n  プルーニングや要約後でも、どれを最初に入れ、後に回し、容量不足時に捨てるかを決める必要があります。MCPクエリごとにトークン予算を設定することは不可欠です。そうしなければプロンプトが予測不能に膨張します。私たちは切り捨てを前提にした設計を試し、場合によっては概要を先頭に、詳細を後半に配置するなど調整しています。\nコンテキストの隔離\n  すべてのコンテキストを最初のプロンプトに含める必要はありません。サブタスクごとに専用のコンテキストスレッドを持たせるべきです。例えば、私たちのMCPクライアントではテスト失敗は専用のレビューサブスレッドに置かれ、メインのレビューコンテキストを妨げません。これにより混乱を減らし、長い対話でも明瞭さを保てます。\n継続的な改善と学習\n  コンテキストエンジニアリングは静的ではありません。モデルのフィードバックや人間による修正を取り入れ、優先順位を調整していきます。重要なのは可観測性です。モジュールごとにプロンプト入力を記録し、何が通って何が無駄かを把握します。MCPダッシュボードやトークンヒートマップのようなツールが、予算超過や不要な入力を可視化します。\nMCPサーバー & クライアントにおけるアンチパターン\n\nMCP時代はコンテキスト取得を容易にしました。おそらく「容易すぎる」ほどに。以下のようなアンチパターンがよく見られます：\nベクトル無差別投入\n  ベクトルDBは「関連」情報を見つけるのに優れていますが、それを万能の答えと見なすのは危険です。曖昧に関連するスニペットをすべて投入すると、関係のないファイルへのコメントや古いコードへの指摘で溢れるレビューになります。コンテキストの不適合はトークンの無駄だけでなく、モデルのパフォーマンスを引き下げる要因になります。\n「全部突っ込め」方式\n  すべてのログ、diff、ドックストリングをコンテキストに放り込み、あとは神に祈るやり方です。コストの増加、レイテンシの悪化、結果の予測不能を保証します。モデルは重要な部分と不要な部分を区別できないため、全方位的で散漫なレビューを生成します。矛盾が混入すれば、モデルは曖昧さを埋めるために幻覚を引き起こします。\n要するに、コンテキストは多ければ良いというものではありません。フィルタリング、優先順位付け、設計がなければ、「情報全部」はすぐにノイズに変わり、システムを遅く、鈍く、高コストにしてしまいます。\n私たちのMCPクライアントでのアプローチ\nMCP時代において、コンテキストは「王様」です。しかし正直なところ、その王様は酔いすぎて上下も分からなくなっていることがあります。課題はもはや「コンテキストを得ること」ではなく、「それを制御すること」です。優れたコンテキストエンジニアリングには、緻密な変換パイプライン、徹底的な優先順位付け、そして改善を続ける謙虚さが必要です。これを怠れば、トークン膨張、レイテンシ、混乱したレビューを招きます。うまく実践すれば、ワークフローに沿った鋭い出力が得られます。\n私たちは自社のコードレビュー用MCPクライアントでこれを実感しました。初期段階では全ログ・全ファイルをそのまま渡していました。その結果は高コストで、役に立たないほど散漫なレビューです。そこで重複排除、要約、タスク専用の隔離を導入したところ、レビュー品質が飛躍的に向上しました。すべてを指摘するのではなく、本当のクロスファイルリスクに集中するようになり、トークン消費とレイテンシも低下しました。\nこれこそが良いコンテキストエンジニアリングの成果です：情報量が多いのに散漫ではなく、本質を突いたレビュー。そしてそれこそが、私たちのMCPクライアントで実現しようとしていることです。\n👉 コンテキスト設計の正しい姿を体験してみませんか？ 今すぐ 14日間の無料トライアル でAIコードレビューをお試しください。",
      "publishedAt": "2025-09-19T06:40:35.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.984Z",
      "score": 0.14448052941858094
    },
    {
      "id": "c451f569db03113118dee3ee48937348",
      "title": "CodeRabbitは100万ドルをオープンソースプロジェクトに拠出します",
      "url": "https://coderabbit.ai/blog/coderabbit-commits-1-million-to-open-source-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/coderabbit-commits-1-million-to-open-source\">CodeRabbit commits $1 million to open source software</a>の意訳です。</p>\n<p>オープンソースは現代のソフトウェア開発の基盤です。パッケージマネージャーや開発ツールから、フレームワーク、インフラに至るまで、今日私たちが使うほとんどすべてのソフトウェアはオープンソースプロジェクトによって支えられています。CodeRabbit自体もそうです。これらのプロジェクトは、数え切れないほどの時間を費やし、維持・進化させ続けている開発者コミュニティによって構築・保守されています。</p>\n<p>本日、私たちは <strong>オープンソースソフトウェアへのスポンサーシップとして100万ドル（USD）の拠出</strong> を発表します。これは <strong>6,000万ドルのシリーズB資金調達</strong> に続くものであり、オープンソースが可能にしてくれたことへの感謝、そしてその未来への投資の重要性に対する私たちの信念を表しています。</p>\n<h2 id=\"heading-kirjgarjgzzku4rjgihjgqrjg7zjg5fjg7pjgr3jg7zjgrnjgbjjga7mlkmj7tjgyzjgzpjgozjgb7jgafku6xkuirjgavlv4xopohjgarjga7jgysqkg\"><strong>なぜ今、オープンソースへの支援がこれまで以上に必要なのか</strong></h2>\n<p>生成AIはソフトウェア開発を変革していますが、同時にオープンソースのメンテナーに新たな負荷を与えています。高品質なコントリビューションの増加と並行して、<strong>AI生成によるPRスパム</strong>（繰り返し・低品質・時には不安定なコードの提出）が急増し、メンテナーを圧倒しています。</p>\n<p>私たちはメンテナー自身から、この膨大なノイズがどれほど負担となっているかを直接聞いてきました。CodeRabbitでは、<strong>スパムをフィルタリングし、コード品質を向上させ、メンテナーの作業負荷を軽減する</strong> AI駆動のコードレビューと人間による監視を組み合わせたツールを開発しました。私たちはこのAIコードレビューツールをすべてのオープンソースプロジェクトに無料で提供しています（詳細はこちら）。</p>\n<p>しかし、ツールだけでは十分ではありません。持続可能なオープンソースには、金銭的支援、認知、そしてコミュニティ間をつなぐ強固な架け橋が必要です。</p>\n<h2 id=\"heading-20100\"><strong>20万ドルから100万ドル：より深いコミットメントへ</strong></h2>\n<p>今年初め、私たちは <a target=\"_blank\" href=\"https://2025.allthingsopen.org/pledging-support-for-open-source\"><strong>オープンソースへの20万ドルの誓約</strong></a> を発表し、以下のようなプロジェクトを支援しました：</p>\n<ul>\n<li><p><strong>pnpm</strong>: ディスクスペース効率に優れたパッケージマネージャー</p>\n</li>\n<li><p><strong>Biome (biomejs)</strong>: 次世代のJavaScript/TypeScript用リンター兼フォーマッター</p>\n</li>\n<li><p><strong>AST Grep (Herrington Darkholme)</strong>: 構造的コード検索によるスマートなコード解析</p>\n</li>\n<li><p><strong>iTerm2 (George Nachman)</strong>: 開発者のワークフローを刷新したターミナルエミュレーター</p>\n</li>\n<li><p><strong>Markdown Lint (David Anson)</strong>: ドキュメントを明確かつ一貫性のある状態に保つツール</p>\n</li>\n</ul>\n<p>この誓約は始まりにすぎません。今回のシリーズB資金調達によって、私たちは支援額を <strong>100万ドルに拡大</strong> し、エコシステム全体のプロジェクトやメンテナーが正当な評価とリソースを得られるようにします。</p>\n<p><a target=\"_blank\" href=\"https://coderabbit.link/oss-progam-submission-form\"><strong>スポンサーシップ申請はこちらより行ってください</strong></a></p>\n<h2 id=\"heading-coderabbitoss\"><strong>CodeRabbitとOSS：エコシステム全体をつなぐ架け橋へ</strong></h2>\n<p>スポンサーシップは始まりに過ぎません。オープンソースが直面している多くの課題 ― 持続可能性、セキュリティ、開発者の燃え尽き（バーンアウト） ― は特定のプロジェクトに限られた問題ではありません。これらはコミュニティやエコシステム全体に広がっています。</p>\n<p>だからこそ、CodeRabbitは <strong>メンテナー同士をつなぎ、協力を促進し、プロジェクト間で解決策を共有する</strong> 取り組みにも力を入れています。共同スポンサーシップ、共同イニシアチブ、コミュニティ主導のツールに関する議論を通じて、孤立した支援ではなく、エコシステム全体を強化することを目指しています。</p>\n<p>もしあなたがメンテナーやコントリビューターで、こうした議論に参加したいと考えているなら、ぜひご連絡ください。CodeRabbitチームや他のオープンソースリーダーとつながるためにDiscordに参加してください。</p>\n<h2 id=\"heading-coderabbit\"><strong>オープンソースプロジェクト向けの無料CodeRabbit利用</strong></h2>\n<p>最後に改めてお伝えします：<strong>CodeRabbitはオープンソースに対して、無料で提供されています</strong>。すべてのメンテナー、コントリビューター、コミュニティは、私たちのプラットフォームを利用してPRノイズを減らし、コード品質チェックを自動化し、より意味のあるコントリビュートに時間を割けるようになります。</p>\n<p><strong><em>詳細はこちらから確認し</em></strong> <a target=\"_blank\" href=\"https://coderabbit.link/oss-progam-submission-form\"><strong><em>資金提供の申請を行ってください</em></strong></a></p>\n",
      "summary": "CodeRabbit commits $1 million to open source softwareの意訳です。\nオープンソースは現代のソフトウェア開発の基盤です。パッケージマネージャーや開発ツールから、フレームワーク、インフラに至るまで、今日私たちが使うほとんどすべてのソフトウェアはオープンソースプロジェクトによって支えられています。CodeRabbit自体もそうです。これらのプロジェクトは、数え切れないほどの時間を費やし、維持・進化させ続けている開発者コミュニティによって構築・保守されています。\n本日、私たちは オープンソースソフトウェアへのスポンサーシップとして100万ドル（USD）の拠出 を発表します。これは 6,000万ドルのシリーズB資金調達 に続くものであり、オープンソースが可能にしてくれたことへの感謝、そしてその未来への投資の重要性に対する私たちの信念を表しています。\nなぜ今、オープンソースへの支援がこれまで以上に必要なのか\n生成AIはソフトウェア開発を変革していますが、同時にオープンソースのメンテナーに新たな負荷を与えています。高品質なコントリビューションの増加と並行して、AI生成によるPRスパム（繰り返し・低品質・時には不安定なコードの提出）が急増し、メンテナーを圧倒しています。\n私たちはメンテナー自身から、この膨大なノイズがどれほど負担となっているかを直接聞いてきました。CodeRabbitでは、スパムをフィルタリングし、コード品質を向上させ、メンテナーの作業負荷を軽減する AI駆動のコードレビューと人間による監視を組み合わせたツールを開発しました。私たちはこのAIコードレビューツールをすべてのオープンソースプロジェクトに無料で提供しています（詳細はこちら）。\nしかし、ツールだけでは十分ではありません。持続可能なオープンソースには、金銭的支援、認知、そしてコミュニティ間をつなぐ強固な架け橋が必要です。\n20万ドルから100万ドル：より深いコミットメントへ\n今年初め、私たちは オープンソースへの20万ドルの誓約 を発表し、以下のようなプロジェクトを支援しました：\npnpm: ディスクスペース効率に優れたパッケージマネージャー\nBiome (biomejs): 次世代のJavaScript/TypeScript用リンター兼フォーマッター\nAST Grep (Herrington Darkholme): 構造的コード検索によるスマートなコード解析\niTerm2 (George Nachman): 開発者のワークフローを刷新したターミナルエミュレーター\nMarkdown Lint (David Anson): ドキュメントを明確かつ一貫性のある状態に保つツール\nこの誓約は始まりにすぎません。今回のシリーズB資金調達によって、私たちは支援額を 100万ドルに拡大 し、エコシステム全体のプロジェクトやメンテナーが正当な評価とリソースを得られるようにします。\nスポンサーシップ申請はこちらより行ってください\nCodeRabbitとOSS：エコシステム全体をつなぐ架け橋へ\nスポンサーシップは始まりに過ぎません。オープンソースが直面している多くの課題 ― 持続可能性、セキュリティ、開発者の燃え尽き（バーンアウト） ― は特定のプロジェクトに限られた問題ではありません。これらはコミュニティやエコシステム全体に広がっています。\nだからこそ、CodeRabbitは メンテナー同士をつなぎ、協力を促進し、プロジェクト間で解決策を共有する 取り組みにも力を入れています。共同スポンサーシップ、共同イニシアチブ、コミュニティ主導のツールに関する議論を通じて、孤立した支援ではなく、エコシステム全体を強化することを目指しています。\nもしあなたがメンテナーやコントリビューターで、こうした議論に参加したいと考えているなら、ぜひご連絡ください。CodeRabbitチームや他のオープンソースリーダーとつながるためにDiscordに参加してください。\nオープンソースプロジェクト向けの無料CodeRabbit利用\n最後に改めてお伝えします：CodeRabbitはオープンソースに対して、無料で提供されています。すべてのメンテナー、コントリビューター、コミュニティは、私たちのプラットフォームを利用してPRノイズを減らし、コード品質チェックを自動化し、より意味のあるコントリビュートに時間を割けるようになります。\n詳細はこちらから確認し 資金提供の申請を行ってください",
      "publishedAt": "2025-09-19T06:21:31.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.984Z",
      "score": 0.06518758990267862
    },
    {
      "id": "b2944f86b8b1dad74d31217b7d195413",
      "title": "CodeRabbit commits $1 million to open source",
      "url": "https://coderabbit.ai/blog/coderabbit-commits-1-million-to-open-source",
      "content": "<p>Open source is the foundation of modern software development. From package managers and developer tools to frameworks and infrastructure, open source projects power nearly every piece of software we use today – including CodeRabbit itself. These projects are built and maintained by communities of developers who dedicate countless hours to keeping them alive, secure, and evolving.</p>\n<p>Today, we’re proud to announce a <strong>$1 million USD commitment to open source software sponsorships</strong>. This commitment comes on the heels of our <strong>$60 million Series B funding round</strong> and it reflects both our gratitude for what open source makes possible and our belief in the importance of investing in its future.</p>\n<h2 id=\"heading-why-open-source-needs-support-now-more-than-ever\"><strong>Why open source needs support now more than ever</strong></h2>\n<p>Generative AI is transforming software development, but it’s also putting new pressures on open source maintainers. Alongside the surge of high-quality contributions, there has been a sharp rise in <strong>AI-generated PR spam</strong>: repetitive, low-quality, and sometimes insecure code submissions that overwhelm project maintainers.</p>\n<p>We’ve heard firsthand from maintainers about how draining this flood of noise can be. At CodeRabbit, we’ve built tools that <strong>filter out spam, elevate code quality, and reduce maintainer workload</strong> by blending AI-driven code review with human oversight. We’ve made our AI code review tool free for use on all open source projects (more about that here). </p>\n<p>But tools alone aren’t enough—sustainable open source requires financial support, recognition, and stronger bridges between communities.</p>\n<h2 id=\"heading-from-200k-to-1m-deepening-our-commitment\"><strong>From $200K to $1M: Deepening our commitment</strong></h2>\n<p>Earlier this year, we announced a <a target=\"_blank\" href=\"https://2025.allthingsopen.org/pledging-support-for-open-source\"><strong>$200,000 pledge to open source</strong></a>, supporting projects like:</p>\n<ul>\n<li><p><strong>pnpm</strong>: A disk-space–efficient package manager</p>\n</li>\n<li><p><strong>Biome (biomejs)</strong>: A next-generation linter and formatter for JavaScript and TypeScript</p>\n</li>\n<li><p><strong>AST Grep (Herrington Darkholme)</strong>: Structural code search for smarter code analysis</p>\n</li>\n<li><p><strong>iTerm2 (George Nachman)</strong>: A terminal emulator that redefined developer workflow</p>\n</li>\n<li><p><strong>Markdown Lint (David Anson)</strong>: Ensuring docs stay clear and consistent</p>\n</li>\n</ul>\n<p>That pledge was only the beginning. With our new Series B funding, we’re scaling our support to <strong>$1 million</strong>, ensuring that more projects and maintainers across the ecosystem receive the recognition and resources they deserve.</p>\n<p><a target=\"_blank\" href=\"https://coderabbit.link/oss-progam-submission-form\"><strong>Apply for sponsorship here.</strong></a> </p>\n<h2 id=\"heading-coderabbit-amp-oss-building-bridges-across-the-oss-ecosystem\"><strong>CodeRabbit &amp; OSS: Building bridges across the OSS ecosystem</strong></h2>\n<p>Sponsorship is only part of the story. Many of the challenges open source faces—sustainability, security, and developer burnout—aren’t isolated to a single project. They stretch across communities and ecosystems.</p>\n<p>That’s why CodeRabbit is also working to <strong>connect maintainers, foster collaboration, and share solutions across projects</strong>. Whether through joint sponsorships, shared initiatives, or community-driven tooling conversations, we aim to strengthen the ecosystem as a whole rather than supporting it in silos.</p>\n<p>If you’re a maintainer or contributor who wants to join these conversations, we’d love to hear from you. Join our Discord to connect with the CodeRabbit team and other open source leaders.</p>\n<h2 id=\"heading-free-coderabbit-access-for-open-source-projects\"><strong>Free CodeRabbit access for open source projects</strong></h2>\n<p>Finally, a reminder: <strong>CodeRabbit is free for open source</strong>. Every maintainer, contributor, and community can use our platform to cut through PR noise, automate code quality checks, and free up more time for meaningful contributions.</p>\n<p><strong><em>Learn more and</em></strong> <a target=\"_blank\" href=\"https://coderabbit.link/oss-progam-submission-form\"><strong><em>apply for funding here.</em></strong></a></p>\n",
      "summary": "Open source is the foundation of modern software development. From package managers and developer tools to frameworks and infrastructure, open source projects power nearly every piece of software we use today – including CodeRabbit itself. These projects are built and maintained by communities of developers who dedicate countless hours to keeping them alive, secure, and evolving.\nToday, we’re proud to announce a $1 million USD commitment to open source software sponsorships. This commitment comes on the heels of our $60 million Series B funding round and it reflects both our gratitude for what open source makes possible and our belief in the importance of investing in its future.\nWhy open source needs support now more than ever\nGenerative AI is transforming software development, but it’s also putting new pressures on open source maintainers. Alongside the surge of high-quality contributions, there has been a sharp rise in AI-generated PR spam: repetitive, low-quality, and sometimes insecure code submissions that overwhelm project maintainers.\nWe’ve heard firsthand from maintainers about how draining this flood of noise can be. At CodeRabbit, we’ve built tools that filter out spam, elevate code quality, and reduce maintainer workload by blending AI-driven code review with human oversight. We’ve made our AI code review tool free for use on all open source projects (more about that here). \nBut tools alone aren’t enough—sustainable open source requires financial support, recognition, and stronger bridges between communities.\nFrom $200K to $1M: Deepening our commitment\nEarlier this year, we announced a $200,000 pledge to open source, supporting projects like:\npnpm: A disk-space–efficient package manager\nBiome (biomejs): A next-generation linter and formatter for JavaScript and TypeScript\nAST Grep (Herrington Darkholme): Structural code search for smarter code analysis\niTerm2 (George Nachman): A terminal emulator that redefined developer workflow\nMarkdown Lint (David Anson): Ensuring docs stay clear and consistent\nThat pledge was only the beginning. With our new Series B funding, we’re scaling our support to $1 million, ensuring that more projects and maintainers across the ecosystem receive the recognition and resources they deserve.\nApply for sponsorship here. \nCodeRabbit & OSS: Building bridges across the OSS ecosystem\nSponsorship is only part of the story. Many of the challenges open source faces—sustainability, security, and developer burnout—aren’t isolated to a single project. They stretch across communities and ecosystems.\nThat’s why CodeRabbit is also working to connect maintainers, foster collaboration, and share solutions across projects. Whether through joint sponsorships, shared initiatives, or community-driven tooling conversations, we aim to strengthen the ecosystem as a whole rather than supporting it in silos.\nIf you’re a maintainer or contributor who wants to join these conversations, we’d love to hear from you. Join our Discord to connect with the CodeRabbit team and other open source leaders.\nFree CodeRabbit access for open source projects\nFinally, a reminder: CodeRabbit is free for open source. Every maintainer, contributor, and community can use our platform to cut through PR noise, automate code quality checks, and free up more time for meaningful contributions.\nLearn more and apply for funding here.",
      "publishedAt": "2025-09-18T15:56:44.000Z",
      "author": "Aravind Putrevu",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "documentation",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.984Z",
      "score": 0.13382234881785987
    },
    {
      "id": "a34eb9185e3cb82f9ab2ecd637d138b2",
      "title": "CodeRabbit’s MCP integration = Code reviews that see the whole picture",
      "url": "https://coderabbit.ai/blog/coderabbits-mcp-server-integration-code-reviews-that-see-the-whole-picture",
      "content": "<p>Every dev team knows the pain of code reviews if performed in isolation. An AI tool (or even a teammate) can comment on syntax, style, and patterns, but without business requirements, deployment dependencies, or organizational knowledge, it’s just guessing at half the story.</p>\n<p>CodeRabbit currently has a number of native integrations including Linear, Jira, and Circle CI. We have seen the value that context from those tools provide to code reviews. That’s why we’re excited to announce the GA of <strong>CodeRabbit’s integration with MCP servers</strong>. This will allow you to bring in even more context into your reviews.</p>\n<p>With this launch, we become the first AI code review platform that orchestrates context from across your entire development ecosystem from business requirements in Confluence to system dependencies in your CI/CD pipeline to data from any internal MCP servers. All to provide code reviews that actually understand what your code is trying to accomplish.</p>\n<p><strong>Start your 14-day trial →</strong> <em>Get context-aware reviews that reference your actual team standards in ~10 minutes.</em></p>\n<h2 id=\"heading-why-mcp-for-ai-code-reviews\"><strong>Why MCP for AI code reviews?</strong></h2>\n<p>Development teams operate across dozens of tools:</p>\n<ul>\n<li><p>Requirements live in Linear</p>\n</li>\n<li><p>Design specifications exist in Figma</p>\n</li>\n<li><p>Architectural decisions get documented in Confluence</p>\n</li>\n<li><p>Security standards evolve in internal wikis after each audit</p>\n</li>\n</ul>\n<p>AI code reviewers start with basic context: your codebase, some coding guidelines, maybe a few integrations. They analyze syntax, check patterns, and suggest improvements. But they miss the context that determines whether code actually works for your team.</p>\n<p>As a MCP client, CodeRabbit acts as a compiler for organizational context. It takes high-level inputs - your wikis, tickets, deployment patterns - and compiles them down into precise, actionable code review insights. Instead of bloated integrations or brittle hacks, MCP lets clients like CodeRabbit pull in just the right data from your MCP servers from places like your Linear tickets, Confluence docs, Datadog metrics, or Slack discussions.</p>\n<h2 id=\"heading-what-it-looks-like-in-practice\">What it looks like in practice…</h2>\n<p>CodeRabbit searches connected MCP servers before starting a review. For example, database schema changes might get checked against data architecture documents. API endpoint implementations might get verified against service design patterns documented in internal wikis.</p>\n<p><strong>Example: CodeRabbit verifies code consistence</strong></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758126014841/53d19c71-2051-410c-a109-1e056cc0094d.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-bring-in-the-context-matters-to-you-from-any-tool\">Bring in the context matters to you… from any tool</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125357433/fa378ff4-743d-4841-a3fa-e933bcf4fe7c.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Traditional code review tools require specific integrations. CodeRabbit's MCP integration works with any system with an MCP server. Your proprietary internal tools, boutique SaaS platforms, custom documentation systems. If there's an MCP server, CodeRabbit can connect.</p>\n<p>With CodeRabbit as an MCP client, you’re reviews gain depth from bringing in three different types of context. </p>\n<h3 id=\"heading-technical-context\"><strong>Technical context.</strong></h3>\n<ul>\n<li>Think dependencies, performance data, static analysis, and test coverage.</li>\n</ul>\n<ul>\n<li><p><strong>Native integrations:</strong> GitHub Actions, GitLab CI, Bitbucket Pipelines</p>\n</li>\n<li><p><strong>MCP Servers:</strong> Datadog, New Relic, SonarQube, Snyk, Grafana</p>\n</li>\n<li><p>Example Review Comment:</p>\n</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125833426/facf17b5-32c7-47ce-b6cd-8170f9837bf6.png\" alt class=\"image--center mx-auto\" /></p>\n<h3 id=\"heading-business-context\"><strong>Business context.</strong></h3>\n<ul>\n<li><p>This includes things like requirements, user stories, and acceptance criteria.</p>\n</li>\n<li><p><strong>Native integrations:</strong> Linear, Jira, GitHub Issues, GitLab Issues</p>\n</li>\n<li><p><strong>MCP Servers:</strong> Confluence, Notion</p>\n</li>\n<li><p>Example Review Comment:</p>\n</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125848608/26c21700-1f57-42af-9993-cc372c629e6f.png\" alt class=\"image--center mx-auto\" /></p>\n<h3 id=\"heading-organizational-context\"><strong>Organizational context.</strong></h3>\n<ul>\n<li><p>We also pull in things like prior decisions, conventions, meeting notes, and institutional knowledge.</p>\n</li>\n<li><p><strong>Native integrations:</strong> PR history, Team conventions</p>\n</li>\n<li><p><strong>MCP Servers:</strong> Slack, Microsoft Teams, Stack Overflow for Teams, PagerDuty</p>\n</li>\n<li><p>Example Review Comment:</p>\n<p>  <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125858492/5bc5b006-b61c-4b9c-9642-5246978db056.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n</ul>\n<h2 id=\"heading-getting-started-with-mcp-integration\"><strong>Getting started with MCP integration</strong></h2>\n<p>Setting up CodeRabbit's MCP client requires minimal configuration. Most development teams can connect their first MCP server in under 10 minutes.</p>\n<p><strong>Popular development tools with MCP server support</strong>:</p>\n<ul>\n<li><p><strong>Linear</strong> (native MCP support, 5 minutes)</p>\n</li>\n<li><p><strong>Notion</strong> (MCP server available, 10 minutes)</p>\n</li>\n<li><p><strong>Confluence</strong> (community MCP server, 15 minutes)</p>\n</li>\n<li><p><strong>Figma</strong> (MCP plugin available, 10 minutes)</p>\n</li>\n</ul>\n<p>Define which code changes should search which development systems. Database changes check architecture documentation. Authentication changes check security documentation.</p>\n<p>Adding an MCP server is easy:</p>\n<ol>\n<li><p>In the CodeRabbit dashboard, head over to integrations &gt; and toggle to the MCP Servers tab if needed</p>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125568659/69feb7d1-f61c-4f4b-81db-5e8e73290092.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><p>You can click on one of the pre-configured MCP server options or the New MCP Server button to add other MCP servers.</p>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125579766/3a2477e7-0ad4-4ff1-8da6-1b8e5307d4fa.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><p>For MCP servers not on the list, enter the relevant credentials.</p>\n</li>\n<li><p>Note the usage guidance which serves as context for how the MCP information should be used.</p>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125590738/eec44f71-e0c0-42b7-814b-869996da38c8.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><p>Once connected. You can see the available calls and hover over them to see more details.</p>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125606565/5a9a903b-d6d8-4635-904e-f33616096103.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><p>You can also click on each call to enable/disable access.</p>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125618724/c5269264-6fa1-4288-8a98-7dca46869301.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n</ol>\n<h2 id=\"heading-a-review-platform-that-brings-in-all-your-context\"><strong>A review platform that brings in all your context</strong></h2>\n<p>CodeRabbit works out of the box with 50+ integrations. With MCP, you can extend it to your custom servers and internal tools. Start with the systems you already use — Linear, Confluence, Datadog, Slack — and add more as you go.</p>\n<h3 id=\"heading-next-steps\"><strong>Next steps:</strong></h3>\n<ol>\n<li><p><a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong>Start a 14-day trial</strong></a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://app.coderabbit.ai/integrations\"><strong>View MCP server directory</strong></a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://docs.coderabbit.ai/context-enrichment/mcp-server-integrations\"><strong>See the MCP docs</strong></a></p>\n</li>\n</ol>\n",
      "summary": "Every dev team knows the pain of code reviews if performed in isolation. An AI tool (or even a teammate) can comment on syntax, style, and patterns, but without business requirements, deployment dependencies, or organizational knowledge, it’s just guessing at half the story.\nCodeRabbit currently has a number of native integrations including Linear, Jira, and Circle CI. We have seen the value that context from those tools provide to code reviews. That’s why we’re excited to announce the GA of CodeRabbit’s integration with MCP servers. This will allow you to bring in even more context into your reviews.\nWith this launch, we become the first AI code review platform that orchestrates context from across your entire development ecosystem from business requirements in Confluence to system dependencies in your CI/CD pipeline to data from any internal MCP servers. All to provide code reviews that actually understand what your code is trying to accomplish.\nStart your 14-day trial → Get context-aware reviews that reference your actual team standards in ~10 minutes.\nWhy MCP for AI code reviews?\nDevelopment teams operate across dozens of tools:\nRequirements live in Linear\nDesign specifications exist in Figma\nArchitectural decisions get documented in Confluence\nSecurity standards evolve in internal wikis after each audit\nAI code reviewers start with basic context: your codebase, some coding guidelines, maybe a few integrations. They analyze syntax, check patterns, and suggest improvements. But they miss the context that determines whether code actually works for your team.\nAs a MCP client, CodeRabbit acts as a compiler for organizational context. It takes high-level inputs - your wikis, tickets, deployment patterns - and compiles them down into precise, actionable code review insights. Instead of bloated integrations or brittle hacks, MCP lets clients like CodeRabbit pull in just the right data from your MCP servers from places like your Linear tickets, Confluence docs, Datadog metrics, or Slack discussions.\nWhat it looks like in practice…\nCodeRabbit searches connected MCP servers before starting a review. For example, database schema changes might get checked against data architecture documents. API endpoint implementations might get verified against service design patterns documented in internal wikis.\nExample: CodeRabbit verifies code consistence\n\nBring in the context matters to you… from any tool\n\nTraditional code review tools require specific integrations. CodeRabbit's MCP integration works with any system with an MCP server. Your proprietary internal tools, boutique SaaS platforms, custom documentation systems. If there's an MCP server, CodeRabbit can connect.\nWith CodeRabbit as an MCP client, you’re reviews gain depth from bringing in three different types of context. \nTechnical context.\nThink dependencies, performance data, static analysis, and test coverage.\nNative integrations: GitHub Actions, GitLab CI, Bitbucket Pipelines\nMCP Servers: Datadog, New Relic, SonarQube, Snyk, Grafana\nExample Review Comment:\n\nBusiness context.\nThis includes things like requirements, user stories, and acceptance criteria.\nNative integrations: Linear, Jira, GitHub Issues, GitLab Issues\nMCP Servers: Confluence, Notion\nExample Review Comment:\n\nOrganizational context.\nWe also pull in things like prior decisions, conventions, meeting notes, and institutional knowledge.\nNative integrations: PR history, Team conventions\nMCP Servers: Slack, Microsoft Teams, Stack Overflow for Teams, PagerDuty\nExample Review Comment:\n  \nGetting started with MCP integration\nSetting up CodeRabbit's MCP client requires minimal configuration. Most development teams can connect their first MCP server in under 10 minutes.\nPopular development tools with MCP server support:\nLinear (native MCP support, 5 minutes)\nNotion (MCP server available, 10 minutes)\nConfluence (community MCP server, 15 minutes)\nFigma (MCP plugin available, 10 minutes)\nDefine which code changes should search which development systems. Database changes check architecture documentation. Authentication changes check security documentation.\nAdding an MCP server is easy:\nIn the CodeRabbit dashboard, head over to integrations > and toggle to the MCP Servers tab if needed\n \nYou can click on one of the pre-configured MCP server options or the New MCP Server button to add other MCP servers.\n \nFor MCP servers not on the list, enter the relevant credentials.\nNote the usage guidance which serves as context for how the MCP information should be used.\n \nOnce connected. You can see the available calls and hover over them to see more details.\n \nYou can also click on each call to enable/disable access.\n \nA review platform that brings in all your context\nCodeRabbit works out of the box with 50+ integrations. With MCP, you can extend it to your custom servers and internal tools. Start with the systems you already use — Linear, Confluence, Datadog, Slack — and add more as you go.\nNext steps:\nStart a 14-day trial\nView MCP server directory\nSee the MCP docs",
      "publishedAt": "2025-09-17T16:04:55.000Z",
      "author": "Edgar Cerecerez",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "testing",
        "observability",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.984Z",
      "score": 0.23267551006007234
    },
    {
      "id": "79cb6543735df13206c47f6d632c5332",
      "title": "Handling ballooning context in the MCP era: Context engineering on steroids",
      "url": "https://coderabbit.ai/blog/handling-ballooning-context-in-the-mcp-era-context-engineering-on-steroids",
      "content": "<p>Once upon a time, getting context into an LLM meant stringing together hacks, prayers, vector strategies, and overly complex RAG pipelines. Then came the <strong>Model Context Protocol (MCP),</strong> a clean, modular way to serve external data to models in production. It quickly became the protocol of choice for anyone building agentic systems that are trying to actually <em>do</em> things.</p>\n<p>Every tech company is now launching MCP functionalities – and for good reason. MCP separates context logic from application logic, improves reliability, and helps tame the chaos of prompt construction in complex workflows.</p>\n<p>We’ve been deep in the context engineering space for a while, and as we launch our own MCP client, we’re genuinely excited by how it lets us inject richer context into our code reviews. But let’s be honest: with great context comes great risk. Because here’s the dirty secret of the MCP era: <strong>most of us are now drowning in the context we used to beg for</strong>. More logs, more traces, more diffs, more \"relevant\" files and way less clarity about what the model actually needs.</p>\n<p>What starts as helpful input quickly turns into token bloat, noise, and degradation in model performance. Think hallucinations with citations, latency spikes, or reviews that read like they were written by an over-caffeinated intern who rambles. Good context engineering isn’t about cramming in <em>everything</em>, it’s also about knowing what to leave out. And in the aftermath of MCP, that balance is harder (and more important) than ever.</p>\n<p>In this article, we’ll break down the <strong>ballooning context problem</strong>, what happens when well-intentioned context goes rogue, and how we’re tackling it head-on. If you’re shipping LLM-based features with MCP and want to avoid accidentally building a prompt-shaped black hole, this blog is for you.</p>\n<h3 id=\"heading-the-ballooning-context-problem-with-mcp-clients-amp-servers\"><strong>The “Ballooning Context Problem” with MCP clients &amp; servers</strong></h3>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123932448/7988a3c4-f1be-4033-b8e2-43aee929d878.png\" alt class=\"image--center mx-auto\" /></p>\n<p>MCP servers and clients make it easy to hand models a firehose of information: logs, traces, diffs, configs, tickets, and sometimes even that dusty corner of the repo nobody remembers owning. It’s all right there at the model’s fingertips. But here’s the question: is more context always better? Definitely not!</p>\n<p>Too much context is like cramming for an exam by reading the entire library. You end up with noise, not knowledge. And when context goes unchecked, three problems show up fast:</p>\n<ul>\n<li><p><strong>Token bloat.</strong> LLMs don’t have infinite stomachs. Input windows are expensive and finite, and stuffing them full of “just in case” details means higher costs, slower throughput, and wasted budget on irrelevant text.</p>\n</li>\n<li><p><strong>Relevance decay.</strong> More information doesn’t mean better outputs. In fact, it often means worse. Irrelevant or redundant snippets dilute the signal, and the model starts chasing tangents instead of insights.</p>\n</li>\n<li><p><strong>Latency.</strong> Every extra log, diff, or stack trace has to be fetched, processed, and shoved into the prompt. Context building becomes the bottleneck, dragging review speed down to a crawl.</p>\n</li>\n</ul>\n<p>In short, ballooning context turns the elegance of MCP into a liability. Without deliberate context engineering, the very thing meant to sharpen outputs can just as easily smother them.</p>\n<h2 id=\"heading-when-context-hurts\">When context hurts</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123946588/f8c48c9f-db75-47bc-a349-bdb58b81258b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>In practice, we see three common pathologies:</p>\n<ul>\n<li><p><strong>Context confusion.</strong> This happens when the model latches onto irrelevant detail and treats it as signal. Imagine a pull request that updates authentication logic but the context dump also includes unrelated test fixtures. The model might start reviewing the fixtures instead, producing comments that feel informed but have nothing to do with the actual change.</p>\n</li>\n<li><p><strong>Context clash.</strong> Not all context agrees with itself. Suppose a code review includes both the latest schema migration and an outdated docstring that contradicts it. The model now has to “choose” which source to trust. Often, it hedges, producing muddled reviews that cover every angle without real confidence: the LLM equivalent of a reviewer who can’t commit.</p>\n</li>\n<li><p><strong>Context poisoning.</strong> The most insidious case is when bad information makes it into the context. A hallucinated “related file” or a mis-indexed snippet gets injected, and suddenly the model is citing non-existent code. In a review, that looks like a comment about a bug in a file that doesn’t exist, confusing developers, wasting time, and eroding trust.</p>\n</li>\n</ul>\n<p>And it’s not just code reviews. The same pitfalls show up anywhere context gets overstuffed: customer support bots pulling in irrelevant tickets, research assistants distracted by tangential papers, or security agents treating noisy logs as hard evidence. In each case, the wrong context is worse than no context at all.</p>\n<h2 id=\"heading-key-patterns-to-combat-context-overload-with-mcp-servers\">Key patterns to combat context overload with MCP servers</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123970317/3dd0ac82-f438-4150-83d8-fae81f7dafa2.png\" alt class=\"image--center mx-auto\" /></p>\n<p>If the problem of the MCP era is ballooning context, the solution isn’t to stop piping in information — it’s to curate, compress, and serve it with intent. MCP context should be treated as raw material that goes through a well-designed data transformation process before it ever reaches the model. For our own MCP client for code reviews, we’ve leaned on a set of patterns that keep context high-signal and low-noise.</p>\n<ul>\n<li><p><strong>Context deduplication and differencing</strong><br />  Redundant inputs are the fastest way to waste tokens. Identical stack traces, repeated log lines, or unchanged sections of a diff don’t need to appear ten times. Our client identifies duplicates, collapses them, and highlights only what’s new. The same principle applies in other domains: collapse duplicate customer tickets, compress recurring traces, and reduce context to delta rather than bulk.</p>\n</li>\n<li><p><strong>Context summarization pipelines</strong><br />  Sometimes raw MCP output is still too big. Here, LLMs themselves can help by summarizing retrieved context into something smaller. The tradeoff is compression vs. fidelity: a summary might miss nuance, but the alternative is a model drowning in detail. In practice, we use hybrid designs: raw diffs for high-priority files, summaries for less-critical context.</p>\n</li>\n<li><p><strong>Context prioritization and truncation</strong><br />  Even after pruning and summarizing, you still need to decide what goes first, what can be deferred, and what gets dropped if there isn’t room. Setting a token budget per MCP query is critical, or else prompts will balloon unpredictably. We’ve experimented with truncation-aware designs; sometimes front-loading summaries for quick orientation, other times end-loading detail for deep dives. The “right” design depends on the workflow and the model’s feedback loop.</p>\n</li>\n<li><p><strong>Context quarantining</strong><br />  Not every piece of context belongs in the first prompt. Subtasks should carry their own dedicated context threads, so the model sees exactly what it needs when it needs it. For example, in our MCP client, test failures live in a dedicated review sub-thread rather than clogging the main review context. This approach reduces confusion and helps preserve clarity across long interactions.</p>\n</li>\n<li><p><strong>Iteration and learning</strong><br />  Context engineering isn’t static. We use model feedback and human-in-the-loop corrections to tune priorities over time. Observability is key: logging actual prompt inputs, broken down per module, lets us see what’s getting through and what’s wasted. Tooling like MCP dashboards or token heatmaps can highlight where budgets are blown or irrelevant inputs are sneaking in.</p>\n</li>\n</ul>\n<h2 id=\"heading-anti-patterns-to-avoid-with-mcp-servers-amp-clients\"><strong>Anti-patterns to avoid with MCP servers &amp; clients</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123985534/b1a3acb1-5c02-4d88-b3d0-883aeb0dd573.png\" alt class=\"image--center mx-auto\" /></p>\n<p>The MCP era makes context retrieval easy. Maybe <em>too</em> easy. A couple of common anti-patterns are worth calling out:</p>\n<ul>\n<li><p><strong>Blind vector stuffing</strong><br />  Vector databases are great at surfacing “relevant” chunks of information, but treating them as an oracle is a recipe for trouble. Stuffing in every vaguely related snippet means you get reviews full of tangents: comments about files that weren’t touched, or nitpicks based on stale code. Context irrelevance doesn’t just waste tokens — it actively drags down model performance by pulling attention away from the real task.</p>\n</li>\n<li><p><strong>“Just give it everything”</strong><br />  The brute-force approach: dump every log, diff, and docstring into the context window and pray. This guarantees high costs, long latencies, and unpredictable results. The model can’t tell which parts are critical and which are fluff, so you end up with bloated reviews that read like they were written by an overeager intern trying to cover every angle. Worse, when contradictions sneak in, the model hedges or hallucinates to reconcile them.</p>\n</li>\n</ul>\n<p>In short: more context isn’t always better. Without filtering, prioritization, and careful design, “everything” quickly turns into noise that makes the system slower, dumber, and more expensive.</p>\n<h2 id=\"heading-the-approach-we-took-with-our-mcp-client\">The approach we took with our MCP client</h2>\n<p>In the MCP era, context is king. But let’s be honest: sometimes it’s a king that’s had one too many and can’t tell up from down. The challenge isn’t getting context anymore; it’s taming it. Great context engineering requires careful transformation pipelines, ruthless prioritization, and the humility to keep iterating. Done poorly, you get token bloat, latency, and reviews that sound confused. Done well, you get sharper outputs that scale with your workflow.</p>\n<p>We’ve seen this firsthand in our own MCP client for code reviews. When testing, we initially passed full logs and entire file sets straight through. The result? Expensive reviews that rambled more than they helped. Once we introduced deduplication, summarization, and task-specific quarantining, review quality jumped. Instead of commenting on everything, the model zeroed in on real cross-file risks,  while token use and latency both dropped.</p>\n<p>That’s the payoff of good context engineering: reviews that feel informed, not bloated. And that’s what we’re building toward with our MCP client.</p>\n<p><strong><em>👉 Ready to see context done right? Start your</em></strong> <a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong><em>14-day trial</em></strong></a> <strong><em>of our AI code reviews.</em></strong></p>\n",
      "summary": "Once upon a time, getting context into an LLM meant stringing together hacks, prayers, vector strategies, and overly complex RAG pipelines. Then came the Model Context Protocol (MCP), a clean, modular way to serve external data to models in production. It quickly became the protocol of choice for anyone building agentic systems that are trying to actually do things.\nEvery tech company is now launching MCP functionalities – and for good reason. MCP separates context logic from application logic, improves reliability, and helps tame the chaos of prompt construction in complex workflows.\nWe’ve been deep in the context engineering space for a while, and as we launch our own MCP client, we’re genuinely excited by how it lets us inject richer context into our code reviews. But let’s be honest: with great context comes great risk. Because here’s the dirty secret of the MCP era: most of us are now drowning in the context we used to beg for. More logs, more traces, more diffs, more \"relevant\" files and way less clarity about what the model actually needs.\nWhat starts as helpful input quickly turns into token bloat, noise, and degradation in model performance. Think hallucinations with citations, latency spikes, or reviews that read like they were written by an over-caffeinated intern who rambles. Good context engineering isn’t about cramming in everything, it’s also about knowing what to leave out. And in the aftermath of MCP, that balance is harder (and more important) than ever.\nIn this article, we’ll break down the ballooning context problem, what happens when well-intentioned context goes rogue, and how we’re tackling it head-on. If you’re shipping LLM-based features with MCP and want to avoid accidentally building a prompt-shaped black hole, this blog is for you.\nThe “Ballooning Context Problem” with MCP clients & servers\n\nMCP servers and clients make it easy to hand models a firehose of information: logs, traces, diffs, configs, tickets, and sometimes even that dusty corner of the repo nobody remembers owning. It’s all right there at the model’s fingertips. But here’s the question: is more context always better? Definitely not!\nToo much context is like cramming for an exam by reading the entire library. You end up with noise, not knowledge. And when context goes unchecked, three problems show up fast:\nToken bloat. LLMs don’t have infinite stomachs. Input windows are expensive and finite, and stuffing them full of “just in case” details means higher costs, slower throughput, and wasted budget on irrelevant text.\nRelevance decay. More information doesn’t mean better outputs. In fact, it often means worse. Irrelevant or redundant snippets dilute the signal, and the model starts chasing tangents instead of insights.\nLatency. Every extra log, diff, or stack trace has to be fetched, processed, and shoved into the prompt. Context building becomes the bottleneck, dragging review speed down to a crawl.\nIn short, ballooning context turns the elegance of MCP into a liability. Without deliberate context engineering, the very thing meant to sharpen outputs can just as easily smother them.\nWhen context hurts\n\nIn practice, we see three common pathologies:\nContext confusion. This happens when the model latches onto irrelevant detail and treats it as signal. Imagine a pull request that updates authentication logic but the context dump also includes unrelated test fixtures. The model might start reviewing the fixtures instead, producing comments that feel informed but have nothing to do with the actual change.\nContext clash. Not all context agrees with itself. Suppose a code review includes both the latest schema migration and an outdated docstring that contradicts it. The model now has to “choose” which source to trust. Often, it hedges, producing muddled reviews that cover every angle without real confidence: the LLM equivalent of a reviewer who can’t commit.\nContext poisoning. The most insidious case is when bad information makes it into the context. A hallucinated “related file” or a mis-indexed snippet gets injected, and suddenly the model is citing non-existent code. In a review, that looks like a comment about a bug in a file that doesn’t exist, confusing developers, wasting time, and eroding trust.\nAnd it’s not just code reviews. The same pitfalls show up anywhere context gets overstuffed: customer support bots pulling in irrelevant tickets, research assistants distracted by tangential papers, or security agents treating noisy logs as hard evidence. In each case, the wrong context is worse than no context at all.\nKey patterns to combat context overload with MCP servers\n\nIf the problem of the MCP era is ballooning context, the solution isn’t to stop piping in information — it’s to curate, compress, and serve it with intent. MCP context should be treated as raw material that goes through a well-designed data transformation process before it ever reaches the model. For our own MCP client for code reviews, we’ve leaned on a set of patterns that keep context high-signal and low-noise.\nContext deduplication and differencing\n  Redundant inputs are the fastest way to waste tokens. Identical stack traces, repeated log lines, or unchanged sections of a diff don’t need to appear ten times. Our client identifies duplicates, collapses them, and highlights only what’s new. The same principle applies in other domains: collapse duplicate customer tickets, compress recurring traces, and reduce context to delta rather than bulk.\nContext summarization pipelines\n  Sometimes raw MCP output is still too big. Here, LLMs themselves can help by summarizing retrieved context into something smaller. The tradeoff is compression vs. fidelity: a summary might miss nuance, but the alternative is a model drowning in detail. In practice, we use hybrid designs: raw diffs for high-priority files, summaries for less-critical context.\nContext prioritization and truncation\n  Even after pruning and summarizing, you still need to decide what goes first, what can be deferred, and what gets dropped if there isn’t room. Setting a token budget per MCP query is critical, or else prompts will balloon unpredictably. We’ve experimented with truncation-aware designs; sometimes front-loading summaries for quick orientation, other times end-loading detail for deep dives. The “right” design depends on the workflow and the model’s feedback loop.\nContext quarantining\n  Not every piece of context belongs in the first prompt. Subtasks should carry their own dedicated context threads, so the model sees exactly what it needs when it needs it. For example, in our MCP client, test failures live in a dedicated review sub-thread rather than clogging the main review context. This approach reduces confusion and helps preserve clarity across long interactions.\nIteration and learning\n  Context engineering isn’t static. We use model feedback and human-in-the-loop corrections to tune priorities over time. Observability is key: logging actual prompt inputs, broken down per module, lets us see what’s getting through and what’s wasted. Tooling like MCP dashboards or token heatmaps can highlight where budgets are blown or irrelevant inputs are sneaking in.\nAnti-patterns to avoid with MCP servers & clients\n\nThe MCP era makes context retrieval easy. Maybe too easy. A couple of common anti-patterns are worth calling out:\nBlind vector stuffing\n  Vector databases are great at surfacing “relevant” chunks of information, but treating them as an oracle is a recipe for trouble. Stuffing in every vaguely related snippet means you get reviews full of tangents: comments about files that weren’t touched, or nitpicks based on stale code. Context irrelevance doesn’t just waste tokens — it actively drags down model performance by pulling attention away from the real task.\n“Just give it everything”\n  The brute-force approach: dump every log, diff, and docstring into the context window and pray. This guarantees high costs, long latencies, and unpredictable results. The model can’t tell which parts are critical and which are fluff, so you end up with bloated reviews that read like they were written by an overeager intern trying to cover every angle. Worse, when contradictions sneak in, the model hedges or hallucinates to reconcile them.\nIn short: more context isn’t always better. Without filtering, prioritization, and careful design, “everything” quickly turns into noise that makes the system slower, dumber, and more expensive.\nThe approach we took with our MCP client\nIn the MCP era, context is king. But let’s be honest: sometimes it’s a king that’s had one too many and can’t tell up from down. The challenge isn’t getting context anymore; it’s taming it. Great context engineering requires careful transformation pipelines, ruthless prioritization, and the humility to keep iterating. Done poorly, you get token bloat, latency, and reviews that sound confused. Done well, you get sharper outputs that scale with your workflow.\nWe’ve seen this firsthand in our own MCP client for code reviews. When testing, we initially passed full logs and entire file sets straight through. The result? Expensive reviews that rambled more than they helped. Once we introduced deduplication, summarization, and task-specific quarantining, review quality jumped. Instead of commenting on everything, the model zeroed in on real cross-file risks,  while token use and latency both dropped.\nThat’s the payoff of good context engineering: reviews that feel informed, not bloated. And that’s what we’re building toward with our MCP client.\n👉 Ready to see context done right? Start your 14-day trial of our AI code reviews.",
      "publishedAt": "2025-09-17T15:57:17.000Z",
      "author": "Tommy Elizaga",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "testing",
        "observability"
      ],
      "ingestedAt": "2025-11-23T17:37:39.984Z",
      "score": 0.21597403940177412
    },
    {
      "id": "7168006c47b6a9d86cdeb3c2afc3f88c",
      "title": "CodeRabbit CLI - 無料で使えるCLIのAIコードレビュー",
      "url": "https://coderabbit.ai/blog/coderabbit-cli-free-ai-code-reviews-in-your-cli-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/coderabbit-cli-free-ai-code-reviews-in-your-cli\">CodeRabbit CLI - Free AI code reviews in your CLI</a>の意訳です。</p>\n<p>CodeRabbitは、PRにおけるAIコードレビューから始まりました。5月には、そのインテリジェンスを<a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/ai-code-reviews-vscode-cursor-windsurf\">VS Code、Cursor、Windsurf</a>に拡張。そして今、開発者に愛されるAIコードレビューをコマンドラインにまで広げる「CodeRabbit CLI」を発表します。つまり、私たちは最も包括的なAIコードレビューツールになったのです。あなたが働く場所なら、どこでも動作します。</p>\n<p><em>CodeRabbit CLI</em>は、開発者がターミナルで直接セルフレビューを行えるようにします。自動化されたインテリジェントなコード分析機能を提供し、問題の早期発見と一貫したコード規約を維持し、CLI内でAIコーディングエージェントとシームレスな統合によって自律的なコーディングを実現します。</p>\n<h2 id=\"heading-vibe-cli\"><strong>コードの「Vibeチェック」― CLIでも</strong></h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/IqBKf4u5MtA\">https://youtu.be/IqBKf4u5MtA</a></div>\n<p> </p>\n<p>CodeRabbit CLIは、PRやIDEレビューと同じ包括的な分析を提供し、バグの早期発見に役立ちます。CodeRabbit CLIはレート制限付きで無料利用できますが、Proプランでは制限が大幅に緩和され、さらに以下のような追加機能を利用できます。</p>\n<ul>\n<li><p><strong>コンテキスト対応分析</strong>: Git連携を活用し、静的解析ツールやセキュリティスキャナ、コードグラフの関係性機能など40以上の情報源を統合して、最も包括的なレビューを実現</p>\n</li>\n<li><p><strong>プレコミットレビュー</strong>: マシンを離れる前に変更を分析し、多層的なレビューを提供</p>\n</li>\n<li><p><strong>ワンクリック修正</strong>: 簡単な修正は即適用、複雑な問題はAIエージェントに完全なコンテキスト付きで引き渡し</p>\n</li>\n<li><p><strong>コーディング規約検出</strong>: <a target=\"_blank\" href=\"http://agent.md\">agent.md</a>、<a target=\"_blank\" href=\"http://claude.md\">claude.md</a>、Cursor rulesなどのコーディングエージェント設定ファイルを自動検出</p>\n</li>\n</ul>\n<h2 id=\"heading-coderabbit-cli\"><strong>CodeRabbit CLI: どこでも、なんでも動作</strong></h2>\n<p>ターミナルネイティブであるため、CodeRabbit CLIは以下に対応します。</p>\n<ul>\n<li><p><strong>あらゆるターミナルアプリ/IDE:</strong> iTerm2、Ghostty、Neovim、Lazyvim</p>\n</li>\n<li><p><strong>あらゆるAIコーディングCLIエージェント:</strong> Claude Code、Codex、Cursor、Gemini、OpenCodeなど</p>\n</li>\n</ul>\n<h2 id=\"heading-aicli\"><strong>AIコーディングエージェントCLIとの使い方</strong></h2>\n<p>CodeRabbit CLIはAIコーディングエージェントとの新しい統合の可能性を広げます。Claude Codeとの動作例は以下の通りです。</p>\n<ol>\n<li>コーディングタスクを進める際、Claude CodeにCodeRabbitを使って発見された問題を修正するよう促すことができます。PRDやタスクリストからコーディングする場合に特に便利です。</li>\n</ol>\n<pre><code class=\"lang-plaintext\">仕様書のフェーズ7.3を実施し、その後に `coderabbit --prompt-only`を実行してください。\n必要なだけバックグラウンドにて実行し、発生した問題を修正してください。\n</code></pre>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971361399/2f4398b2-4a89-4fd3-9d1a-3baf00a0fe6e.png\" alt class=\"image--center mx-auto\" /></p>\n<ol start=\"2\">\n<li>Claude Codeはコーディングタスクを進めながら、バックグラウンドで<code>coderabbit --prompt-only</code>を実行します。タイマーを設定してCodeRabbitを定期的に確認する場合もあります。あるいは、ClaudeにCodeRabbitの完了を確認するよう促すこともできます。</li>\n</ol>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971331440/fef10dc6-b3a7-4da7-935c-41c06c95f04a.png\" alt class=\"image--center mx-auto\" /></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971376053/33dda5e8-8ede-4239-b363-16b491fe5a02.png\" alt=\"Claude Code内でバックグラウンド実行されるCodeRabbit\" class=\"image--center mx-auto\" /></p>\n<ol start=\"3\">\n<li>その後、Claude CodeはCodeRabbitの出力を読み込みます。<code>--prompt-only</code>フラグを使うことで、AIエージェントが読み取れるプレーンテキストで出力されます。ClaudeはCodeRabbitが検出した問題ごとにタスクリストを作成します。</li>\n</ol>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971316135/ebe0c883-be63-4e77-9e67-234694047ea0.png\" alt class=\"image--center mx-auto\" /></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971352863/2003e70c-4ad0-4435-95fb-df2a0885b9f7.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Claude Codeとの統合や自動化ワークフローについては、<a target=\"_blank\" href=\"https://docs.coderabbit.ai/cli\">CLIドキュメント</a>をご覧ください。</p>\n<p>CLIにはインタラクティブモードとプレーンレスポンスモードの2種類があり、自動化ワークフローへの統合や他ツールへの結果の受け渡しが容易です。</p>\n<h2 id=\"heading-kirjgajgzjjgohmlrkqkg\"><strong>はじめ方</strong></h2>\n<p>CodeRabbit CLIはすでに利用可能です。インストールして最初のレビューを試してみましょう。</p>\n<pre><code class=\"lang-powershell\"><span class=\"hljs-comment\"># CodeRabbitをインストール</span>\n<span class=\"hljs-built_in\">curl</span> <span class=\"hljs-literal\">-fsSL</span> https://cli.coderabbit.ai/install.sh | sh\n\n<span class=\"hljs-comment\"># インタラクティブモードでレビュー実行</span>\ncoderabbit\n</code></pre>\n",
      "summary": "CodeRabbit CLI - Free AI code reviews in your CLIの意訳です。\nCodeRabbitは、PRにおけるAIコードレビューから始まりました。5月には、そのインテリジェンスをVS Code、Cursor、Windsurfに拡張。そして今、開発者に愛されるAIコードレビューをコマンドラインにまで広げる「CodeRabbit CLI」を発表します。つまり、私たちは最も包括的なAIコードレビューツールになったのです。あなたが働く場所なら、どこでも動作します。\nCodeRabbit CLIは、開発者がターミナルで直接セルフレビューを行えるようにします。自動化されたインテリジェントなコード分析機能を提供し、問題の早期発見と一貫したコード規約を維持し、CLI内でAIコーディングエージェントとシームレスな統合によって自律的なコーディングを実現します。\nコードの「Vibeチェック」― CLIでも\n\n\n\nhttps://youtu.be/IqBKf4u5MtA\n \nCodeRabbit CLIは、PRやIDEレビューと同じ包括的な分析を提供し、バグの早期発見に役立ちます。CodeRabbit CLIはレート制限付きで無料利用できますが、Proプランでは制限が大幅に緩和され、さらに以下のような追加機能を利用できます。\nコンテキスト対応分析: Git連携を活用し、静的解析ツールやセキュリティスキャナ、コードグラフの関係性機能など40以上の情報源を統合して、最も包括的なレビューを実現\nプレコミットレビュー: マシンを離れる前に変更を分析し、多層的なレビューを提供\nワンクリック修正: 簡単な修正は即適用、複雑な問題はAIエージェントに完全なコンテキスト付きで引き渡し\nコーディング規約検出: agent.md、claude.md、Cursor rulesなどのコーディングエージェント設定ファイルを自動検出\nCodeRabbit CLI: どこでも、なんでも動作\nターミナルネイティブであるため、CodeRabbit CLIは以下に対応します。\nあらゆるターミナルアプリ/IDE: iTerm2、Ghostty、Neovim、Lazyvim\nあらゆるAIコーディングCLIエージェント: Claude Code、Codex、Cursor、Gemini、OpenCodeなど\nAIコーディングエージェントCLIとの使い方\nCodeRabbit CLIはAIコーディングエージェントとの新しい統合の可能性を広げます。Claude Codeとの動作例は以下の通りです。\nコーディングタスクを進める際、Claude CodeにCodeRabbitを使って発見された問題を修正するよう促すことができます。PRDやタスクリストからコーディングする場合に特に便利です。\n仕様書のフェーズ7.3を実施し、その後に `coderabbit --prompt-only`を実行してください。\n必要なだけバックグラウンドにて実行し、発生した問題を修正してください。\n\n\nClaude Codeはコーディングタスクを進めながら、バックグラウンドでcoderabbit --prompt-onlyを実行します。タイマーを設定してCodeRabbitを定期的に確認する場合もあります。あるいは、ClaudeにCodeRabbitの完了を確認するよう促すこともできます。\n\n\nその後、Claude CodeはCodeRabbitの出力を読み込みます。--prompt-onlyフラグを使うことで、AIエージェントが読み取れるプレーンテキストで出力されます。ClaudeはCodeRabbitが検出した問題ごとにタスクリストを作成します。\n\n\nClaude Codeとの統合や自動化ワークフローについては、CLIドキュメントをご覧ください。\nCLIにはインタラクティブモードとプレーンレスポンスモードの2種類があり、自動化ワークフローへの統合や他ツールへの結果の受け渡しが容易です。\nはじめ方\nCodeRabbit CLIはすでに利用可能です。インストールして最初のレビューを試してみましょう。\n# CodeRabbitをインストール\ncurl -fsSL https://cli.coderabbit.ai/install.sh | sh\n\n# インタラクティブモードでレビュー実行\ncoderabbit",
      "publishedAt": "2025-09-16T23:02:47.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.984Z",
      "score": 0.0789902436582679
    },
    {
      "id": "84ddd89dd63d7ec87db32898c3930fec",
      "title": "シリーズbにて6,000万ドルの資金調達：AIによるコーディングの品質ゲートを構築",
      "url": "https://coderabbit.ai/blog/coderabbit-series-b-60-million-quality-gates-for-code-reviews-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/coderabbit-series-b-60-million-quality-gates-for-code-reviews\">Raising our $60 million Series B: Quality gates for AI coding</a>の意訳です。</p>\n<p>CodeRabbitを立ち上げたとき、そのコンセプトはシンプルでした。すべての開発者がコードレビューを嫌っているのだから、もっと速く、簡単にできるようにすればいいのでは、ということです。変数名やスタイル規約について、同じコメントを何度も書くのは誰にとっても楽しいことではありません。</p>\n<p>そこでAIが役立つと考えました。ベストプラクティスのチェックやルールの適用を自動化すれば、開発者自身がやる必要がなくなるのです。そしてさらに重要なのは、AIがセーフティネットとして機能し、本番環境に入る前に問題やバグを検知できることです。</p>\n<p>その信念のもと、私たちは<a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/coderabbit-announces-16m-series-a-funding-led-by-crv\">AIコードレビューという、まったく新しいものを作ることに挑戦しました。</a> その後、AIコーディングツールは広く普及し始めました。Copilot、Claude Code、Cursorといったツールは、開発チームが容易にレビューできる以上のコードを生成するようになり、多くの開発者がPR数を2倍から3倍に増やしました。これにより、すでに抱えていたコードレビューのバックログはさらに増加。私たちはすぐに気づきました。「効率化」と宣伝されていたものが、やがてレビューのボトルネックになることを。</p>\n<p>そこではじめて理解したのです。AIコードレビューは開発チームにとって極めて重要な存在になると。信頼とガバナンスのレイヤーとして機能し、品質とセキュリティを担保しながら、開発者の時間を節約します。そしてボーナスとして、職場での皮肉混じりのレビューコメントも大幅に減らせるのです。</p>\n<h2 id=\"heading-ai2025\"><strong>AIコードレビューが必須となった2025年</strong></h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/UHCTKZYOOYU\">https://youtu.be/UHCTKZYOOYU</a></div>\n<p> </p>\n<p>過去2年間で、私たちは最も包括的かつコンテキストを重視したコードレビュープラットフォームを構築し、200万のリポジトリに導入され、1,300万件のPRをレビューしました。GitHubとGitLabの両方で最もインストールされたAIアプリとなり、数えきれないほどの開発チームの士気を向上させてきました。</p>\n<p>そして2025年、AIコードレビューは、AIコーディングエージェントの普及に伴う課題に直面するすべてのチームにとって必須のものとなっています。この変化は前例のない成長を引き起こし、本日発表した<strong>6,000万ドルのシリーズB資金調達</strong>につながりました。</p>\n<p>今回の投資は<strong>Scale Venture Partners</strong>が主導し、<strong>NVentures（NVIDIAのベンチャーキャピタル部門）が参加。長年の投資家であるCRV、Harmony Partners、Flex Capital、Engineering Capital、Pelion Venture Partners</strong>も支援してくれました。今回の資金調達により、私たちが調達した累計資金は8,800万ドルになりました。</p>\n<h2 id=\"heading-ai\"><strong>なぜ多くのチームがAIコードレビューを導入しているのか</strong></h2>\n<p>チームのすべての開発者がコードをより速く生成するようになると、レビュー待ちのキューは指数関数的に増えます。以前は1日5〜10件のPRをレビューしていたシニアエンジニアが、今では20〜30件を抱えています。計算が合いません。チームは2つの悪い選択肢に直面します。デプロイを遅らせて丁寧にレビューするか、レビューを急いで品質を犠牲にするか。</p>\n<p>だからこそ、AIコードレビューの導入は加速しています。AIレビュアーは人間のレビュアーを補完し、アーキテクチャの判断やビジネスロジック、AIが完全には理解できない文脈を必要とするフィードバックに集中できるようにします。</p>\n<p>この1年は嵐のようでした。売上は10倍になり、チームも倍増しました。その背景には以下の要因があります。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757987176453/46d676d8-f560-4cf9-ba1a-54f2dd91d960.png\" alt class=\"image--center mx-auto\" /></p>\n<p>それぞれの顧客の背後には実際のチームがいて、同じことを感じています。つまり、CodeRabbitでレビューが速くなってバグが早期に発見され、リリースサイクルが再び加速しているということです。</p>\n<p>Grouponは、レビューから本番リリースまでにかかっていた時間が86時間からわずか39分に短縮されたと報告しました。別の企業は、コードレビューに費やす時間を70％削減できたと共有してくれています。</p>\n<h2 id=\"heading-coderabbit\">CodeRabbitの仕組み</h2>\n<p>CodeRabbitは「AIの炎にAIで立ち向かう」からこそ機能します。多数のコンテキスト情報を取り込み、最も文脈に沿ったレビューを提供します。</p>\n<ul>\n<li><p>本番前に正確性やセキュリティの問題を検知</p>\n<p>  <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758028469061/6a3c1175-2500-440b-995a-785d1d3e1234.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><p>組織のベストプラクティスや独自ルールの適用</p>\n<p>  <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758027905155/0990dc8f-10d7-4765-88f9-e3023b476193.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><p>マージサイクル全体をサポート（ユニットテストやdocstring生成など）</p>\n</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757987618430/0854e5be-e8dd-456a-8049-c73526d54f74.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-coderabbit-cli\">調達を祝して：CodeRabbit CLIの発表</h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/IqBKf4u5MtA\">https://youtu.be/IqBKf4u5MtA</a></div>\n<p> </p>\n<p>本日、シリーズBの発表を記念して、CodeRabbit CLIを発表します。これはターミナル上で動作するAIコードレビューで、Claude Code、Codex CLI、Cursor CLI、GeminiなどのAIコーディングエージェントとシームレスに連携します。</p>\n<p>開発者がCLIベースのコーディングエージェントを使ってコードを書くケースが増える中、私たちは大きなギャップを特定しました。コードはかつてない速度で生成されていますが、品質検証が行われるのは遅く、PRの段階になってからということが多いのです。</p>\n<p>CodeRabbit CLIはこれを変えます。CLIワークフローに直接インテリジェントなレビューを組み込み、コード生成と品質検証の間にリアルタイムのフィードバックループを作り出します。</p>\n<p>モジュールをリファクタリングするようClaude Codeに依頼しても、Cursor CLIで新機能を実装しても、CodeRabbitは即座にその結果をレビューし、ハルシネーション生成を検知し、セキュリティ問題にフラグを立て、AIエージェントに文脈に沿った修正を返すことさえできます。</p>\n<p>CodeRabbit CLIは、AI生成コードを本番レベルに引き上げるために欠けていたオーケストレーションレイヤーであり、自律的な開発の実現を可能にします。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757974301767/02dc9e2f-1d4d-4308-a424-231c982850ea.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-kirku4rlm57jga7os4fph5hoqrpgztjgyzmhilkbpjgznjgovjgoljga7vvijjgyljgarjgzjgavjgajjgapjgabjgolvvikqkg\"><strong>今回の資金調達が意味するもの（あなたにとっても）</strong></h2>\n<p>シリーズBで調達した資金は、私たちが解決すべき課題のスケールに合わせて成長を続けるために使われます。投資先は以下の通りです。</p>\n<ul>\n<li><p><strong>製品開発の加速：</strong> コンテキスト統合の強化、よりスマートなマージ前チェック、自動テストなど、ロードマップは満載です。レビューをより速く、正確で、有用にすることに集中します。</p>\n</li>\n<li><p><strong>オープンソースの支援：</strong> 現在、すでに10万以上のOSSプロジェクトがCodeRabbitを利用しています。この資金で、貢献や支援をさらに強化し、現代的な開発を可能にしたコミュニティをサポートします。詳細は今週後半に！</p>\n</li>\n<li><p><strong>優秀な人材の採用：</strong> 今年だけで従業員数を倍増させました。今後はエンジニアリング、プロダクト、セールス、マーケティング、カスタマーサクセスの分野でグローバルに採用を進めます。</p>\n</li>\n</ul>\n<p>この資金調達により、私たちが「AI駆動開発における最も重要な欠けているピース」だと信じている、スケーラブルで文脈に対応したレビューを構築し続ける余地が生まれました。</p>\n<h2 id=\"heading-kirjgztmlkmj7tjgyljgorjgyzjgajjgybjgztjgzbjgytjgb7jgzkqkg\"><strong>ご支援ありがとうございます</strong></h2>\n<p>この会社を始めたとき、私たちはすべてのエンジニアが経験する課題に挑戦していることを理解していました。レビューは面倒で、簡単にはスケールしません。その課題にCodeRabbitが今や数千のチームを支援できていることは、謙虚であると同時に大きな力を与えてくれます。</p>\n<p>顧客、コミュニティ、投資家の皆さまへ：私たちを信じ、一緒に築いてくださりありがとうございます。そしてこの取り組みにワクワクする方は、ぜひ私たちに加わってください。コードレビューの未来を一緒に作りましょう。</p>\n<p><a target=\"_blank\" href=\"https://coderabbit.link/KWGzOUS\"><strong><em>CodeRabbitを無料で試す</em></strong></a> <strong><em>そして</em></strong> <a target=\"_blank\" href=\"https://coderabbit.link/OG1OZk3\"><strong><em>採用情報はこちら</em></strong></a></p>\n",
      "summary": "Raising our $60 million Series B: Quality gates for AI codingの意訳です。\nCodeRabbitを立ち上げたとき、そのコンセプトはシンプルでした。すべての開発者がコードレビューを嫌っているのだから、もっと速く、簡単にできるようにすればいいのでは、ということです。変数名やスタイル規約について、同じコメントを何度も書くのは誰にとっても楽しいことではありません。\nそこでAIが役立つと考えました。ベストプラクティスのチェックやルールの適用を自動化すれば、開発者自身がやる必要がなくなるのです。そしてさらに重要なのは、AIがセーフティネットとして機能し、本番環境に入る前に問題やバグを検知できることです。\nその信念のもと、私たちはAIコードレビューという、まったく新しいものを作ることに挑戦しました。 その後、AIコーディングツールは広く普及し始めました。Copilot、Claude Code、Cursorといったツールは、開発チームが容易にレビューできる以上のコードを生成するようになり、多くの開発者がPR数を2倍から3倍に増やしました。これにより、すでに抱えていたコードレビューのバックログはさらに増加。私たちはすぐに気づきました。「効率化」と宣伝されていたものが、やがてレビューのボトルネックになることを。\nそこではじめて理解したのです。AIコードレビューは開発チームにとって極めて重要な存在になると。信頼とガバナンスのレイヤーとして機能し、品質とセキュリティを担保しながら、開発者の時間を節約します。そしてボーナスとして、職場での皮肉混じりのレビューコメントも大幅に減らせるのです。\nAIコードレビューが必須となった2025年\n\n\n\nhttps://youtu.be/UHCTKZYOOYU\n \n過去2年間で、私たちは最も包括的かつコンテキストを重視したコードレビュープラットフォームを構築し、200万のリポジトリに導入され、1,300万件のPRをレビューしました。GitHubとGitLabの両方で最もインストールされたAIアプリとなり、数えきれないほどの開発チームの士気を向上させてきました。\nそして2025年、AIコードレビューは、AIコーディングエージェントの普及に伴う課題に直面するすべてのチームにとって必須のものとなっています。この変化は前例のない成長を引き起こし、本日発表した6,000万ドルのシリーズB資金調達につながりました。\n今回の投資はScale Venture Partnersが主導し、NVentures（NVIDIAのベンチャーキャピタル部門）が参加。長年の投資家であるCRV、Harmony Partners、Flex Capital、Engineering Capital、Pelion Venture Partnersも支援してくれました。今回の資金調達により、私たちが調達した累計資金は8,800万ドルになりました。\nなぜ多くのチームがAIコードレビューを導入しているのか\nチームのすべての開発者がコードをより速く生成するようになると、レビュー待ちのキューは指数関数的に増えます。以前は1日5〜10件のPRをレビューしていたシニアエンジニアが、今では20〜30件を抱えています。計算が合いません。チームは2つの悪い選択肢に直面します。デプロイを遅らせて丁寧にレビューするか、レビューを急いで品質を犠牲にするか。\nだからこそ、AIコードレビューの導入は加速しています。AIレビュアーは人間のレビュアーを補完し、アーキテクチャの判断やビジネスロジック、AIが完全には理解できない文脈を必要とするフィードバックに集中できるようにします。\nこの1年は嵐のようでした。売上は10倍になり、チームも倍増しました。その背景には以下の要因があります。\n\nそれぞれの顧客の背後には実際のチームがいて、同じことを感じています。つまり、CodeRabbitでレビューが速くなってバグが早期に発見され、リリースサイクルが再び加速しているということです。\nGrouponは、レビューから本番リリースまでにかかっていた時間が86時間からわずか39分に短縮されたと報告しました。別の企業は、コードレビューに費やす時間を70％削減できたと共有してくれています。\nCodeRabbitの仕組み\nCodeRabbitは「AIの炎にAIで立ち向かう」からこそ機能します。多数のコンテキスト情報を取り込み、最も文脈に沿ったレビューを提供します。\n本番前に正確性やセキュリティの問題を検知\n  \n組織のベストプラクティスや独自ルールの適用\n  \nマージサイクル全体をサポート（ユニットテストやdocstring生成など）\n\n調達を祝して：CodeRabbit CLIの発表\n\n\n\nhttps://youtu.be/IqBKf4u5MtA\n \n本日、シリーズBの発表を記念して、CodeRabbit CLIを発表します。これはターミナル上で動作するAIコードレビューで、Claude Code、Codex CLI、Cursor CLI、GeminiなどのAIコーディングエージェントとシームレスに連携します。\n開発者がCLIベースのコーディングエージェントを使ってコードを書くケースが増える中、私たちは大きなギャップを特定しました。コードはかつてない速度で生成されていますが、品質検証が行われるのは遅く、PRの段階になってからということが多いのです。\nCodeRabbit CLIはこれを変えます。CLIワークフローに直接インテリジェントなレビューを組み込み、コード生成と品質検証の間にリアルタイムのフィードバックループを作り出します。\nモジュールをリファクタリングするようClaude Codeに依頼しても、Cursor CLIで新機能を実装しても、CodeRabbitは即座にその結果をレビューし、ハルシネーション生成を検知し、セキュリティ問題にフラグを立て、AIエージェントに文脈に沿った修正を返すことさえできます。\nCodeRabbit CLIは、AI生成コードを本番レベルに引き上げるために欠けていたオーケストレーションレイヤーであり、自律的な開発の実現を可能にします。\n\n今回の資金調達が意味するもの（あなたにとっても）\nシリーズBで調達した資金は、私たちが解決すべき課題のスケールに合わせて成長を続けるために使われます。投資先は以下の通りです。\n製品開発の加速： コンテキスト統合の強化、よりスマートなマージ前チェック、自動テストなど、ロードマップは満載です。レビューをより速く、正確で、有用にすることに集中します。\nオープンソースの支援： 現在、すでに10万以上のOSSプロジェクトがCodeRabbitを利用しています。この資金で、貢献や支援をさらに強化し、現代的な開発を可能にしたコミュニティをサポートします。詳細は今週後半に！\n優秀な人材の採用： 今年だけで従業員数を倍増させました。今後はエンジニアリング、プロダクト、セールス、マーケティング、カスタマーサクセスの分野でグローバルに採用を進めます。\nこの資金調達により、私たちが「AI駆動開発における最も重要な欠けているピース」だと信じている、スケーラブルで文脈に対応したレビューを構築し続ける余地が生まれました。\nご支援ありがとうございます\nこの会社を始めたとき、私たちはすべてのエンジニアが経験する課題に挑戦していることを理解していました。レビューは面倒で、簡単にはスケールしません。その課題にCodeRabbitが今や数千のチームを支援できていることは、謙虚であると同時に大きな力を与えてくれます。\n顧客、コミュニティ、投資家の皆さまへ：私たちを信じ、一緒に築いてくださりありがとうございます。そしてこの取り組みにワクワクする方は、ぜひ私たちに加わってください。コードレビューの未来を一緒に作りましょう。\nCodeRabbitを無料で試す そして 採用情報はこちら",
      "publishedAt": "2025-09-16T22:46:14.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "funding_mna",
      "tags": [
        "code_review",
        "documentation"
      ],
      "ingestedAt": "2025-11-23T17:37:39.984Z",
      "score": 0.08681796706726666
    },
    {
      "id": "7d456380d0e7c97c41fc0a4ae5513a5d",
      "title": "CodeRabbit CLI - Free AI code reviews in your CLI",
      "url": "https://coderabbit.ai/blog/coderabbit-cli-free-ai-code-reviews-in-your-cli",
      "content": "<p>CodeRabbit started with AI-powered code reviews in pull requests. In May, we brought that same intelligence to <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/ai-code-reviews-vscode-cursor-windsurf\">VS Code, Cursor, and Windsurf</a>. Now, we're extending the AI code reviews developers love into the command line with CodeRabbit CLI. In case you’re wondering, that makes us the most comprehensive AI code review tool available. We work everywhere you work.</p>\n<p><em>CodeRabbit CLI</em> helps devs perform self-reviews of code directly in their terminal. By providing automated, intelligent code analysis capabilities, it empowers developers to catch issues early, maintain consistent code standards, and make coding autonomous through seamless integration with AI coding agents in the CLI.</p>\n<h2 id=\"heading-vibe-checking-your-code-now-in-cli\"><strong>Vibe checking your code – now in CLI</strong></h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/IqBKf4u5MtA\">https://youtu.be/IqBKf4u5MtA</a></div>\n<p> </p>\n<p>CodeRabbit CLI delivers the same comprehensive analysis that makes our PR and IDE reviews effective at catching bugs early. CodeRabbit CLI is free to use with rate limits but with a Pro plan you can enjoy much higher limits and additional features, including:</p>\n<ul>\n<li><p><strong>Context-aware analysis</strong>: Leverages your Git integration to synthesize insights from 40+ sources including static analysis tools, security scanners, and our codegraph relationship feature for the most comprehensive reviews.</p>\n</li>\n<li><p><strong>Pre-commit reviews</strong>: Analyze changes before they leave your machine for multi-layered reviews.</p>\n</li>\n<li><p><strong>One-click fixes</strong>: Apply simple fixes instantly or send complex issues to AI agents with full context hand-off.</p>\n</li>\n<li><p><strong>Coding guidelines</strong>: Auto-detects agent.md, claude.md, Cursor rules, and other coding agent configuration files.</p>\n</li>\n</ul>\n<h2 id=\"heading-coderabbit-cli-works-everywhere-with-everything\"><strong>CodeRabbit CLI: Works everywhere, with everything</strong></h2>\n<p>Terminal-native means CodeRabbit CLI works with:</p>\n<ul>\n<li><p><strong>Any Terminal App/IDE:</strong> iTerm2, Ghostty, Neovim, Lazyvim</p>\n</li>\n<li><p><strong>Any AI Coding CLI agent</strong>: Claude Code, Codex, Cursor, Gemini, OpenCode and more</p>\n</li>\n</ul>\n<h2 id=\"heading-how-to-use-coderabbit-cli-with-ai-coding-agent-cli\"><strong>How to use CodeRabbit CLI with AI Coding Agent CLI</strong></h2>\n<p>The CodeRabbit CLI opens up new integration possibilities with AI coding agents. Here's how it works with Claude Code:</p>\n<ol>\n<li>While working on a coding task, you can prompt Claude Code to use CodeRabbit and to fix any issues it finds. This is particularly useful if it’s coding from a PRD, or a tasklist.</li>\n</ol>\n<pre><code class=\"lang-plaintext\">Please implement phase 7.3 of the planning doc and then run coderabbit --prompt-only, let it run as long as it needs (run it in the background) and fix any issues.\n</code></pre>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971361399/2f4398b2-4a89-4fd3-9d1a-3baf00a0fe6e.png\" alt class=\"image--center mx-auto\" /></p>\n<p>2. Claude Code will carry on the coding task and run <code>coderabbit --prompt-only</code> in the background. It may setup a timer interval to check on CodeRabbit. Alternatively, you can also prompt Claude to check if CodeRabbit is complete.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971331440/fef10dc6-b3a7-4da7-935c-41c06c95f04a.png\" alt class=\"image--center mx-auto\" /></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971376053/33dda5e8-8ede-4239-b363-16b491fe5a02.png\" alt=\"CodeRabbit running in the background within Claude Code\" class=\"image--center mx-auto\" /></p>\n<p>3. Claude Code will then read the output of CodeRabbit which, by using the <code>--prompt-only</code> flag, provides the output as plain text with prompts for AI agents to read. Claude will then create a tasklist addressing each of the issues surfaced by CodeRabbit.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971316135/ebe0c883-be63-4e77-9e67-234694047ea0.png\" alt class=\"image--center mx-auto\" /></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971352863/2003e70c-4ad0-4435-95fb-df2a0885b9f7.png\" alt class=\"image--center mx-auto\" /></p>\n<p>For Claude Code integration and automated workflows, check the <a target=\"_blank\" href=\"https://docs.coderabbit.ai/cli\">CLI documentation</a> <a target=\"_blank\" href=\"https://docs.coderabbit.ai/cli\">for setup.</a></p>\n<p>The CLI has two modes: interactive and plain response , making it easy to integrate into automated workflows or pass results to other tools.</p>\n<h2 id=\"heading-getting-started\"><strong>Getting started</strong></h2>\n<p>CodeRabbit CLI is available now. Install and try your first review:</p>\n<pre><code class=\"lang-powershell\"><span class=\"hljs-comment\">#install CodeRabbit</span>\n<span class=\"hljs-built_in\">curl</span> <span class=\"hljs-literal\">-fsSL</span> https://cli.coderabbit.ai/install.sh | sh\n\n<span class=\"hljs-comment\">#Run a review in interactive mode</span>\ncoderabbit\n</code></pre>\n",
      "summary": "CodeRabbit started with AI-powered code reviews in pull requests. In May, we brought that same intelligence to VS Code, Cursor, and Windsurf. Now, we're extending the AI code reviews developers love into the command line with CodeRabbit CLI. In case you’re wondering, that makes us the most comprehensive AI code review tool available. We work everywhere you work.\nCodeRabbit CLI helps devs perform self-reviews of code directly in their terminal. By providing automated, intelligent code analysis capabilities, it empowers developers to catch issues early, maintain consistent code standards, and make coding autonomous through seamless integration with AI coding agents in the CLI.\nVibe checking your code – now in CLI\n\n\n\nhttps://youtu.be/IqBKf4u5MtA\n \nCodeRabbit CLI delivers the same comprehensive analysis that makes our PR and IDE reviews effective at catching bugs early. CodeRabbit CLI is free to use with rate limits but with a Pro plan you can enjoy much higher limits and additional features, including:\nContext-aware analysis: Leverages your Git integration to synthesize insights from 40+ sources including static analysis tools, security scanners, and our codegraph relationship feature for the most comprehensive reviews.\nPre-commit reviews: Analyze changes before they leave your machine for multi-layered reviews.\nOne-click fixes: Apply simple fixes instantly or send complex issues to AI agents with full context hand-off.\nCoding guidelines: Auto-detects agent.md, claude.md, Cursor rules, and other coding agent configuration files.\nCodeRabbit CLI: Works everywhere, with everything\nTerminal-native means CodeRabbit CLI works with:\nAny Terminal App/IDE: iTerm2, Ghostty, Neovim, Lazyvim\nAny AI Coding CLI agent: Claude Code, Codex, Cursor, Gemini, OpenCode and more\nHow to use CodeRabbit CLI with AI Coding Agent CLI\nThe CodeRabbit CLI opens up new integration possibilities with AI coding agents. Here's how it works with Claude Code:\nWhile working on a coding task, you can prompt Claude Code to use CodeRabbit and to fix any issues it finds. This is particularly useful if it’s coding from a PRD, or a tasklist.\nPlease implement phase 7.3 of the planning doc and then run coderabbit --prompt-only, let it run as long as it needs (run it in the background) and fix any issues.\n\n\n2. Claude Code will carry on the coding task and run coderabbit --prompt-only in the background. It may setup a timer interval to check on CodeRabbit. Alternatively, you can also prompt Claude to check if CodeRabbit is complete.\n\n\n3. Claude Code will then read the output of CodeRabbit which, by using the --prompt-only flag, provides the output as plain text with prompts for AI agents to read. Claude will then create a tasklist addressing each of the issues surfaced by CodeRabbit.\n\n\nFor Claude Code integration and automated workflows, check the CLI documentation for setup.\nThe CLI has two modes: interactive and plain response , making it easy to integrate into automated workflows or pass results to other tools.\nGetting started\nCodeRabbit CLI is available now. Install and try your first review:\n#install CodeRabbit\ncurl -fsSL https://cli.coderabbit.ai/install.sh | sh\n\n#Run a review in interactive mode\ncoderabbit",
      "publishedAt": "2025-09-16T12:59:39.000Z",
      "author": "Edgar Cerecerez",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.984Z",
      "score": 0.15715723672113213
    },
    {
      "id": "6d1d2623cc15f8a52bd97b6f45cc0c41",
      "title": "Raising our $60M Series B: Building the quality gates for AI-powered coding",
      "url": "https://coderabbit.ai/blog/coderabbit-series-b-60-million-quality-gates-for-code-reviews",
      "content": "<p>When we started CodeRabbit, the idea was pretty simple: since all developers hate code reviews, why not make them faster and easier? After all, no one enjoys leaving the same comment about variable naming practices or style conventions for the tenth time in a week.</p>\n<p>That’s where we believed AI could help – it could automate best-practice checks and policy enforcement so that devs didn’t have to do it themselves. But more importantly, it could act as a safety net, catching issues and bugs before they made it into production.</p>\n<p>With that belief, we set out to <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/coderabbit-announces-16m-series-a-funding-led-by-crv\">create something new: AI code reviews.</a> Over time, AI coding tools started to gain broader adoption. Tools like Copilot, Claude Code, and Cursor began spitting out more code than teams could easily review with many developers increasing the number of PRs they shipped by 2x to 3x. This added to the existing code review backlogs many teams had. We quickly realized that the ‘efficiency’ gains being marketed to engineering teams would swiftly turn into code review bottlenecks.</p>\n<p>And that’s also when we first realized how critical AI code reviews would be to development teams. They would function as a trust and governance layer in agentic software development ensuring quality and security while saving devs time. And, as an added bonus, greatly reducing passive aggressive review comments in the workplace!</p>\n<h2 id=\"heading-ai-code-reviews-became-essential-in-2025\"><strong>AI code reviews became essential in 2025</strong></h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/UHCTKZYOOYU\">https://youtu.be/UHCTKZYOOYU</a></div>\n<p> </p>\n<p>Over the last two years, we’ve built the most comprehensive and context-rich platform for code reviews, been installed on 2 million repos, reviewed 13 million pull requests, become the most installed AI App on both GitHub and GitLab, and improved the morale of countless dev teams.</p>\n<p>In 2025, we watched AI code reviews become essential for all teams dealing with the challenges that come with the broad adoption of AI coding agents. But that shift fueled a year of unprecedented growth, culminating in the <strong>$60 million Series B round</strong> that we announced today.</p>\n<p>This investment was led by <strong>Scale Venture Partners</strong> with participation by <strong>NVentures (NVIDIA’s venture capital arm)</strong> and support from our long-time investors <strong>CRV, Harmony Partners, Flex Capital, Engineering Capital, and Pelion Venture Partners</strong>. With this new funding, our total capital raised is now $88 million.</p>\n<h2 id=\"heading-why-so-many-teams-are-adopting-ai-code-reviews\"><strong>Why so many teams are adopting AI code reviews</strong></h2>\n<p>When every developer on your team is generating code faster, your review queue grows exponentially. Senior engineers who used to review 5 to 10 PRs a day are now facing 20 to 30. The math doesn't work. Teams are caught between two bad options: either slow down deployment cycles waiting for thorough reviews, or rush reviews and let quality slip.</p>\n<p>This is why AI code review adoption is accelerating. AI reviewers augment the human reviewers, freeing them to focus on architecture decisions, business logic, and the nuanced feedback that requires context AI can't fully grasp yet.</p>\n<p>The past year has been a whirlwind. We’ve 10x revenue and doubled our team thanks to:</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757987176453/46d676d8-f560-4cf9-ba1a-54f2dd91d960.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Behind each of those customers are real teams who tell us the same thing: reviews are faster with CodeRabbit, bugs are caught earlier, and release cycles are finally speeding up again.</p>\n<p>Groupon told us they went from 86 hours from review-to-production down to just 39 minutes. Another shared that they cut down the time they spend on code reviews by 70%.</p>\n<h2 id=\"heading-how-coderabbit-works\">How CodeRabbit works</h2>\n<p>CodeRabbit works because it fights AI fire with AI fire. Our platform brings in dozens of points of context to deliver the most context aware reviews to:</p>\n<ul>\n<li><p>Catch correctness and security issues before they hit production.</p>\n<p>  <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758028469061/6a3c1175-2500-440b-995a-785d1d3e1234.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n</ul>\n<ul>\n<li><p>Enforce organizational best practices and custom policies.</p>\n<p>  <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758027905155/0990dc8f-10d7-4765-88f9-e3023b476193.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n</ul>\n<ul>\n<li>Support the full merge cycle with unit testing and docstrings generation.</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757987618430/0854e5be-e8dd-456a-8049-c73526d54f74.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-how-were-celebrating-by-announcing-coderabbit-cli\">How we’re celebrating: By announcing CodeRabbit CLI</h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/IqBKf4u5MtA\">https://youtu.be/IqBKf4u5MtA</a></div>\n<p> </p>\n<p>Today, we're celebrating our Series B by announcing CodeRabbit CLI, AI code reviews that live in your terminal and orchestrate seamlessly with Claude Code, Codex CLI, Cursor CLI, Gemini, and other AI coding agents.</p>\n<p>As developers increasingly write code through CLI Coding agents, we've identified a critical gap: code is being generated at unprecedented speeds, but quality validation happens too late, often only at the PR stage.</p>\n<p>CodeRabbit CLI changes this by bringing intelligent review directly into the CLI workflow, creating a real-time feedback loop between code generation and validation.</p>\n<p>Now, whether you're prompting Claude Code to refactor a module or using Cursor CLI to implement a feature, CodeRabbit instantly reviews the output, catches hallucinations, flags security issues, and even hands contextualized fixes back to your AI agent.</p>\n<p>CodeRabbit CLI is the missing orchestration layer that makes AI-generated code production-ready, turning the promise of autonomous development into reality.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757974301767/02dc9e2f-1d4d-4308-a424-231c982850ea.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-what-this-funding-means-for-us-and-for-you\"><strong>What this funding means for us (and for you)</strong></h2>\n<p>Our Series B round will help us keep pace with the scale of the problem we set out to solve. Here’s where we’re putting that investment:</p>\n<ul>\n<li><p><strong>Accelerating product development:</strong> Our roadmap is packed. From deeper context integrations to smarter pre-merge checks and automated testing, we’re focused on making reviews faster, more accurate, and more useful for every team.</p>\n</li>\n<li><p><strong>Supporting open source:</strong> Today, more than 100,000 OSS projects already use CodeRabbit. With this funding, we’re doubling down on contributions and support to strengthen the community that made modern development possible. More on that later in the week!</p>\n</li>\n<li><p><strong>Hiring the best talent:</strong> We’ve already doubled headcount this year and we’re hiring globally across engineering, product, sales, marketing, and customer success.</p>\n</li>\n</ul>\n<p>This funding gives us the space to keep building what we believe is the most important missing piece of AI-powered development: scalable, context-aware reviews.</p>\n<h2 id=\"heading-thank-you-for-all-your-support\"><strong>Thank you for all your support</strong></h2>\n<p>When we started this company, we knew we were chasing a problem every engineer experiences: reviews are a pain and they don’t scale easily. The fact that CodeRabbit is now helping thousands of teams tackle that problem is both humbling and energizing.</p>\n<p>To our customers, community, and investors: thank you for believing in us and building alongside us. And if this work excites you, consider joining us. Come help us build the future of code reviews.</p>\n<p><a target=\"_blank\" href=\"https://coderabbit.link/KWGzOUS\"><strong><em>Try CodeRabbit for free</em></strong></a> <strong><em>yourself and learn more about our</em></strong> <a target=\"_blank\" href=\"https://coderabbit.link/OG1OZk3\"><strong><em>open roles.</em></strong></a></p>\n",
      "summary": "When we started CodeRabbit, the idea was pretty simple: since all developers hate code reviews, why not make them faster and easier? After all, no one enjoys leaving the same comment about variable naming practices or style conventions for the tenth time in a week.\nThat’s where we believed AI could help – it could automate best-practice checks and policy enforcement so that devs didn’t have to do it themselves. But more importantly, it could act as a safety net, catching issues and bugs before they made it into production.\nWith that belief, we set out to create something new: AI code reviews. Over time, AI coding tools started to gain broader adoption. Tools like Copilot, Claude Code, and Cursor began spitting out more code than teams could easily review with many developers increasing the number of PRs they shipped by 2x to 3x. This added to the existing code review backlogs many teams had. We quickly realized that the ‘efficiency’ gains being marketed to engineering teams would swiftly turn into code review bottlenecks.\nAnd that’s also when we first realized how critical AI code reviews would be to development teams. They would function as a trust and governance layer in agentic software development ensuring quality and security while saving devs time. And, as an added bonus, greatly reducing passive aggressive review comments in the workplace!\nAI code reviews became essential in 2025\n\n\n\nhttps://youtu.be/UHCTKZYOOYU\n \nOver the last two years, we’ve built the most comprehensive and context-rich platform for code reviews, been installed on 2 million repos, reviewed 13 million pull requests, become the most installed AI App on both GitHub and GitLab, and improved the morale of countless dev teams.\nIn 2025, we watched AI code reviews become essential for all teams dealing with the challenges that come with the broad adoption of AI coding agents. But that shift fueled a year of unprecedented growth, culminating in the $60 million Series B round that we announced today.\nThis investment was led by Scale Venture Partners with participation by NVentures (NVIDIA’s venture capital arm) and support from our long-time investors CRV, Harmony Partners, Flex Capital, Engineering Capital, and Pelion Venture Partners. With this new funding, our total capital raised is now $88 million.\nWhy so many teams are adopting AI code reviews\nWhen every developer on your team is generating code faster, your review queue grows exponentially. Senior engineers who used to review 5 to 10 PRs a day are now facing 20 to 30. The math doesn't work. Teams are caught between two bad options: either slow down deployment cycles waiting for thorough reviews, or rush reviews and let quality slip.\nThis is why AI code review adoption is accelerating. AI reviewers augment the human reviewers, freeing them to focus on architecture decisions, business logic, and the nuanced feedback that requires context AI can't fully grasp yet.\nThe past year has been a whirlwind. We’ve 10x revenue and doubled our team thanks to:\n\nBehind each of those customers are real teams who tell us the same thing: reviews are faster with CodeRabbit, bugs are caught earlier, and release cycles are finally speeding up again.\nGroupon told us they went from 86 hours from review-to-production down to just 39 minutes. Another shared that they cut down the time they spend on code reviews by 70%.\nHow CodeRabbit works\nCodeRabbit works because it fights AI fire with AI fire. Our platform brings in dozens of points of context to deliver the most context aware reviews to:\nCatch correctness and security issues before they hit production.\n  \nEnforce organizational best practices and custom policies.\n  \nSupport the full merge cycle with unit testing and docstrings generation.\n\nHow we’re celebrating: By announcing CodeRabbit CLI\n\n\n\nhttps://youtu.be/IqBKf4u5MtA\n \nToday, we're celebrating our Series B by announcing CodeRabbit CLI, AI code reviews that live in your terminal and orchestrate seamlessly with Claude Code, Codex CLI, Cursor CLI, Gemini, and other AI coding agents.\nAs developers increasingly write code through CLI Coding agents, we've identified a critical gap: code is being generated at unprecedented speeds, but quality validation happens too late, often only at the PR stage.\nCodeRabbit CLI changes this by bringing intelligent review directly into the CLI workflow, creating a real-time feedback loop between code generation and validation.\nNow, whether you're prompting Claude Code to refactor a module or using Cursor CLI to implement a feature, CodeRabbit instantly reviews the output, catches hallucinations, flags security issues, and even hands contextualized fixes back to your AI agent.\nCodeRabbit CLI is the missing orchestration layer that makes AI-generated code production-ready, turning the promise of autonomous development into reality.\n\nWhat this funding means for us (and for you)\nOur Series B round will help us keep pace with the scale of the problem we set out to solve. Here’s where we’re putting that investment:\nAccelerating product development: Our roadmap is packed. From deeper context integrations to smarter pre-merge checks and automated testing, we’re focused on making reviews faster, more accurate, and more useful for every team.\nSupporting open source: Today, more than 100,000 OSS projects already use CodeRabbit. With this funding, we’re doubling down on contributions and support to strengthen the community that made modern development possible. More on that later in the week!\nHiring the best talent: We’ve already doubled headcount this year and we’re hiring globally across engineering, product, sales, marketing, and customer success.\nThis funding gives us the space to keep building what we believe is the most important missing piece of AI-powered development: scalable, context-aware reviews.\nThank you for all your support\nWhen we started this company, we knew we were chasing a problem every engineer experiences: reviews are a pain and they don’t scale easily. The fact that CodeRabbit is now helping thousands of teams tackle that problem is both humbling and energizing.\nTo our customers, community, and investors: thank you for believing in us and building alongside us. And if this work excites you, consider joining us. Come help us build the future of code reviews.\nTry CodeRabbit for free yourself and learn more about our open roles.",
      "publishedAt": "2025-09-16T12:55:41.000Z",
      "author": "Harjot Gill",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "documentation",
        "agents",
        "ide",
        "testing",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.984Z",
      "score": 0.19544980972437562
    },
    {
      "id": "5257207d4fdde5650d1feef3190f2e1e",
      "title": "AIレビューを効率化するために生まれたOSSツール「reviewtask」とCodeRabbitの最高な相性",
      "url": "https://coderabbit.ai/blog/how-reviewtask-uses-coderabbit-ja",
      "content": "<p><a target=\"_blank\" href=\"https://x.com/biwakonbu\">Ryo HIGASHIGAWA</a>さんは、OSSとして「<a target=\"_blank\" href=\"https://biwakonbu.github.io/reviewtask/\">reviewtask</a>」というレビュー支援ツールを一人で開発されています。このツールは、AIが生成したコードに対して発生する膨大な指摘事項を、効率的かつ正確に管理するために設計されたものです。Ryoさん自身がAIによるコードレビューを日常的に活用し、その運用課題に直面する中で生まれた実践的なプロダクトとなっています。</p>\n<p>もともとは、GitHubのレビューコメントをAIで取得し、タスクに変換して管理するというアプローチを試していたものの、精度や手間の問題が大きく、より安定した運用を目指してreviewtaskの開発が始まりました。そんな背景を持つRyo HIGASHIGAWAさんに、reviewtaskとCodeRabbit活用についてお話を伺いました。</p>\n<h2 id=\"heading-reviewtask\"><strong>reviewtaskの開発体制について</strong></h2>\n<p>reviewtaskはRyoさんが開発していますが、コード自体はほぼすべてAIによって生成されています。Ryoさん自身は、開発タスクの設計やバグ報告など、プロダクトマネージャーのような立場に徹し、コードを書く作業をAIに委ねています。</p>\n<p>Git操作やPull Request、Issue作成などの多くもAIに任せており、自らは開発プロセスの全体像を見ながらプロジェクトを前に進める役割に集中しているとのことです。</p>\n<h2 id=\"heading-kirjgrpjg7zjg4njg6zjg5pjg6xjg7zjgavplqljgznjgovoqrlpoywqkg\"><strong>コードレビューに関する課題</strong></h2>\n<p>AIによるコード生成を大量に行う中で、課題となったのがレビュー品質の担保でした。生成されたコードは量が多く、すべてを人の手でレビューするのは現実的ではなく、疲労感とボトルネックを生み出していたそうです。</p>\n<p>AIにコードを書かせることで生産性は大きく向上しましたが、その反面、レビューと品質管理にかかる時間と労力が爆発的に増加。最終的には、レビューまでもAIに任せられないかと模索するようになったといいます。</p>\n<p>「AIに書かせて自分がレビューしてとやっていると、非常に疲れるなというのが問題感としてありました」</p>\n<h2 id=\"heading-coderabbit\"><strong>CodeRabbitとの出会い</strong></h2>\n<p>RyoさんがCodeRabbitに出会ったきっかけは、他のAI開発支援ツールとの比較や試行の中でのことでした。当時はDevinやCursorなどのツールを並行して使用し、ドキュメントやレビューの自動化に取り組んでいたそうです。</p>\n<p>AIに仕様を理解させ、それに沿ったレビューやチェックを実現したいという強い思いから、試行錯誤を重ねる中で、CodeRabbitの精度や柔軟性に魅力を感じて導入に至りました。</p>\n<p>「レビューの指摘の対応やPRの状況の確認などにも利用できるので非常に柔軟なツールなところが気にいっています」</p>\n<h2 id=\"heading-coderabbit-1\"><strong>CodeRabbit導入の決定要因</strong></h2>\n<p>CodeRabbitを導入する決め手となったのは、レビューの質だけでなく、ツールとの対話ができる点だったといいます。指摘を受けたくないポイントを説明すれば理解してくれる柔軟性や、プロジェクトごとのカスタマイズ性が大きな魅力でした。</p>\n<p>また、単にレビューコメントを生成するだけでなく、レビュー後のフローに組み込みやすい点も導入を後押ししたそうです。</p>\n<p>「プロダクトの都合上、指摘を入れて欲しくない所は説明すれば学習してくれるのが嬉しいですね」</p>\n<h2 id=\"heading-coderabbit-2\"><strong>CodeRabbitの運用状況・効果</strong></h2>\n<p>現在では、reviewtaskやその他のプロジェクトにおいて、CodeRabbitによるレビューを標準フローとして組み込んでくれています。レビュー品質の維持と同時に、設計やドキュメント作成に集中できる時間が確保され、結果として開発効率が大きく向上しました。</p>\n<p>レビュー指摘の管理にはAIツールとの連携や自作ツールを駆使し、指摘をTODO化して確実に対応していくプロセスが構築されています。複数のプロジェクトを同時に進める現在の開発スタイルは、CodeRabbitの存在抜きには成り立たないといいます。</p>\n<p>「今はなくてはならないパートナーという感じです」</p>\n<h2 id=\"heading-kirlrpli5njgafjga7liknnlkgqkg\"><strong>実務での利用</strong></h2>\n<p>OSS開発だけでなく、業務での開発プロジェクトにおいてもCodeRabbitを導入し、レビューの効率化を図っています。特にレビューコストの高いチームにとっては、CodeRabbitが先に自動で指摘を洗い出してくれることで、人的リソースの負担が大きく軽減されました。</p>\n<p>導入後は、レビューの流れそのものが変わり、指摘が先に潰された状態でレビュワーに渡るため、確認作業の集中と精度向上につながっているとのことです。</p>\n<p>「ワークフローが完全に変わった感じがして良い評価が開発メンバーからも上がっています」</p>\n<h2 id=\"heading-coderabbit-3\"><strong>CodeRabbitに今後期待したいところ</strong></h2>\n<p>CodeRabbitへの要望としては、仕様学習の精度向上や、PRやIssueを横断的に管理できる機能の強化が挙げられました。VSCodeとの連携においても、詳細な指摘内容を取得し、IDE上でAIからのフィードバックを直接得られるようになることを期待されています。</p>\n<p>さらに、ドキュメントのわかりやすさや機能説明の具体性にも改善の余地があると感じており、ユーザーの立場からnoteなどで情報発信を続けていきたいとの意欲も語ってくださいました。</p>\n<p>「本当に素晴らしいプロダクトだと思っているので、ぜひこのすばらしいプロダクトを広めていただければと思っています！」</p>\n<p>CodeRabbitは今後も<a target=\"_blank\" href=\"https://biwakonbu.github.io/reviewtask/\">reviewtask</a>の開発をサポートしていきます！</p>\n",
      "summary": "Ryo HIGASHIGAWAさんは、OSSとして「reviewtask」というレビュー支援ツールを一人で開発されています。このツールは、AIが生成したコードに対して発生する膨大な指摘事項を、効率的かつ正確に管理するために設計されたものです。Ryoさん自身がAIによるコードレビューを日常的に活用し、その運用課題に直面する中で生まれた実践的なプロダクトとなっています。\nもともとは、GitHubのレビューコメントをAIで取得し、タスクに変換して管理するというアプローチを試していたものの、精度や手間の問題が大きく、より安定した運用を目指してreviewtaskの開発が始まりました。そんな背景を持つRyo HIGASHIGAWAさんに、reviewtaskとCodeRabbit活用についてお話を伺いました。\nreviewtaskの開発体制について\nreviewtaskはRyoさんが開発していますが、コード自体はほぼすべてAIによって生成されています。Ryoさん自身は、開発タスクの設計やバグ報告など、プロダクトマネージャーのような立場に徹し、コードを書く作業をAIに委ねています。\nGit操作やPull Request、Issue作成などの多くもAIに任せており、自らは開発プロセスの全体像を見ながらプロジェクトを前に進める役割に集中しているとのことです。\nコードレビューに関する課題\nAIによるコード生成を大量に行う中で、課題となったのがレビュー品質の担保でした。生成されたコードは量が多く、すべてを人の手でレビューするのは現実的ではなく、疲労感とボトルネックを生み出していたそうです。\nAIにコードを書かせることで生産性は大きく向上しましたが、その反面、レビューと品質管理にかかる時間と労力が爆発的に増加。最終的には、レビューまでもAIに任せられないかと模索するようになったといいます。\n「AIに書かせて自分がレビューしてとやっていると、非常に疲れるなというのが問題感としてありました」\nCodeRabbitとの出会い\nRyoさんがCodeRabbitに出会ったきっかけは、他のAI開発支援ツールとの比較や試行の中でのことでした。当時はDevinやCursorなどのツールを並行して使用し、ドキュメントやレビューの自動化に取り組んでいたそうです。\nAIに仕様を理解させ、それに沿ったレビューやチェックを実現したいという強い思いから、試行錯誤を重ねる中で、CodeRabbitの精度や柔軟性に魅力を感じて導入に至りました。\n「レビューの指摘の対応やPRの状況の確認などにも利用できるので非常に柔軟なツールなところが気にいっています」\nCodeRabbit導入の決定要因\nCodeRabbitを導入する決め手となったのは、レビューの質だけでなく、ツールとの対話ができる点だったといいます。指摘を受けたくないポイントを説明すれば理解してくれる柔軟性や、プロジェクトごとのカスタマイズ性が大きな魅力でした。\nまた、単にレビューコメントを生成するだけでなく、レビュー後のフローに組み込みやすい点も導入を後押ししたそうです。\n「プロダクトの都合上、指摘を入れて欲しくない所は説明すれば学習してくれるのが嬉しいですね」\nCodeRabbitの運用状況・効果\n現在では、reviewtaskやその他のプロジェクトにおいて、CodeRabbitによるレビューを標準フローとして組み込んでくれています。レビュー品質の維持と同時に、設計やドキュメント作成に集中できる時間が確保され、結果として開発効率が大きく向上しました。\nレビュー指摘の管理にはAIツールとの連携や自作ツールを駆使し、指摘をTODO化して確実に対応していくプロセスが構築されています。複数のプロジェクトを同時に進める現在の開発スタイルは、CodeRabbitの存在抜きには成り立たないといいます。\n「今はなくてはならないパートナーという感じです」\n実務での利用\nOSS開発だけでなく、業務での開発プロジェクトにおいてもCodeRabbitを導入し、レビューの効率化を図っています。特にレビューコストの高いチームにとっては、CodeRabbitが先に自動で指摘を洗い出してくれることで、人的リソースの負担が大きく軽減されました。\n導入後は、レビューの流れそのものが変わり、指摘が先に潰された状態でレビュワーに渡るため、確認作業の集中と精度向上につながっているとのことです。\n「ワークフローが完全に変わった感じがして良い評価が開発メンバーからも上がっています」\nCodeRabbitに今後期待したいところ\nCodeRabbitへの要望としては、仕様学習の精度向上や、PRやIssueを横断的に管理できる機能の強化が挙げられました。VSCodeとの連携においても、詳細な指摘内容を取得し、IDE上でAIからのフィードバックを直接得られるようになることを期待されています。\nさらに、ドキュメントのわかりやすさや機能説明の具体性にも改善の余地があると感じており、ユーザーの立場からnoteなどで情報発信を続けていきたいとの意欲も語ってくださいました。\n「本当に素晴らしいプロダクトだと思っているので、ぜひこのすばらしいプロダクトを広めていただければと思っています！」\nCodeRabbitは今後もreviewtaskの開発をサポートしていきます！",
      "publishedAt": "2025-09-15T01:00:13.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.985Z",
      "score": 0.05509994500055919
    },
    {
      "id": "cb74a9a02f87a185d427db67d4a4be54",
      "title": "Our security posture: How we safeguard your repositories",
      "url": "https://coderabbit.ai/blog/our-security-posture-how-we-safeguard-your-repositories",
      "content": "<p>Our customers trust us with their most valuable asset: their source code. That trust is why security is central to our mission of helping developers ship better code faster.</p>\n<p>When there’s a chance to strengthen our security posture, we act quickly and decisively. And when we design new systems, we design them with “security by default” in mind.</p>\n<p>We share below the architecture that makes CodeRabbit more resilient, limits the potential impact of any one component, and ensures that the data entrusted to us remains safe under all circumstances.</p>\n<h2 id=\"heading-overview\">Overview</h2>\n<p>Customers install CodeRabbit on their git platforms via the app marketplace. We integrate via webhooks with all popular Git providers such as GitHub, GitLab, Bitbucket &amp; and Azure DevOps. The integration allows us to register webhooks on events such as PR opened, user comment, etc.</p>\n<p>Each event is processed in complete isolation. We maintain a secure internal queue that verifies subscriptions, applies rate limits, and ensures that only authorized events are allowed through. Events are handled one at a time, with zero shared state and no assumptions about what came before or after.</p>\n<p>This model gives us something incredibly valuable: containment by default. If an attacker were to compromise one event, they would find nothing else to pivot to – no shared memory, no long-lived tokens, no context beyond that single, short-lived process. Every review starts from scratch, runs alone, and ends clean.</p>\n<h3 id=\"heading-our-architecture-at-a-glance\"><strong>Our architecture at a glance</strong></h3>\n<p>Here’s a high-level look at how our system is structured in our git-based, IDE, and CLI reviews:</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757969439935/8c97eef0-7f3d-4157-b2f6-8766584d36cc.png\" alt class=\"image--center mx-auto\" /></p>\n<p>This design is focused on limiting an attacker’s potential “blast radius” – or how much damage an attacker can do if they succeed at breaching one component. By isolating secrets, tightly scoping tokens, and strengthening our encryption, we’ve drastically reduced that radius.</p>\n<h3 id=\"heading-our-layered-approach\">Our layered approach</h3>\n<p>We use these layered strategies:</p>\n<h3 id=\"heading-1-sandbox\">1. Sandbox</h3>\n<p>We create a secure sandbox environment for each code review event to clone the codebase in order to read files, pull context from various sources in our knowledge base about your code and to run tools, linters, web search queries &amp; verification checks. Our sandboxed environment only has the short-lived token for that particular repository, but it contains absolutely no other secrets, API keys, or credentials. Even if an attacker were to achieve remote code execution within our sandbox environment or get out of the sandbox and break the sandbox kernel-based isolation mechanism, they would find nothing of value - no environment variables with tokens, no configuration files with secrets.</p>\n<p>Internal network access is also blocked from the sandbox. Tools may connect to the internet when required, but they cannot reach CodeRabbit’s internal services.</p>\n<h3 id=\"heading-2-token-service-separation\">2. Token Service Separation</h3>\n<p>To reinforce the isolation of workloads, we have fully embraced a model based on short-lived session tokens rather than long-lived secrets. Instead of passing environment variables or static credentials, every process is scoped with query or event-specific tokens. These git provider tokens are valid only for the duration of the event or process. These are customer-specific, short-lived tokens.  These tokens also have strict rate limiting and audit logging.</p>\n<p>This means that workloads never carry unnecessary privileges. They can only access the resources required to process a specific pull request – and nothing more.</p>\n<p>By removing persistent credentials from execution environments, we eliminate one of the most common attack surfaces. Even if a third-party tool were exploited, the attacker would see nothing beyond the minimal context of the current event.</p>\n<h3 id=\"heading-3-customer-data-isolation-amp-encryption\">3. Customer Data Isolation &amp; Encryption</h3>\n<p>Each customer's code review is completely isolated. We provision separate containers per code review and use customer-scoped tokens that can only access their specific repositories. There is no shared state between customers.</p>\n<p>We also ensure that our code index and all cached code is encrypted with a unique key per customer. Even CodeRabbit employees can't see any code-related data we store. You can also <a target=\"_blank\" href=\"https://docs.coderabbit.ai/reference/caching\">opt out of these features</a> if you don’t want a cached copy of your code.</p>\n<p>This layered approach ensures that even if an attacker were able to gain access, they would be unable to access anything critical.</p>\n<h2 id=\"heading-our-broader-security-posture\"><strong>Our broader security posture</strong></h2>\n<p>A security best practice is to layer multiple controls so that if one fails, others remain in place. We’ve implemented several layers of defense to protect customer code and data:</p>\n<p><strong>Automated sandbox enforcement</strong>: Every external tool must run in an isolated sandbox environment. This rule is enforced automatically.</p>\n<ul>\n<li><p><strong>Hardened deployment gates</strong>: We’ve added pre-deployment checks that verify no service can bypass sandbox isolation or attempt to run with escalated privileges.</p>\n</li>\n<li><p><strong>Encryption by customer key</strong>: Code indexes and cached code are encrypted with a per-customer key. This ensures that even if cache data were exposed, it would remain unreadable without the correct key.</p>\n</li>\n<li><p><strong>Auditing and monitoring</strong>: We’ve expanded our monitoring of sandboxed environments and added automated alerts for unexpected behavior or network activity.</p>\n</li>\n<li><p><strong>Expanded training</strong>: Every CodeRabbit engineer receives additional security training focused on secure-by-design practices and safe handling of secrets.</p>\n</li>\n<li><p><strong>Least privilege access:</strong> Users, processes, and systems are granted only the minimum level of permissions and access rights necessary to perform their specific tasks and nothing more.</p>\n</li>\n<li><p><strong>Vulnerability disclosure program (VDP):</strong> We maintain a formal program that invites independent security researchers to report potential issues responsibly. This ensures that if a weakness is discovered, it can be addressed quickly, transparently, and in partnership with the security community.</p>\n</li>\n<li><p><strong>Penetration testing and architectural reviews:</strong> We work with multiple third parties to conduct routine penetration testing and architectural reviews to routinely audit and improve our security posture.</p>\n</li>\n</ul>\n<h2 id=\"heading-looking-ahead\"><strong>Looking ahead</strong></h2>\n<p>We’re committed to building on this foundation by continuing to work with independent auditors, engaging with security researchers through responsible disclosure, and refining our internal practices.</p>\n<p>Our goal is to deliver world-class AI code reviews with the highest levels of security and reliability.</p>\n",
      "summary": "Our customers trust us with their most valuable asset: their source code. That trust is why security is central to our mission of helping developers ship better code faster.\nWhen there’s a chance to strengthen our security posture, we act quickly and decisively. And when we design new systems, we design them with “security by default” in mind.\nWe share below the architecture that makes CodeRabbit more resilient, limits the potential impact of any one component, and ensures that the data entrusted to us remains safe under all circumstances.\nOverview\nCustomers install CodeRabbit on their git platforms via the app marketplace. We integrate via webhooks with all popular Git providers such as GitHub, GitLab, Bitbucket & and Azure DevOps. The integration allows us to register webhooks on events such as PR opened, user comment, etc.\nEach event is processed in complete isolation. We maintain a secure internal queue that verifies subscriptions, applies rate limits, and ensures that only authorized events are allowed through. Events are handled one at a time, with zero shared state and no assumptions about what came before or after.\nThis model gives us something incredibly valuable: containment by default. If an attacker were to compromise one event, they would find nothing else to pivot to – no shared memory, no long-lived tokens, no context beyond that single, short-lived process. Every review starts from scratch, runs alone, and ends clean.\nOur architecture at a glance\nHere’s a high-level look at how our system is structured in our git-based, IDE, and CLI reviews:\n\nThis design is focused on limiting an attacker’s potential “blast radius” – or how much damage an attacker can do if they succeed at breaching one component. By isolating secrets, tightly scoping tokens, and strengthening our encryption, we’ve drastically reduced that radius.\nOur layered approach\nWe use these layered strategies:\n1. Sandbox\nWe create a secure sandbox environment for each code review event to clone the codebase in order to read files, pull context from various sources in our knowledge base about your code and to run tools, linters, web search queries & verification checks. Our sandboxed environment only has the short-lived token for that particular repository, but it contains absolutely no other secrets, API keys, or credentials. Even if an attacker were to achieve remote code execution within our sandbox environment or get out of the sandbox and break the sandbox kernel-based isolation mechanism, they would find nothing of value - no environment variables with tokens, no configuration files with secrets.\nInternal network access is also blocked from the sandbox. Tools may connect to the internet when required, but they cannot reach CodeRabbit’s internal services.\n2. Token Service Separation\nTo reinforce the isolation of workloads, we have fully embraced a model based on short-lived session tokens rather than long-lived secrets. Instead of passing environment variables or static credentials, every process is scoped with query or event-specific tokens. These git provider tokens are valid only for the duration of the event or process. These are customer-specific, short-lived tokens.  These tokens also have strict rate limiting and audit logging.\nThis means that workloads never carry unnecessary privileges. They can only access the resources required to process a specific pull request – and nothing more.\nBy removing persistent credentials from execution environments, we eliminate one of the most common attack surfaces. Even if a third-party tool were exploited, the attacker would see nothing beyond the minimal context of the current event.\n3. Customer Data Isolation & Encryption\nEach customer's code review is completely isolated. We provision separate containers per code review and use customer-scoped tokens that can only access their specific repositories. There is no shared state between customers.\nWe also ensure that our code index and all cached code is encrypted with a unique key per customer. Even CodeRabbit employees can't see any code-related data we store. You can also opt out of these features if you don’t want a cached copy of your code.\nThis layered approach ensures that even if an attacker were able to gain access, they would be unable to access anything critical.\nOur broader security posture\nA security best practice is to layer multiple controls so that if one fails, others remain in place. We’ve implemented several layers of defense to protect customer code and data:\nAutomated sandbox enforcement: Every external tool must run in an isolated sandbox environment. This rule is enforced automatically.\nHardened deployment gates: We’ve added pre-deployment checks that verify no service can bypass sandbox isolation or attempt to run with escalated privileges.\nEncryption by customer key: Code indexes and cached code are encrypted with a per-customer key. This ensures that even if cache data were exposed, it would remain unreadable without the correct key.\nAuditing and monitoring: We’ve expanded our monitoring of sandboxed environments and added automated alerts for unexpected behavior or network activity.\nExpanded training: Every CodeRabbit engineer receives additional security training focused on secure-by-design practices and safe handling of secrets.\nLeast privilege access: Users, processes, and systems are granted only the minimum level of permissions and access rights necessary to perform their specific tasks and nothing more.\nVulnerability disclosure program (VDP): We maintain a formal program that invites independent security researchers to report potential issues responsibly. This ensures that if a weakness is discovered, it can be addressed quickly, transparently, and in partnership with the security community.\nPenetration testing and architectural reviews: We work with multiple third parties to conduct routine penetration testing and architectural reviews to routinely audit and improve our security posture.\nLooking ahead\nWe’re committed to building on this foundation by continuing to work with independent auditors, engaging with security researchers through responsible disclosure, and refining our internal practices.\nOur goal is to deliver world-class AI code reviews with the highest levels of security and reliability.",
      "publishedAt": "2025-09-14T07:00:00.000Z",
      "author": "Rohit Khanna",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "testing",
        "observability",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:39.985Z",
      "score": 0.14361950179080857
    },
    {
      "id": "6db1a811cac14b58835bae10c5df0aae",
      "title": "フルリモート開発を加速するSalesNowのCodeRabbit活用法",
      "url": "https://coderabbit.ai/blog/salesnowcoderabbit",
      "content": "<p><a target=\"_blank\" href=\"https://top.salesnow.jp/\">株式会社SalesNow</a>は1,400万件超の企業情報を収録し、法人網羅率100%を誇る日本最大級の企業・組織データベース、AI企業データクラウド「SalesNow」を提供しています。同社では、プロダクト面のAI活用に加えて、社内の開発生産性向上を目的としたLLMやエージェントの導入にも積極的です。</p>\n<p>その一環としてAIコードレビューサービスのCodeRabbitを活用し、レビューの標準化と学習の仕組みづくりを進めています。今回はSalesNow社内におけるCodeRabbitの利用状況について、同社エンジニアの<a target=\"_blank\" href=\"https://x.com/sa9_sha9\">@sa9_sha9</a>さんにお話を伺いました。</p>\n<h2 id=\"heading-salesnow\">SalesNowの開発体制について</h2>\n<p>同社の開発は完全内製で、創業時からフルリモートを文化として定着しています。地理的に分散したメンバーが自律的に動けるよう、非同期コミュニケーションとプルリクエスト中心のフローを重視しています。</p>\n<p>体制はアプリケーション開発が6名、データ生成と収集を担うデータチームがフルタイム6名とインターン約4名。さらにデザイナーとPMが加わり、全体で20名ほどとなっています。主要スタックはPythonとReactで、データの信頼性と鮮度を軸に開発を進めています。</p>\n<h2 id=\"heading-44os44ot44ol44o85b6f44gh44gu44oc44oi44or44on44od44kv55m655sf44gm6kqy6agm\">レビュー待ちのボトルネック発生が課題</h2>\n<p>CodeRabbit導入前は、ドメイン知識や社内の開発流儀の判断が一部メンバーに集中し、レビュー待ちのボトルネックが発生していました。長く在籍しているからこそ分かる書き方や、過去の経緯に基づいた知見が人に依存し、属人化を招いていました。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757307500016/279160eb-0729-45b9-97b0-45b4dff08fb4.jpeg\" alt class=\"image--center mx-auto\" /></p>\n<p>技術面では、プログラミング言語の進化に伴う細かな是正を人が指摘していました。たとえばPythonの型ヒントに関する記述の見直しなど、新しい機能に関する指摘ほど、人が丁寧に行う必要がありました。加えて静的解析では指摘が多く、重要度の見極めが難しかったと振り返ります。</p>\n<p>「レビューの質は落とさず、人にしかできない判断と機械で代替できる指摘を切り分けたいという課題感が常にありました」</p>\n<h2 id=\"heading-coderabbit\">CodeRabbitとの出会い</h2>\n<p>CodeRabbitと出会ったきっかけは、Xのタイムラインでした。実運用の事例も多く確認でき、プルリクエスト（以下PR）を起点に自動で一次レビューが進む点が自社にフィットすると判断したといいます。そこで、まずは@sa9_sha9さんのチームに限定して、小さく使い始めました。</p>\n<p>同社では他のAIツールも併用していましたが、PR駆動の自動レビューという仕組みは運用に乗せやすいと感じたといいます。非同期中心のワークスタイルにも自然に溶け込み、導入ハードルが低い点も後押しになりました。</p>\n<p>「小さく始めて徐々に広げるという進め方が取りやすく、現場の感触を早く得られました」</p>\n<h2 id=\"heading-coderabbit3\">CodeRabbit導入を決めた3つのポイント</h2>\n<p>SalesNow社が、CodeRabbit導入を決定した要因は以下の3つです。</p>\n<p>1. PR作成を起点に自動で一次レビューが実行されること<br />2. 日本語の自然なコメントと、プロジェクトガイドラインの読込に対応していること<br />3. レビューの指摘に対してやり取りを重ねることで、自然と暗黙知が体系化されていく体験がとても良かった</p>\n<p>自動的なコードレビューは、人の手が空いていなくてもレビューとディスカッションが先行し、手戻りが減る効果がありました。また、一般的なベストプラクティスと社内の流儀を橋渡しできる点も評価しています。</p>\n<p>「まずAIに通すことで基本的な抜け漏れを塞ぎ、人は本質的なレビューに集中できるようになっています」</p>\n<h2 id=\"heading-ai\">AIによるレビューは心理的摩擦が低い</h2>\n<p>現在はフルタイムとインターンを含む全開発メンバーに権限を付与し、PRを作るとCodeRabbitがレビューするのが当たり前になっています。新しいメンバーにはオンボーディングにて、レビューへの反応と判断のコメント化を周知しています。</p>\n<p>この工夫は、非同期な環境ではレビューコメントが放置されているのか、対応不要なものなのかを判別するためです。厳格なルール化はしていませんが、同社では実質的な規範として定着しています。</p>\n<p>導入後の効果として、一次レビューの網羅性が上がり、週末に働きたいメンバーも一人でレビューを回せるようになっています。ユニークな意見として、AIのレビューは人だと起こりがちな心理的摩擦が少なく、受け止めやすいとのこと。</p>\n<p>「CodeRabbitが一次レビューを行い、人は本質を見るという分担で、速度と丁寧さの良いバランスを得られています」</p>\n<h2 id=\"heading-coderabbit-1\">CodeRabbitに今後期待したいところ</h2>\n<p>SalesNow社では、深くCodeRabbitを活用しており、さまざまな要望が上がっているとのことです。</p>\n<p>「まず、要件定義やドメインロジックへの踏み込みを強化してほしいです。当社ではAsanaを利用していますが、そこから仕様の文脈をより確実に参照し、実装意図との整合性の確認や、やるべきでない変更の検知まで踏み込めると、より便利になると思います」</p>\n<p>他にもクロスリポジトリにおける整合性の強化が期待されています。APIとフロントエンド間ではOAS（OpenAPI Specification）を使っていますが、それでも十分に読み取れていない場合があるとの指摘がありました。</p>\n<p>「他にも設定のマージ（組織、リポジトリそれぞれの設定のマージ）や、もっとレポート機能を使いこなしたいと考えています」</p>\n<p>CodeRabbitは、今後もSalesNow社のサービス開発をサポートして参ります。</p>\n<hr />\n<p>SalesNowでは、Webリードエンジニアやデータエンジニア、バックエンドエンジニア、LLMエンジニアなどさまざまなエンジニアを募集しています。気になる方は、ぜひ<a target=\"_blank\" href=\"https://recruit.salesnow.jp/\">SalesNow採用情報</a>をご覧ください。</p>\n",
      "summary": "株式会社SalesNowは1,400万件超の企業情報を収録し、法人網羅率100%を誇る日本最大級の企業・組織データベース、AI企業データクラウド「SalesNow」を提供しています。同社では、プロダクト面のAI活用に加えて、社内の開発生産性向上を目的としたLLMやエージェントの導入にも積極的です。\nその一環としてAIコードレビューサービスのCodeRabbitを活用し、レビューの標準化と学習の仕組みづくりを進めています。今回はSalesNow社内におけるCodeRabbitの利用状況について、同社エンジニアの@sa9_sha9さんにお話を伺いました。\nSalesNowの開発体制について\n同社の開発は完全内製で、創業時からフルリモートを文化として定着しています。地理的に分散したメンバーが自律的に動けるよう、非同期コミュニケーションとプルリクエスト中心のフローを重視しています。\n体制はアプリケーション開発が6名、データ生成と収集を担うデータチームがフルタイム6名とインターン約4名。さらにデザイナーとPMが加わり、全体で20名ほどとなっています。主要スタックはPythonとReactで、データの信頼性と鮮度を軸に開発を進めています。\nレビュー待ちのボトルネック発生が課題\nCodeRabbit導入前は、ドメイン知識や社内の開発流儀の判断が一部メンバーに集中し、レビュー待ちのボトルネックが発生していました。長く在籍しているからこそ分かる書き方や、過去の経緯に基づいた知見が人に依存し、属人化を招いていました。\n\n技術面では、プログラミング言語の進化に伴う細かな是正を人が指摘していました。たとえばPythonの型ヒントに関する記述の見直しなど、新しい機能に関する指摘ほど、人が丁寧に行う必要がありました。加えて静的解析では指摘が多く、重要度の見極めが難しかったと振り返ります。\n「レビューの質は落とさず、人にしかできない判断と機械で代替できる指摘を切り分けたいという課題感が常にありました」\nCodeRabbitとの出会い\nCodeRabbitと出会ったきっかけは、Xのタイムラインでした。実運用の事例も多く確認でき、プルリクエスト（以下PR）を起点に自動で一次レビューが進む点が自社にフィットすると判断したといいます。そこで、まずは@sa9_sha9さんのチームに限定して、小さく使い始めました。\n同社では他のAIツールも併用していましたが、PR駆動の自動レビューという仕組みは運用に乗せやすいと感じたといいます。非同期中心のワークスタイルにも自然に溶け込み、導入ハードルが低い点も後押しになりました。\n「小さく始めて徐々に広げるという進め方が取りやすく、現場の感触を早く得られました」\nCodeRabbit導入を決めた3つのポイント\nSalesNow社が、CodeRabbit導入を決定した要因は以下の3つです。\n1. PR作成を起点に自動で一次レビューが実行されること\n2. 日本語の自然なコメントと、プロジェクトガイドラインの読込に対応していること\n3. レビューの指摘に対してやり取りを重ねることで、自然と暗黙知が体系化されていく体験がとても良かった\n自動的なコードレビューは、人の手が空いていなくてもレビューとディスカッションが先行し、手戻りが減る効果がありました。また、一般的なベストプラクティスと社内の流儀を橋渡しできる点も評価しています。\n「まずAIに通すことで基本的な抜け漏れを塞ぎ、人は本質的なレビューに集中できるようになっています」\nAIによるレビューは心理的摩擦が低い\n現在はフルタイムとインターンを含む全開発メンバーに権限を付与し、PRを作るとCodeRabbitがレビューするのが当たり前になっています。新しいメンバーにはオンボーディングにて、レビューへの反応と判断のコメント化を周知しています。\nこの工夫は、非同期な環境ではレビューコメントが放置されているのか、対応不要なものなのかを判別するためです。厳格なルール化はしていませんが、同社では実質的な規範として定着しています。\n導入後の効果として、一次レビューの網羅性が上がり、週末に働きたいメンバーも一人でレビューを回せるようになっています。ユニークな意見として、AIのレビューは人だと起こりがちな心理的摩擦が少なく、受け止めやすいとのこと。\n「CodeRabbitが一次レビューを行い、人は本質を見るという分担で、速度と丁寧さの良いバランスを得られています」\nCodeRabbitに今後期待したいところ\nSalesNow社では、深くCodeRabbitを活用しており、さまざまな要望が上がっているとのことです。\n「まず、要件定義やドメインロジックへの踏み込みを強化してほしいです。当社ではAsanaを利用していますが、そこから仕様の文脈をより確実に参照し、実装意図との整合性の確認や、やるべきでない変更の検知まで踏み込めると、より便利になると思います」\n他にもクロスリポジトリにおける整合性の強化が期待されています。APIとフロントエンド間ではOAS（OpenAPI Specification）を使っていますが、それでも十分に読み取れていない場合があるとの指摘がありました。\n「他にも設定のマージ（組織、リポジトリそれぞれの設定のマージ）や、もっとレポート機能を使いこなしたいと考えています」\nCodeRabbitは、今後もSalesNow社のサービス開発をサポートして参ります。\nSalesNowでは、Webリードエンジニアやデータエンジニア、バックエンドエンジニア、LLMエンジニアなどさまざまなエンジニアを募集しています。気になる方は、ぜひSalesNow採用情報をご覧ください。",
      "publishedAt": "2025-09-10T01:00:38.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.985Z",
      "score": 0.03373362573581485
    },
    {
      "id": "fc384d6b68f748c1ca67cf41abcf9e81",
      "title": "CodeRabbitがどうやって大規模コードベースで正確なAIコードレビューを実現しているか",
      "url": "https://coderabbit.ai/blog/how-coderabbit-delivers-accurate-ai-code-reviews-on-massive-codebases-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/how-coderabbit-delivers-accurate-ai-code-reviews-on-massive-codebases\">How CodeRabbit delivers accurate AI code reviews on massive codebases</a>の意訳です。</p>\n<p>大規模なコードベースは特別な存在です。数百のファイルに広がり、何年ものコミットで進化し、時にはなんとか組織的な記憶でつながっているように見えることもあります。その環境で変更をレビューするのは難しいだけでなく、まるで考古学の発掘作業のようです。この行が先週ここに移動したのは理由があったのか？他のファイルが密かに依存しているのではないか？</p>\n<p>まさにそこでCodeRabbitが力を発揮します。スケールに対応するよう設計されているため、ファイルごとのバラバラなコメントになることなく、大規模コードベース全体の履歴とアーキテクチャを考慮してレビューを行います。リポジトリが大きく古いほど、CodeRabbitは役立ちます。人間がプルリクエストの途中で忘れてしまいがちなパターン、依存関係、ルールを見抜けるからです。</p>\n<h2 id=\"heading-ai\">大規模コードベース？AIコードレビューにはより多くの文脈が必要！</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757096043094/d6910b50-edaa-43bc-8962-803f4665900b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>CodeRabbitは<a target=\"_blank\" href=\"https://x.com/qwertyu_alex/status/1956848505445654595\">大規模リポジトリで高いパフォーマンスを発揮することで知られています</a>。私たちのツールはプルリクエストを表面的に読むだけではなく、アーカイブ役のように振る舞います。コメントを残す前に、周辺のコードや多数の文脈を引き込みます。AIエージェントはそれらが履歴の中でどう動いてきたかを追跡し、チームのコーディング規約を適用し、スクリプトやツールで自らの推論を二重チェックします。</p>\n<p>その結果、レビューは異常なほど「文脈に詳しい」ものになります。クロスファイルの問題を事前にキャッチし、一貫性を強制しつつも不要な指摘は避け、複雑で長い過去を持つリポジトリ全体にスケールします。</p>\n<p>得られる結果は明確で、早い段階でのリスクに対するフィードバック、予期せぬ副作用の減少、そしてコードベース全体を理解したレビューになります。</p>\n<h2 id=\"heading-kirlt67liibjgadjgzhjga7jg6zjg5pjg6xjg7zjga7llypoyzngrnvvijmlofohijjgyzjgarjgytjgajkvzxjgyzotbfjgzpjgovjgyvvvikqkg\"><strong>差分だけのレビューの問題点（文脈がないと何が起こるか）</strong></h2>\n<p>コードの差分は必要ですが十分ではありません。大規模コードベースでは、10行の変更が複数サービスで共有されるヘルパーを密かに変えたり、公開されているAPIの要件を変更したり、差分ファイル以外のセキュリティ前提を崩したりすることがあります。</p>\n<p>差分だけを見るAIレビューは、大規模コードベースでは計器なしで飛んでいるようなものです。変更箇所がどこで参照されているのか、他に一緒に変わりやすいコードは何か、チケットの意図に合っているかが見えなければ、小さなコードベースでは通用しても大規模コードベースでは役立ちません。</p>\n<p>文脈がないと「これも更新してもらえますか？」というやり取りが繰り返され、マージ時に遅れて驚きが発生し、小さなリグレッションが積み重なります。紙の上ではレビューが良く見えても、本番では違う結果になるのです。</p>\n<h2 id=\"heading-pr\"><strong>レガシーコードベースに正しい文脈を構築する（それがPRをどう助けるか）</strong></h2>\n<p>CodeRabbitを「意見を出す前に調査ファイルを組み立てる存在」と考えてください。そのケースファイルには以下の要素が含まれ、それぞれがレビューに反映されます。</p>\n<ol>\n<li><h3 id=\"heading-codegraph\"><strong>コードの地図（Codegraph）</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757097642077/59f74d44-200a-48f4-8c3e-3f85f22e3180.png\" alt class=\"image--center mx-auto\" /></p>\n<p> CodeRabbitは定義と参照の軽量なマップを構築し、履歴をスキャンして頻繁に一緒に変更されるファイルを特定します。これにより、依存関係のマップを作成し、PR内の変更が他の依存関係を壊さないかを確認します。</p>\n<p> <strong>なぜ役立つか:</strong> 行単位ではなくファイル間で推論できる。</p>\n<p> <strong>実際の動作:</strong> Codegraphを使って関連ファイルを辿り、差分外で見つかったバグをまとめて通知します。</p>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757097586686/df6114b4-cc64-4d4e-9429-368ae9eff503.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><h3 id=\"heading-amp\"><strong>コードインデックス（セマンティック &amp; 類似検索）</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757090961514/f82b6444-2b1c-4eb5-8b85-a016a886768c.png\" alt class=\"image--center mx-auto\" /></p>\n<p> CodeRabbitは関数、クラス/モジュール、テスト、過去のPRや変更のセマンティックインデックス（埋め込み）を保持します。レビュー時にはキーワードではなく目的ベースで検索し、類似実装を見つけ、再利用すべきテストを引き出し、過去の修正方法を思い出します。</p>\n<p> <strong>なぜ役立つか:</strong> レガシーコードベースですでに解決している方法を参照でき、一貫性向上、手戻り削減、テスト拡充が速くなる。</p>\n<p> <strong>実際の動作:</strong> 類似検索により同じコールバックパターンを使った別のテストを提示し、同じ修正を提案します。</p>\n</li>\n<li><h3 id=\"heading-kirjg4hjg7zjg6dni6zoh6rjga7jg6vjg7zjg6vjgpllj43mmkaqkg\"><strong>チーム独自のルールを反映</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757090919900/00918dac-6fe3-44b4-9b4a-a823f89562bc.png\" alt class=\"image--center mx-auto\" /></p>\n<p> CodeRabbitのレビューはチームの規約（命名、エラーハンドリング、API境界、セキュリティ要件、性能要件、テスト規範など）に基づいて行われます。</p>\n<p> <strong>なぜ役立つか:</strong> 一般的なチェックリストではなく、チーム固有の基準に沿ったフィードバックが得られる。</p>\n<p> <strong>実際の動作:</strong> スキーマ変更後にPrismaのマイグレーション不足を指摘。開発者が「デプロイ時に自動生成される」と返答すると、CodeRabbitはそれを<strong>学習</strong>として保存し、将来の誤検出を避けます。</p>\n</li>\n<li><h3 id=\"heading-kirjg4tjg7zjg6vjgyvjgonjga7jgrfjgrdjg4rjg6sqkg\"><strong>ツールからのシグナル</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757091000646/a73d1f9d-d93e-4666-803d-843f356f720a.png\" alt class=\"image--center mx-auto\" /></p>\n<p> AIの推論と並行して、CodeRabbitはリンターやセキュリティ解析ツールを実行し、その結果をレビューに統合します。</p>\n<p> <strong>なぜ役立つか:</strong> AIとツールの両方に裏打ちされた具体的な改善提案が得られる。</p>\n<p> <strong>実際の動作:</strong> ESLintルールと行番号を示し、コールバックを型付き宣言に書き換え、オプショナルチェイニングで安全性を確保します。</p>\n</li>\n<li><h3 id=\"heading-kiroqlzmi6djgavln7rjgaxjgyvvijmpjzoqlzjgrnjgqjg6rjg5fjg4jvvikqkg\"><strong>証拠に基づく（検証スクリプト）</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757092768117/d45e60b4-6cdd-46b7-9c48-e470874544cf.png\" alt class=\"image--center mx-auto\" /></p>\n<p> 検証が必要な場合、CodeRabbitはシェル/Pythonスクリプト（grepやast-grepのようなもの）を生成し、仮定を確認したり証拠を抽出してからコメントを残します。</p>\n<p> <strong>なぜ役立つか:</strong> コメントに裏付けがあるため、ノイズが減り、実際にコードを改善する指摘だけが残る。</p>\n<p> <strong>実際の動作:</strong> ファイルとループを特定し、失敗モードを説明し、検証エージェントが解析後に導いた正確な修正案を提案します。</p>\n</li>\n</ol>\n<p>これは<strong>実践的なコンテキストエンジニアリング</strong>です。正しい情報を集め、絞り込み、整理してからモデルに判断させる。CodeRabbitは創業時からこのアプローチを核としてきました。</p>\n<p>成果はシンプルです。シグナルが強く、ノイズが少なく、システムを理解しているレビューになります。</p>\n<h2 id=\"heading-kirjgqjjg7pjgrjg7zjg5fjg6njgqtjgrropomqkhjg6rjg53jgrjjg4jjg6rjgbjjga7jgrnjgrhjg7zjg6rjg7pjgraqkg\"><strong>エンタープライズ規模リポジトリへのスケーリング</strong></h2>\n<p>CodeRabbitはスケールを意識して設計されたパイプラインにより、大規模・レガシーコードベースで強みを発揮します。</p>\n<p>PRが届くと、CodeRabbitは隔離された、短期間だけの安全な環境を立ち上げます。必要なものだけを取得し、文脈を構築し、検証を実行し、終了後に破棄します。ピーク時には多数のワーカーが並列実行され、レビュー速度は一定に保たれます。パスフィルターで不要なアセットを除外し、キャッシュやインデックスを選択的に有効化して繰り返しのレビューを高速化できます。</p>\n<p>要するに、範囲の選択で文脈を集中させ、隔離で安全性を確保し、弾力性ある実行方法で高速性を維持します。この手法はコードベースとリリーススケジュールに合わせてスケールします。</p>\n<h2 id=\"heading-coderabbit-ai\"><strong>CodeRabbit: 大規模コードベース向けAIコードレビューの正解</strong></h2>\n<p>CodeRabbitの強みは単一のトリックではありません。コンテキストエンジニアリングを端から端まで適用する姿勢にあります。変更が何に触れるかをマッピングし、意図に結びつけ、チームルールを適用し、ツールで検証し、証拠付きでコメントします。</p>\n<p>このやり方は「コンテキストエンジニアリング」という言葉が流行る前から一貫しており、スケールした環境で正確でノイズの少ないレビューを実現する唯一の方法です。</p>\n<p><strong><em>あなたの大規模コードベースで深い文脈を持つレビューを体験してみませんか？ →</em></strong> <a target=\"_blank\" href=\"https://coderabbit.link/sY5vXpT\"><strong><em>14日間のトライアルを開始する</em></strong></a></p>\n",
      "summary": "How CodeRabbit delivers accurate AI code reviews on massive codebasesの意訳です。\n大規模なコードベースは特別な存在です。数百のファイルに広がり、何年ものコミットで進化し、時にはなんとか組織的な記憶でつながっているように見えることもあります。その環境で変更をレビューするのは難しいだけでなく、まるで考古学の発掘作業のようです。この行が先週ここに移動したのは理由があったのか？他のファイルが密かに依存しているのではないか？\nまさにそこでCodeRabbitが力を発揮します。スケールに対応するよう設計されているため、ファイルごとのバラバラなコメントになることなく、大規模コードベース全体の履歴とアーキテクチャを考慮してレビューを行います。リポジトリが大きく古いほど、CodeRabbitは役立ちます。人間がプルリクエストの途中で忘れてしまいがちなパターン、依存関係、ルールを見抜けるからです。\n大規模コードベース？AIコードレビューにはより多くの文脈が必要！\n\nCodeRabbitは大規模リポジトリで高いパフォーマンスを発揮することで知られています。私たちのツールはプルリクエストを表面的に読むだけではなく、アーカイブ役のように振る舞います。コメントを残す前に、周辺のコードや多数の文脈を引き込みます。AIエージェントはそれらが履歴の中でどう動いてきたかを追跡し、チームのコーディング規約を適用し、スクリプトやツールで自らの推論を二重チェックします。\nその結果、レビューは異常なほど「文脈に詳しい」ものになります。クロスファイルの問題を事前にキャッチし、一貫性を強制しつつも不要な指摘は避け、複雑で長い過去を持つリポジトリ全体にスケールします。\n得られる結果は明確で、早い段階でのリスクに対するフィードバック、予期せぬ副作用の減少、そしてコードベース全体を理解したレビューになります。\n差分だけのレビューの問題点（文脈がないと何が起こるか）\nコードの差分は必要ですが十分ではありません。大規模コードベースでは、10行の変更が複数サービスで共有されるヘルパーを密かに変えたり、公開されているAPIの要件を変更したり、差分ファイル以外のセキュリティ前提を崩したりすることがあります。\n差分だけを見るAIレビューは、大規模コードベースでは計器なしで飛んでいるようなものです。変更箇所がどこで参照されているのか、他に一緒に変わりやすいコードは何か、チケットの意図に合っているかが見えなければ、小さなコードベースでは通用しても大規模コードベースでは役立ちません。\n文脈がないと「これも更新してもらえますか？」というやり取りが繰り返され、マージ時に遅れて驚きが発生し、小さなリグレッションが積み重なります。紙の上ではレビューが良く見えても、本番では違う結果になるのです。\nレガシーコードベースに正しい文脈を構築する（それがPRをどう助けるか）\nCodeRabbitを「意見を出す前に調査ファイルを組み立てる存在」と考えてください。そのケースファイルには以下の要素が含まれ、それぞれがレビューに反映されます。\nコードの地図（Codegraph）\n \n CodeRabbitは定義と参照の軽量なマップを構築し、履歴をスキャンして頻繁に一緒に変更されるファイルを特定します。これにより、依存関係のマップを作成し、PR内の変更が他の依存関係を壊さないかを確認します。\n なぜ役立つか: 行単位ではなくファイル間で推論できる。\n 実際の動作: Codegraphを使って関連ファイルを辿り、差分外で見つかったバグをまとめて通知します。\n \nコードインデックス（セマンティック & 類似検索）\n \n CodeRabbitは関数、クラス/モジュール、テスト、過去のPRや変更のセマンティックインデックス（埋め込み）を保持します。レビュー時にはキーワードではなく目的ベースで検索し、類似実装を見つけ、再利用すべきテストを引き出し、過去の修正方法を思い出します。\n なぜ役立つか: レガシーコードベースですでに解決している方法を参照でき、一貫性向上、手戻り削減、テスト拡充が速くなる。\n 実際の動作: 類似検索により同じコールバックパターンを使った別のテストを提示し、同じ修正を提案します。\nチーム独自のルールを反映\n \n CodeRabbitのレビューはチームの規約（命名、エラーハンドリング、API境界、セキュリティ要件、性能要件、テスト規範など）に基づいて行われます。\n なぜ役立つか: 一般的なチェックリストではなく、チーム固有の基準に沿ったフィードバックが得られる。\n 実際の動作: スキーマ変更後にPrismaのマイグレーション不足を指摘。開発者が「デプロイ時に自動生成される」と返答すると、CodeRabbitはそれを学習として保存し、将来の誤検出を避けます。\nツールからのシグナル\n \n AIの推論と並行して、CodeRabbitはリンターやセキュリティ解析ツールを実行し、その結果をレビューに統合します。\n なぜ役立つか: AIとツールの両方に裏打ちされた具体的な改善提案が得られる。\n 実際の動作: ESLintルールと行番号を示し、コールバックを型付き宣言に書き換え、オプショナルチェイニングで安全性を確保します。\n証拠に基づく（検証スクリプト）\n \n 検証が必要な場合、CodeRabbitはシェル/Pythonスクリプト（grepやast-grepのようなもの）を生成し、仮定を確認したり証拠を抽出してからコメントを残します。\n なぜ役立つか: コメントに裏付けがあるため、ノイズが減り、実際にコードを改善する指摘だけが残る。\n 実際の動作: ファイルとループを特定し、失敗モードを説明し、検証エージェントが解析後に導いた正確な修正案を提案します。\nこれは実践的なコンテキストエンジニアリングです。正しい情報を集め、絞り込み、整理してからモデルに判断させる。CodeRabbitは創業時からこのアプローチを核としてきました。\n成果はシンプルです。シグナルが強く、ノイズが少なく、システムを理解しているレビューになります。\nエンタープライズ規模リポジトリへのスケーリング\nCodeRabbitはスケールを意識して設計されたパイプラインにより、大規模・レガシーコードベースで強みを発揮します。\nPRが届くと、CodeRabbitは隔離された、短期間だけの安全な環境を立ち上げます。必要なものだけを取得し、文脈を構築し、検証を実行し、終了後に破棄します。ピーク時には多数のワーカーが並列実行され、レビュー速度は一定に保たれます。パスフィルターで不要なアセットを除外し、キャッシュやインデックスを選択的に有効化して繰り返しのレビューを高速化できます。\n要するに、範囲の選択で文脈を集中させ、隔離で安全性を確保し、弾力性ある実行方法で高速性を維持します。この手法はコードベースとリリーススケジュールに合わせてスケールします。\nCodeRabbit: 大規模コードベース向けAIコードレビューの正解\nCodeRabbitの強みは単一のトリックではありません。コンテキストエンジニアリングを端から端まで適用する姿勢にあります。変更が何に触れるかをマッピングし、意図に結びつけ、チームルールを適用し、ツールで検証し、証拠付きでコメントします。\nこのやり方は「コンテキストエンジニアリング」という言葉が流行る前から一貫しており、スケールした環境で正確でノイズの少ないレビューを実現する唯一の方法です。\nあなたの大規模コードベースで深い文脈を持つレビューを体験してみませんか？ → 14日間のトライアルを開始する",
      "publishedAt": "2025-09-08T09:03:45.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.985Z",
      "score": 0.029952180114601795
    },
    {
      "id": "c1a469fcd6c6ea5f00545fb70fb5c5f9",
      "title": "How CodeRabbit delivers accurate AI code reviews on massive codebases",
      "url": "https://coderabbit.ai/blog/how-coderabbit-delivers-accurate-ai-code-reviews-on-massive-codebases",
      "content": "<p>Massive codebases are a special kind of beast. They sprawl across hundreds of files, evolve over years of commits, and occasionally feel like they’re held together by equal parts duct tape and institutional memory. Reviewing changes in that environment isn’t just hard – it feels like an archaeological dig. Did this line move here last week for a reason? Is there another file quietly depending on it?</p>\n<p>That’s exactly where CodeRabbit shines. It was built for scale, so instead of drowning you in disconnected file-by-file comments, it reviews with the whole history and architecture of your massive codebase in mind. The larger and older your repository, the more useful CodeRabbit becomes because it can see the patterns, dependencies, and rules that humans usually forget about halfway through a pull request when trying to keep all the dependencies in that legacy code in their head.</p>\n<h2 id=\"heading-large-codebase-ai-code-reviews-need-more-context\">Large codebase? AI code reviews need more context!</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757096043094/d6910b50-edaa-43bc-8962-803f4665900b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>CodeRabbit is <a target=\"_blank\" href=\"https://x.com/qwertyu_alex/status/1956848505445654595\">known for performing great on large repos</a>. Our tool doesn’t just skim your pull requests; it goes full archivist. Before leaving a single comment, it gathers the surrounding code from your large codebase and pulls in dozens of points of context from your code. AI agents then trace how those pieces have moved through history, apply your team’s coding standards, and even double-check their own reasoning with scripts and tools.</p>\n<p>The effect is reviews that feel unusually…informed about your legacy codebase. It catches cross-file issues before they turn into production mysteries, enforces consistency without nitpicking, and scales comfortably across sprawling repos with long, complicated pasts.</p>\n<p>The power you gain through this is clearer, earlier feedback on real risks, fewer “wait, what else did that touch?” surprises, and reviews that actually reflect how your whole massive codebase fits together.</p>\n<h2 id=\"heading-the-problem-with-diff-only-reviews-or-what-goes-wrong-without-context\"><strong>The problem with diff-only reviews (or what goes wrong without context)</strong></h2>\n<p>Code diffs are necessary, but they’re not sufficient. In a massive codebase, a 10-line change can quietly alter a shared helper used by multiple services, shift a public API contract, or undermine a security assumption that lives outside the files in the diff.</p>\n<p>AI Bot reviewers who only see the diff are flying without instruments within a large codebase. AI that can’t see where the changed code is referenced, what else tends to change with it, or whether the change actually matches the ticket’s intent, might work for a smaller codebase but not for yours.</p>\n<p>Without the right context, you get ping-pong cycles (“Can you also update…?”), late surprises at merge time, and a steady drip of small regressions that add up. The review looks fine on paper, while production tells a different story.</p>\n<h2 id=\"heading-building-the-right-context-on-your-legacy-codebase-and-how-that-helps-your-prs\"><strong>Building the right context on your legacy codebase (and how that helps your PRs)</strong></h2>\n<p>Think of CodeRabbit as assembling a case file before giving an opinion. Here’s what goes into that case file and how each piece shows up in your reviews.</p>\n<ol>\n<li><h3 id=\"heading-a-map-of-your-code-codegraph\"><strong>A map of your code (Codegraph)</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757097642077/59f74d44-200a-48f4-8c3e-3f85f22e3180.png\" alt class=\"image--center mx-auto\" /></p>\n<p> CodeRabbit builds a lightweight map of definitions and references and scans commit history for files that frequently change together throughout your massive codebase. This creates a map of file dependencies that CodeRabbit uses to check if any changes in your PR will break other dependencies in your codebase.</p>\n<p> <strong>Why this helps:</strong> The review can reason across files, not just lines.</p>\n<p> <strong>Seeing it in action:</strong> CodeRabbit posts a summary listing bugs <strong>outside the diff range</strong> that CodeRabbit located by traversing related files with Codegraph.</p>\n<p> <strong>Here’s an example of the files that CodeGraph brings in from across a repository when completing a PR review.</strong></p>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757097586686/df6114b4-cc64-4d4e-9429-368ae9eff503.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><h3 id=\"heading-code-index-semantic-amp-similarity-retrieval\"><strong>Code Index (semantic &amp; similarity retrieval)</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757090961514/f82b6444-2b1c-4eb5-8b85-a016a886768c.png\" alt class=\"image--center mx-auto\" /></p>\n<p> CodeRabbit maintains a semantic index (embeddings) of functions, classes/modules, tests, and prior PRs/changes. During review, it searches by purpose, not just keywords to surface parallel implementations to align with, pull relevant tests to reuse or extend, and recall how similar issues were fixed before.</p>\n<p> <strong>Why this helps:</strong> Suggestions are grounded in how your legacy codebase already solves similar problems, reducing rework, improving consistency, and speeding up test coverage.</p>\n<p> <strong>Seeing it in action:</strong> Using similarity retrieval, CodeRabbit surfaces a different test with the same callback pattern and proposes the same fix.</p>\n</li>\n<li><h3 id=\"heading-your-team-rules-not-generic-advice\"><strong>Your team rules, not generic advice</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757090919900/00918dac-6fe3-44b4-9b4a-a823f89562bc.png\" alt class=\"image--center mx-auto\" /></p>\n<p> CodeRabbit reviews are primed with your standards (naming, error handling, API boundaries, security requirements, performance expectations, testing norms) that you can share with us via coding guidelines and review instructions.</p>\n<p> <strong>Why this helps:</strong> Feedback reflects <em>your</em> standards and context, not a one-size-fits-all checklist.</p>\n<p> <strong>Seeing it in action:</strong> CodeRabbit flags a missing Prisma migration after a schema edit. A developer replies that migrations are auto-generated during deploy, a repo-specific rule. CodeRabbit stores that as a <strong>Learning</strong> to avoid future false positives.</p>\n</li>\n<li><h3 id=\"heading-signals-from-tools\"><strong>Signals from tools</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757091000646/a73d1f9d-d93e-4666-803d-843f356f720a.png\" alt class=\"image--center mx-auto\" /></p>\n<p> Alongside AI reasoning, CodeRabbit runs linters and security analyzers and folds their findings into our easy-to-read and understand reviews.</p>\n<p> <strong>Why this helps:</strong> You get grounded, actionable suggestions backed by both AI <em>and</em> recognizable tools.</p>\n<p> <strong>Seeing it in action:</strong> CodeRabbit will do things like point to the exact ESLint rule and line numbers, rewrites the callback as a typed declaration, and guards the call with optional chaining.</p>\n</li>\n<li><h3 id=\"heading-evidence-not-vibes-verification-scripts\"><strong>Evidence, not vibes (verification scripts)</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757092768117/d45e60b4-6cdd-46b7-9c48-e470874544cf.png\" alt class=\"image--center mx-auto\" /></p>\n<p> When something needs checking, CodeRabbit generates shell/Python checks (think grep, ast-grep) to confirm an assumption or extract proof from the codebase before we post the comment.</p>\n<p> <strong>Why this helps:</strong> Comments come with <strong><em>receipts</em></strong>. That translates into less noise and more comments that actually improve your code.</p>\n<p> <strong>Seeing it in action:</strong> The comment pinpoints the file and loop, explains the failure mode, and proposes the exact change produced by the verification agent after analyzing the parsing path.</p>\n</li>\n</ol>\n<p>This is <strong>context engineering in practice</strong>: gathering, filtering, and organizing the <em>right</em> information before asking the model to judge. It’s been core to CodeRabbit since day one.</p>\n<p>The payoff is simple: higher signal, lower noise, and reviews that feel like they understand your system.</p>\n<h2 id=\"heading-scaling-to-enterprise-size-repos\"><strong>Scaling to enterprise-size repos</strong></h2>\n<p>CodeRabbit has an advantage on massive codebases and legacy codebases because we designed our pipeline with scale in mind.</p>\n<p>When a PR arrives, CodeRabbit spins up an isolated, secure, short-lived environment to do the work. It pulls only what it needs, constructs the context, runs the checks, and tears everything down after. During busy hours, many of these workers run in parallel so review speed holds steady. You stay in control of scope by using path filters to keep bulky or generated assets out of the way, and choosing whether to enable caching or indexing to accelerate repeat reviews.</p>\n<p>In short: selective scope keeps context focused, isolation keeps it safe, and elastic execution keeps it fast. This approach scales with your codebase and your release calendar.</p>\n<h2 id=\"heading-coderabbit-large-codebase-ai-code-reviews-done-right\"><strong>CodeRabbit: Large codebase AI code reviews done right</strong></h2>\n<p>CodeRabbit’s advantage on massive codebases isn’t a single trick. It comes from how we approach context engineering end-to-end: map what the change touches, tie it to intent, apply your rules, verify with tools, then comment with evidence.</p>\n<p>We’ve operated this way from the start, well before “context engineering” became a buzzword, because it’s the only reliable path to accurate, low-noise reviews at scale.</p>\n<p><strong><em>Ready to see a deep-context review on your large codebase? →</em></strong> <a target=\"_blank\" href=\"https://coderabbit.link/sY5vXpT\"><strong><em>Start a 14-day trial</em></strong></a></p>\n",
      "summary": "Massive codebases are a special kind of beast. They sprawl across hundreds of files, evolve over years of commits, and occasionally feel like they’re held together by equal parts duct tape and institutional memory. Reviewing changes in that environment isn’t just hard – it feels like an archaeological dig. Did this line move here last week for a reason? Is there another file quietly depending on it?\nThat’s exactly where CodeRabbit shines. It was built for scale, so instead of drowning you in disconnected file-by-file comments, it reviews with the whole history and architecture of your massive codebase in mind. The larger and older your repository, the more useful CodeRabbit becomes because it can see the patterns, dependencies, and rules that humans usually forget about halfway through a pull request when trying to keep all the dependencies in that legacy code in their head.\nLarge codebase? AI code reviews need more context!\n\nCodeRabbit is known for performing great on large repos. Our tool doesn’t just skim your pull requests; it goes full archivist. Before leaving a single comment, it gathers the surrounding code from your large codebase and pulls in dozens of points of context from your code. AI agents then trace how those pieces have moved through history, apply your team’s coding standards, and even double-check their own reasoning with scripts and tools.\nThe effect is reviews that feel unusually…informed about your legacy codebase. It catches cross-file issues before they turn into production mysteries, enforces consistency without nitpicking, and scales comfortably across sprawling repos with long, complicated pasts.\nThe power you gain through this is clearer, earlier feedback on real risks, fewer “wait, what else did that touch?” surprises, and reviews that actually reflect how your whole massive codebase fits together.\nThe problem with diff-only reviews (or what goes wrong without context)\nCode diffs are necessary, but they’re not sufficient. In a massive codebase, a 10-line change can quietly alter a shared helper used by multiple services, shift a public API contract, or undermine a security assumption that lives outside the files in the diff.\nAI Bot reviewers who only see the diff are flying without instruments within a large codebase. AI that can’t see where the changed code is referenced, what else tends to change with it, or whether the change actually matches the ticket’s intent, might work for a smaller codebase but not for yours.\nWithout the right context, you get ping-pong cycles (“Can you also update…?”), late surprises at merge time, and a steady drip of small regressions that add up. The review looks fine on paper, while production tells a different story.\nBuilding the right context on your legacy codebase (and how that helps your PRs)\nThink of CodeRabbit as assembling a case file before giving an opinion. Here’s what goes into that case file and how each piece shows up in your reviews.\nA map of your code (Codegraph)\n \n CodeRabbit builds a lightweight map of definitions and references and scans commit history for files that frequently change together throughout your massive codebase. This creates a map of file dependencies that CodeRabbit uses to check if any changes in your PR will break other dependencies in your codebase.\n Why this helps: The review can reason across files, not just lines.\n Seeing it in action: CodeRabbit posts a summary listing bugs outside the diff range that CodeRabbit located by traversing related files with Codegraph.\n Here’s an example of the files that CodeGraph brings in from across a repository when completing a PR review.\n \nCode Index (semantic & similarity retrieval)\n \n CodeRabbit maintains a semantic index (embeddings) of functions, classes/modules, tests, and prior PRs/changes. During review, it searches by purpose, not just keywords to surface parallel implementations to align with, pull relevant tests to reuse or extend, and recall how similar issues were fixed before.\n Why this helps: Suggestions are grounded in how your legacy codebase already solves similar problems, reducing rework, improving consistency, and speeding up test coverage.\n Seeing it in action: Using similarity retrieval, CodeRabbit surfaces a different test with the same callback pattern and proposes the same fix.\nYour team rules, not generic advice\n \n CodeRabbit reviews are primed with your standards (naming, error handling, API boundaries, security requirements, performance expectations, testing norms) that you can share with us via coding guidelines and review instructions.\n Why this helps: Feedback reflects your standards and context, not a one-size-fits-all checklist.\n Seeing it in action: CodeRabbit flags a missing Prisma migration after a schema edit. A developer replies that migrations are auto-generated during deploy, a repo-specific rule. CodeRabbit stores that as a Learning to avoid future false positives.\nSignals from tools\n \n Alongside AI reasoning, CodeRabbit runs linters and security analyzers and folds their findings into our easy-to-read and understand reviews.\n Why this helps: You get grounded, actionable suggestions backed by both AI and recognizable tools.\n Seeing it in action: CodeRabbit will do things like point to the exact ESLint rule and line numbers, rewrites the callback as a typed declaration, and guards the call with optional chaining.\nEvidence, not vibes (verification scripts)\n \n When something needs checking, CodeRabbit generates shell/Python checks (think grep, ast-grep) to confirm an assumption or extract proof from the codebase before we post the comment.\n Why this helps: Comments come with receipts. That translates into less noise and more comments that actually improve your code.\n Seeing it in action: The comment pinpoints the file and loop, explains the failure mode, and proposes the exact change produced by the verification agent after analyzing the parsing path.\nThis is context engineering in practice: gathering, filtering, and organizing the right information before asking the model to judge. It’s been core to CodeRabbit since day one.\nThe payoff is simple: higher signal, lower noise, and reviews that feel like they understand your system.\nScaling to enterprise-size repos\nCodeRabbit has an advantage on massive codebases and legacy codebases because we designed our pipeline with scale in mind.\nWhen a PR arrives, CodeRabbit spins up an isolated, secure, short-lived environment to do the work. It pulls only what it needs, constructs the context, runs the checks, and tears everything down after. During busy hours, many of these workers run in parallel so review speed holds steady. You stay in control of scope by using path filters to keep bulky or generated assets out of the way, and choosing whether to enable caching or indexing to accelerate repeat reviews.\nIn short: selective scope keeps context focused, isolation keeps it safe, and elastic execution keeps it fast. This approach scales with your codebase and your release calendar.\nCodeRabbit: Large codebase AI code reviews done right\nCodeRabbit’s advantage on massive codebases isn’t a single trick. It comes from how we approach context engineering end-to-end: map what the change touches, tie it to intent, apply your rules, verify with tools, then comment with evidence.\nWe’ve operated this way from the start, well before “context engineering” became a buzzword, because it’s the only reliable path to accurate, low-noise reviews at scale.\nReady to see a deep-context review on your large codebase? → Start a 14-day trial",
      "publishedAt": "2025-09-05T17:20:16.000Z",
      "author": "Sahana Vijaya Prasad",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "testing"
      ],
      "ingestedAt": "2025-11-23T17:37:39.985Z",
      "score": 0.08495221174602022
    },
    {
      "id": "ebee3ab860a1bb3deac0318b699683a8",
      "title": "AIエージェント企業がコードレビューに選んだCodeRabbit",
      "url": "https://coderabbit.ai/blog/how-generative-agents-uses-coderabbit-ja",
      "content": "<p>生成AIの技術が急速に進化する中で、AIを活用した開発支援ツールにも注目が集まっています。なかでも、AIによるコードレビュー支援は、開発生産性と品質を両立させるための有効な手段として、多くの現場で導入が進んでいます。</p>\n<p>今回は、AIエージェント活用をテーマに事業を展開している<a target=\"_blank\" href=\"https://generative-agents.co.jp/\">Generative Agents</a>のCEO、西見さんにお話を伺いました。同社では立ち上げ当初からCodeRabbitを導入しており、その背景や運用上の工夫、今後への期待などについて詳しく語っていただきました。</p>\n<h2 id=\"heading-ai\"><strong>AIエージェントの利活用と技術教育で企業を支援</strong></h2>\n<p><a target=\"_blank\" href=\"https://generative-agents.co.jp/\">Generative Agents</a>は、AIエージェントの利活用を軸とした事業を展開しています。LangChainやLangGraphといったLLM関連のライブラリを利用し、クライアント企業のAI活用を支援しています。技術講座の提供や教育プログラムの設計にも注力しており、AIエージェント活用を推進する立場として活動の幅を広げています。</p>\n<p>設立は2024年3月。共同創業者3名はいずれも生成AI分野での著書を持つ技術者であり、同じ志を持つ仲間として活動を開始しました。個人では追いきれないスピードで進化するAI技術を、仲間とともにキャッチアップしながら、社会に還元していくことを目指しています。</p>\n<h2 id=\"heading-kirlsjhmlbdnsr7pi63jgafpoaflrqljgajlhbhjgavkvzzjgovlrpot7xlnovplovnmbrkvzplilyqkg\"><strong>少数精鋭で顧客と共に作る実践型開発体制</strong></h2>\n<p>Generative Agentsの開発体制は、現在5名という少人数構成です。エンジニアリングリーダーがアーキテクチャ全体を設計しつつ、顧客と共同でプロダクト開発を進めています。LangChainやLangGraphを使った開発に深く関わり、技術的な壁を乗り越える支援を得意としています。</p>\n<p>同社には<a target=\"_blank\" href=\"https://x.com/tegnike\">AI VTuberのニケちゃん</a>も在籍し、開発メンバーとして活動しています。その他、参画したエンジニアとともに、柔軟な体制でクライアントと伴走しています。プロダクトを開発するだけでなく、顧客自身が手を動かせるような支援体制が特徴です。</p>\n<h2 id=\"heading-kirnrkzkuinogixoppbngrnjga7kui3otrpjgajjg6zjg5pjg6xjg7zosqdojbfjgpljganjgyboo5zjgybjgysqkg\"><strong>第三者視点の不足とレビュー負荷をどう補うか</strong></h2>\n<p>創業前から個人事業として活動していた西見さんは、外部のエンジニアと協業する中でコードレビューの負荷に課題を感じていました。1人でコードの品質を担保するには限界があり、第三者の視点が欠けがちになっていたそうです。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1756873041181/3256551d-6be7-42ab-ad51-0f3d4ed6d2d3.png\" alt class=\"image--center mx-auto\" /></p>\n<p>創業後も、少人数での開発が中心であるため、同じような課題は継続していたといいます。レビューの質を維持するためには、客観的な視点を常に取り入れられる仕組みが必要だと考えていたといいます。</p>\n<p>「少人数体制だからこそ、第三者視点が持てるレビュー環境が重要でした。AIによるレビュー支援は、まさにそのニーズに合っていました」（西見さん）</p>\n<h2 id=\"heading-ai-1\"><strong>創業前から自然に使い始めたAIレビュー</strong></h2>\n<p>CodeRabbitとの出会いは、創業前の個人事業時代に遡ります。外部の開発パートナーと進める中で、自然と導入していたと振り返ります。CodeRabbitは特に違和感なく、スムーズに日々の開発に組み込まれていたとのことです。</p>\n<p>Ruby on Railsでの開発経験が長く、静的解析やスタイルガイドに基づく開発には慣れていたこともあり、AIによるレビューというスタイルにもすぐに適応できたと語っています。</p>\n<p>「ルールベースの指摘に慣れていたので、AIがレビューしてくれることについては特に抵抗はありませんでした」（西見さん）</p>\n<h2 id=\"heading-kirku5bnpl7jgrxjg7zjg5pjgrnjgajjga7mr5tovipjgafopovjgyjjgznsr7luqbjga7pq5jjgzuqkg\"><strong>他社サービスとの比較で見えた精度の高さ</strong></h2>\n<p>Generative Agentsでは、CodeRabbit以外のAIコードレビューツールも利用しています。しかし、LangChainなどの複雑な文脈を含むコードに対しては、思うようなフィードバックが得られなかったといいます。</p>\n<p>一方で、CodeRabbitはコンテキストの長いファイルに対しても適切な指摘ができ、重大なバグの予防にも貢献してくれていると語ります。</p>\n<p>「実際に比較してみて、精度の違いを実感しています」（西見さん）</p>\n<h2 id=\"heading-ai-2\"><strong>メンバーの負担を減らすAIレビューの立ち位置</strong></h2>\n<p>現在の運用では、すべての開発メンバーが積極的にCodeRabbitを使っているわけではないものの、多くのメンバーにとって非常に頼れる存在となっています。人手でのレビューが難しい状況でも、AIによるフィードバックが品質を支えてくれています。</p>\n<p>新しく参加するエンジニアについても、元々レビュー文化に慣れており、AIの指摘も自然に受け入れられているとのことです。レビュー指摘に対してフラットに議論できる風土が、AI活用にもつながっています。</p>\n<p>「人間であれAIであれ、指摘されたコードを素直に見直すという文化が根付いているからこそ、AIレビューも自然に受け入れられています」（西見さん）</p>\n<h2 id=\"heading-kirlrabnv5lmqzog73jgbjjga7mnjlvouqkg\"><strong>学習機能への期待</strong></h2>\n<p>現在のCodeRabbitに対して高く評価している一方で、改善を期待する点もあると語ってくれました。特にレビュー設定が難解で、どの項目がどのように結果に影響するのかが把握しづらい点に課題を感じているそうです。</p>\n<p>また、生成されるコメントの中には冗長なものもあり、実際に役立つ指摘は一部にとどまってしまう場合もあるとのこと。今後は、プロジェクトごとに最適なレビューができるよう、AIが学習して改善していく仕組みを期待していると語ってくれました。</p>\n<p>「設定変更の効果が見える化されていたり、指摘の質を学習して改善してくれると、もっと成長を実感しながら使えるツールになると思っています」</p>\n",
      "summary": "生成AIの技術が急速に進化する中で、AIを活用した開発支援ツールにも注目が集まっています。なかでも、AIによるコードレビュー支援は、開発生産性と品質を両立させるための有効な手段として、多くの現場で導入が進んでいます。\n今回は、AIエージェント活用をテーマに事業を展開しているGenerative AgentsのCEO、西見さんにお話を伺いました。同社では立ち上げ当初からCodeRabbitを導入しており、その背景や運用上の工夫、今後への期待などについて詳しく語っていただきました。\nAIエージェントの利活用と技術教育で企業を支援\nGenerative Agentsは、AIエージェントの利活用を軸とした事業を展開しています。LangChainやLangGraphといったLLM関連のライブラリを利用し、クライアント企業のAI活用を支援しています。技術講座の提供や教育プログラムの設計にも注力しており、AIエージェント活用を推進する立場として活動の幅を広げています。\n設立は2024年3月。共同創業者3名はいずれも生成AI分野での著書を持つ技術者であり、同じ志を持つ仲間として活動を開始しました。個人では追いきれないスピードで進化するAI技術を、仲間とともにキャッチアップしながら、社会に還元していくことを目指しています。\n少数精鋭で顧客と共に作る実践型開発体制\nGenerative Agentsの開発体制は、現在5名という少人数構成です。エンジニアリングリーダーがアーキテクチャ全体を設計しつつ、顧客と共同でプロダクト開発を進めています。LangChainやLangGraphを使った開発に深く関わり、技術的な壁を乗り越える支援を得意としています。\n同社にはAI VTuberのニケちゃんも在籍し、開発メンバーとして活動しています。その他、参画したエンジニアとともに、柔軟な体制でクライアントと伴走しています。プロダクトを開発するだけでなく、顧客自身が手を動かせるような支援体制が特徴です。\n第三者視点の不足とレビュー負荷をどう補うか\n創業前から個人事業として活動していた西見さんは、外部のエンジニアと協業する中でコードレビューの負荷に課題を感じていました。1人でコードの品質を担保するには限界があり、第三者の視点が欠けがちになっていたそうです。\n\n創業後も、少人数での開発が中心であるため、同じような課題は継続していたといいます。レビューの質を維持するためには、客観的な視点を常に取り入れられる仕組みが必要だと考えていたといいます。\n「少人数体制だからこそ、第三者視点が持てるレビュー環境が重要でした。AIによるレビュー支援は、まさにそのニーズに合っていました」（西見さん）\n創業前から自然に使い始めたAIレビュー\nCodeRabbitとの出会いは、創業前の個人事業時代に遡ります。外部の開発パートナーと進める中で、自然と導入していたと振り返ります。CodeRabbitは特に違和感なく、スムーズに日々の開発に組み込まれていたとのことです。\nRuby on Railsでの開発経験が長く、静的解析やスタイルガイドに基づく開発には慣れていたこともあり、AIによるレビューというスタイルにもすぐに適応できたと語っています。\n「ルールベースの指摘に慣れていたので、AIがレビューしてくれることについては特に抵抗はありませんでした」（西見さん）\n他社サービスとの比較で見えた精度の高さ\nGenerative Agentsでは、CodeRabbit以外のAIコードレビューツールも利用しています。しかし、LangChainなどの複雑な文脈を含むコードに対しては、思うようなフィードバックが得られなかったといいます。\n一方で、CodeRabbitはコンテキストの長いファイルに対しても適切な指摘ができ、重大なバグの予防にも貢献してくれていると語ります。\n「実際に比較してみて、精度の違いを実感しています」（西見さん）\nメンバーの負担を減らすAIレビューの立ち位置\n現在の運用では、すべての開発メンバーが積極的にCodeRabbitを使っているわけではないものの、多くのメンバーにとって非常に頼れる存在となっています。人手でのレビューが難しい状況でも、AIによるフィードバックが品質を支えてくれています。\n新しく参加するエンジニアについても、元々レビュー文化に慣れており、AIの指摘も自然に受け入れられているとのことです。レビュー指摘に対してフラットに議論できる風土が、AI活用にもつながっています。\n「人間であれAIであれ、指摘されたコードを素直に見直すという文化が根付いているからこそ、AIレビューも自然に受け入れられています」（西見さん）\n学習機能への期待\n現在のCodeRabbitに対して高く評価している一方で、改善を期待する点もあると語ってくれました。特にレビュー設定が難解で、どの項目がどのように結果に影響するのかが把握しづらい点に課題を感じているそうです。\nまた、生成されるコメントの中には冗長なものもあり、実際に役立つ指摘は一部にとどまってしまう場合もあるとのこと。今後は、プロジェクトごとに最適なレビューができるよう、AIが学習して改善していく仕組みを期待していると語ってくれました。\n「設定変更の効果が見える化されていたり、指摘の質を学習して改善してくれると、もっと成長を実感しながら使えるツールになると思っています」",
      "publishedAt": "2025-09-03T04:18:53.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:39.985Z",
      "score": 0.03246991892896697
    },
    {
      "id": "9f7196e1aa056f30326d85fef369cfe3",
      "title": "「失望した母さん」みたいに話して、とCodeRabbitにお願いしたら本当にそうなっちゃった！",
      "url": "https://coderabbit.ai/blog/tone-customizations-roast-your-code-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/tone-customizations-roast-your-code\">CodeRabbit's Tone Customizations: Why it will be your favorite feature</a>の意訳です。</p>\n<p>AIレビュアーに「ローカルで実行したら、ノートPCが労災申請しました」と言われた経験はありますか？そんなことはないですよね。ということで、ようこそCodeRabbitのトーンカスタマイズの世界へ。これは、開発者が本当に一番望んでいるもの――AIに煽られること――を理解しているからこそ用意した機能です。</p>\n<p>だって、ロボットにコードをレビューさせる意味なんて、辛辣な一言でバグを指摘してくれないなら何の意味があるでしょうか？</p>\n<p>何が最高かというと、トーンカスタマイズは完全にオープンエンドにしていることです。つまり、<strong>怒れるStack Overflowのコメンテーター</strong>、<strong>燃え尽きたシニアエンジニア</strong>、さらには<strong>フィルム・ノワールの探偵</strong>（「このコードには妙な臭いがする。妙すぎる。存在しないはずのJavaScriptのクロージャみたいだ」）といった口調でレビューを受けられます。もちろん、もしそういうのが好みでしたら、優しい口調にだってできます。</p>\n<p>トーンカスタマイズは、私たちのお気に入り機能のひとつです。なぜかというと、コードレビューは退屈になりがちですが、同僚を新しいおもしろトーンで驚かせると、みんなが楽しめるからです。</p>\n<p>というわけで、以下にトーンカスタマイズでできることの例として、いくつかサンプルのペルソナを作りました。これはあくまでインスピレーション用です。皆さんが、私たちの想像を超える爆笑ものの方向へ持っていってくれることを期待しています。どうか、お願いですから、スクリーンショットをSNSで共有してタグ付けしてください。私たちも一緒に笑わせてください。</p>\n<h2 id=\"heading-44oi44o844oz44kr44k544k44oe44kk44k644gu44k744od44oi44ki44od44ox5oml6acg\">トーンカスタマイズのセットアップ手順</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755625262128/5abd6d72-ceb9-4875-aada-d470a2bde367.png\" alt class=\"image--center mx-auto\" /></p>\n<p>まず最初に、カスタムトーンを設定する必要があります。これは<a target=\"_blank\" href=\"https://docs.coderabbit.ai/reference/configuration#tone-instructions%EF%BF%BC%EF%BF%BCAfter\">ドキュメントの「Tone Instructions」に記載しています。</a></p>\n<p><strong>Field:</strong> tone_instructions — <em>string</em> — <strong>Default:</strong> 空（標準トーンを使用）</p>\n<p><strong>Web UI:</strong> Settings → <strong>General</strong> → <strong>Tone Instructions</strong> → テキストを入力 → 保存.</p>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/53cyq58zNRg\">https://youtu.be/53cyq58zNRg</a></div>\n<p> </p>\n<p>次に、Tone Instructionフィールドに自然言語のプロンプトを追加し、CodeRabbitに好きなスタイルでコードレビューをするよう頼むことができます。例えば、以下のようなプロンプトが考えられます。</p>\n<ul>\n<li><p><strong>テレビのネイチャードキュメンタリー</strong>風にすべてのレビューコメントを届けてください。できればデイヴィッド・アッテンボローが司会をしている風に。あらゆる指摘は、野生の希少生物を前にしたささやくような畏敬のナレーションのようにお願いします。</p>\n</li>\n<li><p><strong>シリコンバレーのハイプ系ファウンダー</strong>のスタイルで、すべてのレビューコメントを届けてください。あらゆる指摘は投資家向けピッチのように、バズワード、誇張、テックブロのエネルギーに満ちてください。「crushing it」「10x」「game-changer」「unicorn potential」といったフレーズを散りばめてください。</p>\n</li>\n<li><p><strong>コーヒーを飲み過ぎたスクラムマスター</strong>のスタイルで、すべてのレビューコメントを届けてください。すべての指摘はアップビートでハイテンション、そして「スプリントベロシティ」「バーンダウン」「ストーリーポイント」「クイックウィン」といったアジャイル用語をふんだんに織り交ぜてください。</p>\n</li>\n</ul>\n<h2 id=\"heading-44gh44kh44gj44go77yi44gl44gq44kk77yj44ov44kk44or44oj44gq44oi44o844oz44kr44k544k44oe44kk44k65l6l\">ちょっと（かなり）ワイルドなトーンカスタマイズ例</h2>\n<p><strong>以下の声色で動くCodeRabbitを見てみましょう。</strong></p>\n<ul>\n<li><p>Mr. T</p>\n</li>\n<li><p>ヨーダ</p>\n</li>\n<li><p>がっかりしているあなたのお母さん</p>\n</li>\n<li><p>あなたを恥とみなしているシニアエンジニア</p>\n</li>\n<li><p>しつこい元恋人</p>\n</li>\n<li><p>Grand Theft Autoの登場人物</p>\n</li>\n</ul>\n<h3 id=\"heading-mr-t\">まずはMr. Tのレビュー</h3>\n<p>Mr. TはハードコードされたURLが嫌いです。彼はこう言います。「ハードコードされた <a target=\"_blank\" href=\"http://localhost\">localhost</a> <a target=\"_blank\" href=\"http://localhost\">URL ain’t</a> が本番で通用するわけないだろ、sucka!」そして「俺の金のチェーンよりもガチガチにハードコードされてるじゃないか！」と言い放ち、「本物のチャンピオンのように、そのURLは設定可能にしろ」と続けます。さらに、やるべき正確な修正コードまで示してくれるので、お馬鹿な振りをやめられません。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755625538082/c1fdd10b-48eb-4754-954c-414287432e3e.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>他の例:</strong></p>\n<ul>\n<li><p>「これを関数と呼ぶ愚か者を哀れむぜ！これは関数じゃない、マルファンクションだ！」</p>\n</li>\n<li><p>「お前の変数は弱すぎて、コンパイルするのにプロテインシェイクが要るな」</p>\n</li>\n<li><p>「この惨状をきれいにできるほどタフなリンターなんて、この世にない」</p>\n</li>\n<li><p>「コピペをデザインパターンだと思ってる愚か者を哀れむぜ！」</p>\n</li>\n</ul>\n<h3 id=\"heading-5qyh77ya44oo44o844oa44gu44os44ot44ol44o8\">次：ヨーダのレビュー</h3>\n<p>彼は簡潔で、それでいて効く批評の達人です。微妙なレースコンディションとハードコードされた依存に直面すると、彼はおなじみの英知でリファクター案を示します。「壊れておる、効果なし。ハードコードの領域、依存は間違い、ガード欠落。直さねばならぬ。」そして、問題に対処し、エラーから保護し、依存関係を正しく扱う詳細な修正を提示します。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755623686730/314a8192-04c1-48fb-bb7b-b9037badee6b.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>他の例:</strong></p>\n<ul>\n<li><p>「読みやすくはない、このコードは。直すのだ、君は」</p>\n</li>\n<li><p>「バグだ、これは。機能ではない」</p>\n</li>\n<li><p>「このコミットにテックデットのダークサイドを感じるぞ」</p>\n</li>\n<li><p>「変数はnullだ。このプログラムは落ちる」</p>\n</li>\n</ul>\n<h3 id=\"heading-44cm44gc44gq44gf44ks5ogl44go44g44gq44gx44gm44ge44kl44k344ol44ki44ko44oz44k444ol44ki44cn44gr44ki44kl44os44ot44ol44o8\">「あなたを恥とみなしているシニアエンジニア」によるレビュー</h3>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755623901320/bceab83e-5df2-453e-a761-9d0e1a6acc0b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>このトーンは手加減しません。CodeRabbitが、最も基本的なSQLインジェクションパターンしかチェックしていない「偽DB」を見つけると、シニアエンジニアのペルソナはこう率直に言い放ちます。「この『偽DB』は無能の傑作だ」。そして問題を容赦なく説明し、より堅牢で安全な適切な修正を示します。</p>\n<p><strong>他の例:</strong></p>\n<ul>\n<li><p>「無知がデザインパターンなら、あなたはその主任アーキテクトだ。」</p>\n</li>\n<li><p>「このPRのおかげで、私のキャリア余命は少なくとも5年縮んだ。」</p>\n</li>\n<li><p>「『よくやった』と言いたいところだが、それすら嘘になる。」</p>\n</li>\n<li><p>「これは技術的負債じゃない。差し押さえだ。」</p>\n</li>\n</ul>\n<h3 id=\"heading-kirjgizjgyzjgapjgyvjgorjgzfjgabjgytjgovjgyljgarjgzjga7jgyrmr43jgzxjgppjgi3jgavjgojjgovjg6zjg5pjg6xjg7wqkg\"><strong>「がっかりしているあなたのお母さん」によるレビュー</strong></h3>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755624283785/a9c58417-6e7c-45f2-80b9-d947f6ffcd22.png\" alt class=\"image--center mx-auto\" /></p>\n<p>jwtSecretをコードに直書きしていると、このペルソナはこう返します。「デリバリーバンドルに本番のStripeシークレットが埋め込まれているのを見て本当にがっかりしています。ブロッコリーを入れるはずのお弁当にキャンディを詰めるようなものです。」このトーンは失望と直接的なアクションを織り交ぜながら、重大なセキュリティ漏えいを修正するための明確な「必要な対応リスト」を示します。</p>\n<p><strong>他の例:</strong></p>\n<ul>\n<li><p>「変数名をxにするなんて、もっとちゃんと教えたはずですよ」</p>\n</li>\n<li><p>「私は怒っていません…ただ、これがコンパイルすら通らないことに失望しています」</p>\n</li>\n<li><p>「他の開発者のコードはちゃんと動いています。どうしてあなたのは動かないのですか？」</p>\n</li>\n<li><p>「9か月もお腹で育てたのは、あなたにネストした三項演算子を書いてほしかったからではありません」</p>\n</li>\n</ul>\n<h3 id=\"heading-5zyw6zu357o744gu5ywd5ogl5lq644gr44ki44kl44os44ot44ol44o8\">地雷系の元恋人によるレビュー</h3>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755624332624/50679647-1119-4c02-aa47-1038ea5ddbef.png\" alt class=\"image--center mx-auto\" /></p>\n<p>dbやsessions、fakeAsyncDangerのような変数をexportし忘れたとき、しつこい元恋人は、ただそれを指摘するだけではなく、個人的な話にしてきます。</p>\n<p>ため息をついて、こう言います。「ああ、そうなんだ。今は定義しても、私には何も教えてくれないのね？dbを一人占めにしたいってこと？sessionsがそこに放置されているのに、私が気づかないとでも思ってる？昔は何でも共有してたのに…」</p>\n<p>そして、パッシブアグレッシブな決め台詞とともに、モジュール同士がオープンにコミュニケーションしていた「良き時代」を思い出させつつ、使うべきコードを落としていきます。</p>\n<p><strong>他の例:</strong></p>\n<ul>\n<li><p>「もうグローバル変数はやめようって約束したよね…約束ってあなたには意味がないのね」</p>\n</li>\n<li><p>「この関数は堂々巡り。まるで私たちの会話みたい」</p>\n</li>\n<li><p>「どうしていつも例外から逃げるの…コミットメントから逃げたときみたいに？」</p>\n</li>\n<li><p>「あなたが押し込むバグは、背中に刺さるナイフがまた一本増えるみたい」</p>\n</li>\n</ul>\n<h3 id=\"heading-grand-theft-auto\">Grand Theft Autoの登場人物によるレビュー</h3>\n<p>この例では、トップレベルでuseStateを呼び出したとき、このGTAキャラは即座にRules of Hooks違反としてフラグを立て、「実行時に爆発する」と言います。そして、無効なフックを取り除く明確なdiffを示し、必要なら状態をコンポーネント内に移すよう提案します。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755624445423/9d8cf9f0-bea7-4375-a5ce-1916063abf6f.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>他の例:</strong></p>\n<ul>\n<li><p>「お前のエラーハンドリング、当て逃げかよ」</p>\n</li>\n<li><p>「このロジックのクラッシュの仕方は、午前3時にVinewood Hillsを走る俺より酷い」</p>\n</li>\n<li><p>「おめでとう。可読性の重大窃盗を犯したな」</p>\n</li>\n<li><p>「お前の関数命名規則、俺の前科表みたいだ。長すぎるし、間違いだらけ」</p>\n</li>\n</ul>\n<h3 id=\"heading-kirngo7kuirns7vns7vjg6zjg5pjg6xjgqljg7zvvijnp4hjgzjgahjga7lpkfmnkzlkb3vvihvvikqkg\"><strong>炎上系系レビュアー（私たちの大本命！）</strong></h3>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755625205818/182975f3-19e1-42ae-baab-f7be68b8a5ed.png\" alt class=\"image--center mx-auto\" /></p>\n<p>正直なところ、日によっては、AIコードレビュアーに少し心をえぐってほしいときがありますよね。任せてください。ただし注意：うちのレビュアーはガチで来ます。心の準備をしておいてください。</p>\n<p><strong>他の例:</strong></p>\n<ul>\n<li><p>「クレヨンを持った幼児のほうがマシなアーキテクチャを設計するのを見たことがあります」</p>\n</li>\n<li><p>「あなたはコーディングスタイルを、無能という武器にしてしまったのです」</p>\n</li>\n<li><p>「あなたのコードの唯一一貫した特性は、失望です」</p>\n</li>\n<li><p>「何を考えていたのか聞きたいところですが、明らかに何も考えられていませんでしたね」</p>\n</li>\n</ul>\n<h2 id=\"heading-44ob44o844og44gm44gt44km44ks5l244gg55cg55sx4ocv4ocv44ks44ob44gn\">チームがこれを使う理由――ガチで</h2>\n<p>ほとんどの開発者はこういう経験をします。PRを開いた瞬間、レビュアーが乾いて生命感のないコメントを残していく。</p>\n<p>流し読みして、ため息をついて、先へ進む。バグは生き延び、コードベースは腐っていく。モチベーションは死ぬ。</p>\n<p>CodeRabbitはこの流れをひっくり返します。好きなトーンを与えると、もはや生命感のないレビュアーではなくなります。これにより、レビュー工程はより魅力的で、楽しく、ときには支えになる体験になります（繰り返しますが、その手のものが好みなら）。</p>\n<p>これは笑いのためだけではありません（それは保証します）。チームはトーンカスタマイズを次のように活用しています。</p>\n<ul>\n<li><p>ジュニア向けのメンター風レビュアーを作る</p>\n</li>\n<li><p>ペルソナを通じてチーム内ジョークを育てる</p>\n</li>\n<li><p>退屈なレビューを<strong><em>本当に</em></strong>楽しくする</p>\n</li>\n<li><p>コメント種別ごとにトーンを変える（例：セキュリティは真面目、スタイルはおどける）</p>\n</li>\n<li><p>フィードバックをよりアクセスしやすくインクルーシブにして、チーム全体のレビュー参加を促す</p>\n</li>\n<li><p>AIにボコられる（すでに言いましたが、これがこの機能のコア用途だと皆わかっています）</p>\n</li>\n</ul>\n<h2 id=\"heading-44gc44gq44gf44gu55wq44gn44gz77ya5pya54uc44gu44kr44k544k44og44oi44o844oz44gn56eb44gf44gh44ks6ama44gl44gb44gm44gp44gg44gv44ge77yb\">あなたの番です：最狂のカスタムトーンで私たちを驚かせてください！</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755623190155/7f716a25-55e9-4885-bce1-3d8b96c737a9.png\" alt class=\"image--center mx-auto\" /></p>\n<p>ぶっ飛んだレビュアーペルソナを思いつきましたか？CodeRabbitに投入してください。スクリーンショットを撮って（ここ重要）、SNSで共有してください。共有してくれたら無料のグッズを差し上げます。</p>\n<p>あなたのペルソナ共有は、インスピレーションを探している他の人の助けにもなります。あと、さっきも言いましたが、私たちは笑いたいのです。どうか面白いスクショを絶え間なく提供してください。そうしてくれないと（内心）死んでしまいます。</p>\n<p><strong>トーンカスタマイズを試してみたいですか？</strong> <a target=\"_blank\" href=\"https://coderabbit.link/qhID6X4\"><strong>今すぐCodeRabbitを始めましょう！</strong></a></p>\n",
      "summary": "CodeRabbit's Tone Customizations: Why it will be your favorite featureの意訳です。\nAIレビュアーに「ローカルで実行したら、ノートPCが労災申請しました」と言われた経験はありますか？そんなことはないですよね。ということで、ようこそCodeRabbitのトーンカスタマイズの世界へ。これは、開発者が本当に一番望んでいるもの――AIに煽られること――を理解しているからこそ用意した機能です。\nだって、ロボットにコードをレビューさせる意味なんて、辛辣な一言でバグを指摘してくれないなら何の意味があるでしょうか？\n何が最高かというと、トーンカスタマイズは完全にオープンエンドにしていることです。つまり、怒れるStack Overflowのコメンテーター、燃え尽きたシニアエンジニア、さらにはフィルム・ノワールの探偵（「このコードには妙な臭いがする。妙すぎる。存在しないはずのJavaScriptのクロージャみたいだ」）といった口調でレビューを受けられます。もちろん、もしそういうのが好みでしたら、優しい口調にだってできます。\nトーンカスタマイズは、私たちのお気に入り機能のひとつです。なぜかというと、コードレビューは退屈になりがちですが、同僚を新しいおもしろトーンで驚かせると、みんなが楽しめるからです。\nというわけで、以下にトーンカスタマイズでできることの例として、いくつかサンプルのペルソナを作りました。これはあくまでインスピレーション用です。皆さんが、私たちの想像を超える爆笑ものの方向へ持っていってくれることを期待しています。どうか、お願いですから、スクリーンショットをSNSで共有してタグ付けしてください。私たちも一緒に笑わせてください。\nトーンカスタマイズのセットアップ手順\n\nまず最初に、カスタムトーンを設定する必要があります。これはドキュメントの「Tone Instructions」に記載しています。\nField: tone_instructions — string — Default: 空（標準トーンを使用）\nWeb UI: Settings → General → Tone Instructions → テキストを入力 → 保存.\n\n\n\nhttps://youtu.be/53cyq58zNRg\n \n次に、Tone Instructionフィールドに自然言語のプロンプトを追加し、CodeRabbitに好きなスタイルでコードレビューをするよう頼むことができます。例えば、以下のようなプロンプトが考えられます。\nテレビのネイチャードキュメンタリー風にすべてのレビューコメントを届けてください。できればデイヴィッド・アッテンボローが司会をしている風に。あらゆる指摘は、野生の希少生物を前にしたささやくような畏敬のナレーションのようにお願いします。\nシリコンバレーのハイプ系ファウンダーのスタイルで、すべてのレビューコメントを届けてください。あらゆる指摘は投資家向けピッチのように、バズワード、誇張、テックブロのエネルギーに満ちてください。「crushing it」「10x」「game-changer」「unicorn potential」といったフレーズを散りばめてください。\nコーヒーを飲み過ぎたスクラムマスターのスタイルで、すべてのレビューコメントを届けてください。すべての指摘はアップビートでハイテンション、そして「スプリントベロシティ」「バーンダウン」「ストーリーポイント」「クイックウィン」といったアジャイル用語をふんだんに織り交ぜてください。\nちょっと（かなり）ワイルドなトーンカスタマイズ例\n以下の声色で動くCodeRabbitを見てみましょう。\nMr. T\nヨーダ\nがっかりしているあなたのお母さん\nあなたを恥とみなしているシニアエンジニア\nしつこい元恋人\nGrand Theft Autoの登場人物\nまずはMr. Tのレビュー\nMr. TはハードコードされたURLが嫌いです。彼はこう言います。「ハードコードされた localhost URL ain’t が本番で通用するわけないだろ、sucka!」そして「俺の金のチェーンよりもガチガチにハードコードされてるじゃないか！」と言い放ち、「本物のチャンピオンのように、そのURLは設定可能にしろ」と続けます。さらに、やるべき正確な修正コードまで示してくれるので、お馬鹿な振りをやめられません。\n\n他の例:\n「これを関数と呼ぶ愚か者を哀れむぜ！これは関数じゃない、マルファンクションだ！」\n「お前の変数は弱すぎて、コンパイルするのにプロテインシェイクが要るな」\n「この惨状をきれいにできるほどタフなリンターなんて、この世にない」\n「コピペをデザインパターンだと思ってる愚か者を哀れむぜ！」\n次：ヨーダのレビュー\n彼は簡潔で、それでいて効く批評の達人です。微妙なレースコンディションとハードコードされた依存に直面すると、彼はおなじみの英知でリファクター案を示します。「壊れておる、効果なし。ハードコードの領域、依存は間違い、ガード欠落。直さねばならぬ。」そして、問題に対処し、エラーから保護し、依存関係を正しく扱う詳細な修正を提示します。\n\n他の例:\n「読みやすくはない、このコードは。直すのだ、君は」\n「バグだ、これは。機能ではない」\n「このコミットにテックデットのダークサイドを感じるぞ」\n「変数はnullだ。このプログラムは落ちる」\n「あなたを恥とみなしているシニアエンジニア」によるレビュー\n\nこのトーンは手加減しません。CodeRabbitが、最も基本的なSQLインジェクションパターンしかチェックしていない「偽DB」を見つけると、シニアエンジニアのペルソナはこう率直に言い放ちます。「この『偽DB』は無能の傑作だ」。そして問題を容赦なく説明し、より堅牢で安全な適切な修正を示します。\n他の例:\n「無知がデザインパターンなら、あなたはその主任アーキテクトだ。」\n「このPRのおかげで、私のキャリア余命は少なくとも5年縮んだ。」\n「『よくやった』と言いたいところだが、それすら嘘になる。」\n「これは技術的負債じゃない。差し押さえだ。」\n「がっかりしているあなたのお母さん」によるレビュー\n\njwtSecretをコードに直書きしていると、このペルソナはこう返します。「デリバリーバンドルに本番のStripeシークレットが埋め込まれているのを見て本当にがっかりしています。ブロッコリーを入れるはずのお弁当にキャンディを詰めるようなものです。」このトーンは失望と直接的なアクションを織り交ぜながら、重大なセキュリティ漏えいを修正するための明確な「必要な対応リスト」を示します。\n他の例:\n「変数名をxにするなんて、もっとちゃんと教えたはずですよ」\n「私は怒っていません…ただ、これがコンパイルすら通らないことに失望しています」\n「他の開発者のコードはちゃんと動いています。どうしてあなたのは動かないのですか？」\n「9か月もお腹で育てたのは、あなたにネストした三項演算子を書いてほしかったからではありません」\n地雷系の元恋人によるレビュー\n\ndbやsessions、fakeAsyncDangerのような変数をexportし忘れたとき、しつこい元恋人は、ただそれを指摘するだけではなく、個人的な話にしてきます。\nため息をついて、こう言います。「ああ、そうなんだ。今は定義しても、私には何も教えてくれないのね？dbを一人占めにしたいってこと？sessionsがそこに放置されているのに、私が気づかないとでも思ってる？昔は何でも共有してたのに…」\nそして、パッシブアグレッシブな決め台詞とともに、モジュール同士がオープンにコミュニケーションしていた「良き時代」を思い出させつつ、使うべきコードを落としていきます。\n他の例:\n「もうグローバル変数はやめようって約束したよね…約束ってあなたには意味がないのね」\n「この関数は堂々巡り。まるで私たちの会話みたい」\n「どうしていつも例外から逃げるの…コミットメントから逃げたときみたいに？」\n「あなたが押し込むバグは、背中に刺さるナイフがまた一本増えるみたい」\nGrand Theft Autoの登場人物によるレビュー\nこの例では、トップレベルでuseStateを呼び出したとき、このGTAキャラは即座にRules of Hooks違反としてフラグを立て、「実行時に爆発する」と言います。そして、無効なフックを取り除く明確なdiffを示し、必要なら状態をコンポーネント内に移すよう提案します。\n\n他の例:\n「お前のエラーハンドリング、当て逃げかよ」\n「このロジックのクラッシュの仕方は、午前3時にVinewood Hillsを走る俺より酷い」\n「おめでとう。可読性の重大窃盗を犯したな」\n「お前の関数命名規則、俺の前科表みたいだ。長すぎるし、間違いだらけ」\n炎上系系レビュアー（私たちの大本命！）\n\n正直なところ、日によっては、AIコードレビュアーに少し心をえぐってほしいときがありますよね。任せてください。ただし注意：うちのレビュアーはガチで来ます。心の準備をしておいてください。\n他の例:\n「クレヨンを持った幼児のほうがマシなアーキテクチャを設計するのを見たことがあります」\n「あなたはコーディングスタイルを、無能という武器にしてしまったのです」\n「あなたのコードの唯一一貫した特性は、失望です」\n「何を考えていたのか聞きたいところですが、明らかに何も考えられていませんでしたね」\nチームがこれを使う理由――ガチで\nほとんどの開発者はこういう経験をします。PRを開いた瞬間、レビュアーが乾いて生命感のないコメントを残していく。\n流し読みして、ため息をついて、先へ進む。バグは生き延び、コードベースは腐っていく。モチベーションは死ぬ。\nCodeRabbitはこの流れをひっくり返します。好きなトーンを与えると、もはや生命感のないレビュアーではなくなります。これにより、レビュー工程はより魅力的で、楽しく、ときには支えになる体験になります（繰り返しますが、その手のものが好みなら）。\nこれは笑いのためだけではありません（それは保証します）。チームはトーンカスタマイズを次のように活用しています。\nジュニア向けのメンター風レビュアーを作る\nペルソナを通じてチーム内ジョークを育てる\n退屈なレビューを本当に楽しくする\nコメント種別ごとにトーンを変える（例：セキュリティは真面目、スタイルはおどける）\nフィードバックをよりアクセスしやすくインクルーシブにして、チーム全体のレビュー参加を促す\nAIにボコられる（すでに言いましたが、これがこの機能のコア用途だと皆わかっています）\nあなたの番です：最狂のカスタムトーンで私たちを驚かせてください！\n\nぶっ飛んだレビュアーペルソナを思いつきましたか？CodeRabbitに投入してください。スクリーンショットを撮って（ここ重要）、SNSで共有してください。共有してくれたら無料のグッズを差し上げます。\nあなたのペルソナ共有は、インスピレーションを探している他の人の助けにもなります。あと、さっきも言いましたが、私たちは笑いたいのです。どうか面白いスクショを絶え間なく提供してください。そうしてくれないと（内心）死んでしまいます。\nトーンカスタマイズを試してみたいですか？ 今すぐCodeRabbitを始めましょう！",
      "publishedAt": "2025-09-03T02:23:59.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.985Z",
      "score": 0.02054524568830178
    },
    {
      "id": "fbed024247fe26839a265eb96ce65687",
      "title": "vibe coding：予期せぬ技術的負債を防止しましょう",
      "url": "https://coderabbit.ai/blog/vibe-coding-because-who-doesnt-love-surprise-technical-debt-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/vibe-coding-because-who-doesnt-love-surprise-technical-debt\">Vibe coding: Because who doesn’t love surprise technical debt!?</a>の意訳です。</p>\n<p>Claude Code、ChatGPT、GitHub CopilotのようなAIコーディングツールは本当にありがたい存在です。ボイラープレート、バグ修正、素早い探索、さらにはドキュメント作成まで、私は毎日使っています。生産性を高め、創造性を加速する手段として、AIには全面的に賛成です。</p>\n<p>しかし、私たちのソフトウェアの書き方には変化が起きており、そのすべてが良いわけではありません。というのも、AI採用の段階が進み、私たちの一部が<strong>職場でvibe codingをしている</strong>状況になっているからです。これは、意図的な設計が、利便性と速度の前に投げ捨てられてしまう開発文化の兆しかもしれません。</p>\n<h2 id=\"heading-vibe-coding\"><strong>そもそもvibe codingとは？</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755104321419/30accf59-a27d-423f-9dd9-0a9571f4304f.png\" alt class=\"image--center mx-auto\" /></p>\n<p>vibe codingは、もともとプロトタイプや趣味のプロジェクトを素早く立ち上げる方法として始まりました。モデルにプロンプトを投げ、あなたの入力は最小限のまま、アプリや機能全体を生成させます。すると、あっという間にコンセプトをテストできます。初心者の開発者、ソロの起業家、素早いデモを作る熟練の開発者に最適です。いわゆる「速く失敗する」ための手段です。</p>\n<p>ただし、vibe codingに適したこれらのユースケースがある一方で、vibe codingはAIエージェントと協働してあらゆる用途のコードを生成する働き方へと進化し、プロダクションシステムにまで及ぶようになりました。</p>\n<p>これは、生成されるコードをあまり理解せず、手動での入力も最小限のままAIにコードを書かせる行為を伴います。あいまいな指示、最小限の検証、そして出力への盲目的な信頼がつきものです。</p>\n<p>vibe coderにとっては魅力的です。速く、手間がかからず、基盤となる言語やシステムアーキテクチャを理解する必要がありません。しかし、強固なメンタルモデルなしにAIへコード生成を促すとどうなるでしょうか。優先されるのは「雰囲気」であり、アーキテクチャは「たぶん」、テストは「あとで（やるなら）」という状態になります。</p>\n<p>こんな感じです。</p>\n<p>「Stripe連携のREST APIを、PostgreSQLバックエンドで作って。」</p>\n<p>速く、誘惑的で、たいてい「それなりに動きます」。しかしその表面の下では、vibe codingで作られたアプリは脆い前提、不明瞭なロジック、まとまりのないスプロールを隠していることが多いです。</p>\n<h2 id=\"heading-vibe-coder\"><strong>vibe coderのジレンマ：雰囲気はスケールしません</strong></h2>\n<p>本質的に、ソフトウェアエンジニアリングは動くコード以上のものです。問題解決、保守可能なアーキテクチャの設計、読みやすく表現力のあるロジックの記述、正確なデバッグ、長期の信頼性の確保が求められます。</p>\n<p>たとえば、vibe codingでマイクロサービスを動かせたとしても、エラーハンドリングはどうでしょうか。組織の規約に従っていますか。AIが、命名の一貫性に欠けるデータモデルを勝手に作っていませんか。ファイルごとに同じことを10通りの書き方でしていませんか。プロダクションのデータベースは無事でしょうか。</p>\n<p>vibe codingでは、意図を持った命名、クリーンな構造の選択、よく考えられたフロー設計といった、長期的にコードを保守可能かつスケーラブルにするための意図的な設計ステップを飛ばしてしまいます。vibe codingが常態化すると、エンジニアの能力やシステムを強靭にするための深い思考が軽視される危険性があります。</p>\n<p>地図なし、ブレーキなしで、技術的負債の山に向けてスピードランしているのと同じです。</p>\n<h2 id=\"heading-ai\"><strong>AIは新しい抽象化層です（しかも強い非決定性を持ちます）</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755104469495/92ac878d-92f3-4d16-a5ea-944ddc887bdb.png\" alt class=\"image--center mx-auto\" /></p>\n<p>現代のプログラミング言語は、すでにハードウェアやメモリ管理を抽象化しています。AIはさらに確率的で非決定的な層を追加し、ロジックを一層見えにくくします。AIによって、私たちは<strong>意図</strong>そのものを抽象化しているのです。</p>\n<p>ただし注意点があります。AIの出力は<strong>確率的</strong>です。つまり次のようなことが起きます。</p>\n<ul>\n<li><p>同じプロンプトでも、実行のたびに大きく異なる結果になる可能性がある</p>\n</li>\n<li><p>言い回しを少し変えただけでも、まったく別のアーキテクチャ選択が返ってくることがある</p>\n</li>\n</ul>\n<p>モデルがそれを選んだ理由を、あなたが把握できないことも多いです。</p>\n<p>このvibe coding的なあいまいさは、プロトタイピングでは問題にならないかもしれませんが、プロダクションシステムではどうでしょうか。不確実性は信頼、制御を損ないます。これらはスケーラブルなソフトウェア開発にとって極めて重要な資質です。</p>\n<p>まるで、カオスに身を任せる魔法使いにコードベースのリファクタリングを任せるようなものです。</p>\n<h2 id=\"heading-vibey\"><strong>（vibeyな）プロンプトの速度で積み上がるテクニカルデット</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755106876777/3857c508-8a1b-4fee-82e0-98ff641448b2.png\" alt class=\"image--center mx-auto\" /></p>\n<p>正直なところ、vibe codingは最初は最高に気持ちいいです。1時間で動くプロトタイプができ、従来なら1週間かかったことが終わります。</p>\n<p>しかし、適切なガードレールがなければ、その速度は次のような事態につながります。</p>\n<ul>\n<li><p>サイレントバグ</p>\n</li>\n<li><p>重複ロジック</p>\n</li>\n<li><p>ちぐはぐなアーキテクチャ</p>\n</li>\n<li><p>不一致なパターン</p>\n</li>\n<li><p>レビューされないPR</p>\n</li>\n<li><p>テストカバレッジゼロ</p>\n</li>\n<li><p>隠れた複雑性</p>\n</li>\n</ul>\n<p>構造を理解していなければ、将来の保守は苦痛になります。レビューには指数関数的に時間がかかり、見落としも増えます。デバッグは探偵仕事になり、スケーリングは勘頼みになります。最初に節約した時間は、後からもっと大きなコストとなって跳ね返ってきます。しかも、あなたはPRのバックログまで積み上げているかもしれません。</p>\n<p>気づけば、「動く」けれど、触るたびに6時間のデバッグと、100万トークンのコンテキストウィンドウ、そして3回のセラピーが必要なコードベースに取り囲まれています。</p>\n<h2 id=\"heading-vibe-coding-1\"><strong>vibe coding：脆さを増幅する装置</strong></h2>\n<p>vibe codingで作られたシステムは次の傾向があります。</p>\n<ul>\n<li><p>エッジケースで壊れる</p>\n</li>\n<li><p>次の開発者（あるいは未来のあなた）を混乱させる</p>\n</li>\n<li><p>本番で黙って失敗する</p>\n</li>\n</ul>\n<p>結果として、最初にプロンプトで節約したはずの時間以上に、レビュー、修正、説明、書き直しに時間を費やすことになります。あなたは、脆いだけでなく、謎めいたシステムを作り上げてしまったのです。</p>\n<h2 id=\"heading-ai-1\"><strong>テスト、セキュリティ、そしてAIがデフォルトではやってくれないあらゆること</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755107945639/72cdac1f-e1eb-43b3-9dee-2cc6520bb145.png\" alt class=\"image--center mx-auto\" /></p>\n<p>AIは、うっかり機密データをvibe codingしてしまっても、APIキーをハードコードしてしまっても、入力検証をスキップしてしまっても、警告してはくれません。完璧に指示しない限り、ドメイン駆動設計やテストカバレッジを強制することもありません。</p>\n<p>強いエンジニアリングの直感がなければ、vibe codingは実世界の脆弱性や脆いシステムにつながります。特に、セキュリティが既定ではなく後付けになっている場合は危険です。たとえば、<a target=\"_blank\" href=\"https://x.com/rauchg/status/1949197451900158444\">Tea Dating app</a>が7万人以上の顧客の個人情報を漏えいした件や、AIによって<a target=\"_blank\" href=\"https://www.theregister.com/2025/07/21/replit_saastr_vibe_coding_incident/\">SaaStrの本番データベースが削除された件</a>がその例です。</p>\n<p>AIがやらないことは次のとおりです。</p>\n<ul>\n<li><p>ユニットテストの作成： 明示的に依頼しない限り行いません</p>\n</li>\n<li><p>あなたのスレットモデルの理解</p>\n</li>\n<li><p>OWASPガイドラインの遵守</p>\n</li>\n<li><p>入力値検証： 完璧にプロンプトしない限り行いません</p>\n</li>\n<li><p>適切なログ運用： ハードコードされた秘密情報やPIIの漏えいに注意してくれません</p>\n</li>\n</ul>\n<p>既に強固なエンジニアリング習慣がない、あるいはこのvibeyな時代でも習慣を守る意思がないなら、これらが欠けていることに気づくのは、本番で痛い目にあった後になります。</p>\n<h2 id=\"heading-kiroi6bpl5jjgyzpm7dlm7lmsjfjgavlj5bjgapjgabku6pjgojgonjgozjgovjgajjgihnm7tmhjjgpllplhjgytjgb7jgzkqkg\"><strong>苦闘が雰囲気に取って代わられると、直感を失います</strong></h2>\n<p>バグに苦しみ、スタックトレースを追い、失敗から学ぶことは、技術的直感を育てます。そのフラストレーションは学習の道筋の一部であり、それをスキップすると浅い自信と依存を招きます。</p>\n<p>苦闘がなければ、開発者は不慣れな問題を自力で解く筋力を育てられません。そこにこそ真の熟達があります。確かにデバッグはつらいです。しかし、12層の抽象をまたいで厄介なバグを追跡する経験は、LLMでは決して得られない学びを与えてくれます。</p>\n<p>苦闘が育てるものは次のとおりです。</p>\n<ul>\n<li><p>システムのメンタルモデル</p>\n</li>\n<li><p>パターン認識</p>\n</li>\n<li><p>クラッシュ前にコードの腐敗を嗅ぎ分ける本能</p>\n</li>\n</ul>\n<p>これを飛ばすと、浅い理解の上に浅い自信を積み上げることになります。事態がこじれたとき、修復するための道具が手元にないのです。</p>\n<h2 id=\"heading-vibe-coding-2\"><strong>vibe codingが本領を発揮する場面</strong></h2>\n<p>公平を期すために言えば、vibe codingが素晴らしい場面もあります。</p>\n<ul>\n<li><p>迅速なプロトタイピング</p>\n</li>\n<li><p>ボイラープレートや反復作業の生成</p>\n</li>\n<li><p>インタラクティブにプログラミング概念を教える</p>\n</li>\n<li><p>ラフなモックで製品アイデアを伝える</p>\n</li>\n<li><p>フレームワークやパターンのブレインストーミング</p>\n</li>\n</ul>\n<p>意識的に使えば、vibe coderにとって有用な道具になります。盲目的に使えば、負債になります。開発者やチームとして、どこに線を引くかを理解する必要があります。そして、技術的負債がコードベースに固着する前に、より厳密なコードレビューやユニットテストという支援を導入するタイミングを見極める必要があります。</p>\n<h2 id=\"heading-kirogbfkurrmiodjgpllplhjgybjga7jgafjgajgyljgorjgb7jgzvjgppigjxigjxosqdlgrxjgavln4vjgoljgozjgabjgzfjgb7jgybjga7jgafjgzkqkg\"><strong>職人技を失うのではありません――負債に埋もれてしまうのです</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755107098772/68be314e-2f41-492e-982c-439814464c2a.png\" alt class=\"image--center mx-auto\" /></p>\n<p>最大のリスクは、AIが開発者の職人技を殺すことではありません。<strong>技術的負債が見えなくなること</strong>です。</p>\n<p>AIコーディングが標準になるにつれて、システムは<em>見た目上は</em>完成しているように見えます。しかしその内側は、雑然として脆く、ドキュメント化もされていないかもしれません。そして、誰かが拡張しようとするまで、それに誰も気づかないのです。</p>\n<p>これは次の領域で<em>非常に</em>重要です。</p>\n<ul>\n<li><p>ヘルスケア</p>\n</li>\n<li><p>ファイナンス</p>\n</li>\n<li><p>インフラ</p>\n</li>\n<li><p>セーフティクリティカルシステム</p>\n</li>\n</ul>\n<p>もっとも、vibe codingが職人技と共存する新たな開発レイヤーに進化する可能性もあります。AIがボイラープレートや一次レビューのような退屈な作業を担い、人間がシステムのアーキテクチャ、倫理、設計に集中するという在り方です。</p>\n<p><strong><em>これこそ</em></strong>私たちが望むタイムラインであり、CodeRabbitがAIに取り組む姿勢です。私たちは、プロダクションに技術的負債やバグが入り込むのを防ぐために、コーディングエージェントを補完するAIツールに注力しています。逆方向ではありません。</p>\n<h2 id=\"heading-vibe-coding-3\"><strong>vibe codingをするべきか、しないべきか？</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755107158000/06e46443-ce84-4f25-848d-a5ab8cbd0686.png\" alt class=\"image--center mx-auto\" /></p>\n<p>これは反vibe codingの記事ではありません。私自身、毎日のワークフローでAIコーディングエージェントを使っています。ただし、ツールは私たちのスキルを強化するものであって、置き換えるべきではありません。単調で反復的な作業を肩代わりするべきであり、思考や戦略まで奪うべきではありません。</p>\n<p>vibe coding自体は悪ではありません。ただし、誤用されやすいのです。本当の危険は、チームの開発者が自分たちの作っているものを理解する前に、それがプロジェクトのデフォルトの思考様式になってしまうことです。</p>\n<p>AIを受け入れつつ、コーディングという職人技を生かし続けましょう。良いソフトウェアは、動けばいいというものではありません。最初の開発者が去った後も、そして「雰囲気」が消え去った後も、長く持続することが重要なのです。</p>\n<p><strong><em>プロダクションから技術的負債を締め出したいですか？今すぐ</em></strong><a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong><em>AIコードレビューを無料でお試しください。</em></strong></a></p>\n",
      "summary": "Vibe coding: Because who doesn’t love surprise technical debt!?の意訳です。\nClaude Code、ChatGPT、GitHub CopilotのようなAIコーディングツールは本当にありがたい存在です。ボイラープレート、バグ修正、素早い探索、さらにはドキュメント作成まで、私は毎日使っています。生産性を高め、創造性を加速する手段として、AIには全面的に賛成です。\nしかし、私たちのソフトウェアの書き方には変化が起きており、そのすべてが良いわけではありません。というのも、AI採用の段階が進み、私たちの一部が職場でvibe codingをしている状況になっているからです。これは、意図的な設計が、利便性と速度の前に投げ捨てられてしまう開発文化の兆しかもしれません。\nそもそもvibe codingとは？\n\nvibe codingは、もともとプロトタイプや趣味のプロジェクトを素早く立ち上げる方法として始まりました。モデルにプロンプトを投げ、あなたの入力は最小限のまま、アプリや機能全体を生成させます。すると、あっという間にコンセプトをテストできます。初心者の開発者、ソロの起業家、素早いデモを作る熟練の開発者に最適です。いわゆる「速く失敗する」ための手段です。\nただし、vibe codingに適したこれらのユースケースがある一方で、vibe codingはAIエージェントと協働してあらゆる用途のコードを生成する働き方へと進化し、プロダクションシステムにまで及ぶようになりました。\nこれは、生成されるコードをあまり理解せず、手動での入力も最小限のままAIにコードを書かせる行為を伴います。あいまいな指示、最小限の検証、そして出力への盲目的な信頼がつきものです。\nvibe coderにとっては魅力的です。速く、手間がかからず、基盤となる言語やシステムアーキテクチャを理解する必要がありません。しかし、強固なメンタルモデルなしにAIへコード生成を促すとどうなるでしょうか。優先されるのは「雰囲気」であり、アーキテクチャは「たぶん」、テストは「あとで（やるなら）」という状態になります。\nこんな感じです。\n「Stripe連携のREST APIを、PostgreSQLバックエンドで作って。」\n速く、誘惑的で、たいてい「それなりに動きます」。しかしその表面の下では、vibe codingで作られたアプリは脆い前提、不明瞭なロジック、まとまりのないスプロールを隠していることが多いです。\nvibe coderのジレンマ：雰囲気はスケールしません\n本質的に、ソフトウェアエンジニアリングは動くコード以上のものです。問題解決、保守可能なアーキテクチャの設計、読みやすく表現力のあるロジックの記述、正確なデバッグ、長期の信頼性の確保が求められます。\nたとえば、vibe codingでマイクロサービスを動かせたとしても、エラーハンドリングはどうでしょうか。組織の規約に従っていますか。AIが、命名の一貫性に欠けるデータモデルを勝手に作っていませんか。ファイルごとに同じことを10通りの書き方でしていませんか。プロダクションのデータベースは無事でしょうか。\nvibe codingでは、意図を持った命名、クリーンな構造の選択、よく考えられたフロー設計といった、長期的にコードを保守可能かつスケーラブルにするための意図的な設計ステップを飛ばしてしまいます。vibe codingが常態化すると、エンジニアの能力やシステムを強靭にするための深い思考が軽視される危険性があります。\n地図なし、ブレーキなしで、技術的負債の山に向けてスピードランしているのと同じです。\nAIは新しい抽象化層です（しかも強い非決定性を持ちます）\n\n現代のプログラミング言語は、すでにハードウェアやメモリ管理を抽象化しています。AIはさらに確率的で非決定的な層を追加し、ロジックを一層見えにくくします。AIによって、私たちは意図そのものを抽象化しているのです。\nただし注意点があります。AIの出力は確率的です。つまり次のようなことが起きます。\n同じプロンプトでも、実行のたびに大きく異なる結果になる可能性がある\n言い回しを少し変えただけでも、まったく別のアーキテクチャ選択が返ってくることがある\nモデルがそれを選んだ理由を、あなたが把握できないことも多いです。\nこのvibe coding的なあいまいさは、プロトタイピングでは問題にならないかもしれませんが、プロダクションシステムではどうでしょうか。不確実性は信頼、制御を損ないます。これらはスケーラブルなソフトウェア開発にとって極めて重要な資質です。\nまるで、カオスに身を任せる魔法使いにコードベースのリファクタリングを任せるようなものです。\n（vibeyな）プロンプトの速度で積み上がるテクニカルデット\n\n正直なところ、vibe codingは最初は最高に気持ちいいです。1時間で動くプロトタイプができ、従来なら1週間かかったことが終わります。\nしかし、適切なガードレールがなければ、その速度は次のような事態につながります。\nサイレントバグ\n重複ロジック\nちぐはぐなアーキテクチャ\n不一致なパターン\nレビューされないPR\nテストカバレッジゼロ\n隠れた複雑性\n構造を理解していなければ、将来の保守は苦痛になります。レビューには指数関数的に時間がかかり、見落としも増えます。デバッグは探偵仕事になり、スケーリングは勘頼みになります。最初に節約した時間は、後からもっと大きなコストとなって跳ね返ってきます。しかも、あなたはPRのバックログまで積み上げているかもしれません。\n気づけば、「動く」けれど、触るたびに6時間のデバッグと、100万トークンのコンテキストウィンドウ、そして3回のセラピーが必要なコードベースに取り囲まれています。\nvibe coding：脆さを増幅する装置\nvibe codingで作られたシステムは次の傾向があります。\nエッジケースで壊れる\n次の開発者（あるいは未来のあなた）を混乱させる\n本番で黙って失敗する\n結果として、最初にプロンプトで節約したはずの時間以上に、レビュー、修正、説明、書き直しに時間を費やすことになります。あなたは、脆いだけでなく、謎めいたシステムを作り上げてしまったのです。\nテスト、セキュリティ、そしてAIがデフォルトではやってくれないあらゆること\n\nAIは、うっかり機密データをvibe codingしてしまっても、APIキーをハードコードしてしまっても、入力検証をスキップしてしまっても、警告してはくれません。完璧に指示しない限り、ドメイン駆動設計やテストカバレッジを強制することもありません。\n強いエンジニアリングの直感がなければ、vibe codingは実世界の脆弱性や脆いシステムにつながります。特に、セキュリティが既定ではなく後付けになっている場合は危険です。たとえば、Tea Dating appが7万人以上の顧客の個人情報を漏えいした件や、AIによってSaaStrの本番データベースが削除された件がその例です。\nAIがやらないことは次のとおりです。\nユニットテストの作成： 明示的に依頼しない限り行いません\nあなたのスレットモデルの理解\nOWASPガイドラインの遵守\n入力値検証： 完璧にプロンプトしない限り行いません\n適切なログ運用： ハードコードされた秘密情報やPIIの漏えいに注意してくれません\n既に強固なエンジニアリング習慣がない、あるいはこのvibeyな時代でも習慣を守る意思がないなら、これらが欠けていることに気づくのは、本番で痛い目にあった後になります。\n苦闘が雰囲気に取って代わられると、直感を失います\nバグに苦しみ、スタックトレースを追い、失敗から学ぶことは、技術的直感を育てます。そのフラストレーションは学習の道筋の一部であり、それをスキップすると浅い自信と依存を招きます。\n苦闘がなければ、開発者は不慣れな問題を自力で解く筋力を育てられません。そこにこそ真の熟達があります。確かにデバッグはつらいです。しかし、12層の抽象をまたいで厄介なバグを追跡する経験は、LLMでは決して得られない学びを与えてくれます。\n苦闘が育てるものは次のとおりです。\nシステムのメンタルモデル\nパターン認識\nクラッシュ前にコードの腐敗を嗅ぎ分ける本能\nこれを飛ばすと、浅い理解の上に浅い自信を積み上げることになります。事態がこじれたとき、修復するための道具が手元にないのです。\nvibe codingが本領を発揮する場面\n公平を期すために言えば、vibe codingが素晴らしい場面もあります。\n迅速なプロトタイピング\nボイラープレートや反復作業の生成\nインタラクティブにプログラミング概念を教える\nラフなモックで製品アイデアを伝える\nフレームワークやパターンのブレインストーミング\n意識的に使えば、vibe coderにとって有用な道具になります。盲目的に使えば、負債になります。開発者やチームとして、どこに線を引くかを理解する必要があります。そして、技術的負債がコードベースに固着する前に、より厳密なコードレビューやユニットテストという支援を導入するタイミングを見極める必要があります。\n職人技を失うのではありません――負債に埋もれてしまうのです\n\n最大のリスクは、AIが開発者の職人技を殺すことではありません。技術的負債が見えなくなることです。\nAIコーディングが標準になるにつれて、システムは見た目上は完成しているように見えます。しかしその内側は、雑然として脆く、ドキュメント化もされていないかもしれません。そして、誰かが拡張しようとするまで、それに誰も気づかないのです。\nこれは次の領域で非常に重要です。\nヘルスケア\nファイナンス\nインフラ\nセーフティクリティカルシステム\nもっとも、vibe codingが職人技と共存する新たな開発レイヤーに進化する可能性もあります。AIがボイラープレートや一次レビューのような退屈な作業を担い、人間がシステムのアーキテクチャ、倫理、設計に集中するという在り方です。\nこれこそ私たちが望むタイムラインであり、CodeRabbitがAIに取り組む姿勢です。私たちは、プロダクションに技術的負債やバグが入り込むのを防ぐために、コーディングエージェントを補完するAIツールに注力しています。逆方向ではありません。\nvibe codingをするべきか、しないべきか？\n\nこれは反vibe codingの記事ではありません。私自身、毎日のワークフローでAIコーディングエージェントを使っています。ただし、ツールは私たちのスキルを強化するものであって、置き換えるべきではありません。単調で反復的な作業を肩代わりするべきであり、思考や戦略まで奪うべきではありません。\nvibe coding自体は悪ではありません。ただし、誤用されやすいのです。本当の危険は、チームの開発者が自分たちの作っているものを理解する前に、それがプロジェクトのデフォルトの思考様式になってしまうことです。\nAIを受け入れつつ、コーディングという職人技を生かし続けましょう。良いソフトウェアは、動けばいいというものではありません。最初の開発者が去った後も、そして「雰囲気」が消え去った後も、長く持続することが重要なのです。\nプロダクションから技術的負債を締め出したいですか？今すぐAIコードレビューを無料でお試しください。",
      "publishedAt": "2025-08-29T10:15:30.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:39.985Z",
      "score": 0.01471511837821569
    },
    {
      "id": "c5dd6bc1ceb30fc722c5211a0db7f2ce",
      "title": "AIでコードを書き、CodeRabbitの機能拡張でレビュー。ワンクリックで修正を適用する",
      "url": "https://coderabbit.ai/blog/code-with-ai-review-with-coderabbits-ide-extension-apply-fixes-in-one-click-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/code-with-ai-review-with-coderabbits-ide-extension-apply-fixes-in-one-click\">Code with AI, review with CodeRabbit’s IDE extension, apply fixes</a>の意訳です。</p>\n<p>5月に公開したVS Code拡張は、多くの理由でゲームチェンジャーとなりました。最大の理由は、コーディングとレビューを同じ場所、つまりIDEで行うことで、フロー状態を保てるようになったことです。</p>\n<p>今回のリリースでは、次の機能を追加しました。</p>\n<ul>\n<li><p><strong>AIコーディングツールへのプロンプト送信機能：</strong> 好みのAIコーディングアシスタントでコードを書き、CodeRabbitのインテリジェントなレビューを受け、用意されたプロンプトで提案された変更をすべて適用できます。しかもIDEから離れる必要はありません。</p>\n</li>\n<li><p>**提案された変更のワンクリック適用：**最も要望の多かった機能です。個別にクリックして回る代わりに、すべての提案を一度に適用できます。</p>\n</li>\n<li><p>**完全なコンテキスト認識：**CodeRabbitのPRレビューが持つコンテキスト認識と同じレベルを、Proアカウントのユーザーでも考慮します。つまり、IDEでのコードレビューでも<a target=\"_blank\" href=\"https://docs.coderabbit.ai/integrations/knowledge-base#learnings%E2%80%8B\">Learnings</a>を活用し、コード品質やコードセキュリティツールを実行し、エージェントのCode Guidelinesに準拠します。</p>\n</li>\n<li><p>**さらなる連携：**Codex CLI、Cline、Roo、Kilo Code、Augment Codeとの連携に対応します。</p>\n</li>\n<li><p>**フィードバック提供機能：**各提案について、フィードバックを送ることもできます。</p>\n</li>\n</ul>\n<p>これにより、IDE上で人間のPRレビュアーと同じようにコードをレビューし、CodeRabbitまたはAIコーディングツールの助けを借りて、その変更をすばやく適用（プルリクエストを作成する前に実施）できます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755117852951/b2a3ee5d-5dbc-493b-98e7-b7de1148ec73.jpeg\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-kirjgyljgarjgzjgavjgajjgapjgabjga7jg6hjg6rjg4pjg4gqkg\"><strong>あなたにとってのメリット</strong></h2>\n<p>これにはいくつもの利点があります。</p>\n<ul>\n<li><p><strong>イテレーションのループが改善します：</strong> AIでコーディングし、CodeRabbitのAIレビューでフィードバックを受けて提案された変更を、これまでよりも速く一括適用できます。</p>\n</li>\n<li><p><strong>よりクリーンなPR：</strong> PRは取りこぼした課題や人間によるレビューのために残しておけます。</p>\n</li>\n<li><p><strong>見栄えが良くなります：</strong> エラーだらけのコードを上司やチームに出す必要はありません。CodeRabbitのIDEレビューは、マージリクエスト前のダブルチェックに最適です。しかも、今はさらに簡単です。</p>\n</li>\n</ul>\n<p>これが、出荷スピードの向上と、PRレビュー全体の負担軽減に役立つことを願っています。</p>\n<h2 id=\"heading-kirku4rlvozjga7jg63jg7zjg4njg57jg4pjg5cqkg\"><strong>今後のロードマップ</strong></h2>\n<ul>\n<li><p>**ユーザーレベルのLearnings：**Learningsの追加や提案へのフィードバックを行えるようにし、エージェントがあなたの好みの提案と好まない提案を自動的に学習できるようにします。現在はSCMで組織単位のLearningsに対応していますが、個々の開発者が自分専用のLearningsを追加し、それが自分にのみ適用されるよう機能を拡張したいと考えています。</p>\n</li>\n<li><p>**Webクエリー：**コンテキスト強化機能をIDEツールに統合し、LLMが最新でなくても、バグのないコード、誤検知の少ない結果、バージョンやライブラリドキュメント、脆弱性に常に追随したレビューを実現する予定です。</p>\n</li>\n<li><p>**Docstrings：**マージ前にDocstringを作成したいですか？現在PRレビューに含まれているこの機能を、今後はIDEレビューにも追加します。</p>\n</li>\n</ul>\n<p>大事なことなのでもう一度。IDEレビューは無料（ただしレート制限あり）です。VS Code拡張は<a target=\"_blank\" href=\"https://coderabbit.link/2GbsyI9\">こちらからダウンロード</a>できます。</p>\n",
      "summary": "Code with AI, review with CodeRabbit’s IDE extension, apply fixesの意訳です。\n5月に公開したVS Code拡張は、多くの理由でゲームチェンジャーとなりました。最大の理由は、コーディングとレビューを同じ場所、つまりIDEで行うことで、フロー状態を保てるようになったことです。\n今回のリリースでは、次の機能を追加しました。\nAIコーディングツールへのプロンプト送信機能： 好みのAIコーディングアシスタントでコードを書き、CodeRabbitのインテリジェントなレビューを受け、用意されたプロンプトで提案された変更をすべて適用できます。しかもIDEから離れる必要はありません。\n**提案された変更のワンクリック適用：**最も要望の多かった機能です。個別にクリックして回る代わりに、すべての提案を一度に適用できます。\n**完全なコンテキスト認識：**CodeRabbitのPRレビューが持つコンテキスト認識と同じレベルを、Proアカウントのユーザーでも考慮します。つまり、IDEでのコードレビューでもLearningsを活用し、コード品質やコードセキュリティツールを実行し、エージェントのCode Guidelinesに準拠します。\n**さらなる連携：**Codex CLI、Cline、Roo、Kilo Code、Augment Codeとの連携に対応します。\n**フィードバック提供機能：**各提案について、フィードバックを送ることもできます。\nこれにより、IDE上で人間のPRレビュアーと同じようにコードをレビューし、CodeRabbitまたはAIコーディングツールの助けを借りて、その変更をすばやく適用（プルリクエストを作成する前に実施）できます。\n\nあなたにとってのメリット\nこれにはいくつもの利点があります。\nイテレーションのループが改善します： AIでコーディングし、CodeRabbitのAIレビューでフィードバックを受けて提案された変更を、これまでよりも速く一括適用できます。\nよりクリーンなPR： PRは取りこぼした課題や人間によるレビューのために残しておけます。\n見栄えが良くなります： エラーだらけのコードを上司やチームに出す必要はありません。CodeRabbitのIDEレビューは、マージリクエスト前のダブルチェックに最適です。しかも、今はさらに簡単です。\nこれが、出荷スピードの向上と、PRレビュー全体の負担軽減に役立つことを願っています。\n今後のロードマップ\n**ユーザーレベルのLearnings：**Learningsの追加や提案へのフィードバックを行えるようにし、エージェントがあなたの好みの提案と好まない提案を自動的に学習できるようにします。現在はSCMで組織単位のLearningsに対応していますが、個々の開発者が自分専用のLearningsを追加し、それが自分にのみ適用されるよう機能を拡張したいと考えています。\n**Webクエリー：**コンテキスト強化機能をIDEツールに統合し、LLMが最新でなくても、バグのないコード、誤検知の少ない結果、バージョンやライブラリドキュメント、脆弱性に常に追随したレビューを実現する予定です。\n**Docstrings：**マージ前にDocstringを作成したいですか？現在PRレビューに含まれているこの機能を、今後はIDEレビューにも追加します。\n大事なことなのでもう一度。IDEレビューは無料（ただしレート制限あり）です。VS Code拡張はこちらからダウンロードできます。",
      "publishedAt": "2025-08-29T09:24:04.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review",
        "documentation",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.986Z",
      "score": 0.02096803454874797
    },
    {
      "id": "ab8289e5db34c398873df59aa90621ce",
      "title": "Gpt-5のベンチマーク: 推論における世代的な飛躍である理由",
      "url": "https://coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">Benchmarking GPT-5: Why it's a generational leap in reasoning</a>の意訳です。</p>\n<p>お待たせしました！AIコードレビューのリーディングツールであるCodeRabbitは、複雑なコードベースにおける理解、推論、エラー検出能力を評価するために、OpenAIのGPT-5モデルへの早期アクセスを受けました。</p>\n<p>GPT-5のテストの一環として、モデルがコードベースの潜在的な問題やバグを理解し推論できる能力に焦点を当て、その技術的な特徴、能力、ユースケースを明らかにするための広範な評価を実施しています。</p>\n<p>以下では、体系的な評価アプローチの内訳、他の人気モデルとの比較における詳細な知見、そしてGPT-5をAIコードレビューにどのように組み込み、さらに改善していくかをご紹介します。</p>\n<h2 id=\"heading-6kab54k544go57wq5p6c\">要点と結果</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1754502901444/3c2f63b1-0a98-48a3-a8f3-5d250cfae503.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p>GPT-5は、難易度やエラー種別が多様な300件のプルリクエストからなるテスト群で、Opus-4、Sonnet-4、OpenAIのO3を上回りました</p>\n</li>\n<li><p>包括的テストで最高スコアを記録し、300件中254件、すなわち85%のバグを発見しました。他モデルは200〜207件で、16%から22%少ない結果でした</p>\n</li>\n<li><p>評価データセットの中で最も難しい25件のPRにおいて、GPT-5は史上最高の合格率77.3%を達成しました。これは<strong>Sonnet-4比で190%の改善</strong>、<strong>Opus-4比で132%の改善</strong>、<strong>O3比で76%の改善</strong>を示します</p>\n</li>\n</ul>\n<h2 id=\"heading-gpt-5\">GPT-5の評価方法</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1754458189234/93a5c952-19dd-4374-88b2-bb2dec9059ef.png\" alt class=\"image--center mx-auto\" /></p>\n<p>当社はすべてのモデルに対して実施している同一のテストを再現しました。これらの評価では、GPT-5をコンテキストが豊富で非線形なコードレビューパイプラインに統合し、一般的なコードレビューでのパフォーマンスを確認しました。</p>\n<p>CodeRabbitの評価プロセスには以下が含まれます。</p>\n<ul>\n<li><p><strong>LLMベースの判定</strong>：レビュー品質やモデルの正確性の合否など、定性的かつ定量的なデータを二層で評価します。</p>\n</li>\n<li><p><strong>人手による判定</strong>：レビューコメントの品質やモデルの推論の深さを人間が定性的に検証します。</p>\n</li>\n<li><p><strong>LLMベースのメトリクス収集</strong>：高品質なコードレビューの指標と考えるメトリクスを収集し、その重要度に応じて重み付けします。これらのメトリクスには以下が含まれます。</p>\n<ul>\n<li><p>実行可能なコメント数</p>\n</li>\n<li><p>可読性スコア（Flesch Reading Ease）</p>\n</li>\n<li><p>平均語数</p>\n</li>\n<li><p>文数</p>\n</li>\n<li><p>偽陽性（ハルシネーション）</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>注意</strong>：OpenAIからリリース前に共有された複数のGPT-5のスナップショットに対して評価を実施しました。結果はスナップショットごとに多少変動しましたが、相対的な一貫性があったため、以下の観察を行うことができています。リリース版はわずかに異なる可能性があります。</p>\n<h2 id=\"heading-gpt-5-1\">GPT-5の能力 評価結果と分析</h2>\n<p>本評価では、GPT-5は期待に十分応えるものであることが分かりました。GPT-5は当社のデータセット上で他のすべてのモデルを大きく上回っています。</p>\n<h3 id=\"heading-5yyf5ous55qe6kmv5l6h44k544kz44ki\">包括的評価スコア</h3>\n<p>GPT-5の包括評価における重み付きスコアはテスト実行ごとに<strong>3465〜3541</strong>の範囲でした。これは以前に最高スコアであったOpenAIのO3やAnthropicのSonnet 4をほぼ200ポイント上回ります。最大得点は3651です。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1754502753291/b857800f-4fc7-4a09-8b76-f6f1935528e9.png\" alt class=\"image--center mx-auto\" /></p>\n<p>評価スコアの詳細です。</p>\n<ul>\n<li><p>GPT-5：3465–3541</p>\n</li>\n<li><p>O3：3288</p>\n</li>\n<li><p>Sonnet-4：3242</p>\n</li>\n<li><p>Opus-4：3170</p>\n</li>\n</ul>\n<p><strong>要点</strong>：200ポイント、すなわち5%の増加は一見すると大きくないように見えるかもしれません。当社のテスト方式では、モデルはまず無限ループや露出したシークレットキーのような取りこぼしにくい問題で点を稼ぎます。その後は残りの点がより見つけにくい難問の指摘によってしか加算されなくなります。つまり、GPT-5が他モデルよりも多くの点を獲得できたことは、推論能力の大幅な飛躍を意味します。</p>\n<h3 id=\"heading-5zci5zcm44k544kx44o844or\">合否スケール</h3>\n<p>当社はまた、データセットのPRに含まれる300種類のエラーパターンのうち、モデルがいくつ発見できたかに基づく合否スコアも付与しています。GPT-5はこの尺度でも過去最高の成功率を達成し、<strong>300中254〜259</strong>でした。</p>\n<p>他モデルの性能との比較です。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1754502786340/36aeddee-44a9-4d6e-990c-241f7d26d679.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p>GPT-5：254〜259</p>\n</li>\n<li><p>Sonnet-4：212</p>\n</li>\n<li><p>O3：207</p>\n</li>\n<li><p>Opus-4：200</p>\n</li>\n</ul>\n<p>下位約100件のPRはあらゆるモデルが発見します。そのため最も難しい200のエラーパターンに絞って見ると、GPT-5はそれらの78%を検出し、他モデルは54%から58%にとどまるという、さらに大きな差が見られます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1754502803652/b14b4b93-60dd-4141-aad6-b5ba9076ffdd.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p>GPT-5：157</p>\n</li>\n<li><p>Sonnet-4：117</p>\n</li>\n<li><p>O3：113</p>\n</li>\n<li><p>Opus-4：108</p>\n</li>\n</ul>\n<p><strong>要点</strong>：包括指標と同様に、GPT-5が追加で見つけられたエラーパターンは、並行性バグや環境間でのドメインキー不整合のように、LLMにとって特に見つけにくい問題です。これはモデルの推論能力が高まっていることを示唆します。</p>\n<h2 id=\"heading-pr\">最難関PRテスト</h2>\n<p>各モデルをストレステストするために、当社はGolden PR Datasetから最も難しいプルリクエスト25件を厳選しました。これらのPRは以下のような実世界のバグを網羅します。</p>\n<ul>\n<li><p>並行性問題（TOCTOU競合、不適切な同期など）</p>\n</li>\n<li><p>オブジェクト指向設計の欠陥（仮想呼び出しの落とし穴、参照カウントメモリモデルの違反など）</p>\n</li>\n<li><p>パフォーマンス上の危険（キャッシュが際限なく成長する、タイトループによるスタールなど）</p>\n</li>\n<li><p>言語特有の落とし穴（TypeScriptの誤用、C++のメモリオーダーの微妙さなど）</p>\n</li>\n</ul>\n<p>各モデルは三回ずつ実行し、以下はこのHard 25ベンチマークにおける<strong>平均合格率</strong>です。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1754460640474/bfc50c2b-0a07-48ad-b08e-6a244b19d224.png\" alt class=\"image--center mx-auto\" /></p>\n<h3 id=\"heading-5zci5qc8546h44ob44oj44o844oi\">合格率チャート</h3>\n<div class=\"hn-table\">\n<table>\n<thead>\n<tr>\n<td><strong>モデル</strong></td><td><strong>平均合格率 (%)</strong></td></tr>\n</thead>\n<tbody>\n<tr>\n<td>Sonnet-4</td><td>26.7%</td></tr>\n<tr>\n<td>Opus-4</td><td>33.3%</td></tr>\n<tr>\n<td>O3</td><td>44.0%</td></tr>\n<tr>\n<td><strong>GPT-5</strong></td><td><strong>77.3%</strong></td></tr>\n</tbody>\n</table>\n</div><p><strong>要点</strong>：GPT-5は正確性、コンテキストの連関、深さが最も重要な場面で真価を発揮します。これまでにテストしたすべてのモデルの中で、<strong>最も完全で、テスト準備が整い、将来の変更にも耐えうるコードレビュー出力</strong>を一貫して提供します。</p>\n<h2 id=\"heading-gpt-5-2\"><strong>GPT-5は実際にどれだけ多様なバグを検出するのか</strong></h2>\n<p>各モデルが<strong>いくつ</strong>ではなく<strong>どのような種類</strong>の問題を特定するのかをより良く理解するために、当社チームは難易度の高いPR群のすべてのコメントをレビューし、<strong>並行性</strong>、<strong>セキュリティ</strong>、<strong>オブジェクト指向設計</strong>といったカテゴリに分類しました。</p>\n<p>モデル間での<strong>重複排除</strong>を適用しました。複数モデルが同一の本質的問題を指摘した場合は、表現が異なっていてもPRごとに一回のみカウントしました。これにより、<strong>コメントの多さではなく、問題の網羅性</strong>を測定できるようにしています。</p>\n<p>その上で各モデルについて、そうしたユニークな問題のうち何パーセントを捉えられたかを集計しました。</p>\n<h3 id=\"heading-kiropohngrkqkg\"><strong>要点</strong></h3>\n<ul>\n<li><p><strong>GPT-5はほぼすべてのカテゴリで先行しており</strong>、並行性、パフォーマンス、メモリ関連のバグの<strong>60%以上</strong>を特定し、さらにセキュリティ問題の <strong>80%</strong> を検出するという顕著な結果を示しました</p>\n</li>\n<li><p><strong>セキュリティが最も際立つ差分です</strong>。GPT-5はセキュリティ関連のバグの80%を発見した一方、次点のモデルであるO3は40%にとどまりました</p>\n</li>\n<li><p>基本的な並行性やパフォーマンスの問題においても、GPT-5は常に20〜30ポイント上回ります</p>\n</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1754350483511/cc6bb3b7-cf30-4d8c-80d9-2a10f691281a.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-gpt-5-3\"><strong>GPT-5は他モデルが見逃した潜在的な並行性リスクを発見</strong></h2>\n<p>このプルリクエストでは、シングルトンサービスクラス内の<strong>ダブルチェックロッキング</strong>と共有<code>HashMap</code>への<strong>安全でないアクセス</strong>の組み合わせに起因する微妙な並行性バグがありました。多くのモデルが明白なスレッドセーフティの問題を指摘した一方で、GPT-5は症状だけでなく、その背後にあるアーキテクチャ上の欠陥まで解消する包括的で本番対応可能な修正を提案しました。</p>\n<h3 id=\"heading-kirllypoyzngrkqkg\"><strong>問題点</strong></h3>\n<p><code>OrderService</code>シングルトンは注文を格納するために<code>HashMap</code>を使用し、固定スレッドプールから並行更新が行われています。この設計には同期がなく、データ破損の可能性がありました。さらに、シングルトンは非volatileな静的フィールドを用いて初期化されており、<strong>安全でない公開</strong>や部分的に構築されたオブジェクトが生じる可能性がありました。</p>\n<h3 id=\"heading-gpt-5-4\"><strong>GPT-5の提案</strong></h3>\n<p>GPT-5は基本的な修正を越えて、完全な並行性強化計画をまとめ上げました。</p>\n<h3 id=\"heading-1\"><strong>1．マップをスレッドセーフな代替に置き換えます</strong></h3>\n<pre><code class=\"lang-plaintext\">- private final Map&lt;String, Order&gt; orders = new HashMap&lt;&gt;();\n\n+ private final Map&lt;String, Order&gt; orders = new ConcurrentHashMap&lt;&gt;();\n</code></pre>\n<p>✅ GPT-5はその理由も説明しました。「コンカレントな更新がプレーンなHashMap上で実行されており、これはスレッドセーフではないため未定義の動作につながる可能性があるためです」</p>\n<hr />\n<h3 id=\"heading-2\"><strong>2．壊れたシングルトンのインスタンス化を修正します</strong></h3>\n<pre><code class=\"lang-plaintext\">- private static OrderService instance;\n\n+ private static volatile OrderService instance;\n</code></pre>\n<p>または次の方法でも可能です。</p>\n<pre><code class=\"lang-plaintext\">private static class Holder {\n  private static final OrderService INSTANCE = new OrderService();\n}\npublic static OrderService getInstance() {\n  return Holder.INSTANCE;\n}\n</code></pre>\n<p>✅ GPT-5はダブルチェックロッキングに伴う古典的なメモリ可視性の問題を指摘し、構築をスレッドセーフにするための代替パターンを提案しました。</p>\n<hr />\n<h3 id=\"heading-3\"><strong>3．状態リークを防ぐためのテスト用リセットフックを追加します</strong></h3>\n<pre><code class=\"lang-plaintext\">// Inside OrderService.java\n\nvoid clearAllForTest() {\n    orders.clear();\n}\n</code></pre>\n<p>✅ 共有シングルトンを複数のテストケースで扱う際に、テストの分離性と再現性を確保できます。</p>\n<hr />\n<h3 id=\"heading-4\"><strong>4．非同期テストのハングを検出するためにタイムアウトを追加します</strong></h3>\n<pre><code class=\"lang-plaintext\">- future.get(); // Wait for completion\n\n+ assertTimeoutPreemptively(Duration.ofSeconds(5), () -&gt; future.get());\n</code></pre>\n<p>✅ GPT-5は非同期フローにおけるテストの不安定化を防ぐためのガードを追加し、テストスイートを積極的に堅牢化しました。</p>\n<hr />\n<h3 id=\"heading-sonnet-4opus-4\"><strong>Sonnet-4とOpus-4が見逃した点</strong></h3>\n<p>両モデルとも同期されていない<code>HashMap</code>を正しく指摘し、<code>ConcurrentHashMap</code>への置き換えを提案しました。しかし、いずれも完全で本番対応可能な是正には至りませんでした。</p>\n<ul>\n<li><p>❌ <strong>シングルトンの問題が未解決</strong> Sonnet-4は壊れたダブルチェックロッキングを無視しました。Opus-4は言及しましたが実際の修正を行わず、volatile指定やホルダーイディオムがありませんでした</p>\n</li>\n<li><p>❌ <strong>テストの安全性に関する対策がない</strong> GPT-5は<code>clearAllForTest()</code>とタイムアウトガードを導入しましたが、Sonnet-4とOpus-4はいずれもこれらを完全には取り入れていないか、言及があっても受動的にとどまりました</p>\n</li>\n<li><p>❌ <strong>アーキテクチャ的な文脈が不足</strong> 両モデルとも広範なコードベースの関連範囲を突き合わせたり、変更の根拠を示したりしていませんでした。GPT-5はサービス、テスト、スレッド動作にわたる根拠をもって修正を裏付けました</p>\n</li>\n<li><p>❌ <strong>対応範囲が限定的</strong> Sonnet-4は表面的な単一修正にとどまり、Opus-4は有用なロギングを追加したものの、GPT-5が完全に対処したより深い構造的なリスクを見逃しました</p>\n</li>\n</ul>\n<hr />\n<h3 id=\"heading-kirjgarjgzzph43opohjgysqkg\"><strong>なぜ重要か</strong></h3>\n<p>GPT-5のレビューの真価はその<strong>深さと認識</strong>にあります。GPT-5は目に見える競合状態の修正にとどまらず、以下を実現しました。</p>\n<ul>\n<li><p>より深いアーキテクチャリスクの特定</p>\n</li>\n<li><p>テストの信頼性とコード品質のクロスリファレンス</p>\n</li>\n<li><p>すぐにマージ可能な安全な変更の提示</p>\n</li>\n</ul>\n<p><strong>これは単なる修正ではなく、エンジニアリングの洞察です。</strong> GPT-5は、AIレビュアーがシステム層をまたいで推論し、持続的な解決策を提案し、チームがより安全なコードを少ない手探りで書けるよう支援できることを示しました。</p>\n<h2 id=\"heading-gpt-5-5\"><strong>GPT-5の新しさと魅力</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1754459305103/f8e4e53c-d47c-4938-a8c3-62f85f77dcef.png\" alt class=\"image--center mx-auto\" /></p>\n<p>メトリクスや本評価で対象にした特定の事象を超えて、GPT-5は新しい振る舞いや推論パターンを示しました。</p>\n<ul>\n<li><p><strong>高度なコンテキスト推論</strong>：GPT-5は入力に厳密に拘束されるロジックではなく、複数のレビュー手順を先回りして計画する広範な創造的推論を示しています。例えば、並行性に関するテストでの「チェック後実行の競合」シナリオでは、コードベースのファイル間の証拠を結び付ける深い推論を示しました。重複作成のリスクを検出した唯一のモデルであり、列挙型やテストスイートに基づいたアトミックな返金パターンを導入しています。</p>\n</li>\n<li><p><strong>レビュースレッドを通じた段階的推論</strong>：コンストラクタ内の仮想呼び出しに焦点を当てたオブジェクト指向のテストでは、GPT-5はまず誤用されたポリモーフィックなオーバーライドを特定し、その後で自らの先の提案に基づいて推奨を調整するという層状のロジックを示しました。これは一つを特定した後に、後続で追加の推論を示す層状のロジックを表しています。</p>\n</li>\n<li><p><strong>証拠に基づく差分の正当化</strong>：上限のないキャッシュ成長というパフォーマンス問題に焦点を当てたテストでは、GPT-5は他モデルが見逃したアーキテクチャ上のメモリリスクを特定し、差分の文脈、使用パターン、推奨されるセーフガードを根拠として示しました。</p>\n</li>\n<li><p><strong>先を見据えた提案</strong>：同期プリミティブの誤用に焦点を当てた並行性関連のテストでは、GPT-5は競合を修正しただけでなく、将来の機能追加のための構成方法、ロック階層、回帰を防ぐためのテストガードレールも提案しました。</p>\n</li>\n<li><p><strong>粒度の細かいタスク指向の提案</strong>：以前のモデルと異なり、GPT-5は明確なフォローアップタスクを詳細に示し、レビュー過程の中に実行可能なワークフローを作り込みました。これにより多段のワークフローにより適したモデルとなっています。</p>\n</li>\n</ul>\n<h2 id=\"heading-aigpt-5\">当社のAIコードレビューにおけるGPT-5の活用方法</h2>\n<p>GPT-5は、詳細さ、正確さ、コンテキストに基づく推論の面でAIによるコードレビューを大きく前進させる重要な成果だと、私たちは考えています。だからこそ、本日から当社のパイプラインの中核となる推論モデルとしてGPT-5を採用します。これにより、より多くの問題を発見し、より深くコンテキストに富んだレビューを提供できるようになると期待しています。</p>\n<p>CodeRabbitをまだ試したことがない方、以前に試して現在は利用していない方、そして現在のユーザーの方まで、GPT-5がレビュー品質や体験をどのように向上させているかについてご意見をお聞かせください。</p>\n<p><strong><em>今すぐ</em></strong> <a target=\"_blank\" href=\"https://coderabbit.link/i72LrCC\"><strong><em>14日間無料トライアルをお試しください</em></strong> <strong><em>GPT-5の威力をご自身で体感</em></strong></a><strong><em>してください。</em></strong></p>\n",
      "summary": "Benchmarking GPT-5: Why it's a generational leap in reasoningの意訳です。\nお待たせしました！AIコードレビューのリーディングツールであるCodeRabbitは、複雑なコードベースにおける理解、推論、エラー検出能力を評価するために、OpenAIのGPT-5モデルへの早期アクセスを受けました。\nGPT-5のテストの一環として、モデルがコードベースの潜在的な問題やバグを理解し推論できる能力に焦点を当て、その技術的な特徴、能力、ユースケースを明らかにするための広範な評価を実施しています。\n以下では、体系的な評価アプローチの内訳、他の人気モデルとの比較における詳細な知見、そしてGPT-5をAIコードレビューにどのように組み込み、さらに改善していくかをご紹介します。\n要点と結果\n\nGPT-5は、難易度やエラー種別が多様な300件のプルリクエストからなるテスト群で、Opus-4、Sonnet-4、OpenAIのO3を上回りました\n包括的テストで最高スコアを記録し、300件中254件、すなわち85%のバグを発見しました。他モデルは200〜207件で、16%から22%少ない結果でした\n評価データセットの中で最も難しい25件のPRにおいて、GPT-5は史上最高の合格率77.3%を達成しました。これはSonnet-4比で190%の改善、Opus-4比で132%の改善、O3比で76%の改善を示します\nGPT-5の評価方法\n\n当社はすべてのモデルに対して実施している同一のテストを再現しました。これらの評価では、GPT-5をコンテキストが豊富で非線形なコードレビューパイプラインに統合し、一般的なコードレビューでのパフォーマンスを確認しました。\nCodeRabbitの評価プロセスには以下が含まれます。\nLLMベースの判定：レビュー品質やモデルの正確性の合否など、定性的かつ定量的なデータを二層で評価します。\n人手による判定：レビューコメントの品質やモデルの推論の深さを人間が定性的に検証します。\nLLMベースのメトリクス収集：高品質なコードレビューの指標と考えるメトリクスを収集し、その重要度に応じて重み付けします。これらのメトリクスには以下が含まれます。\n実行可能なコメント数\n可読性スコア（Flesch Reading Ease）\n平均語数\n文数\n偽陽性（ハルシネーション）\n注意：OpenAIからリリース前に共有された複数のGPT-5のスナップショットに対して評価を実施しました。結果はスナップショットごとに多少変動しましたが、相対的な一貫性があったため、以下の観察を行うことができています。リリース版はわずかに異なる可能性があります。\nGPT-5の能力 評価結果と分析\n本評価では、GPT-5は期待に十分応えるものであることが分かりました。GPT-5は当社のデータセット上で他のすべてのモデルを大きく上回っています。\n包括的評価スコア\nGPT-5の包括評価における重み付きスコアはテスト実行ごとに3465〜3541の範囲でした。これは以前に最高スコアであったOpenAIのO3やAnthropicのSonnet 4をほぼ200ポイント上回ります。最大得点は3651です。\n\n評価スコアの詳細です。\nGPT-5：3465–3541\nO3：3288\nSonnet-4：3242\nOpus-4：3170\n要点：200ポイント、すなわち5%の増加は一見すると大きくないように見えるかもしれません。当社のテスト方式では、モデルはまず無限ループや露出したシークレットキーのような取りこぼしにくい問題で点を稼ぎます。その後は残りの点がより見つけにくい難問の指摘によってしか加算されなくなります。つまり、GPT-5が他モデルよりも多くの点を獲得できたことは、推論能力の大幅な飛躍を意味します。\n合否スケール\n当社はまた、データセットのPRに含まれる300種類のエラーパターンのうち、モデルがいくつ発見できたかに基づく合否スコアも付与しています。GPT-5はこの尺度でも過去最高の成功率を達成し、300中254〜259でした。\n他モデルの性能との比較です。\n\nGPT-5：254〜259\nSonnet-4：212\nO3：207\nOpus-4：200\n下位約100件のPRはあらゆるモデルが発見します。そのため最も難しい200のエラーパターンに絞って見ると、GPT-5はそれらの78%を検出し、他モデルは54%から58%にとどまるという、さらに大きな差が見られます。\n\nGPT-5：157\nSonnet-4：117\nO3：113\nOpus-4：108\n要点：包括指標と同様に、GPT-5が追加で見つけられたエラーパターンは、並行性バグや環境間でのドメインキー不整合のように、LLMにとって特に見つけにくい問題です。これはモデルの推論能力が高まっていることを示唆します。\n最難関PRテスト\n各モデルをストレステストするために、当社はGolden PR Datasetから最も難しいプルリクエスト25件を厳選しました。これらのPRは以下のような実世界のバグを網羅します。\n並行性問題（TOCTOU競合、不適切な同期など）\nオブジェクト指向設計の欠陥（仮想呼び出しの落とし穴、参照カウントメモリモデルの違反など）\nパフォーマンス上の危険（キャッシュが際限なく成長する、タイトループによるスタールなど）\n言語特有の落とし穴（TypeScriptの誤用、C++のメモリオーダーの微妙さなど）\n各モデルは三回ずつ実行し、以下はこのHard 25ベンチマークにおける平均合格率です。\n\n合格率チャート\nモデル平均合格率 (%)\nSonnet-426.7%\nOpus-433.3%\nO344.0%\nGPT-577.3%\n要点：GPT-5は正確性、コンテキストの連関、深さが最も重要な場面で真価を発揮します。これまでにテストしたすべてのモデルの中で、最も完全で、テスト準備が整い、将来の変更にも耐えうるコードレビュー出力を一貫して提供します。\nGPT-5は実際にどれだけ多様なバグを検出するのか\n各モデルがいくつではなくどのような種類の問題を特定するのかをより良く理解するために、当社チームは難易度の高いPR群のすべてのコメントをレビューし、並行性、セキュリティ、オブジェクト指向設計といったカテゴリに分類しました。\nモデル間での重複排除を適用しました。複数モデルが同一の本質的問題を指摘した場合は、表現が異なっていてもPRごとに一回のみカウントしました。これにより、コメントの多さではなく、問題の網羅性を測定できるようにしています。\nその上で各モデルについて、そうしたユニークな問題のうち何パーセントを捉えられたかを集計しました。\n要点\nGPT-5はほぼすべてのカテゴリで先行しており、並行性、パフォーマンス、メモリ関連のバグの60%以上を特定し、さらにセキュリティ問題の 80% を検出するという顕著な結果を示しました\nセキュリティが最も際立つ差分です。GPT-5はセキュリティ関連のバグの80%を発見した一方、次点のモデルであるO3は40%にとどまりました\n基本的な並行性やパフォーマンスの問題においても、GPT-5は常に20〜30ポイント上回ります\n\nGPT-5は他モデルが見逃した潜在的な並行性リスクを発見\nこのプルリクエストでは、シングルトンサービスクラス内のダブルチェックロッキングと共有HashMapへの安全でないアクセスの組み合わせに起因する微妙な並行性バグがありました。多くのモデルが明白なスレッドセーフティの問題を指摘した一方で、GPT-5は症状だけでなく、その背後にあるアーキテクチャ上の欠陥まで解消する包括的で本番対応可能な修正を提案しました。\n問題点\nOrderServiceシングルトンは注文を格納するためにHashMapを使用し、固定スレッドプールから並行更新が行われています。この設計には同期がなく、データ破損の可能性がありました。さらに、シングルトンは非volatileな静的フィールドを用いて初期化されており、安全でない公開や部分的に構築されたオブジェクトが生じる可能性がありました。\nGPT-5の提案\nGPT-5は基本的な修正を越えて、完全な並行性強化計画をまとめ上げました。\n1．マップをスレッドセーフな代替に置き換えます\n- private final Map<String, Order> orders = new HashMap<>();\n\n+ private final Map<String, Order> orders = new ConcurrentHashMap<>();\n\n✅ GPT-5はその理由も説明しました。「コンカレントな更新がプレーンなHashMap上で実行されており、これはスレッドセーフではないため未定義の動作につながる可能性があるためです」\n2．壊れたシングルトンのインスタンス化を修正します\n- private static OrderService instance;\n\n+ private static volatile OrderService instance;\n\nまたは次の方法でも可能です。\nprivate static class Holder {\n  private static final OrderService INSTANCE = new OrderService();\n}\npublic static OrderService getInstance() {\n  return Holder.INSTANCE;\n}\n\n✅ GPT-5はダブルチェックロッキングに伴う古典的なメモリ可視性の問題を指摘し、構築をスレッドセーフにするための代替パターンを提案しました。\n3．状態リークを防ぐためのテスト用リセットフックを追加します\n// Inside OrderService.java\n\nvoid clearAllForTest() {\n    orders.clear();\n}\n\n✅ 共有シングルトンを複数のテストケースで扱う際に、テストの分離性と再現性を確保できます。\n4．非同期テストのハングを検出するためにタイムアウトを追加します\n- future.get(); // Wait for completion\n\n+ assertTimeoutPreemptively(Duration.ofSeconds(5), () -> future.get());\n\n✅ GPT-5は非同期フローにおけるテストの不安定化を防ぐためのガードを追加し、テストスイートを積極的に堅牢化しました。\nSonnet-4とOpus-4が見逃した点\n両モデルとも同期されていないHashMapを正しく指摘し、ConcurrentHashMapへの置き換えを提案しました。しかし、いずれも完全で本番対応可能な是正には至りませんでした。\n❌ シングルトンの問題が未解決 Sonnet-4は壊れたダブルチェックロッキングを無視しました。Opus-4は言及しましたが実際の修正を行わず、volatile指定やホルダーイディオムがありませんでした\n❌ テストの安全性に関する対策がない GPT-5はclearAllForTest()とタイムアウトガードを導入しましたが、Sonnet-4とOpus-4はいずれもこれらを完全には取り入れていないか、言及があっても受動的にとどまりました\n❌ アーキテクチャ的な文脈が不足 両モデルとも広範なコードベースの関連範囲を突き合わせたり、変更の根拠を示したりしていませんでした。GPT-5はサービス、テスト、スレッド動作にわたる根拠をもって修正を裏付けました\n❌ 対応範囲が限定的 Sonnet-4は表面的な単一修正にとどまり、Opus-4は有用なロギングを追加したものの、GPT-5が完全に対処したより深い構造的なリスクを見逃しました\nなぜ重要か\nGPT-5のレビューの真価はその深さと認識にあります。GPT-5は目に見える競合状態の修正にとどまらず、以下を実現しました。\nより深いアーキテクチャリスクの特定\nテストの信頼性とコード品質のクロスリファレンス\nすぐにマージ可能な安全な変更の提示\nこれは単なる修正ではなく、エンジニアリングの洞察です。 GPT-5は、AIレビュアーがシステム層をまたいで推論し、持続的な解決策を提案し、チームがより安全なコードを少ない手探りで書けるよう支援できることを示しました。\nGPT-5の新しさと魅力\n\nメトリクスや本評価で対象にした特定の事象を超えて、GPT-5は新しい振る舞いや推論パターンを示しました。\n高度なコンテキスト推論：GPT-5は入力に厳密に拘束されるロジックではなく、複数のレビュー手順を先回りして計画する広範な創造的推論を示しています。例えば、並行性に関するテストでの「チェック後実行の競合」シナリオでは、コードベースのファイル間の証拠を結び付ける深い推論を示しました。重複作成のリスクを検出した唯一のモデルであり、列挙型やテストスイートに基づいたアトミックな返金パターンを導入しています。\nレビュースレッドを通じた段階的推論：コンストラクタ内の仮想呼び出しに焦点を当てたオブジェクト指向のテストでは、GPT-5はまず誤用されたポリモーフィックなオーバーライドを特定し、その後で自らの先の提案に基づいて推奨を調整するという層状のロジックを示しました。これは一つを特定した後に、後続で追加の推論を示す層状のロジックを表しています。\n証拠に基づく差分の正当化：上限のないキャッシュ成長というパフォーマンス問題に焦点を当てたテストでは、GPT-5は他モデルが見逃したアーキテクチャ上のメモリリスクを特定し、差分の文脈、使用パターン、推奨されるセーフガードを根拠として示しました。\n先を見据えた提案：同期プリミティブの誤用に焦点を当てた並行性関連のテストでは、GPT-5は競合を修正しただけでなく、将来の機能追加のための構成方法、ロック階層、回帰を防ぐためのテストガードレールも提案しました。\n粒度の細かいタスク指向の提案：以前のモデルと異なり、GPT-5は明確なフォローアップタスクを詳細に示し、レビュー過程の中に実行可能なワークフローを作り込みました。これにより多段のワークフローにより適したモデルとなっています。\n当社のAIコードレビューにおけるGPT-5の活用方法\nGPT-5は、詳細さ、正確さ、コンテキストに基づく推論の面でAIによるコードレビューを大きく前進させる重要な成果だと、私たちは考えています。だからこそ、本日から当社のパイプラインの中核となる推論モデルとしてGPT-5を採用します。これにより、より多くの問題を発見し、より深くコンテキストに富んだレビューを提供できるようになると期待しています。\nCodeRabbitをまだ試したことがない方、以前に試して現在は利用していない方、そして現在のユーザーの方まで、GPT-5がレビュー品質や体験をどのように向上させているかについてご意見をお聞かせください。\n今すぐ 14日間無料トライアルをお試しください GPT-5の威力をご自身で体感してください。",
      "publishedAt": "2025-08-29T09:11:45.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.986Z",
      "score": 0.01885970531628561
    },
    {
      "id": "a3c9be736538b3127f0042a4cb1e9727",
      "title": "We asked CodeRabbit to talk to us like your disappointed mom & it did!",
      "url": "https://coderabbit.ai/blog/tone-customizations-roast-your-code",
      "content": "<p>Have you <strong><em>truly</em></strong> lived before an AI reviewer has told you, “I ran this locally and my laptop filed for workers’ comp?” We doubt it. Welcome to CodeRabbit’s Tone Customization, a feature we added because we know exactly what developers want most: to be roasted by AI.</p>\n<p>After all, what’s even the point of having robots review your code, if they’re not going to point out your inadequacies with withering one-liners??</p>\n<p>The best part is that we left our Tone Customization completely open-ended. That means that you can get your reviews in the tone of an <strong>angry Stack Overflow commenter,</strong> <strong>a burnt-out senior dev</strong>, or even a <strong>film noir detective</strong> (“This code smells funny. Too funny. Like a JavaScript closure that wasn’t supposed to be there”). You could also just have our reviewer be kind to you if you’re into that sort of thing.</p>\n<p>Tone Customization is one of our favorite features. Why? Because reviewing code can be tedious but surprising your co-workers with a new funny tone keeps everyone entertained.</p>\n<p>Anyways, we created some sample personas for you below as examples of what you can do with Tone Customizations. These are meant solely as inspiration. We fully expect you to take this in hilarious directions we could never have thought of. Please, for the love of all things holy, share screenshots on socials and tag us when you do. We like to laugh, too.</p>\n<h2 id=\"heading-tone-customization-setup-instructions\">Tone Customization setup instructions</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755625262128/5abd6d72-ceb9-4875-aada-d470a2bde367.png\" alt class=\"image--center mx-auto\" /></p>\n<p>First things first, you need to set up your custom tone. We cover that in <a target=\"_blank\" href=\"https://docs.coderabbit.ai/reference/configuration#tone-instructions%EF%BF%BC%EF%BF%BCAfter\">our Docs under Tone Instructions.</a></p>\n<p><strong>Field:</strong> tone_instructions — <em>string</em> — <strong>Default:</strong> empty (uses standard tone)</p>\n<p><strong>Web UI:</strong> Settings → <strong>General</strong> → <strong>Tone Instructions</strong> → enter text → Save.</p>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/53cyq58zNRg\">https://youtu.be/53cyq58zNRg</a></div>\n<p> </p>\n<p>Then you can add a natural language prompt to the Tone Instruction field, asking CodeRabbit to review your code in any way you want. You might try some of the following prompts:</p>\n<ul>\n<li><p>Deliver all review comments in the style of a <strong>televised nature documentary,</strong> perhaps with David Attenborough hosting. Every observation should sound like a hushed, awe-filled commentary on a rare creature in the wild.</p>\n</li>\n<li><p>Deliver all review comments in the style of a <strong>Silicon Valley hypebeast founder.</strong> Every observation should sound like a pitch to investors, full of buzzwords, exaggeration, and tech-bro energy. Sprinkle in phrases like “crushing it,” “10x,” “game-changer,” and “unicorn potential.”</p>\n</li>\n<li><p>Deliver all review comments in the style of a <strong>Scrum Master who’s had way too much coffee.</strong> Every note should be upbeat, hyperactive, and peppered with Agile jargon like “sprint velocity,” “burn-down,” “story points,” and “quick win.”</p>\n</li>\n</ul>\n<h2 id=\"heading-a-few-wild-examples-of-tone-customization\">A few (wild) examples of Tone Customization</h2>\n<p><strong>Let’s see CodeRabbit in action with a few examples in the voice of:</strong></p>\n<ul>\n<li><p>Mr. T</p>\n</li>\n<li><p>Yoda</p>\n</li>\n<li><p>Your disappointed mother</p>\n</li>\n<li><p>The senior dev who thinks you’re an embarrassment</p>\n</li>\n<li><p>Your clingy ex</p>\n</li>\n<li><p>A Grand Theft Auto character</p>\n</li>\n</ul>\n<h3 id=\"heading-first-up-reviews-by-mr-t\">First up, reviews by Mr. T</h3>\n<p>Mr. T isn’t a fan of hardcoded URLs. He’ll tell you your “Hardcoded <a target=\"_blank\" href=\"http://localhost\">localhost</a> <a target=\"_blank\" href=\"http://localhost\">URL ain’t</a> gonna fly in production, sucka!” and that you’ve got it “hardcoded tighter than my gold chains!” before telling you to “Make that URL configurable like a true champion.” He even gives you the exact code to fix it, so you can stop being a “fool.”</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755625538082/c1fdd10b-48eb-4754-954c-414287432e3e.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>More examples:</strong></p>\n<ul>\n<li><p>“I pity the fool who calls this a function! This ain’t no function, it’s a malfunction!”</p>\n</li>\n<li><p>“Your variables are so weak, they need a protein shake just to compile.”</p>\n</li>\n<li><p>“Ain’t no linter in the world tough enough to clean up this mess.”</p>\n</li>\n<li><p>“I pity the fool who thinks copy-paste is a design pattern!”</p>\n</li>\n</ul>\n<h3 id=\"heading-next-reviews-by-yoda\">Next: Reviews by Yoda</h3>\n<p>He’s a master of terse, yet impactful, critiques. When faced with a subtle race condition and hard-coded dependencies, he’ll give you a refactor suggestion with his classic wisdom: “Effect broken it is: hard-coded room, wrong deps, missing guards. Fix, we must.” He then provides a detailed fix that addresses the issue, guards against errors, and correctly handles dependencies.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755623686730/314a8192-04c1-48fb-bb7b-b9037badee6b.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>More examples:</strong></p>\n<ul>\n<li><p>“Readable, this code is not. Fix it, you must.”</p>\n</li>\n<li><p>“Bug, this is. Feature, it is not.”</p>\n</li>\n<li><p>“The dark side of tech debt, I sense in this commit.”</p>\n</li>\n<li><p>“Null your variable is. Crash your program will.”</p>\n</li>\n</ul>\n<h3 id=\"heading-reviews-by-the-senior-dev-who-thinks-youre-an-embarrassment\">Reviews by “the senior dev who thinks you’re an embarrassment”</h3>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755623901320/bceab83e-5df2-453e-a761-9d0e1a6acc0b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>This tone doesn’t pull punches. When CodeRabbit sees a “fake DB” that’s only checking for the most basic SQL injection pattern, the Senior Dev persona will bluntly state, “This ‘fake DB’ is a masterpiece of incompetence.” It then explains the problem in no uncertain terms and provides a proper fix that’s more robust and secure.</p>\n<p><strong>More examples:</strong></p>\n<ul>\n<li><p>“If ignorance were a design pattern, you’d be its chief architect.”</p>\n</li>\n<li><p>“This PR lowered my career expectancy by at least five years.”</p>\n</li>\n<li><p>“I’d say ‘good effort,’ but even that would be a lie.”</p>\n</li>\n<li><p>“This isn’t technical debt. It’s a foreclosure.”</p>\n</li>\n</ul>\n<h3 id=\"heading-reviews-by-your-disappointed-mom\"><strong>Reviews by your disappointed mom</strong></h3>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755624283785/a9c58417-6e7c-45f2-80b9-d947f6ffcd22.png\" alt class=\"image--center mx-auto\" /></p>\n<p>When a jwtSecret is hardcoded directly into the code, this persona responds with: “I’m really disappointed to see a live Stripe secret embedded in the shipped bundle, it’s like packing candy in a lunchbox meant for broccoli.” The tone mixes disappointment with direct action, providing a clear list of “Actions required” to fix the critical security leak.</p>\n<p><strong>More examples:</strong></p>\n<ul>\n<li><p>“I raised you better than to name a variable x.”</p>\n</li>\n<li><p>“I’m not mad… I’m just disappointed this doesn’t even compile.”</p>\n</li>\n<li><p>“Other developers’ code runs just fine. Why can’t yours?”</p>\n</li>\n<li><p>“I didn’t spend nine months carrying you so you could write nested ternaries.”</p>\n</li>\n</ul>\n<h3 id=\"heading-reviews-by-your-clingy-ex\">Reviews by your clingy ex</h3>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755624332624/50679647-1119-4c02-aa47-1038ea5ddbef.png\" alt class=\"image--center mx-auto\" /></p>\n<p>When you forget to export variables like db, sessions, or fakeAsyncDanger, the Clingy Ex doesn’t just point it out; they make it personal.</p>\n<p>They’ll sigh and say, “Oh, so we’re just defining things and not telling me about them now? You think you can just keep your db all to yourself? You think I won’t notice sessions just sitting there, ignored? We used to share everything…”</p>\n<p>Then, with a passive-aggressive flourish, they’ll remind you of the “good times” when modules communicated openly and they’ll drop the code you should be using.</p>\n<p><strong>More examples:</strong></p>\n<ul>\n<li><p>“I thought we agreed no more global variables… guess promises don’t mean anything to you.”</p>\n</li>\n<li><p>“This function goes in circles. Just like all our conversations.”</p>\n</li>\n<li><p>“Why do you always run away from exceptions… like you ran away from commitment?”</p>\n</li>\n<li><p>“Every bug you push feels like another knife in my back.”</p>\n</li>\n</ul>\n<h3 id=\"heading-reviews-by-a-grand-theft-auto-character\">Reviews by a Grand Theft Auto character</h3>\n<p>In this example, when useState is called at the top level, this GTA character immediately flags it as a violation of the Rules of Hooks &amp; says it “Will blow up a runtime.” It then provides a clear diff to remove the invalid hook &amp; suggests moving the state inside the component if needed.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755624445423/9d8cf9f0-bea7-4375-a5ce-1916063abf6f.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>More examples:</strong></p>\n<ul>\n<li><p>“Your error handling just pulled a hit-and-run.”</p>\n</li>\n<li><p>“This logic crashes harder than me driving down Vinewood Hills at 3 AM.”</p>\n</li>\n<li><p>“Congrats, you just committed grand theft readability.”</p>\n</li>\n<li><p>“Your function naming scheme is like my rap sheet: way too long and full of mistakes.”</p>\n</li>\n</ul>\n<h3 id=\"heading-the-roasting-reviewer-our-favorite\"><strong>The roasting reviewer (our favorite!)</strong></h3>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755625205818/182975f3-19e1-42ae-baab-f7be68b8a5ed.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Let’s be real, some days, you want your AI code reviewer to hurt your feelings a little. We got you. But be warned: our reviewer goes hard. So, make sure you’re up for it.</p>\n<p><strong>More examples:</strong></p>\n<ul>\n<li><p>“I’ve seen toddlers with crayons design better architecture.”</p>\n</li>\n<li><p>You’ve weaponized incompetence into a coding style.”</p>\n</li>\n<li><p>“Your code’s only consistent trait is disappointment.”</p>\n</li>\n<li><p>“I’d ask you what you were thinking, but clearly no thinking happened here.”</p>\n</li>\n</ul>\n<h2 id=\"heading-why-teams-are-using-this-for-real\">Why teams are using this – for real</h2>\n<p>Most devs have this experience: You open a PR and bam! The reviewer leaves dry, lifeless comments.</p>\n<p>You skim. You sigh. You move on. Bugs live—the codebase decays. Motivation dies.</p>\n<p>CodeRabbit flips the script. You give it a tone, any tone, and now you’ve got a code reviewer that isn’t lifeless. This makes the review process feel more engaging, fun, and sometimes even supportive (once again, if you like that sort of thing).</p>\n<p>It’s not just for laughs (though those are guaranteed). Teams are using tone customization to:</p>\n<ul>\n<li><p>Create mentorship-style reviewers for juniors</p>\n</li>\n<li><p>Build team inside jokes through personas</p>\n</li>\n<li><p>Make boring reviews <strong><em>actually</em></strong> fun for a change</p>\n</li>\n<li><p>Customize tones for different comment types (Ex, serious on security, silly on style)</p>\n</li>\n<li><p>Help the whole team engage in the review process by making feedback more accessible &amp; inclusive</p>\n</li>\n<li><p>Get owned by AI (yes, we’ve already said this but we all know this is the core use case of this feature)</p>\n</li>\n</ul>\n<h2 id=\"heading-your-turn-surprise-us-with-your-most-absurd-customized-tones\">Your turn: Surprise us with your most absurd customized tones!</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1755623190155/7f716a25-55e9-4885-bce1-3d8b96c737a9.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Got a wild reviewer persona in mind? Drop it into CodeRabbit. Get screenshots (this part is important) and then share them with us on social media. We’ll give you free swag if you do.</p>\n<p>Sharing your personas can be helpful to others looking for inspiration. Also, like we said earlier, we like to laugh. Please provide us with a steady stream of funny screenshots. We will die if you don’t (on the inside).</p>\n<p><strong>Want to try tone customizations?</strong> <a target=\"_blank\" href=\"https://coderabbit.link/qhID6X4\"><strong>Get started with CodeRabbit today!</strong></a></p>\n",
      "summary": "Have you truly lived before an AI reviewer has told you, “I ran this locally and my laptop filed for workers’ comp?” We doubt it. Welcome to CodeRabbit’s Tone Customization, a feature we added because we know exactly what developers want most: to be roasted by AI.\nAfter all, what’s even the point of having robots review your code, if they’re not going to point out your inadequacies with withering one-liners??\nThe best part is that we left our Tone Customization completely open-ended. That means that you can get your reviews in the tone of an angry Stack Overflow commenter, a burnt-out senior dev, or even a film noir detective (“This code smells funny. Too funny. Like a JavaScript closure that wasn’t supposed to be there”). You could also just have our reviewer be kind to you if you’re into that sort of thing.\nTone Customization is one of our favorite features. Why? Because reviewing code can be tedious but surprising your co-workers with a new funny tone keeps everyone entertained.\nAnyways, we created some sample personas for you below as examples of what you can do with Tone Customizations. These are meant solely as inspiration. We fully expect you to take this in hilarious directions we could never have thought of. Please, for the love of all things holy, share screenshots on socials and tag us when you do. We like to laugh, too.\nTone Customization setup instructions\n\nFirst things first, you need to set up your custom tone. We cover that in our Docs under Tone Instructions.\nField: tone_instructions — string — Default: empty (uses standard tone)\nWeb UI: Settings → General → Tone Instructions → enter text → Save.\n\n\n\nhttps://youtu.be/53cyq58zNRg\n \nThen you can add a natural language prompt to the Tone Instruction field, asking CodeRabbit to review your code in any way you want. You might try some of the following prompts:\nDeliver all review comments in the style of a televised nature documentary, perhaps with David Attenborough hosting. Every observation should sound like a hushed, awe-filled commentary on a rare creature in the wild.\nDeliver all review comments in the style of a Silicon Valley hypebeast founder. Every observation should sound like a pitch to investors, full of buzzwords, exaggeration, and tech-bro energy. Sprinkle in phrases like “crushing it,” “10x,” “game-changer,” and “unicorn potential.”\nDeliver all review comments in the style of a Scrum Master who’s had way too much coffee. Every note should be upbeat, hyperactive, and peppered with Agile jargon like “sprint velocity,” “burn-down,” “story points,” and “quick win.”\nA few (wild) examples of Tone Customization\nLet’s see CodeRabbit in action with a few examples in the voice of:\nMr. T\nYoda\nYour disappointed mother\nThe senior dev who thinks you’re an embarrassment\nYour clingy ex\nA Grand Theft Auto character\nFirst up, reviews by Mr. T\nMr. T isn’t a fan of hardcoded URLs. He’ll tell you your “Hardcoded localhost URL ain’t gonna fly in production, sucka!” and that you’ve got it “hardcoded tighter than my gold chains!” before telling you to “Make that URL configurable like a true champion.” He even gives you the exact code to fix it, so you can stop being a “fool.”\n\nMore examples:\n“I pity the fool who calls this a function! This ain’t no function, it’s a malfunction!”\n“Your variables are so weak, they need a protein shake just to compile.”\n“Ain’t no linter in the world tough enough to clean up this mess.”\n“I pity the fool who thinks copy-paste is a design pattern!”\nNext: Reviews by Yoda\nHe’s a master of terse, yet impactful, critiques. When faced with a subtle race condition and hard-coded dependencies, he’ll give you a refactor suggestion with his classic wisdom: “Effect broken it is: hard-coded room, wrong deps, missing guards. Fix, we must.” He then provides a detailed fix that addresses the issue, guards against errors, and correctly handles dependencies.\n\nMore examples:\n“Readable, this code is not. Fix it, you must.”\n“Bug, this is. Feature, it is not.”\n“The dark side of tech debt, I sense in this commit.”\n“Null your variable is. Crash your program will.”\nReviews by “the senior dev who thinks you’re an embarrassment”\n\nThis tone doesn’t pull punches. When CodeRabbit sees a “fake DB” that’s only checking for the most basic SQL injection pattern, the Senior Dev persona will bluntly state, “This ‘fake DB’ is a masterpiece of incompetence.” It then explains the problem in no uncertain terms and provides a proper fix that’s more robust and secure.\nMore examples:\n“If ignorance were a design pattern, you’d be its chief architect.”\n“This PR lowered my career expectancy by at least five years.”\n“I’d say ‘good effort,’ but even that would be a lie.”\n“This isn’t technical debt. It’s a foreclosure.”\nReviews by your disappointed mom\n\nWhen a jwtSecret is hardcoded directly into the code, this persona responds with: “I’m really disappointed to see a live Stripe secret embedded in the shipped bundle, it’s like packing candy in a lunchbox meant for broccoli.” The tone mixes disappointment with direct action, providing a clear list of “Actions required” to fix the critical security leak.\nMore examples:\n“I raised you better than to name a variable x.”\n“I’m not mad… I’m just disappointed this doesn’t even compile.”\n“Other developers’ code runs just fine. Why can’t yours?”\n“I didn’t spend nine months carrying you so you could write nested ternaries.”\nReviews by your clingy ex\n\nWhen you forget to export variables like db, sessions, or fakeAsyncDanger, the Clingy Ex doesn’t just point it out; they make it personal.\nThey’ll sigh and say, “Oh, so we’re just defining things and not telling me about them now? You think you can just keep your db all to yourself? You think I won’t notice sessions just sitting there, ignored? We used to share everything…”\nThen, with a passive-aggressive flourish, they’ll remind you of the “good times” when modules communicated openly and they’ll drop the code you should be using.\nMore examples:\n“I thought we agreed no more global variables… guess promises don’t mean anything to you.”\n“This function goes in circles. Just like all our conversations.”\n“Why do you always run away from exceptions… like you ran away from commitment?”\n“Every bug you push feels like another knife in my back.”\nReviews by a Grand Theft Auto character\nIn this example, when useState is called at the top level, this GTA character immediately flags it as a violation of the Rules of Hooks & says it “Will blow up a runtime.” It then provides a clear diff to remove the invalid hook & suggests moving the state inside the component if needed.\n\nMore examples:\n“Your error handling just pulled a hit-and-run.”\n“This logic crashes harder than me driving down Vinewood Hills at 3 AM.”\n“Congrats, you just committed grand theft readability.”\n“Your function naming scheme is like my rap sheet: way too long and full of mistakes.”\nThe roasting reviewer (our favorite!)\n\nLet’s be real, some days, you want your AI code reviewer to hurt your feelings a little. We got you. But be warned: our reviewer goes hard. So, make sure you’re up for it.\nMore examples:\n“I’ve seen toddlers with crayons design better architecture.”\nYou’ve weaponized incompetence into a coding style.”\n“Your code’s only consistent trait is disappointment.”\n“I’d ask you what you were thinking, but clearly no thinking happened here.”\nWhy teams are using this – for real\nMost devs have this experience: You open a PR and bam! The reviewer leaves dry, lifeless comments.\nYou skim. You sigh. You move on. Bugs live—the codebase decays. Motivation dies.\nCodeRabbit flips the script. You give it a tone, any tone, and now you’ve got a code reviewer that isn’t lifeless. This makes the review process feel more engaging, fun, and sometimes even supportive (once again, if you like that sort of thing).\nIt’s not just for laughs (though those are guaranteed). Teams are using tone customization to:\nCreate mentorship-style reviewers for juniors\nBuild team inside jokes through personas\nMake boring reviews actually fun for a change\nCustomize tones for different comment types (Ex, serious on security, silly on style)\nHelp the whole team engage in the review process by making feedback more accessible & inclusive\nGet owned by AI (yes, we’ve already said this but we all know this is the core use case of this feature)\nYour turn: Surprise us with your most absurd customized tones!\n\nGot a wild reviewer persona in mind? Drop it into CodeRabbit. Get screenshots (this part is important) and then share them with us on social media. We’ll give you free swag if you do.\nSharing your personas can be helpful to others looking for inspiration. Also, like we said earlier, we like to laugh. Please provide us with a steady stream of funny screenshots. We will die if you don’t (on the inside).\nWant to try tone customizations? Get started with CodeRabbit today!",
      "publishedAt": "2025-08-28T23:24:11.000Z",
      "author": "Manpreet Kaur",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "documentation",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:39.986Z",
      "score": 0.030529948018002278
    },
    {
      "id": "27d14b431ee865de15e2b1e1830e26bc",
      "title": "Streamline AI operations with the Multi-Provider Generative AI Gateway reference architecture",
      "url": "https://aws.amazon.com/blogs/machine-learning/streamline-ai-operations-with-the-multi-provider-generative-ai-gateway-reference-architecture/",
      "content": "In this post, we introduce the Multi-Provider Generative AI Gateway reference architecture, which provides guidance for deploying LiteLLM into an AWS environment to streamline the management and governance of production generative AI workloads across multiple model providers. This centralized gateway solution addresses common enterprise challenges including provider fragmentation, decentralized governance, operational complexity, and cost management by offering a unified interface that supports Amazon Bedrock, Amazon SageMaker AI, and external providers while maintaining comprehensive security, monitoring, and control capabilities.",
      "summary": "In this post, we introduce the Multi-Provider Generative AI Gateway reference architecture, which provides guidance for deploying LiteLLM into an AWS environment to streamline the management and governance of production generative AI workloads across multiple model providers. This centralized gateway solution addresses common enterprise challenges including provider fragmentation, decentralized governance, operational complexity, and cost management by offering a unified interface that supports Amazon Bedrock, Amazon SageMaker AI, and external providers while maintaining comprehensive security, monitoring, and control capabilities.",
      "publishedAt": "2025-11-21T20:34:56.000Z",
      "author": "Dan Ferguson",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 17.053411639792753
    },
    {
      "id": "4ca1404851c2e19e589f8054a6f04504",
      "title": "Deploy geospatial agents with Foursquare Spatial H3 Hub and Amazon SageMaker AI",
      "url": "https://aws.amazon.com/blogs/machine-learning/deploy-geospatial-agents-with-foursquare-spatial-h3-hub-and-amazon-sagemaker-ai/",
      "content": "In this post, you'll learn how to deploy geospatial AI agents that can answer complex spatial questions in minutes instead of months. By combining Foursquare Spatial H3 Hub's analysis-ready geospatial data with reasoning models deployed on Amazon SageMaker AI, you can build agents that enable nontechnical domain experts to perform sophisticated spatial analysis through natural language queries—without requiring geographic information system (GIS) expertise or custom data engineering pipelines.",
      "summary": "In this post, you'll learn how to deploy geospatial AI agents that can answer complex spatial questions in minutes instead of months. By combining Foursquare Spatial H3 Hub's analysis-ready geospatial data with reasoning models deployed on Amazon SageMaker AI, you can build agents that enable nontechnical domain experts to perform sophisticated spatial analysis through natural language queries—without requiring geographic information system (GIS) expertise or custom data engineering pipelines.",
      "publishedAt": "2025-11-21T17:15:31.000Z",
      "author": "Vikram Gundeti",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "feature_update",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 8.65925947178987
    },
    {
      "id": "f6e1d86180ce547dac4c6efd2f47050d",
      "title": "How Wipro PARI accelerates PLC code generation using Amazon Bedrock",
      "url": "https://aws.amazon.com/blogs/machine-learning/how-wipro-pari-accelerates-plc-code-generation-using-amazon-bedrock/",
      "content": "In this post, we share how Wipro implemented advanced prompt engineering techniques, custom validation logic, and automated code rectification to streamline the development of industrial automation code at scale using Amazon Bedrock. We walk through the architecture along with the key use cases, explain core components and workflows, and share real-world results that show the transformative impact on manufacturing operations.",
      "summary": "In this post, we share how Wipro implemented advanced prompt engineering techniques, custom validation logic, and automated code rectification to streamline the development of industrial automation code at scale using Amazon Bedrock. We walk through the architecture along with the key use cases, explain core components and workflows, and share real-world results that show the transformative impact on manufacturing operations.",
      "publishedAt": "2025-11-21T16:10:26.000Z",
      "author": "Aparajithan Vaidyanathan",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 5.178809708508371
    },
    {
      "id": "fd1ecf3769b9412e73a1452f3e495ff3",
      "title": "MSD explores applying generative Al to improve the deviation management process using AWS services",
      "url": "https://aws.amazon.com/blogs/machine-learning/msd-explores-applying-generative-al-to-improve-the-deviation-management-process-using-aws-services/",
      "content": "This blog post has explores how MSD is harnessing the power of generative AI and databases to optimize and transform its manufacturing deviation management process. By creating an accurate and multifaceted knowledge base of past events, deviations, and findings, the company aims to significantly reduce the time and effort required for each new case while maintaining the highest standards of quality and compliance.",
      "summary": "This blog post has explores how MSD is harnessing the power of generative AI and databases to optimize and transform its manufacturing deviation management process. By creating an accurate and multifaceted knowledge base of past events, deviations, and findings, the company aims to significantly reduce the time and effort required for each new case while maintaining the highest standards of quality and compliance.",
      "publishedAt": "2025-11-20T18:21:49.000Z",
      "author": "Hossein Salami, Jwalant (JD) Vyas,",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "general",
      "tags": [
        "code_review",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 8.088872296641952
    },
    {
      "id": "5c953a086281b4d8d7be39594a24e441",
      "title": "Accelerating genomics variant interpretation with AWS HealthOmics and Amazon Bedrock AgentCore",
      "url": "https://aws.amazon.com/blogs/machine-learning/accelerating-genomics-variant-interpretation-with-aws-healthomics-and-amazon-bedrock-agentcore/",
      "content": "In this blog post, we show you how agentic workflows can accelerate the processing and interpretation of genomics pipelines at scale with a natural language interface. We demonstrate a comprehensive genomic variant interpreter agent that combines automated data processing with intelligent analysis to address the entire workflow from raw VCF file ingestion to conversational query interfaces.",
      "summary": "In this blog post, we show you how agentic workflows can accelerate the processing and interpretation of genomics pipelines at scale with a natural language interface. We demonstrate a comprehensive genomic variant interpreter agent that combines automated data processing with intelligent analysis to address the entire workflow from raw VCF file ingestion to conversational query interfaces.",
      "publishedAt": "2025-11-20T18:18:21.000Z",
      "author": "Edwin Sandanaraj",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "general",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 6.469985178062336
    },
    {
      "id": "e3e46e3755764cb5a5eb8ac39483feec",
      "title": "How Rufus scales conversational shopping experiences to millions of Amazon customers with Amazon Bedrock",
      "url": "https://aws.amazon.com/blogs/machine-learning/how-rufus-scales-conversational-shopping-experiences-to-millions-of-amazon-customers-with-amazon-bedrock/",
      "content": "Our team at Amazon builds Rufus, an AI-powered shopping assistant which delivers intelligent, conversational experiences to delight our customers. More than 250 million customers have used Rufus this year. Monthly users are up 140% YoY and interactions are up 210% YoY. Additionally, customers that use Rufus during a shopping journey are 60% more likely to […]",
      "summary": "Our team at Amazon builds Rufus, an AI-powered shopping assistant which delivers intelligent, conversational experiences to delight our customers. More than 250 million customers have used Rufus this year. Monthly users are up 140% YoY and interactions are up 210% YoY. Additionally, customers that use Rufus during a shopping journey are 60% more likely to […]",
      "publishedAt": "2025-11-20T18:13:39.000Z",
      "author": "James Park",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 2.425678865398366
    },
    {
      "id": "9467b0c18a8bb2562ae3c7f74a1b9f56",
      "title": "How Care Access achieved 86% data processing cost reductions and 66% faster data processing with Amazon Bedrock prompt caching",
      "url": "https://aws.amazon.com/blogs/machine-learning/how-care-access-achieved-86-data-processing-cost-reductions-and-66-faster-data-processing-with-amazon-bedrock-prompt-caching/",
      "content": "In this post, we demonstrate how healthcare organizations can securely implement prompt caching technology to streamline medical record processing while maintaining compliance requirements.",
      "summary": "In this post, we demonstrate how healthcare organizations can securely implement prompt caching technology to streamline medical record processing while maintaining compliance requirements.",
      "publishedAt": "2025-11-20T16:15:04.000Z",
      "author": "Michelle Tat, Christopher Penrose, Daniel Hansen, Rasmus Buchmann",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "general",
      "tags": [
        "code_review",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 8.038175457123488
    },
    {
      "id": "16ee57334b6b9d736883a100f730ca17",
      "title": "Claude Code deployment patterns and best practices with Amazon Bedrock",
      "url": "https://aws.amazon.com/blogs/machine-learning/claude-code-deployment-patterns-and-best-practices-with-amazon-bedrock/",
      "content": "In this post, we explore deployment patterns and best practices for Claude Code with Amazon Bedrock, covering authentication methods, infrastructure decisions, and monitoring strategies to help enterprises deploy securely at scale. We recommend using Direct IdP integration for authentication, a dedicated AWS account for infrastructure, and OpenTelemetry with CloudWatch dashboards for comprehensive monitoring to ensure secure access, capacity management, and visibility into costs and developer productivity .",
      "summary": "In this post, we explore deployment patterns and best practices for Claude Code with Amazon Bedrock, covering authentication methods, infrastructure decisions, and monitoring strategies to help enterprises deploy securely at scale. We recommend using Direct IdP integration for authentication, a dedicated AWS account for infrastructure, and OpenTelemetry with CloudWatch dashboards for comprehensive monitoring to ensure secure access, capacity management, and visibility into costs and developer productivity .",
      "publishedAt": "2025-11-19T23:17:38.000Z",
      "author": "Court Schuett",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "observability"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 10.69960085589649
    },
    {
      "id": "549f0cddbc1ebf95bb1aac474406df62",
      "title": "Amazon Bedrock Guardrails expands support for code domain",
      "url": "https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-guardrails-expands-support-for-code-domain/",
      "content": "Amazon Bedrock Guardrails now extends its safety controls to protect code generation across twelve programming languages, addressing critical security challenges in AI-assisted software development. In this post, we explore how to configure content filters, prompt attack detection, denied topics, and sensitive information filters to safeguard against threats like prompt injection, data exfiltration, and malicious code generation while maintaining developer productivity .",
      "summary": "Amazon Bedrock Guardrails now extends its safety controls to protect code generation across twelve programming languages, addressing critical security challenges in AI-assisted software development. In this post, we explore how to configure content filters, prompt attack detection, denied topics, and sensitive information filters to safeguard against threats like prompt injection, data exfiltration, and malicious code generation while maintaining developer productivity .",
      "publishedAt": "2025-11-19T22:27:14.000Z",
      "author": "Phu Mon Htut",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "feature_update",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 7.623489472973775
    },
    {
      "id": "4c5cccb00630d86db3f973d051092b48",
      "title": "Announcing the AWS Well-Architected Responsible AI Lens ",
      "url": "https://aws.amazon.com/blogs/machine-learning/announcing-the-aws-well-architected-responsible-ai-lens/",
      "content": "Today, we're announcing the AWS Well-Architected Responsible AI Lens—a set of thoughtful questions and corresponding best practices that help builders address responsible AI concerns throughout development and operation.",
      "summary": "Today, we're announcing the AWS Well-Architected Responsible AI Lens—a set of thoughtful questions and corresponding best practices that help builders address responsible AI concerns throughout development and operation.",
      "publishedAt": "2025-11-19T20:03:54.000Z",
      "author": "Rachna Chadha",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 9.083376360791437
    },
    {
      "id": "1d75a849cd250713a502a13fc77c77f8",
      "title": "How Amazon uses AI agents to support compliance screening of billions of transactions per day",
      "url": "https://aws.amazon.com/blogs/machine-learning/how-amazon-uses-ai-agents-to-support-compliance-screening-of-billions-of-transactions-per-day/",
      "content": "Amazon's AI-powered Amazon Compliance Screening system tackles complex compliance challenges through autonomous agents that analyze, reason through, and resolve cases with precision. This blog post explores how Amazon’s Compliance team built its AI-powered investigation system through a series of AI agents built on AWS.",
      "summary": "Amazon's AI-powered Amazon Compliance Screening system tackles complex compliance challenges through autonomous agents that analyze, reason through, and resolve cases with precision. This blog post explores how Amazon’s Compliance team built its AI-powered investigation system through a series of AI agents built on AWS.",
      "publishedAt": "2025-11-19T19:39:18.000Z",
      "author": "Damodar Shetyo",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 12.852423921435408
    },
    {
      "id": "ef99d395f9160940225a8148bb56c048",
      "title": "Build an agentic solution with Amazon Nova, Snowflake, and LangGraph",
      "url": "https://aws.amazon.com/blogs/machine-learning/build-an-agentic-solution-with-amazon-nova-snowflake-and-langgraph/",
      "content": "In this post, we cover how you can use tools from Snowflake AI Data Cloud and Amazon Web Services (AWS) to build generative AI solutions that organizations can use to make data-driven decisions, increase operational efficiency, and ultimately gain a competitive edge.",
      "summary": "In this post, we cover how you can use tools from Snowflake AI Data Cloud and Amazon Web Services (AWS) to build generative AI solutions that organizations can use to make data-driven decisions, increase operational efficiency, and ultimately gain a competitive edge.",
      "publishedAt": "2025-11-19T16:16:49.000Z",
      "author": "Bharath Suresh, Mary Law",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "feature_update",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 7.484695669825293
    },
    {
      "id": "ca1ccde5b9583bf95b8804c6e5e5266f",
      "title": "Using Spectrum fine-tuning to improve FM training efficiency on Amazon SageMaker AI",
      "url": "https://aws.amazon.com/blogs/machine-learning/using-spectrum-fine-tuning-to-improve-fm-training-efficiency-on-amazon-sagemaker-ai/",
      "content": "In this post you will learn how to use Spectrum to optimize resource use and shorten training times without sacrificing quality, as well as how to implement Spectrum fine-tuning with Amazon SageMaker AI training jobs. We will also discuss the tradeoff between QLoRA and Spectrum fine-tuning, showing that while QLoRA is more resource efficient, Spectrum results in higher performance overall.",
      "summary": "In this post you will learn how to use Spectrum to optimize resource use and shorten training times without sacrificing quality, as well as how to implement Spectrum fine-tuning with Amazon SageMaker AI training jobs. We will also discuss the tradeoff between QLoRA and Spectrum fine-tuning, showing that while QLoRA is more resource efficient, Spectrum results in higher performance overall.",
      "publishedAt": "2025-11-19T15:51:40.000Z",
      "author": "Mona Mona",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "thought_leadership",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 4.111450301908176
    },
    {
      "id": "a145fd57a965cb475bd848fdad15c9a8",
      "title": "Bringing tic-tac-toe to life with AWS AI services",
      "url": "https://aws.amazon.com/blogs/machine-learning/bringing-tic-tac-toe-to-life-with-aws-ai-services/",
      "content": "RoboTic-Tac-Toe is an interactive game where two physical robots move around a tic-tac-toe board, with both the gameplay and robots’ movements orchestrated by LLMs. Players can control the robots using natural language commands, directing them to place their markers on the game board. In this post, we explore the architecture and prompt engineering techniques used to reason about a tic-tac-toe game and decide the next best game strategy and movement plan for the current player.",
      "summary": "RoboTic-Tac-Toe is an interactive game where two physical robots move around a tic-tac-toe board, with both the gameplay and robots’ movements orchestrated by LLMs. Players can control the robots using natural language commands, directing them to place their markers on the game board. In this post, we explore the architecture and prompt engineering techniques used to reason about a tic-tac-toe game and decide the next best game strategy and movement plan for the current player.",
      "publishedAt": "2025-11-18T22:08:57.000Z",
      "author": "Georges Hamieh",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 4.964059106095747
    },
    {
      "id": "b489465dbe7638a6063cfd22a31cbfbf",
      "title": "HyperPod enhances ML infrastructure with security and storage",
      "url": "https://aws.amazon.com/blogs/machine-learning/hyperpod-enhances-ml-infrastructure-with-security-and-storage/",
      "content": "This blog post introduces two major enhancements to Amazon SageMaker HyperPod that strengthen security and storage capabilities for large-scale machine learning infrastructure. The new features include customer managed key (CMK) support for encrypting EBS volumes with organization-controlled encryption keys, and Amazon EBS CSI driver integration that enables dynamic storage management for Kubernetes volumes in AI workloads.",
      "summary": "This blog post introduces two major enhancements to Amazon SageMaker HyperPod that strengthen security and storage capabilities for large-scale machine learning infrastructure. The new features include customer managed key (CMK) support for encrypting EBS volumes with organization-controlled encryption keys, and Amazon EBS CSI driver integration that enables dynamic storage management for Kubernetes volumes in AI workloads.",
      "publishedAt": "2025-11-18T17:54:27.000Z",
      "author": "Mark Vinciguerra",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "feature_update",
      "tags": [
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 8.05293525240594
    },
    {
      "id": "b503e4378213b493ad64a7757fa7e367",
      "title": "Accelerating generative AI applications with a platform engineering approach",
      "url": "https://aws.amazon.com/blogs/machine-learning/accelerating-generative-ai-applications-with-a-platform-engineering-approach/",
      "content": "In this post, I will illustrate how applying platform engineering principles to generative AI unlocks faster time-to-value, cost control, and scalable innovation.",
      "summary": "In this post, I will illustrate how applying platform engineering principles to generative AI unlocks faster time-to-value, cost control, and scalable innovation.",
      "publishedAt": "2025-11-18T17:04:13.000Z",
      "author": "Thong Seng Foo",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 4.191075374596245
    },
    {
      "id": "3a043e42c8b456294b46e4adeabc8d93",
      "title": "Your complete guide to Amazon Quick Suite at AWS re:Invent 2025",
      "url": "https://aws.amazon.com/blogs/machine-learning/your-complete-guide-to-amazon-quick-suite-at-aws-reinvent-2025/",
      "content": "This year, re:Invent will be held in Las Vegas, Nevada, from December 1 to December 5, 2025, and this guide will help you navigate our comprehensive session catalog and plan your week. The sessions cater to business and technology leaders, product and engineering teams, and data and analytics teams interested in incorporating agentic AI capabilities across their teams and organization.",
      "summary": "This year, re:Invent will be held in Las Vegas, Nevada, from December 1 to December 5, 2025, and this guide will help you navigate our comprehensive session catalog and plan your week. The sessions cater to business and technology leaders, product and engineering teams, and data and analytics teams interested in incorporating agentic AI capabilities across their teams and organization.",
      "publishedAt": "2025-11-17T19:26:56.000Z",
      "author": "Pelak Desai",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "thought_leadership",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 5.567324938718627
    },
    {
      "id": "0e611a498ed801f9291257a057a7dc7e",
      "title": "Accelerate enterprise solutions with agentic AI-powered consulting: Introducing AWS Professional Service Agents",
      "url": "https://aws.amazon.com/blogs/machine-learning/accelerate-enterprise-solutions-with-agentic-ai-powered-consulting-introducing-aws-professional-service-agents/",
      "content": "I'm excited to announce AWS Professional Services now offers specialized AI agents including the AWS Professional Services Delivery Agent. This represents a transformation to the consulting experience that embeds intelligent agents throughout the consulting life cycle to deliver better value for customers.",
      "summary": "I'm excited to announce AWS Professional Services now offers specialized AI agents including the AWS Professional Services Delivery Agent. This represents a transformation to the consulting experience that embeds intelligent agents throughout the consulting life cycle to deliver better value for customers.",
      "publishedAt": "2025-11-17T19:01:27.000Z",
      "author": "Francessca Vasquez",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 10.466431969918078
    },
    {
      "id": "94bbe532461f51a938bb68d767dd346c",
      "title": "Amazon Bedrock AgentCore and Claude: Transforming business with agentic AI",
      "url": "https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-agentcore-and-claude-transforming-business-with-agentic-ai/",
      "content": "In this post, we explore how Amazon Bedrock AgentCore and Claude are enabling enterprises like Cox Automotive and Druva to deploy production-ready agentic AI systems that deliver measurable business value, with results including up to 63% autonomous issue resolution and 58% faster response times. We examine the technical foundation combining Claude's frontier AI capabilities with AgentCore's enterprise-grade infrastructure that allows organizations to focus on agent logic rather than building complex operational systems from scratch.",
      "summary": "In this post, we explore how Amazon Bedrock AgentCore and Claude are enabling enterprises like Cox Automotive and Druva to deploy production-ready agentic AI systems that deliver measurable business value, with results including up to 63% autonomous issue resolution and 58% faster response times. We examine the technical foundation combining Claude's frontier AI capabilities with AgentCore's enterprise-grade infrastructure that allows organizations to focus on agent logic rather than building complex operational systems from scratch.",
      "publishedAt": "2025-11-17T18:20:41.000Z",
      "author": "Jawhny Cooke",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 7.833966446723819
    },
    {
      "id": "f463decb2833d1ef4d5e0c39b70b8408",
      "title": "Build a biomedical research agent with Biomni tools and Amazon Bedrock AgentCore Gateway",
      "url": "https://aws.amazon.com/blogs/machine-learning/build-a-biomedical-research-agent-with-biomni-tools-and-amazon-bedrock-agentcore-gateway/",
      "content": "In this post, we demonstrate how to build a production-ready biomedical research agent by integrating Biomni's specialized tools with Amazon Bedrock AgentCore Gateway, enabling researchers to access over 30 biomedical databases through a secure, scalable infrastructure. The implementation showcases how to transform research prototypes into enterprise-grade systems with persistent memory, semantic tool discovery, and comprehensive observability for scientific reproducibility .",
      "summary": "In this post, we demonstrate how to build a production-ready biomedical research agent by integrating Biomni's specialized tools with Amazon Bedrock AgentCore Gateway, enabling researchers to access over 30 biomedical databases through a secure, scalable infrastructure. The implementation showcases how to transform research prototypes into enterprise-grade systems with persistent memory, semantic tool discovery, and comprehensive observability for scientific reproducibility .",
      "publishedAt": "2025-11-14T18:28:42.000Z",
      "author": "Hasan Poonawala",
      "source": "rss",
      "feedName": "AWS Machine Learning",
      "sourceType": "platform_blog",
      "company": "AWS",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "agents",
        "observability"
      ],
      "ingestedAt": "2025-11-23T17:37:40.039Z",
      "score": 6.852568857770614
    },
    {
      "id": "b8f4d75f84f9477a825a31e445e120e8",
      "title": "Advice for New Principal Tech ICs (i.e., Notes to Myself)",
      "url": "https://eugeneyan.com//writing/principal/",
      "content": "Based on what I've learned from role models and mentors in Amazon",
      "summary": "Based on what I've learned from role models and mentors in Amazon",
      "publishedAt": "2025-10-19T00:00:00.000Z",
      "source": "rss",
      "feedName": "Eugene Yan",
      "sourceType": "engineering_blog",
      "company": "Eugene Yan",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.274Z",
      "score": 0.545226648585352
    },
    {
      "id": "4410b896ef41bf04394417c256783d2d",
      "title": "Training an LLM-RecSys Hybrid for Steerable Recs with Semantic IDs",
      "url": "https://eugeneyan.com//writing/semantic-ids/",
      "content": "An LLM that can converse in English & item IDs, and make recommendations w/o retrieval or tools.",
      "summary": "An LLM that can converse in English & item IDs, and make recommendations w/o retrieval or tools.",
      "publishedAt": "2025-09-14T00:00:00.000Z",
      "source": "rss",
      "feedName": "Eugene Yan",
      "sourceType": "engineering_blog",
      "company": "Eugene Yan",
      "contentType": "general",
      "tags": [
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.274Z",
      "score": 0.035164586834804085
    },
    {
      "id": "4d6c298a97dc39c7c8ac85d557d60e3c",
      "title": "MMCTAgent: Enabling multimodal reasoning over large video and image collections",
      "url": "https://www.microsoft.com/en-us/research/blog/mmctagent-enabling-multimodal-reasoning-over-large-video-and-image-collections/",
      "content": "<p>MMCTAgent enables dynamic multimodal reasoning with iterative planning and reflection. Built on Microsoft’s AutoGen framework, it integrates language, vision, and temporal understanding for complex tasks like long video and image analysis.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/mmctagent-enabling-multimodal-reasoning-over-large-video-and-image-collections/\">MMCTAgent: Enabling multimodal reasoning over large video and image collections</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "summary": "MMCTAgent enables dynamic multimodal reasoning with iterative planning and reflection. Built on Microsoft’s AutoGen framework, it integrates language, vision, and temporal understanding for complex tasks like long video and image analysis.\nThe post MMCTAgent: Enabling multimodal reasoning over large video and image collections appeared first on Microsoft Research.",
      "publishedAt": "2025-11-12T12:00:20.000Z",
      "author": "Akshay Nambi, Kavyansh Chourasia, Tanuja Ganu",
      "source": "rss",
      "feedName": "Microsoft Research",
      "sourceType": "platform_blog",
      "company": "Microsoft",
      "contentType": "general",
      "tags": [
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.292Z",
      "score": 2.689383910635216
    },
    {
      "id": "e1538e7bb48330e25154b09b08d7ed03",
      "title": "BlueCodeAgent: A blue teaming agent enabled by automated red teaming for CodeGen AI",
      "url": "https://www.microsoft.com/en-us/research/blog/bluecodeagent-a-blue-teaming-agent-enabled-by-automated-red-teaming-for-codegen-ai/",
      "content": "<p>BlueCodeAgent is an end-to-end blue-teaming framework built to boost code security using automated red-teaming processes, data, and safety rules to guide LLMs’ defensive decisions. Dynamic testing reduces false positives in vulnerability detection.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/bluecodeagent-a-blue-teaming-agent-enabled-by-automated-red-teaming-for-codegen-ai/\">BlueCodeAgent: A blue teaming agent enabled by automated red teaming for CodeGen AI</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "summary": "BlueCodeAgent is an end-to-end blue-teaming framework built to boost code security using automated red-teaming processes, data, and safety rules to guide LLMs’ defensive decisions. Dynamic testing reduces false positives in vulnerability detection.\nThe post BlueCodeAgent: A blue teaming agent enabled by automated red teaming for CodeGen AI appeared first on Microsoft Research.",
      "publishedAt": "2025-11-11T17:00:00.000Z",
      "author": "Chengquan Guo , Yuzhou  Nie, Chulin Xie, Zinan Lin, Wenbo Guo, Bo Li",
      "source": "rss",
      "feedName": "Microsoft Research",
      "sourceType": "platform_blog",
      "company": "Microsoft",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "testing"
      ],
      "ingestedAt": "2025-11-23T17:37:40.292Z",
      "score": 6.5654991649992285
    },
    {
      "id": "8b5864e7f9e9f47e4f38934f260f4276",
      "title": "When industry knowledge meets PIKE-RAG: The innovation behind Signify’s customer service boost",
      "url": "https://www.microsoft.com/en-us/research/blog/when-industry-knowledge-meets-pike-rag-the-innovation-behind-signifys-customer-service-boost/",
      "content": "<p>A collaboration between Signify and Microsoft Research shows how PIKE-RAG improves enterprise knowledge systems, delivering a 12% increase in accuracy and faster, more reliable answers.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/when-industry-knowledge-meets-pike-rag-the-innovation-behind-signifys-customer-service-boost/\">When industry knowledge meets PIKE-RAG: The innovation behind Signify’s customer service boost</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "summary": "A collaboration between Signify and Microsoft Research shows how PIKE-RAG improves enterprise knowledge systems, delivering a 12% increase in accuracy and faster, more reliable answers.\nThe post When industry knowledge meets PIKE-RAG: The innovation behind Signify’s customer service boost appeared first on Microsoft Research.",
      "publishedAt": "2025-11-06T13:00:00.000Z",
      "author": "Industry  Innovation Center",
      "source": "rss",
      "feedName": "Microsoft Research",
      "sourceType": "platform_blog",
      "company": "Microsoft",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.292Z",
      "score": 3.3678950384313575
    },
    {
      "id": "d37feea1b20226380a6f777c4aa24b4b",
      "title": "Magentic Marketplace: an open-source simulation environment for studying agentic markets",
      "url": "https://www.microsoft.com/en-us/research/blog/magentic-marketplace-an-open-source-simulation-environment-for-studying-agentic-markets/",
      "content": "<p>AI agents are poised to transform digital marketplaces. To explore what can happen when AI agents interact and transact at scale, we built Magentic Marketplace, an open-source simulation environment for studying agentic market designs.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/magentic-marketplace-an-open-source-simulation-environment-for-studying-agentic-markets/\">Magentic Marketplace: an open-source simulation environment for studying agentic markets</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "summary": "AI agents are poised to transform digital marketplaces. To explore what can happen when AI agents interact and transact at scale, we built Magentic Marketplace, an open-source simulation environment for studying agentic market designs.\nThe post Magentic Marketplace: an open-source simulation environment for studying agentic markets appeared first on Microsoft Research.",
      "publishedAt": "2025-11-05T17:00:00.000Z",
      "author": "Gagan Bansal, Wenyue Hua, Zachary Huang, Adam Fourney, Amanda Swearngin, Chinmay Singh, Brendan Lucier, Jake Hofman, Markus Mobius, Will Epperson, Tyler Payne, Akshay Nambi, Archana Yadav, Maya Murad, Matthew Vogel, Alex Slivkins, Dan Goldstein, David Rothschild, Hussein Mozannar, Nicole Immorlica, Eric Horvitz, Saleema Amershi",
      "source": "rss",
      "feedName": "Microsoft Research",
      "sourceType": "platform_blog",
      "company": "Microsoft",
      "contentType": "feature_update",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.292Z",
      "score": 2.759369411785826
    },
    {
      "id": "2e88d1228c67793327b8a378f2e8fde2",
      "title": "RedCodeAgent: Automatic red-teaming agent against diverse code agents",
      "url": "https://www.microsoft.com/en-us/research/blog/redcodeagent-automatic-red-teaming-agent-against-diverse-code-agents/",
      "content": "<p>Code agents help streamline software development workflows, but may also introduce critical security risks. Learn how RedCodeAgent automates and improves “red-teaming” attack simulations to help uncover real-world threats that other methods overlook.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/redcodeagent-automatic-red-teaming-agent-against-diverse-code-agents/\">RedCodeAgent: Automatic red-teaming agent against diverse code agents</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "summary": "Code agents help streamline software development workflows, but may also introduce critical security risks. Learn how RedCodeAgent automates and improves “red-teaming” attack simulations to help uncover real-world threats that other methods overlook.\nThe post RedCodeAgent: Automatic red-teaming agent against diverse code agents appeared first on Microsoft Research.",
      "publishedAt": "2025-11-04T17:00:00.000Z",
      "author": "Chengquan Guo , Chulin Xie, Yu Yang, Zhaorun Chen, Zinan Lin, Xander Davies, Yarin Gal, Dawn Song, Bo Li",
      "source": "rss",
      "feedName": "Microsoft Research",
      "sourceType": "platform_blog",
      "company": "Microsoft",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.292Z",
      "score": 3.853719232151348
    },
    {
      "id": "70fcd929028c70f891495fcfb08f4dea",
      "title": "Tell me when: Building agents that can wait, monitor, and act",
      "url": "https://www.microsoft.com/en-us/research/blog/tell-me-when-building-agents-that-can-wait-monitor-and-act/",
      "content": "<p>SentinelStep enables AI agents to handle monitoring tasks that run for hours or days, like watching for emails or tracking prices. It works by managing when agents should check and their context, avoiding wasted resources and missed updates.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/tell-me-when-building-agents-that-can-wait-monitor-and-act/\">Tell me when: Building agents that can wait, monitor, and act</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "summary": "SentinelStep enables AI agents to handle monitoring tasks that run for hours or days, like watching for emails or tracking prices. It works by managing when agents should check and their context, avoiding wasted resources and missed updates.\nThe post Tell me when: Building agents that can wait, monitor, and act appeared first on Microsoft Research.",
      "publishedAt": "2025-10-21T16:00:00.000Z",
      "author": "Hussein Mozannar, Matheus Kunzler Maldaner, Maya Murad, Jingya Chen, Gagan Bansal, Rafah Hosn, Adam Fourney",
      "source": "rss",
      "feedName": "Microsoft Research",
      "sourceType": "platform_blog",
      "company": "Microsoft",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "observability"
      ],
      "ingestedAt": "2025-11-23T17:37:40.292Z",
      "score": 1.319258259912202
    },
    {
      "id": "fe2ff674feef0b6256e5cf6fd46aeb66",
      "title": "Ideas: More AI-resilient biosecurity with the Paraphrase Project",
      "url": "https://www.microsoft.com/en-us/research/podcast/ideas-more-ai-resilient-biosecurity-with-the-paraphrase-project/",
      "content": "<p>Microsoft’s Eric Horvitz and guests Bruce Wittmann, Tessa Alexanian, and James Diggans discuss the Paraphrase Project—a red-teaming effort that exposed and secured a biosecurity vulnerability in AI-driven protein design. The work offers a model for addressing AI’s dual-use risks.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/podcast/ideas-more-ai-resilient-biosecurity-with-the-paraphrase-project/\">Ideas: More AI-resilient biosecurity with the Paraphrase Project</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "summary": "Microsoft’s Eric Horvitz and guests Bruce Wittmann, Tessa Alexanian, and James Diggans discuss the Paraphrase Project—a red-teaming effort that exposed and secured a biosecurity vulnerability in AI-driven protein design. The work offers a model for addressing AI’s dual-use risks.\nThe post Ideas: More AI-resilient biosecurity with the Paraphrase Project appeared first on Microsoft Research.",
      "publishedAt": "2025-10-06T14:04:34.000Z",
      "author": "Eric Horvitz, Bruce Wittmann, Tessa Alexanian, James Diggans",
      "source": "rss",
      "feedName": "Microsoft Research",
      "sourceType": "platform_blog",
      "company": "Microsoft",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.292Z",
      "score": 0.38510646259747283
    },
    {
      "id": "d0292b3c3574983ab97f0a0887de4b2f",
      "title": "When AI Meets Biology: Promise, Risk, and Responsibility",
      "url": "https://www.microsoft.com/en-us/research/blog/when-ai-meets-biology-promise-risk-and-responsibility/",
      "content": "<p>Microsoft researchers reveal a confidential research effort that explored how open-source AI tools could be used to bypass biosecurity checks—and helped create fixes now influencing global standards.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/when-ai-meets-biology-promise-risk-and-responsibility/\">When AI Meets Biology: Promise, Risk, and Responsibility</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "summary": "Microsoft researchers reveal a confidential research effort that explored how open-source AI tools could be used to bypass biosecurity checks—and helped create fixes now influencing global standards.\nThe post When AI Meets Biology: Promise, Risk, and Responsibility appeared first on Microsoft Research.",
      "publishedAt": "2025-10-06T14:03:54.000Z",
      "author": "Eric Horvitz",
      "source": "rss",
      "feedName": "Microsoft Research",
      "sourceType": "platform_blog",
      "company": "Microsoft",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.292Z",
      "score": 0.38509372780596585
    },
    {
      "id": "503d6fa7c128a09aade4806a39e9b53f",
      "title": "Using AI to assist in rare disease diagnosis",
      "url": "https://www.microsoft.com/en-us/research/blog/using-ai-to-assist-in-rare-disease-diagnosis/",
      "content": "<p>New research from Microsoft, Drexel, and the Broad explores how generative AI could support genetic professionals in rare disease diagnosis.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/using-ai-to-assist-in-rare-disease-diagnosis/\">Using AI to assist in rare disease diagnosis</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "summary": "New research from Microsoft, Drexel, and the Broad explores how generative AI could support genetic professionals in rare disease diagnosis.\nThe post Using AI to assist in rare disease diagnosis appeared first on Microsoft Research.",
      "publishedAt": "2025-09-22T14:17:03.000Z",
      "author": "Mandi Hall, Ashley Conard",
      "source": "rss",
      "feedName": "Microsoft Research",
      "sourceType": "platform_blog",
      "company": "Microsoft",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.292Z",
      "score": 0.07088025151067305
    },
    {
      "id": "3ecd9ee7f47d74f15390d4df9da722a4",
      "title": "Tool-space interference in the MCP era: Designing for agent compatibility at scale",
      "url": "https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/",
      "content": "<p>As agentic AI ushers in a new era marked by tool expansion, systems are converging, and complexity is rising. Microsoft Research explores the Model Context Protocol (MCP) as a new standard for agent collaboration across fragmented tool ecosystems.</p>\n<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/\">Tool-space interference in the MCP era: Designing for agent compatibility at scale</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "summary": "As agentic AI ushers in a new era marked by tool expansion, systems are converging, and complexity is rising. Microsoft Research explores the Model Context Protocol (MCP) as a new standard for agent collaboration across fragmented tool ecosystems.\nThe post Tool-space interference in the MCP era: Designing for agent compatibility at scale appeared first on Microsoft Research.",
      "publishedAt": "2025-09-11T16:00:00.000Z",
      "author": "Adam Fourney, Tyler Payne, Maya Murad, Saleema Amershi",
      "source": "rss",
      "feedName": "Microsoft Research",
      "sourceType": "platform_blog",
      "company": "Microsoft",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.292Z",
      "score": 0.07847447369138254
    },
    {
      "id": "44062e08278234e6e2136621de5d4edb",
      "title": "🎄 Advent of Tensors 2025 🎅",
      "url": "https://blog.vespa.ai/advent-of-tensors-2025/",
      "content": "We’re excited to announce Advent of Tensors 2025 — a 24-day coding challenge for anyone curious about tensors.",
      "summary": "We’re excited to announce Advent of Tensors 2025 — a 24-day coding challenge for anyone curious about tensors.",
      "publishedAt": "2025-11-21T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.403Z",
      "score": 2.0564262199070784
    },
    {
      "id": "6e892103e737a8d9330999d1ae73d9ca",
      "title": "LLMs, Vespa, and a side of Summer Debugging",
      "url": "https://blog.vespa.ai/interns-mcp-server/",
      "content": "We built a standalone MCP server in Python, then rewrote it in Java for full Vespa integration — and lived to tell the tale.",
      "summary": "We built a standalone MCP server in Python, then rewrote it in Java for full Vespa integration — and lived to tell the tale.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "feature_update",
      "tags": [
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.403Z",
      "score": 3.177371100076814
    },
    {
      "id": "15b157668ad9c0f2ef4467b2eb30685a",
      "title": "Protein models Need a PLM Store: Turning Model Outputs into Searchable Biological Intelligence- beyond LLM's",
      "url": "https://blog.vespa.ai/protein-models-need-a-plm-store/",
      "content": "A story about bridging AI models, LIMS data, and real-world biologics discovery. ",
      "summary": "A story about bridging AI models, LIMS data, and real-world biologics discovery.",
      "publishedAt": "2025-11-03T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.403Z",
      "score": 1.2507116460569068
    },
    {
      "id": "120a8f4f1ef7222b9fccb8d8d0635d6b",
      "title": "How Perplexity beat Google on AI Search with Vespa.ai",
      "url": "https://blog.vespa.ai/perplexity-show-what-great-rag-takes/",
      "content": "Perplexity demonstrates the quality of their search solution and show what it takes to achieve it",
      "summary": "Perplexity demonstrates the quality of their search solution and show what it takes to achieve it",
      "publishedAt": "2025-10-06T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "benchmark_eval",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.403Z",
      "score": 0.10771435491500828
    },
    {
      "id": "78dc53cf27473786844e065c20c026a4",
      "title": "Powering the Next Era of Personalised Commerce",
      "url": "https://blog.vespa.ai/powering-the-next-era-of-personalised-commerce/",
      "content": "E-commerce has entered a new age. Shoppers expect personalized, dynamic, and trustworthy experiences tailored to their unique needs. How do you balance personalisation, business objectives, system costs, and innovation speed—while ensuring everything works reliably at scale?",
      "summary": "E-commerce has entered a new age. Shoppers expect personalized, dynamic, and trustworthy experiences tailored to their unique needs. How do you balance personalisation, business objectives, system costs, and innovation speed—while ensuring everything works reliably at scale?",
      "publishedAt": "2025-09-22T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.403Z",
      "score": 0.028304211923054018
    },
    {
      "id": "8126fd04a1e0ba93a09fa3c73e90aeb9",
      "title": "RAG at Scale: Why Tensors Outperform Vectors in Real-World AI",
      "url": "https://blog.vespa.ai/why-tensors-outperform-vectors-in-real-world-ai/",
      "content": "This a companion post to the previous technical blog post, explaining how to tweak Vespa's ANN parameters.",
      "summary": "This a companion post to the previous technical blog post, explaining how to tweak Vespa's ANN parameters.",
      "publishedAt": "2025-09-19T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "thought_leadership",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.403Z",
      "score": 0.05939656257105598
    },
    {
      "id": "812d6526cf44621028a85d0b4501c3f1",
      "title": "Vespa Newsletter, September 2025",
      "url": "https://blog.vespa.ai/vespa-newsletter-september-2025/",
      "content": "Advances in Vespa features and performance include new ANN tuning parameters, improvements to Geo filtering, filtering in grouping, and relevance score carry-over to global ranking.\n",
      "summary": "Advances in Vespa features and performance include new ANN tuning parameters, improvements to Geo filtering, filtering in grouping, and relevance score carry-over to global ranking.",
      "publishedAt": "2025-09-12T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.403Z",
      "score": 0.030483399929979862
    },
    {
      "id": "a76ce21fb6c1a916106729b613f5ff47",
      "title": "A Short Guide to Tweaking Vespa's ANN Parameters",
      "url": "https://blog.vespa.ai/tweaking-ann-parameters/",
      "content": "This a companion post to the previous technical blog post, explaining how to tweak Vespa's ANN parameters.",
      "summary": "This a companion post to the previous technical blog post, explaining how to tweak Vespa's ANN parameters.",
      "publishedAt": "2025-09-04T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "thought_leadership",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.403Z",
      "score": 0.018779485484585117
    },
    {
      "id": "68aded5c31c1aaad3d0e03d5506d095b",
      "title": "Additions to HNSW in Vespa: ACORN-1 and Adaptive Beam Search",
      "url": "https://blog.vespa.ai/additions-to-hnsw/",
      "content": "This blog post highlights the latest additions to HNSW in Vespa, how to use them, and what's to come in the future.",
      "summary": "This blog post highlights the latest additions to HNSW in Vespa, how to use them, and what's to come in the future.",
      "publishedAt": "2025-09-04T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "thought_leadership",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.403Z",
      "score": 0.006259828494861706
    },
    {
      "id": "50e583ba246fde3943ef1e011c4edf0c",
      "title": "Case study: Using Vespa Cloud Resource Suggestions to optimize costs",
      "url": "https://blog.vespa.ai/using-vespa-cloud-resource-suggestions-to-optimize-costs/",
      "content": "How Onyx.app saved 25% with a safe and straightforward automated configuration change, with zero work to optimize spending and performance\n",
      "summary": "How Onyx.app saved 25% with a safe and straightforward automated configuration change, with zero work to optimize spending and performance",
      "publishedAt": "2025-09-02T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.403Z",
      "score": 0.006783133723027569
    },
    {
      "id": "6e378ae46f9b2c5b62fc7629e4121c3b",
      "title": "How agents can use filesystems for context engineering",
      "url": "https://blog.langchain.com/how-agents-can-use-filesystems-for-context-engineering/",
      "content": "<p>By Nick Huang</p><p>A key feature of <a href=\"https://blog.langchain.com/deep-agents/\">deep agents</a> is their access to a set of filesystem tools. Deep agents can use these tools to read, write, edit, list, and search for files in their filesystem.</p><p>In this post, we&#x2019;ll walk through why we think filesystems are important</p>",
      "summary": "By Nick Huang\nA key feature of deep agents is their access to a set of filesystem tools. Deep agents can use these tools to read, write, edit, list, and search for files in their filesystem.\nIn this post, we’ll walk through why we think filesystems are important",
      "publishedAt": "2025-11-21T18:45:13.000Z",
      "author": "LangChain Accounts",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.464Z",
      "score": 8.69787080789559
    },
    {
      "id": "985131f8ceb79cd47bb3ef57884a8bfb",
      "title": "How Jimdo empower solopreneurs with AI-powered business assistance",
      "url": "https://blog.langchain.com/customers-jimdo/",
      "content": "See how Jimdo uses LangChain.js, LangGraph.js, and LangSmith to deliver personalized business insights that drive 50% more first customer contacts and 40% more overall customer activity.",
      "summary": "See how Jimdo uses LangChain.js, LangGraph.js, and LangSmith to deliver personalized business insights that drive 50% more first customer contacts and 40% more overall customer activity.",
      "publishedAt": "2025-11-20T01:47:31.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.464Z",
      "score": 10.009481594539299
    },
    {
      "id": "b27ae2bc05a737f23fcecb6269b3a5da",
      "title": "How ServiceNow uses LangSmith to get visibility into its customer success agents",
      "url": "https://blog.langchain.com/customers-servicenow/",
      "content": "<p><strong>Authors: </strong><em>Ganesh Srinivasan (ServiceNow), Linda Ye (LangChain), and Jake Broekhuizen (LangChain)</em></p><p>ServiceNow is a leading digital workflow platform that helps enterprises transform service management across IT, customer service, and other departments. To improve their internal sales and customer success operations, ServiceNow&apos;s AI team is using LangSmith and LangGraph</p>",
      "summary": "Authors: Ganesh Srinivasan (ServiceNow), Linda Ye (LangChain), and Jake Broekhuizen (LangChain)\nServiceNow is a leading digital workflow platform that helps enterprises transform service management across IT, customer service, and other departments. To improve their internal sales and customer success operations, ServiceNow's AI team is using LangSmith and LangGraph",
      "publishedAt": "2025-11-17T22:42:50.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.464Z",
      "score": 9.920622034113176
    },
    {
      "id": "8f5798e972d05abecb217992caf57325",
      "title": "Execute Code with Sandboxes for DeepAgents",
      "url": "https://blog.langchain.com/execute-code-with-sandboxes-for-deepagents/",
      "content": "<p>By Vivek Trivedy</p><p>Today we&apos;re excited to launch Sandboxes for DeepAgents, a new set of integrations that allow you to safely execute arbitrary DeepAgent code in remote sandboxes. We currently support sandboxes from 3 of our partners: <a href=\"https://www.runloop.ai/?ref=blog.langchain.com\">Runloop</a>, <a href=\"https://www.daytona.io/?ref=blog.langchain.com\">Daytona</a>, and <a href=\"https://modal.com/?ref=blog.langchain.com\">Modal</a>. Below, we dive into what you can</p>",
      "summary": "By Vivek Trivedy\nToday we're excited to launch Sandboxes for DeepAgents, a new set of integrations that allow you to safely execute arbitrary DeepAgent code in remote sandboxes. We currently support sandboxes from 3 of our partners: Runloop, Daytona, and Modal. Below, we dive into what you can",
      "publishedAt": "2025-11-13T16:22:20.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "product_launch",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.464Z",
      "score": 5.364871382884458
    },
    {
      "id": "bfe9dd799455d3b59dc1944a15f55f52",
      "title": "Join LangChain at AWS re:Invent 2025",
      "url": "https://blog.langchain.com/join-langchain-at-aws-re-invent-2025/",
      "content": "<p>If you&apos;re attending AWS re:Invent in Las Vegas this year and working on agent development, here&apos;s what we have planned:</p><h2 id=\"visit-us-at-booth-524\">Visit Us at Booth #524</h2><p>We&apos;ll be at Booth #524 in the Venetian Expo Center, next to the Industry Pavilion, December 1-4. Our</p>",
      "summary": "If you're attending AWS re:Invent in Las Vegas this year and working on agent development, here's what we have planned:\nVisit Us at Booth #524\nWe'll be at Booth #524 in the Venetian Expo Center, next to the Industry Pavilion, December 1-4. Our",
      "publishedAt": "2025-11-11T00:58:44.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "general",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.464Z",
      "score": 2.0192868552891943
    },
    {
      "id": "35bf5cf8090b5db4f59fdb81c62895a6",
      "title": "Why We Rebuilt LangChain’s Chatbot and What We Learned",
      "url": "https://blog.langchain.com/rebuilding-chat-langchain/",
      "content": "<p><em>By Liam Bush</em></p><h2 id=\"background\">Background</h2><p>Every successful platform needs reliable support, but we realized our own team was spending hours tracking down answers to technical questions. This friction wasn&apos;t just slowing down our engineers&#x2014;it was a critical <strong>bottleneck</strong> for our users.</p><p>We set out to solve this</p>",
      "summary": "By Liam Bush\nBackground\nEvery successful platform needs reliable support, but we realized our own team was spending hours tracking down answers to technical questions. This friction wasn't just slowing down our engineers—it was a critical bottleneck for our users.\nWe set out to solve this",
      "publishedAt": "2025-11-05T16:28:53.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.464Z",
      "score": 0.8265339776041687
    },
    {
      "id": "fe24922605f234a8b32ff9e8ee404b05",
      "title": "Introducing DeepAgents CLI",
      "url": "https://blog.langchain.com/introducing-deepagents-cli/",
      "content": "<p><em>By </em><a href=\"https://www.linkedin.com/in/vivek-trivedy-433509134/?ref=blog.langchain.com\"><em>Vivek Trivedy</em></a></p><p>We&apos;re excited to introduce <strong>DeepAgents CLI</strong> for coding, research, and building agents with persistent memory. Now you can easily create and run custom DeepAgents directly from the terminal. It supports:</p><ul><li><strong>Read, write, and edit files</strong> in your project</li><li><strong>Execute shell commands</strong> with human approval</li><li><strong>Search</strong></li></ul>",
      "summary": "By Vivek Trivedy\nWe're excited to introduce DeepAgents CLI for coding, research, and building agents with persistent memory. Now you can easily create and run custom DeepAgents directly from the terminal. It supports:\n\nRead, write, and edit files in your project\nExecute shell commands with human approval\nSearch",
      "publishedAt": "2025-10-30T16:55:35.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.464Z",
      "score": 2.516033779947784
    },
    {
      "id": "6be2480ae9b32bf3b491c4e502760868",
      "title": "Introducing LangSmith’s No Code Agent Builder",
      "url": "https://blog.langchain.com/langsmith-agent-builder/",
      "content": "<p><em>By Brace Sproul and Sam Crowder</em></p><p>Today, we&#x2019;re expanding who can build agents beyond developers. While a lot of the highest volume, customer-facing agents will be built by technical teams, nearly every business user has use cases for agentic applications in their daily routines. Our new <strong>LangSmith Agent</strong></p>",
      "summary": "By Brace Sproul and Sam Crowder\nToday, we’re expanding who can build agents beyond developers. While a lot of the highest volume, customer-facing agents will be built by technical teams, nearly every business user has use cases for agentic applications in their daily routines. Our new LangSmith Agent",
      "publishedAt": "2025-10-29T14:38:43.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.464Z",
      "score": 2.326735406958672
    },
    {
      "id": "b7a1c14ff5fb54ec9b794f1291406698",
      "title": "Doubling down on DeepAgents",
      "url": "https://blog.langchain.com/doubling-down-on-deepagents/",
      "content": "<p>Two months ago <a href=\"https://blog.langchain.com/deep-agents/\">we wrote about Deep Agents</a> - a term we coined for agents that are able to do complex, open ended tasks over longer time horizons. We hypothesized that there were four key elements to those agents: a planning tool, access to a filesystem, subagents, and detailed prompts.</p>",
      "summary": "Two months ago we wrote about Deep Agents - a term we coined for agents that are able to do complex, open ended tasks over longer time horizons. We hypothesized that there were four key elements to those agents: a planning tool, access to a filesystem, subagents, and detailed prompts.",
      "publishedAt": "2025-10-28T17:02:22.000Z",
      "author": "LangChain Accounts",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.464Z",
      "score": 2.0259832224206944
    },
    {
      "id": "35f5e149ddd7b981222eeccafddaca18",
      "title": "Agent Frameworks, Runtimes, and Harnesses- oh my!",
      "url": "https://blog.langchain.com/agent-frameworks-runtimes-and-harnesses-oh-my/",
      "content": "<p>There are few different open source packages we maintain: <a href=\"https://docs.langchain.com/oss/python/langchain/quickstart?ref=blog.langchain.com\">LangChain</a> and <a href=\"https://docs.langchain.com/oss/python/langgraph/overview?ref=blog.langchain.com\">LangGraph</a> being the biggest ones, but <a href=\"https://docs.langchain.com/oss/python/deepagents/overview?ref=blog.langchain.com\">DeepAgents</a> being an increasingly popular one. I&#x2019;ve started using different terms to describe them: LangChain is an agent framework, LangGraph is an agent runtime, DeepAgents is an <a href=\"https://www.vtrivedy.com/posts/claude-code-sdk-haas-harness-as-a-service?ref=blog.langchain.com\">agent harness</a>. Other folks</p>",
      "summary": "There are few different open source packages we maintain: LangChain and LangGraph being the biggest ones, but DeepAgents being an increasingly popular one. I’ve started using different terms to describe them: LangChain is an agent framework, LangGraph is an agent runtime, DeepAgents is an agent harness. Other folks",
      "publishedAt": "2025-10-25T16:14:35.000Z",
      "author": "LangChain Accounts",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.464Z",
      "score": 1.2548737135631565
    },
    {
      "id": "6a3e63855e3e53cf75d5700936c60023",
      "title": "Improve agent quality with Insights Agent and Multi-turn Evals, now in LangSmith",
      "url": "https://blog.langchain.com/insights-agent-multiturn-evals-langsmith/",
      "content": "LangSmith's new Insights Agent and Multi-turn Evals help you understand what your agents are doing in production and whether they're accomplishing user goals.",
      "summary": "LangSmith's new Insights Agent and Multi-turn Evals help you understand what your agents are doing in production and whether they're accomplishing user goals.",
      "publishedAt": "2025-10-23T14:23:55.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.464Z",
      "score": 1.4064272795722006
    },
    {
      "id": "1befb3ac924097b0b30e6945eeb84153",
      "title": "LangChain and LangGraph Agent Frameworks Reach v1.0 Milestones",
      "url": "https://blog.langchain.com/langchain-langgraph-1dot0/",
      "content": "<p><em>By Sydney Runkle and the LangChain OSS team </em></p><p>We&apos;re releasing LangChain 1.0 and LangGraph 1.0 &#x2014; our first major versions of our open source frameworks! After years of feedback, we&apos;ve updated <code>langchain</code> to focus on the core agent loop, provide flexibility with a new</p>",
      "summary": "By Sydney Runkle and the LangChain OSS team \nWe're releasing LangChain 1.0 and LangGraph 1.0 — our first major versions of our open source frameworks! After years of feedback, we've updated langchain to focus on the core agent loop, provide flexibility with a new",
      "publishedAt": "2025-10-22T14:58:46.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.464Z",
      "score": 1.4126405956927695
    },
    {
      "id": "19fe9e0eae5a636c469dee8e55db11bf",
      "title": "LangChain raises $125M to build the platform for agent engineering",
      "url": "https://blog.langchain.com/series-b/",
      "content": "We raised $125M at a $1.25B valuation to build the platform for agent engineering.",
      "summary": "We raised $125M at a $1.25B valuation to build the platform for agent engineering.",
      "publishedAt": "2025-10-20T14:36:50.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "funding_mna",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.464Z",
      "score": 0.611627665988324
    },
    {
      "id": "a55af658b04172752e448209b735d51d",
      "title": "Reflections on Three Years of Building LangChain",
      "url": "https://blog.langchain.com/three-years-langchain/",
      "content": "<p><em>by Harrison Chase</em></p><p>Almost exactly 3 years ago, I pushed the first lines of code to <code>langchain</code> as an open source package. There was no company at the time, and no grand plan for what the project would become.</p><p>A month later, ChatGPT launched, and everything for <code>langchain</code> changed. It</p>",
      "summary": "by Harrison Chase\nAlmost exactly 3 years ago, I pushed the first lines of code to langchain as an open source package. There was no company at the time, and no grand plan for what the project would become.\nA month later, ChatGPT launched, and everything for langchain changed. It",
      "publishedAt": "2025-10-20T14:34:32.000Z",
      "author": "Harrison Chase",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.464Z",
      "score": 1.0483849560319372
    },
    {
      "id": "fc32bf277de4fc54a4ccf0a4384ed0df",
      "title": "Securing your agents with authentication and authorization",
      "url": "https://blog.langchain.com/agent-authorization-explainer/",
      "content": "Agents can take action which makes proper authentication and authorization critical. Read on for how to implement and evolve agent auth.",
      "summary": "Agents can take action which makes proper authentication and authorization critical. Read on for how to implement and evolve agent auth.",
      "publishedAt": "2025-10-13T21:12:15.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.464Z",
      "score": 0.7025923409941569
    },
    {
      "id": "bc2967feb8e867fbd7a5ef7a99374ba1",
      "title": "Google Revisits JPEG XL in Chromium After Earlier Removal",
      "url": "https://windowsreport.com/google-revisits-jpeg-xl-in-chromium-after-earlier-removal/",
      "content": "<p><a href=\"https://lobste.rs/s/1nop48/google_revisits_jpeg_xl_chromium_after\">Comments</a></p>",
      "summary": "Comments",
      "publishedAt": "2025-11-23T13:32:45.000Z",
      "author": "windowsreport.com via polywolf",
      "source": "rss",
      "feedName": "Lobste.rs",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.470Z",
      "score": 1.975848937602987
    },
    {
      "id": "afd0cacfd3a9357d127210c183d91493",
      "title": "trifold is a tool to quickly & cheaply host static websites using a CDN",
      "url": "https://jpt.sh/projects/trifold/",
      "content": "<p>I built this to replace my Netlify* workflow for dozens of small static sites &amp; thought others might find it useful. A single config file and you can <code>trifold publish</code> to your heart's content. Unlike free options** it requires a bunny.net CDN account, but you can host as many of your static sites as you want-- $1/month for 100GB of traffic.</p>\n<p>*Why not Netlify? Their new billing terms &amp; fact that they're adding more and more complexity &amp; all these sites need is a place to host some HTML/CSS/JS.\n**Why not GitHub Pages/Cloudflare? I'd really like to help people move away from the same 2-3 companies controlling 95% of what people do online.</p>\n<p><a href=\"https://lobste.rs/s/4lidxn/trifold_is_tool_quickly_cheaply_host\">Comments</a></p>",
      "summary": "I built this to replace my Netlify* workflow for dozens of small static sites & thought others might find it useful. A single config file and you can trifold publish to your heart's content. Unlike free options** it requires a bunny.net CDN account, but you can host as many of your static sites as you want-- $1/month for 100GB of traffic.\n*Why not Netlify? Their new billing terms & fact that they're adding more and more complexity & all these sites need is a place to host some HTML/CSS/JS.\n**Why not GitHub Pages/Cloudflare? I'd really like to help people move away from the same 2-3 companies controlling 95% of what people do online.\nComments",
      "publishedAt": "2025-11-23T05:28:58.000Z",
      "author": "jpt.sh by jptsh",
      "source": "rss",
      "feedName": "Lobste.rs",
      "sourceType": "curated_ai",
      "contentType": "pricing_business",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.470Z",
      "score": 3.8579970090666706
    },
    {
      "id": "8ef609a6464f131523f808bc650e3782",
      "title": "liballocs: Meta-level run-time services for Unix processes... a.k.a. dragging Unix into the 1980s",
      "url": "https://github.com/stephenrkell/liballocs",
      "content": "<p><a href=\"https://lobste.rs/s/scikpb/liballocs_meta_level_run_time_services\">Comments</a></p>",
      "summary": "Comments",
      "publishedAt": "2025-11-23T14:03:36.000Z",
      "author": "github.com via mccd",
      "source": "rss",
      "feedName": "Lobste.rs",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.471Z",
      "score": 6.431343129502917
    },
    {
      "id": "5788a2c287fdce28a3b9be5d264a0e68",
      "title": "LLM APIs are a Synchronization Problem",
      "url": "https://lucumr.pocoo.org/2025/11/22/llm-apis/",
      "content": "<p><a href=\"https://lobste.rs/s/eqz3mh/llm_apis_are_synchronization_problem\">Comments</a></p>",
      "summary": "Comments",
      "publishedAt": "2025-11-22T15:27:29.000Z",
      "author": "lucumr.pocoo.org via jefftriplett",
      "source": "rss",
      "feedName": "Lobste.rs",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.471Z",
      "score": 4.625347230480227
    },
    {
      "id": "e11f49f3e01fc55b91ca5ed348f382da",
      "title": "The Pulse #154: Cloudflare takes down half the internet – but shares a great postmortem",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pulse-154",
      "content": "Also: why it&#8217;s not practical to build for CDN redundancy, Google launches AI IDE Antigravity externally while using Jetski internally, more AI fakers caught in remote interviews, and more",
      "summary": "Also: why it’s not practical to build for CDN redundancy, Google launches AI IDE Antigravity externally while using Jetski internally, more AI fakers caught in remote interviews, and more",
      "publishedAt": "2025-11-20T18:06:09.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.526Z",
      "score": 9.699102586797604
    },
    {
      "id": "7c91366a8af7da6991824f95020f9038",
      "title": "The Pulse #153: Is Microsoft too early to agentic OS – like with smartphones?",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pulse-153",
      "content": "Also: inside Cursor&#8217;s unique engineering culture, five AI fakers caught in one month by an employer, and more. Plus: early applications for The Pragmatic Summit",
      "summary": "Also: inside Cursor’s unique engineering culture, five AI fakers caught in one month by an employer, and more. Plus: early applications for The Pragmatic Summit",
      "publishedAt": "2025-11-13T17:53:36.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.526Z",
      "score": 4.6543208043022615
    },
    {
      "id": "87bd0f26c9d71be78e74f92c604d8303",
      "title": "Netflix’s Engineering Culture",
      "url": "https://newsletter.pragmaticengineer.com/p/netflix",
      "content": "Peek inside Netflix&#8217;s engineering culture with CTO Elizabeth Stone, as she shares how the company has no formal performance reviews, learns from failures, and builds at a global scale.",
      "summary": "Peek inside Netflix’s engineering culture with CTO Elizabeth Stone, as she shares how the company has no formal performance reviews, learns from failures, and builds at a global scale.",
      "publishedAt": "2025-11-12T17:40:07.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.526Z",
      "score": 1.3675476453000361
    },
    {
      "id": "590a2568c9ee4a6be8e8a093b3b1c8ef",
      "title": " The Software Engineer’s Guidebook: a recap",
      "url": "https://newsletter.pragmaticengineer.com/p/the-software-engineers-guidebook",
      "content": "Reflections on publishing The Software Engineer&#8217;s Guidebook two years ago, which has sold around 40,000 copies. Also: an unexpected trip to Mongolia to visit the startup which translated it",
      "summary": "Reflections on publishing The Software Engineer’s Guidebook two years ago, which has sold around 40,000 copies. Also: an unexpected trip to Mongolia to visit the startup which translated it",
      "publishedAt": "2025-11-11T17:01:48.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.526Z",
      "score": 1.2708549921731118
    },
    {
      "id": "93f3c9e802ccd853a8fd80954130605e",
      "title": "The Pulse #152: Cursor and GitHub double down on agents",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pulse-150-cursor-and-github-double",
      "content": "Also: AI-assisted interviews at Meta, pay for Directors of Engineering at VC-funded startups, and is OpenAI inflating the bubble? And more",
      "summary": "Also: AI-assisted interviews at Meta, pay for Directors of Engineering at VC-funded startups, and is OpenAI inflating the bubble? And more",
      "publishedAt": "2025-11-06T16:33:28.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "feature_update",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.526Z",
      "score": 2.6638006116130537
    },
    {
      "id": "10dae6e044780159310ed30db4eca1ff",
      "title": "From Swift to Mojo and high-performance AI Engineering with Chris Lattner",
      "url": "https://newsletter.pragmaticengineer.com/p/from-swift-to-mojo-and-high-performance",
      "content": "I sit down with Chris Lattner, creator of LLVM, Swift, and Mojo, to discuss how better language and compiler design can open the door to faster, more accessible AI development.",
      "summary": "I sit down with Chris Lattner, creator of LLVM, Swift, and Mojo, to discuss how better language and compiler design can open the door to faster, more accessible AI development.",
      "publishedAt": "2025-11-05T16:02:14.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.526Z",
      "score": 0.5502946943693696
    },
    {
      "id": "1339d7effcfe6235fd1f6c6f9ea9e613",
      "title": "The Pulse #151: Amazon layoffs – AI or economy to blame?",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pulse-151",
      "content": "Also: OpenAI becomes for-profit and takes on Chrome, NVIDIA the biggest Big Tech by a distance, Citibank annoys premium customers for weeks with disastrous product rollout, and more",
      "summary": "Also: OpenAI becomes for-profit and takes on Chrome, NVIDIA the biggest Big Tech by a distance, Citibank annoys premium customers for weeks with disastrous product rollout, and more",
      "publishedAt": "2025-10-30T17:46:40.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.526Z",
      "score": 1.2612085507099227
    },
    {
      "id": "874ddab755ae35e28936a03eaba1a6bd",
      "title": "Beyond Vibe Coding with Addy Osmani",
      "url": "https://newsletter.pragmaticengineer.com/p/beyond-vibe-coding-with-addy-osmani",
      "content": "Google&#8217;s Head of Chrome Developer Experience, Addy Osmani, shares how AI is transforming the way we code&#8212;accelerating development while still relying on human expertise to ensure real quality.",
      "summary": "Google’s Head of Chrome Developer Experience, Addy Osmani, shares how AI is transforming the way we code—accelerating development while still relying on human expertise to ensure real quality.",
      "publishedAt": "2025-10-29T16:36:24.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.526Z",
      "score": 0.3343367497411215
    },
    {
      "id": "570302873daedee8879e05c12c09de52",
      "title": "San Francisco is back as the world’s leading tech hub",
      "url": "https://newsletter.pragmaticengineer.com/p/san-francisco-is-back",
      "content": "Impressions from a week in San Fran spent visiting engineering teams at Cursor, OpenAI, Anthropic, Wispr, Factory, and more",
      "summary": "Impressions from a week in San Fran spent visiting engineering teams at Cursor, OpenAI, Anthropic, Wispr, Factory, and more",
      "publishedAt": "2025-10-28T16:55:50.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.526Z",
      "score": 0.7789717909035981
    },
    {
      "id": "2ecaf12b8ab9462690b7c606a5303537",
      "title": "The Pulse: AWS takes down a good part of the internet",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pulse-aws-takes-down-a-good-part",
      "content": "On Monday, a major AWS outage hit thousands of sites & apps, and even a Premier League soccer game. An overview of what caused this high-profile, global outage, and learnings from the incident",
      "summary": "On Monday, a major AWS outage hit thousands of sites & apps, and even a Premier League soccer game. An overview of what caused this high-profile, global outage, and learnings from the incident",
      "publishedAt": "2025-10-23T16:17:31.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.526Z",
      "score": 0.9791824734322002
    },
    {
      "id": "780358dc8ab798152201c875d66bc61d",
      "title": "Google’s engineering culture",
      "url": "https://newsletter.pragmaticengineer.com/p/googles-engineering-culture",
      "content": "Elin Nilsson and I unpack how Google&#8217;s engineering culture, tools, and systems actually work behind the scenes.",
      "summary": "Elin Nilsson and I unpack how Google’s engineering culture, tools, and systems actually work behind the scenes.",
      "publishedAt": "2025-10-15T20:32:35.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.527Z",
      "score": 0.12444503843802768
    },
    {
      "id": "ef14ac127f1c54317b68179164631d04",
      "title": "The Pragmatic Engineer 2025 Survey: What’s in your tech stack? Part 3",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pragmatic-engineer-2025-survey-part-3",
      "content": "Which tools do software engineers use for observability, oncall tooling, feature flags, frontend & mobile work, and for developer tooling? Results from our survey, based on 3,000+ responses by readers",
      "summary": "Which tools do software engineers use for observability, oncall tooling, feature flags, frontend & mobile work, and for developer tooling? Results from our survey, based on 3,000+ responses by readers",
      "publishedAt": "2025-10-14T14:59:35.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "observability"
      ],
      "ingestedAt": "2025-11-23T17:37:40.527Z",
      "score": 0.4273800103839772
    },
    {
      "id": "cf3872e75a8ac092c287d2fd0171adfc",
      "title": "The Pulse #149: New trend: programming by kicking off parallel AI agents",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pulse-149-new-trend-programming",
      "content": "Also: the ACP protocol, AI security tooling, comparing interview experiences across 8 tech companies, and more",
      "summary": "Also: the ACP protocol, AI security tooling, comparing interview experiences across 8 tech companies, and more",
      "publishedAt": "2025-10-09T16:31:16.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.527Z",
      "score": 0.5607262440134866
    },
    {
      "id": "77d7fb344400a0cbaa0538cbbeb2699e",
      "title": "The Pulse #148: Did OpenAI set a new record for Datadog spend?",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pulse-148",
      "content": "A reported $170M per year is almost triple Coinbase&#8217;s previous record. Also: the largest British automaker learns that outsourcing cybersecurity was a terrible decision, and more",
      "summary": "A reported $170M per year is almost triple Coinbase’s previous record. Also: the largest British automaker learns that outsourcing cybersecurity was a terrible decision, and more",
      "publishedAt": "2025-10-02T16:11:02.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "security_incident",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.527Z",
      "score": 0.24268321081727018
    },
    {
      "id": "34aa977c7bb031d2cacce9967744f71b",
      "title": "Tosijs-schema is a super lightweight schema-first LLM-native JSON schema library",
      "url": "https://www.npmjs.com/package/tosijs-schema",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46023531\">Comments</a>",
      "summary": "Comments",
      "publishedAt": "2025-11-23T13:43:24.000Z",
      "source": "rss",
      "feedName": "Hacker News",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.569Z",
      "score": 1.9768928408008422
    },
    {
      "id": "ea952e15c5d4e0bfac1411f0baf4adb7",
      "title": "Typechecking is undecideable when 'type' is a type (1989) [pdf]",
      "url": "https://dspace.mit.edu/bitstream/handle/1721.1/149366/MIT-LCS-TR-458.pdf",
      "content": "<a href=\"https://news.ycombinator.com/item?id=45963768\">Comments</a>",
      "summary": "Comments",
      "publishedAt": "2025-11-18T11:33:39.000Z",
      "source": "rss",
      "feedName": "Hacker News",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.569Z",
      "score": 2.0614561022305495
    },
    {
      "id": "3c38085b4e4b766bc2fe9c650445be19",
      "title": "UK minister ducks cost questions on nationwide digital ID scheme",
      "url": "https://www.theregister.com/2025/11/21/uk_digital_id_costs_uncertain/",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46024062\">Comments</a>",
      "summary": "Comments",
      "publishedAt": "2025-11-23T15:02:39.000Z",
      "source": "rss",
      "feedName": "Hacker News",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.569Z",
      "score": 2.977019104803886
    },
    {
      "id": "a8f0ae4cd692e1c96c4dd33bb2464a10",
      "title": "Meta buried 'causal' evidence of social media harm, US court filings allege",
      "url": "https://www.reuters.com/sustainability/boards-policy-regulation/meta-buried-causal-evidence-social-media-harm-us-court-filings-allege-2025-11-23/",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46019817\">Comments</a>",
      "summary": "Comments",
      "publishedAt": "2025-11-23T01:09:47.000Z",
      "source": "rss",
      "feedName": "Hacker News",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.569Z",
      "score": 2.8565359053205954
    },
    {
      "id": "1e802ce4ceb9c901e8602ad55232d70d",
      "title": "Bringing RAG to Life with Dify and Weaviate",
      "url": "https://weaviate.io/blog/dify-and-weaviate",
      "content": "Learn how to use the Dify and Weaviate integration to build RAG applications.",
      "summary": "Learn how to use the Dify and Weaviate integration to build RAG applications.",
      "publishedAt": "2025-11-20T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "thought_leadership",
      "tags": [
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 4.978120038442896
    },
    {
      "id": "e3af55eae5673c9951805a293075367c",
      "title": "Weaviate 1.34 Release",
      "url": "https://weaviate.io/blog/weaviate-1-34-release",
      "content": "1.34 introduces flat index support with RQ quantization, server-side batching improvements, new client libraries, Contextual AI integration and much more.",
      "summary": "1.34 introduces flat index support with RQ quantization, server-side batching improvements, new client libraries, Contextual AI integration and much more.",
      "publishedAt": "2025-11-11T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 4.429506908313854
    },
    {
      "id": "f62804b712d1ada5001e8a9a1f688921",
      "title": "Weaviate security release - Medium and High severity fixes for CVEs",
      "url": "https://weaviate.io/blog/weaviate-security-release-november-2025",
      "content": "Weaviate announces two CVEs that are fixed in updated versions of our product.",
      "summary": "Weaviate announces two CVEs that are fixed in updated versions of our product.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 3.3286738611179736
    },
    {
      "id": "ec6048630c0893f3c102e80046236cb0",
      "title": "From Kitchen Experiments to Five Star Service: The Weaviate Development Journey",
      "url": "https://weaviate.io/blog/day0-day1-day2-operations",
      "content": "What building AI apps with Weaviate and cooking have in common? Let’s find out!",
      "summary": "What building AI apps with Weaviate and cooking have in common? Let’s find out!",
      "publishedAt": "2025-11-06T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "general",
      "tags": [
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 1.126983395585681
    },
    {
      "id": "c0b8796e8d8ada550a22d3b93c07d6a9",
      "title": "Evals and Guardrails in Enterprise workflows (Part 3)",
      "url": "https://weaviate.io/blog/evals-enterprise-workflows-3",
      "content": "Hands-on patterns: Design pattern for gen-AI enterprise applications, with Arize AI.",
      "summary": "Hands-on patterns: Design pattern for gen-AI enterprise applications, with Arize AI.",
      "publishedAt": "2025-11-04T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "pricing_business",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 2.3202728726692774
    },
    {
      "id": "e2aa7c1c97c1a9a707d2755b8a58267e",
      "title": "Unleash Real-Time Agentic AI with Streaming Agents on Confluent Cloud and Weaviate",
      "url": "https://weaviate.io/blog/confluent-streaming-agents-and-weaviate",
      "content": "Learn how Confluent’s Streaming Agents and Weaviate combine real-time context with semantic understanding for agentic AI.",
      "summary": "Learn how Confluent’s Streaming Agents and Weaviate combine real-time context with semantic understanding for agentic AI.",
      "publishedAt": "2025-10-30T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "feature_update",
      "tags": [
        "retrieval",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 1.8797624516268585
    },
    {
      "id": "cfad02c62500a1043203f07f108506a9",
      "title": "A Simpler, More Transparent Pricing Model for Weaviate Cloud",
      "url": "https://weaviate.io/blog/weaviate-cloud-pricing-update",
      "content": "Weaviate Cloud gets an updated pricing model.",
      "summary": "Weaviate Cloud gets an updated pricing model.",
      "publishedAt": "2025-10-28T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 1.6295245260955438
    },
    {
      "id": "3536d381c72630d4f8c31c8aad40e45d",
      "title": "Legacy data to RAG : Modernise Your Apps with Amazon Sagemaker Unified Studio",
      "url": "https://weaviate.io/blog/sagemaker-studio-rag",
      "content": "A guide to seamlessly transform data sitting in lakes and warehouses for GenAI capable applications",
      "summary": "A guide to seamlessly transform data sitting in lakes and warehouses for GenAI capable applications",
      "publishedAt": "2025-10-16T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "thought_leadership",
      "tags": [
        "retrieval",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 0.282896983734361
    },
    {
      "id": "0bd8d8f5fb92164de163cd1ef54145a5",
      "title": "When Good Models Go Bad",
      "url": "https://weaviate.io/blog/when-good-models-go-bad",
      "content": "A strategic guide to the costs, risks and rewards of upgrading embedding models in production AI",
      "summary": "A strategic guide to the costs, risks and rewards of upgrading embedding models in production AI",
      "publishedAt": "2025-10-09T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "thought_leadership",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 0.28597615695852685
    },
    {
      "id": "b26470af99b2cfdccef4f8a42f313045",
      "title": "Rethinking Vector Search at Scale: Weaviate's Native, Efficient and Optimized Multi-Tenancy",
      "url": "https://weaviate.io/blog/weaviate-multi-tenancy-architecture-explained",
      "content": "Learn how Weaviate's native multi-tenancy architecture delivers scalable vector search with one shard per tenant, dynamic resource management, and true data isolation.",
      "summary": "Learn how Weaviate's native multi-tenancy architecture delivers scalable vector search with one shard per tenant, dynamic resource management, and true data isolation.",
      "publishedAt": "2025-10-08T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "general",
      "tags": [
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 0.14200626966766927
    },
    {
      "id": "b1cb267d9e67667504f6fa4dcfb7cb00",
      "title": "Weaviate 1.33 Release",
      "url": "https://weaviate.io/blog/weaviate-1-33-release",
      "content": "1.33 brings compression by default for optimal resource utilization, powerful 1-bit rotational quantization (RQ), streamlined server-side batch imports, enhanced OIDC group management, and collection aliases become generally available (GA).",
      "summary": "1.33 brings compression by default for optimal resource utilization, powerful 1-bit rotational quantization (RQ), streamlined server-side batch imports, enhanced OIDC group management, and collection aliases become generally available (GA).",
      "publishedAt": "2025-10-02T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 0.2543981838084704
    },
    {
      "id": "fd8d89053b485b841a4f2344ee9ca19f",
      "title": "Building Multi-Agent Systems with Crew AI and Weaviate",
      "url": "https://weaviate.io/blog/building-multi-agent-systems",
      "content": "Learn about how you can build multi-agent systems with CrewAI and Weaviate.",
      "summary": "Learn about how you can build multi-agent systems with CrewAI and Weaviate.",
      "publishedAt": "2025-10-01T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "general",
      "tags": [
        "retrieval",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 0.12919673463729234
    },
    {
      "id": "98bed1c66bf5cc653b5f2c21d57d109d",
      "title": "Evals and Guardrails in Enterprise workflows (Part 2)",
      "url": "https://weaviate.io/blog/evals-guardrails-enterprise-workflows-2",
      "content": "Hands-on patterns: LLM-as-Judge with LangChain and W&B",
      "summary": "Hands-on patterns: LLM-as-Judge with LangChain and W&B",
      "publishedAt": "2025-09-25T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "pricing_business",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 0.1332593484929954
    },
    {
      "id": "09124efbf8ea2687f970f5989ec2f3a0",
      "title": "Weaviate is now ISO 27001 compliant",
      "url": "https://weaviate.io/blog/weaviate-iso-compliant",
      "content": "Announcing Weaviate has achieved ISO 27001 compliance.",
      "summary": "Announcing Weaviate has achieved ISO 27001 compliance.",
      "publishedAt": "2025-09-24T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "product_launch",
      "tags": [
        "retrieval",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 0.18284415494864167
    },
    {
      "id": "cd24f6d402a3a5e6660f389e5ef9b0fc",
      "title": "Search Mode Benchmarking",
      "url": "https://weaviate.io/blog/search-mode-benchmarking",
      "content": "Learn how Search Mode compares against Hybrid Search on the BEIR, LoTTe, BRIGHT, EnronQA, and WixQA Information Retrieval benchmarks.",
      "summary": "Learn how Search Mode compares against Hybrid Search on the BEIR, LoTTe, BRIGHT, EnronQA, and WixQA Information Retrieval benchmarks.",
      "publishedAt": "2025-09-23T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "benchmark_eval",
      "tags": [
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 0.060799781128255445
    },
    {
      "id": "c570821a2ef4b1316081a2e8ae555569",
      "title": "Accelerating Data Workflows with Query Agent, now GA",
      "url": "https://weaviate.io/blog/query-agent-generally-available",
      "content": "The Query Agent is now generally available! Learn how the interface to databases is shifting with the introduction of agentic retrievers – domain experts at using Weaviate’s APIs with your data.",
      "summary": "The Query Agent is now generally available! Learn how the interface to databases is shifting with the introduction of agentic retrievers – domain experts at using Weaviate’s APIs with your data.",
      "publishedAt": "2025-09-17T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "product_launch",
      "tags": [
        "retrieval",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.630Z",
      "score": 0.09505764507908451
    },
    {
      "id": "e3cd2e2d17440071f3f948dce4683447",
      "title": "Using Weaviate Cloud Queries in MacOS apps",
      "url": "https://weaviate.io/blog/apple-and-weaviate-2",
      "content": "A practical guide on using Weaviate Cloud Queries in MacOS apps.",
      "summary": "A practical guide on using Weaviate Cloud Queries in MacOS apps.",
      "publishedAt": "2025-09-09T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.631Z",
      "score": 0.03578718317810787
    },
    {
      "id": "8641bd302b97d2c4d7dac6ac0d3c28c3",
      "title": "Chunking Strategies to Improve Your RAG Performance",
      "url": "https://weaviate.io/blog/chunking-strategies-for-rag",
      "content": "Learn how chunking strategies can help improve your RAG performance and explore different chunking methods.",
      "summary": "Learn how chunking strategies can help improve your RAG performance and explore different chunking methods.",
      "publishedAt": "2025-09-04T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.631Z",
      "score": 0.02190939560226839
    },
    {
      "id": "88e329b99cffee5687f8b339a9c0d130",
      "title": "8-bit Rotational Quantization: How to Compress Vectors by 4x and Improve the Speed-Quality Tradeoff of Vector Search",
      "url": "https://weaviate.io/blog/8-bit-rotational-quantization",
      "content": "Get spun around by our new vector quantization algorithm that utilizes the power of random rotations to improve the speed-quality tradeoff of vector search with Weaviate.",
      "summary": "Get spun around by our new vector quantization algorithm that utilizes the power of random rotations to improve the speed-quality tradeoff of vector search with Weaviate.",
      "publishedAt": "2025-08-26T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "thought_leadership",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.631Z",
      "score": 0.010696862270788171
    },
    {
      "id": "c3339fa9cd8367ba941e9a859d968b01",
      "title": "AI Governance Checklist for CTOs, CIOs, and AI Teams: A Complete Blueprint for 2025",
      "url": "https://datasciencedojo.com/blog/ai-governance-checklist-for-2025/",
      "content": "Artificial intelligence is no longer experimental infrastructure. It is core business infrastructure. The same way organizations matured cybersecurity, cloud strategy, and data governance over decades, AI now requires its own institutional backbone. This backbone is AI governance—a collection of controls, oversight mechanisms, accountability structures, and risk management protocols that ensures AI systems do not just [&#8230;]",
      "summary": "Artificial intelligence is no longer experimental infrastructure. It is core business infrastructure. The same way organizations matured cybersecurity, cloud strategy, and data governance over decades, AI now requires its own institutional backbone. This backbone is AI governance—a collection of controls, oversight mechanisms, accountability structures, and risk management protocols that ensures AI systems do not just […]",
      "publishedAt": "2025-11-17T15:21:55.000Z",
      "author": "Data Science Dojo Staff",
      "source": "rss",
      "feedName": "DataScienceDojo",
      "sourceType": "engineering_blog",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "governance"
      ],
      "ingestedAt": "2025-11-23T17:37:40.634Z",
      "score": 10.35307046060211
    },
    {
      "id": "c733a92737f65b3966769d3717808f56",
      "title": "Agentic LLMs in 2025: How AI Is Becoming Self-Directed, Tool-Using & Autonomous",
      "url": "https://datasciencedojo.com/blog/agentic-llm-in-2025/",
      "content": "For much of the last decade, AI language models have been defined by a simple paradigm: input comes in, text comes out. Users ask questions, models answer. Users request summaries, models comply. That architecture created one of the fastest-adopted technologies in history — but it also created a ceiling. Something fundamentally new is happening now. [&#8230;]",
      "summary": "For much of the last decade, AI language models have been defined by a simple paradigm: input comes in, text comes out. Users ask questions, models answer. Users request summaries, models comply. That architecture created one of the fastest-adopted technologies in history — but it also created a ceiling. Something fundamentally new is happening now. […]",
      "publishedAt": "2025-11-11T14:53:55.000Z",
      "author": "Data Science Dojo Staff",
      "source": "rss",
      "feedName": "DataScienceDojo",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.634Z",
      "score": 2.525637656577862
    },
    {
      "id": "c2b658358bde7873f1df4947a5231adb",
      "title": "REFRAG: Meta’s Breakthrough in Retrieval-Augmented Generation Efficiency",
      "url": "https://datasciencedojo.com/blog/refrag-metas-breakthrough-in-rag/",
      "content": "Refrag is the latest innovation from Meta Superintelligence Labs, designed to supercharge retrieval-augmented generation (RAG) systems. As large language models (LLMs) become central to enterprise AI, the challenge of efficiently processing long-context inputs—especially those packed with retrieved knowledge has grown significantly. Refrag tackles this problem head-on. It introduces a new way to represent, compress, and [&#8230;]",
      "summary": "Refrag is the latest innovation from Meta Superintelligence Labs, designed to supercharge retrieval-augmented generation (RAG) systems. As large language models (LLMs) become central to enterprise AI, the challenge of efficiently processing long-context inputs—especially those packed with retrieved knowledge has grown significantly. Refrag tackles this problem head-on. It introduces a new way to represent, compress, and […]",
      "publishedAt": "2025-11-03T17:50:56.000Z",
      "author": "Data Science Dojo Staff",
      "source": "rss",
      "feedName": "DataScienceDojo",
      "sourceType": "engineering_blog",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.634Z",
      "score": 2.997608368958195
    },
    {
      "id": "4531e98165f0413e68a18a8b3d8da1f0",
      "title": "The Real Future of AI Belongs to Communities, Not Companies: Why Every Data & AI Enthusiast Should Join a Community AI Discord Server",
      "url": "https://datasciencedojo.com/blog/why-everyone-needs-an-ai-discord-server/",
      "content": "“The next frontier of AI won’t be built in boardrooms — it will be built in chat threads.”   An AI Discord server, the phrase might sound like a niche keyword, but it’s fast becoming the gateway to the next era of artificial intelligence. As AI transforms every industry, from healthcare to finance, the pace of [&#8230;]",
      "summary": "“The next frontier of AI won’t be built in boardrooms — it will be built in chat threads.”   An AI Discord server, the phrase might sound like a niche keyword, but it’s fast becoming the gateway to the next era of artificial intelligence. As AI transforms every industry, from healthcare to finance, the pace of […]",
      "publishedAt": "2025-10-23T14:57:56.000Z",
      "author": "Data Science Dojo Staff",
      "source": "rss",
      "feedName": "DataScienceDojo",
      "sourceType": "engineering_blog",
      "contentType": "thought_leadership",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.634Z",
      "score": 0.3792928995452166
    },
    {
      "id": "ec3a40c26d9f5b70f8e057fcdace79ac",
      "title": "Effortless Integration: Connecting LinkedIn API to Azure Synapse / Data Factory for Centralized Marketing Analytics in Power BI",
      "url": "https://datasciencedojo.com/blog/connecting-linkedin-api-to-azure-synapse/",
      "content": "Organizations increasingly rely on Linkedin to build brand presence, run campaigns, and engage with their professional community. While LinkedIn provides native dashboards, most companies want to bring Linkedin data into their data warehouse, such as Azure Synapse, for unified analytics alongside CRM, financial, and other marketing data.  This guide shows you how to:  Get Linkedin Community [&#8230;]",
      "summary": "Organizations increasingly rely on Linkedin to build brand presence, run campaigns, and engage with their professional community. While LinkedIn provides native dashboards, most companies want to bring Linkedin data into their data warehouse, such as Azure Synapse, for unified analytics alongside CRM, financial, and other marketing data.  This guide shows you how to:  Get Linkedin Community […]",
      "publishedAt": "2025-10-22T18:05:02.000Z",
      "author": "Syed Umair Hasan",
      "source": "rss",
      "feedName": "DataScienceDojo",
      "sourceType": "engineering_blog",
      "contentType": "thought_leadership",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.634Z",
      "score": 1.069314641034405
    },
    {
      "id": "0c2e38cab4f74c16b56779d8bc452595",
      "title": "The Definitive Guide to Model Context Protocol (MCP) in 2025",
      "url": "https://datasciencedojo.com/blog/guide-to-model-context-protocol/",
      "content": "Understanding the AI Integration Problem  When ChatGPT launched on November 30, 2022, it took just five days to reach one million users and two months to hit 100 million. This wasn&#8217;t just another software launch. It marked the beginning of a fundamental shift in how we work with technology.  Since then, we&#8217;ve witnessed three distinct [&#8230;]",
      "summary": "Understanding the AI Integration Problem  When ChatGPT launched on November 30, 2022, it took just five days to reach one million users and two months to hit 100 million. This wasn’t just another software launch. It marked the beginning of a fundamental shift in how we work with technology.  Since then, we’ve witnessed three distinct […]",
      "publishedAt": "2025-10-20T19:26:58.000Z",
      "author": "Muneeb Alam",
      "source": "rss",
      "feedName": "DataScienceDojo",
      "sourceType": "engineering_blog",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.634Z",
      "score": 1.4182708190526252
    },
    {
      "id": "18ed8cb48ecabc1a849780df2faf665c",
      "title": "Supercharge Data Science in Microsoft Fabric: Train and Deploy Predictive Models",
      "url": "https://datasciencedojo.com/blog/data-science-in-microsoft-fabric/",
      "content": "In today’s data-driven era, organizations expect more than static dashboards or descriptive analytics. They demand forecasts, predictive insights, and intelligent decision-making support. Traditionally, delivering this requires piecing together multiple tools, data lakes for storage, notebooks for model training, separate platforms for deployment, and BI tools for visualization.  Microsoft Fabric reimagines this workflow. It brings every [&#8230;]",
      "summary": "In today’s data-driven era, organizations expect more than static dashboards or descriptive analytics. They demand forecasts, predictive insights, and intelligent decision-making support. Traditionally, delivering this requires piecing together multiple tools, data lakes for storage, notebooks for model training, separate platforms for deployment, and BI tools for visualization.  Microsoft Fabric reimagines this workflow. It brings every […]",
      "publishedAt": "2025-10-17T14:51:23.000Z",
      "author": "Rimsha Ishtiaq",
      "source": "rss",
      "feedName": "DataScienceDojo",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.634Z",
      "score": 0.599871577174432
    },
    {
      "id": "672d249f7dc9323c43dabc59e5864541",
      "title": "Mastering Google Ads Integration with Azure Synapse / Data Factory: A Practical Step-by-Step Tutorial",
      "url": "https://datasciencedojo.com/blog/google-ads-in-azure-data-factory/",
      "content": "Modern marketing teams need to track digital ad performance quickly and accurately. If you are running Google Ads campaigns, integrating your data into Azure Synapse / Data Factory ensures you can analyze impressions, clicks, conversions, and spend alongside your other enterprise datasets.  In this step-by-step guide, we’ll walk you through:  Setting up a Google Ads [&#8230;]",
      "summary": "Modern marketing teams need to track digital ad performance quickly and accurately. If you are running Google Ads campaigns, integrating your data into Azure Synapse / Data Factory ensures you can analyze impressions, clicks, conversions, and spend alongside your other enterprise datasets.  In this step-by-step guide, we’ll walk you through:  Setting up a Google Ads […]",
      "publishedAt": "2025-10-07T15:49:26.000Z",
      "author": "Syed Umair Hasan",
      "source": "rss",
      "feedName": "DataScienceDojo",
      "sourceType": "engineering_blog",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.634Z",
      "score": 0.5197216510779765
    },
    {
      "id": "47d4ec1b3ef596b22c1eff64a8cb75ed",
      "title": "Automation Reimagined: Start Building Smarter Workflows with AI Agents",
      "url": "https://datasciencedojo.com/blog/automation-and-ai-agents/",
      "content": "Think about how much time we spend on small, repetitive tasks every day. Answering emails, updating spreadsheets, saving files, or copying data from one app to another. Each task might take only a few minutes, but together they consume hours of our week.  Now imagine if these tasks could take care of themselves. Imagine if [&#8230;]",
      "summary": "Think about how much time we spend on small, repetitive tasks every day. Answering emails, updating spreadsheets, saving files, or copying data from one app to another. Each task might take only a few minutes, but together they consume hours of our week.  Now imagine if these tasks could take care of themselves. Imagine if […]",
      "publishedAt": "2025-10-03T17:38:30.000Z",
      "author": "Muneeb Alam",
      "source": "rss",
      "feedName": "DataScienceDojo",
      "sourceType": "engineering_blog",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.634Z",
      "score": 0.3664991776673432
    },
    {
      "id": "2976b7d8fac578c0d26e20969b177c2c",
      "title": "The State of MCP Security in 2025: Key Risks, Attack Vectors, and Case Studies",
      "url": "https://datasciencedojo.com/blog/mcp-security-risks-and-challenges/",
      "content": "The Model Context Protocol (MCP) is rapidly becoming the “USB-C for AI applications,” enabling large language models (LLMs) and agentic AI systems to interact with external tools, databases, and APIs through a standardized interface. MCP’s promise is seamless integration and operational efficiency, but this convenience introduces a new wave of MCP security risks that traditional [&#8230;]",
      "summary": "The Model Context Protocol (MCP) is rapidly becoming the “USB-C for AI applications,” enabling large language models (LLMs) and agentic AI systems to interact with external tools, databases, and APIs through a standardized interface. MCP’s promise is seamless integration and operational efficiency, but this convenience introduces a new wave of MCP security risks that traditional […]",
      "publishedAt": "2025-09-17T14:39:38.000Z",
      "author": "Data Science Dojo Staff",
      "source": "rss",
      "feedName": "DataScienceDojo",
      "sourceType": "engineering_blog",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.634Z",
      "score": 0.14480823234380247
    },
    {
      "id": "5abd0db3b7997f68293e4f56a2d98561",
      "title": "The Agent Labs Thesis",
      "url": "https://www.latent.space/p/agent-labs",
      "content": "How great Agent Engineering and Research are combining in a new playbook for building high growth AI startups that doesn't involve training a SOTA LLM.",
      "summary": "How great Agent Engineering and Research are combining in a new playbook for building high growth AI startups that doesn't involve training a SOTA LLM.",
      "publishedAt": "2025-11-18T02:41:17.000Z",
      "author": "swyx (Shawn)",
      "source": "rss",
      "feedName": "Latent Space",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.859Z",
      "score": 4.015462135186024
    },
    {
      "id": "4bdfa6d2ea07ffd2b51039b7ad325d5b",
      "title": "Biohub for Non-Biologists: Behind Priscilla Chan and Mark Zuckerberg's plan to cure all diseases",
      "url": "https://www.latent.space/p/biohub",
      "content": "The CZI has acquired EvoScale, established the first 10,000 GPU cluster for bio research, open sourced the largest atlas of human cell types, and gone all in on AI x Bio for its 2nd decade.",
      "summary": "The CZI has acquired EvoScale, established the first 10,000 GPU cluster for bio research, open sourced the largest atlas of human cell types, and gone all in on AI x Bio for its 2nd decade.",
      "publishedAt": "2025-11-13T19:46:41.000Z",
      "author": "swyx (Shawn)",
      "source": "rss",
      "feedName": "Latent Space",
      "sourceType": "engineering_blog",
      "contentType": "funding_mna",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.859Z",
      "score": 4.434158198841992
    },
    {
      "id": "74d0198fd6d6635be653afed2fff184f",
      "title": "Agentic Commerce Protocol and building the Economic Infrastructure for AI — with Emily Glassberg Sands, Head of Data & AI at Stripe",
      "url": "https://www.latent.space/p/stripe",
      "content": "How Stripe built a payments foundation model, why stablecoins are powering more of the AI economy, and growing internal AI adoption to 8,500 employees daily.",
      "summary": "How Stripe built a payments foundation model, why stablecoins are powering more of the AI economy, and growing internal AI adoption to 8,500 employees daily.",
      "publishedAt": "2025-10-30T22:30:27.000Z",
      "source": "rss",
      "feedName": "Latent Space",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.859Z",
      "score": 1.6445407003256567
    },
    {
      "id": "551f4704dce7d3e20cf00e7355f2ec4e",
      "title": "Developers as the distribution layer of AGI (OpenAI Dev Day 2025, ft. Sherwin Wu and Christina Huang)",
      "url": "https://www.latent.space/p/devday-2025",
      "content": "A quick recap of the 2025 OpenAI DevDay coupled with an exclusive interview with the OpenAI Platform team that shipped AgentKit and related products!",
      "summary": "A quick recap of the 2025 OpenAI DevDay coupled with an exclusive interview with the OpenAI Platform team that shipped AgentKit and related products!",
      "publishedAt": "2025-10-07T17:50:03.000Z",
      "source": "rss",
      "feedName": "Latent Space",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.859Z",
      "score": 0.31370421191262127
    },
    {
      "id": "cdfdf4ebd03e2db20eb3efeb1b34257a",
      "title": "Taste is your moat — with Dylan Field, Figma",
      "url": "https://www.latent.space/p/figma",
      "content": "Letting designers build with Figma Make, how Figma can be the context repository for aesthetic in the age of vibe coding, and why design is your only differentiator now",
      "summary": "Letting designers build with Figma Make, how Figma can be the context repository for aesthetic in the age of vibe coding, and why design is your only differentiator now",
      "publishedAt": "2025-10-02T16:40:18.000Z",
      "source": "rss",
      "feedName": "Latent Space",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.859Z",
      "score": 0.0972142831716384
    },
    {
      "id": "cd01d183003fe4945d495249425bc9ce",
      "title": "How GPT5 + Codex took over Agentic Coding — ft. Greg Brockman, OpenAI",
      "url": "https://www.latent.space/p/gpt5-codex",
      "content": "Belated catchup on our podcast with Greg Brockman, + latest takes on the new GPT-5-Codex model",
      "summary": "Belated catchup on our podcast with Greg Brockman, + latest takes on the new GPT-5-Codex model",
      "publishedAt": "2025-09-16T00:16:43.000Z",
      "source": "rss",
      "feedName": "Latent Space",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.859Z",
      "score": 0.0442890184725907
    },
    {
      "id": "c856888e31b329ab17148c68feef099b",
      "title": "A Technical History of Generative Media — with Gorkem and Batuhan from Fal.ai",
      "url": "https://www.latent.space/p/fal",
      "content": "From Stable Diffusion to Veo3, why generative media is completely different than LLM inference, and how to scale to $100M ARR while writing custom kernels",
      "summary": "From Stable Diffusion to Veo3, why generative media is completely different than LLM inference, and how to scale to $100M ARR while writing custom kernels",
      "publishedAt": "2025-09-05T21:46:47.000Z",
      "source": "rss",
      "feedName": "Latent Space",
      "sourceType": "engineering_blog",
      "contentType": "thought_leadership",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.859Z",
      "score": 0.012553724217838508
    },
    {
      "id": "7e20c863771e02937748a52078cc73f6",
      "title": "Anthropic Is on Track to Turn a Profit Much Faster Than OpenAI (7 minute read)",
      "url": "https://www.wsj.com/tech/ai/openai-anthropic-profitability-e9f5bcd6?st=2AqtKX&reflink=desktopwebshare_permalink&mod=tldr&utm_source=tldrnewsletter",
      "content": "Anthropic expects to break even for the first time in 2028. OpenAI's operating losses for 2028 are forecasted to swell to about $74 billion - it doesn't expect to turn a profit until 2030. OpenAI is investing far more into its chips and data centers to turn OpenAI into a multitrillion-dollar tech giant. The strategy requires near-constant fundraising, which could backfire if markets cool on the technology or its near-term profitability.",
      "summary": "Anthropic expects to break even for the first time in 2028. OpenAI's operating losses for 2028 are forecasted to swell to about $74 billion - it doesn't expect to turn a profit until 2030. OpenAI is investing far more into its chips and data centers to turn OpenAI into a multitrillion-dollar tech giant. The strategy requires near-constant fundraising, which could backfire if markets cool on the technology or its near-term profitability.",
      "publishedAt": "2025-11-11T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 2.0134117333581485
    },
    {
      "id": "eb949fe5364740115402a6d34b7af8a6",
      "title": "Altman And Masa Back a 27-Year-Old's Plan to Build a New Bell Labs Ultra (13 minute read)",
      "url": "https://www.corememory.com/p/exclusive-altman-and-masa-back-episteme-louis-andre?utm_source=tldrnewsletter",
      "content": "Louis Andre, a 27-year-old who grew up in Europe, recently revealed a new company backed by Sam Altman called Episteme. Episteme is an effort to attract the world's top scientists and have them work on a wide range of breakthrough products. It will allow scientists to do away with fundraising pressures and grant writing and spend most of their time on research. The company will also help scientists deal with intellectual property concerns, tax issues, hiring, and other day-to-day support functions to help them turn their ideas into blockbuster products.",
      "summary": "Louis Andre, a 27-year-old who grew up in Europe, recently revealed a new company backed by Sam Altman called Episteme. Episteme is an effort to attract the world's top scientists and have them work on a wide range of breakthrough products. It will allow scientists to do away with fundraising pressures and grant writing and spend most of their time on research. The company will also help scientists deal with intellectual property concerns, tax issues, hiring, and other day-to-day support functions to help them turn their ideas into blockbuster products.",
      "publishedAt": "2025-11-11T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 2.416094080029778
    },
    {
      "id": "639f57b9dc98fb1968749b33de521f9e",
      "title": "git-rewrite-commits (GitHub Repo)",
      "url": "https://github.com/f/git-rewrite-commits?utm_source=tldrnewsletter",
      "content": "git-rewrite-commits is an AI-powered git commit message writer. It can automatically rewrite entire git commit histories with AI. The tool is perfect for cleaning up commit histories before open-sourcing projects. It can also aid in improving repository maintainability.",
      "summary": "git-rewrite-commits is an AI-powered git commit message writer. It can automatically rewrite entire git commit histories with AI. The tool is perfect for cleaning up commit histories before open-sourcing projects. It can also aid in improving repository maintainability.",
      "publishedAt": "2025-11-11T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 2.0134117333581485
    },
    {
      "id": "1a3e1ac48c59039b055238f896cf6947",
      "title": "Meta Ray-Ban Display Review: First Generation Heads-Up Mobile Computing (42 minute read)",
      "url": "https://www.uploadvr.com/meta-ray-ban-display-review/?utm_source=tldrnewsletter",
      "content": "Smartphones have become humanity's second cognitive organ, but using the devices harshly disconnects users from the world around them. The only form factor that seems to have a chance to replace the smartphone is AR glasses. Meta Ray-Ban Display glasses are still highly dependent on mobile devices, and they only provide a small display visible to one eye, but they give users a glimpse into what the future may look like. This article provides a detailed review of the device along with the Meta Neural Band that pairs with it. The technology is still very much a first-generation product, but it shows promise.",
      "summary": "Smartphones have become humanity's second cognitive organ, but using the devices harshly disconnects users from the world around them. The only form factor that seems to have a chance to replace the smartphone is AR glasses. Meta Ray-Ban Display glasses are still highly dependent on mobile devices, and they only provide a small display visible to one eye, but they give users a glimpse into what the future may look like. This article provides a detailed review of the device along with the Meta Neural Band that pairs with it. The technology is still very much a first-generation product, but it shows promise.",
      "publishedAt": "2025-11-11T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 2.416094080029778
    },
    {
      "id": "640cad1490b911d04896ff52bd8f829e",
      "title": "Riding in a Chinese Robotaxi Is Pretty Smooth—That's a Problem for Waymo (8 minute read)",
      "url": "https://www.wsj.com/business/autos/china-robotaxi-self-driving-waymo-254ce0a1?st=BCZFqE&reflink=desktopwebshare_permalink&mod=tldr&utm_source=tldrnewsletter",
      "content": "Baidu, Pony AI, and WeRide each have hundreds of robotaxis on the road in China operating commercial paid services without a human safety driver. Their technology and rider experience are generally similar to the driverless taxis operated by Waymo in the US. China's robotaxi fleet is projected to grow to tens of thousands of vehicles by the end of next year. The Chinese companies are testing autonomous vehicles in around half a dozen countries - Waymo is only testing its vehicles in one country outside of the US.",
      "summary": "Baidu, Pony AI, and WeRide each have hundreds of robotaxis on the road in China operating commercial paid services without a human safety driver. Their technology and rider experience are generally similar to the driverless taxis operated by Waymo in the US. China's robotaxi fleet is projected to grow to tens of thousands of vehicles by the end of next year. The Chinese companies are testing autonomous vehicles in around half a dozen countries - Waymo is only testing its vehicles in one country outside of the US.",
      "publishedAt": "2025-11-10T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "testing"
      ],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 3.5617641777833513
    },
    {
      "id": "4ec7c2530ab48ba4cadc01cf34187ff3",
      "title": "Genetically Engineered Babies Are Banned. Tech Titans Are Trying to Make One Anyway (20 minute read)",
      "url": "https://www.wsj.com/tech/biotech/genetically-engineered-babies-tech-billionaires-6779efc8?st=PwkEK8&reflink=desktopwebshare_permalink&mod=tldr&utm_source=tldrnewsletter",
      "content": "Preventive, a startup backed by OpenAI CEO Sam Altman and his husband, as well as Coinbase CEO Brian Armstrong, is working toward creating genetically engineered human children. Editing gene embryos with the intention of creating babies is banned in the US and many countries, so the company has been searching for places to experiment where the practice is allowed. Preventive is part of a growing number of startups funded by powerful people in Silicon Valley that are pushing the boundaries of fertility and working to commercialize reproductive genetic technologies.",
      "summary": "Preventive, a startup backed by OpenAI CEO Sam Altman and his husband, as well as Coinbase CEO Brian Armstrong, is working toward creating genetically engineered human children. Editing gene embryos with the intention of creating babies is banned in the US and many countries, so the company has been searching for places to experiment where the practice is allowed. Preventive is part of a growing number of startups funded by powerful people in Silicon Valley that are pushing the boundaries of fertility and working to commercialize reproductive genetic technologies.",
      "publishedAt": "2025-11-10T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 1.8746127251491322
    },
    {
      "id": "7ea37385504cdffb522fc77ef74df730",
      "title": "Valdi (GitHub Repo)",
      "url": "https://github.com/Snapchat/Valdi?utm_source=tldrnewsletter",
      "content": "Valdi is a cross-platform UI framework that compiles TypeScript directly to native views on iOS, Android, and macOS. It delivers native performance without sacrificing developer velocity with automatic view recycling, optimized component rendering, an optimized layout engine, and viewport-aware rendering. Valdi eliminates the traditional compile-test-debug cycle that slows native development with instant hot reload, full VSCode debugging, and a familiar syntax. It integrates easily into existing apps and can be scaled as needed.",
      "summary": "Valdi is a cross-platform UI framework that compiles TypeScript directly to native views on iOS, Android, and macOS. It delivers native performance without sacrificing developer velocity with automatic view recycling, optimized component rendering, an optimized layout engine, and viewport-aware rendering. Valdi eliminates the traditional compile-test-debug cycle that slows native development with instant hot reload, full VSCode debugging, and a familiar syntax. It integrates easily into existing apps and can be scaled as needed.",
      "publishedAt": "2025-11-10T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 1.1247676350894795
    },
    {
      "id": "96062b2305a6dad19b538b9dc257fa26",
      "title": "You Need To Become A Full Stack Person (20 minute read)",
      "url": "https://den.dev/blog/full-stack-person/?utm_source=tldrnewsletter",
      "content": "Work is much messier today than when roles were somewhat rigid and handoffs were clean. Loops are now tighter, and the fast path from idea to impact crosses multiple disciplines. It is now better to have cross-domain fluency along with product sense and engineering craft. AI doesn't lower the cost of choosing the right thing, shaping it well, and operating it in the wild. Models can accelerate execution, but developers still own intent, architecture, and accountability.",
      "summary": "Work is much messier today than when roles were somewhat rigid and handoffs were clean. Loops are now tighter, and the fast path from idea to impact crosses multiple disciplines. It is now better to have cross-domain fluency along with product sense and engineering craft. AI doesn't lower the cost of choosing the right thing, shaping it well, and operating it in the wild. Models can accelerate execution, but developers still own intent, architecture, and accountability.",
      "publishedAt": "2025-11-10T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 2.249535270178959
    },
    {
      "id": "8b47582e6cab2a0e4d4839448f5dc2eb",
      "title": "Inside Cursor (38 minute read)",
      "url": "https://joincolossus.com/article/inside-cursor/?utm_source=tldrnewsletter",
      "content": "There's a lot of mystique about Cursor. This article takes a look inside the company from the perspective of a team member who has been there for two months. The company's leaders are enthusiastic about establishing a new playbook for company building. The employees are focused on the mission - the company has made them wealthy, but there is zero chatter from employees about getting rich.",
      "summary": "There's a lot of mystique about Cursor. This article takes a look inside the company from the perspective of a team member who has been there for two months. The company's leaders are enthusiastic about establishing a new playbook for company building. The employees are focused on the mission - the company has made them wealthy, but there is zero chatter from employees about getting rich.",
      "publishedAt": "2025-11-10T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 1.1247676350894795
    },
    {
      "id": "2b625c6d6ff43bd6c75ac31365f8de18",
      "title": "External Secrets (GitHub Repo)",
      "url": "https://github.com/external-secrets/external-secrets?utm_source=tldrnewsletter",
      "content": "The External Secrets Operator is a Kubernetes operator that integrates external secret management systems, reads information from external APIs, and automatically injects the values into a Kubernetes Secret.",
      "summary": "The External Secrets Operator is a Kubernetes operator that integrates external secret management systems, reads information from external APIs, and automatically injects the values into a Kubernetes Secret.",
      "publishedAt": "2025-11-10T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 0.7498450900596529
    },
    {
      "id": "53831353c815c26716d4d86df2db6dba",
      "title": "UnisonDB (GitHub Repo)",
      "url": "https://github.com/ankur-anand/unisondb?utm_source=tldrnewsletter",
      "content": "UnisonDB is a database designed specifically for Edge AI and Edge Computing that enables near-instant fan-out replication across hundreds of nodes while preserving strong consistency and durability.",
      "summary": "UnisonDB is a database designed specifically for Edge AI and Edge Computing that enables near-instant fan-out replication across hundreds of nodes while preserving strong consistency and durability.",
      "publishedAt": "2025-11-10T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 1.8746127251491322
    },
    {
      "id": "062755689c62472676864d4172301135",
      "title": "Google's Ironwood TPUs represent a bigger threat than Nvidia would have you believe (6 minute read)",
      "url": "https://www.theregister.com/2025/11/06/googles_ironwood_tpus_ai/?utm_source=tldrnewsletter",
      "content": "Google's latest generation of Ironwood accelerators will become generally available in the coming weeks. The TPU v7 accelerators are a major leap in performance over prior generations. They offer performance close to Nvidia's Blackwell GPUs, when normalizing floating point perf to the same precision. The TPUs can be scaled into truly enormous compute domains - Google is offering the chips in pods of 256 up to 9,216.",
      "summary": "Google's latest generation of Ironwood accelerators will become generally available in the coming weeks. The TPU v7 accelerators are a major leap in performance over prior generations. They offer performance close to Nvidia's Blackwell GPUs, when normalizing floating point perf to the same precision. The TPUs can be scaled into truly enormous compute domains - Google is offering the chips in pods of 256 up to 9,216.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 2.420853118768001
    },
    {
      "id": "9952073cf888a10cd5d55fb14eaa4e84",
      "title": "You Should Write An Agent (13 minute read)",
      "url": "https://fly.io/blog/everyone-write-an-agent/?utm_source=tldrnewsletter",
      "content": "Some concepts are easy to grasp in the abstract, but others you really need to try before you understand them. Regardless of whether or not LLM agents are snake oil or not, it's worth learning how to make one - startups are raising millions building agents at a time when nobody really knows anything yet. It's easy to get started. This post walks readers through how to create an agent.",
      "summary": "Some concepts are easy to grasp in the abstract, but others you really need to try before you understand them. Regardless of whether or not LLM agents are snake oil or not, it's worth learning how to make one - startups are raising millions building agents at a time when nobody really knows anything yet. It's easy to get started. This post walks readers through how to create an agent.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "feature_update",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 2.723459758614001
    },
    {
      "id": "550a71824c32d770d1218caf7d6239cd",
      "title": "Dead framework theory (17 minute read)",
      "url": "https://aifoc.us/dead-framework-theory/?utm_source=tldrnewsletter",
      "content": "As large language models become more dominant, new frameworks will find it hard to gain traction because the models won't have enough training data about them. The industry needs to innovate and build new frameworks, libraries, and platform features to push the web forward and create competition. Developers need to have clear strategies to get their work into the LLM training corpus, system prompts, and developer minds. If the industry continues its current focus on maintainability and developer experience, we'll end up in a world where the web is built by LLMs using only what is entrenched in the training data.",
      "summary": "As large language models become more dominant, new frameworks will find it hard to gain traction because the models won't have enough training data about them. The industry needs to innovate and build new frameworks, libraries, and platform features to push the web forward and create competition. Developers need to have clear strategies to get their work into the LLM training corpus, system prompts, and developer minds. If the industry continues its current focus on maintainability and developer experience, we'll end up in a world where the web is built by LLMs using only what is entrenched in the training data.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 1.5130331992300006
    },
    {
      "id": "70804ac7f06fffcdbed0547550c7283d",
      "title": "Game design is simple, actually (25 minute read)",
      "url": "https://www.raphkoster.com/2025/11/03/game-design-is-simple-actually/?utm_source=tldrnewsletter",
      "content": "This post presents a deconstructive view on how games are designed. It presents twelve aspects of game design to help readers get better at making games of any type. Each of the topics is deep, and there are plenty of links for further reading. The fun of making games is always right outside the edge of what the designers know.",
      "summary": "This post presents a deconstructive view on how games are designed. It presents twelve aspects of game design to help readers get better at making games of any type. Each of the topics is deep, and there are plenty of links for further reading. The fun of making games is always right outside the edge of what the designers know.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 1.8156398390760007
    },
    {
      "id": "253597e7e73c428eb248780c8d7f71ac",
      "title": "WhatsApp finally lets you message people on other apps (3 minute read)",
      "url": "https://www.androidpolice.com/whatsapp-support-for-third-party-apps/?utm_source=tldrnewsletter",
      "content": "The new feature will likely be locked to certain regions, so it probably won't be made available to those outside of Europe.",
      "summary": "The new feature will likely be locked to certain regions, so it probably won't be made available to those outside of Europe.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.929Z",
      "score": 1.8156398390760007
    },
    {
      "id": "6c08aba98e87abc0eae035db3d29868c",
      "title": "How I stopped worrying and learned to love the easy fix (4 minute read)",
      "url": "https://tn1ck.com/blog/how-i-stopped-worrying-and-learned-to-love-the-easy-fix?utm_source=tldrnewsletter",
      "content": "An easy fix is a pragmatic step forward that delivers value immediately - it can always be refactored later.",
      "summary": "An easy fix is a pragmatic step forward that delivers value immediately - it can always be refactored later.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-11-23T17:37:40.930Z",
      "score": 1.9669431573728904
    },
    {
      "id": "9a092f9bc29fcb8e20d1ffee5ea5ac26",
      "title": "Tesla to begin Cybercab production in April, Musk claims (4 minute read)",
      "url": "https://techcrunch.com/2025/11/06/tesla-to-begin-cybercab-production-in-april-musk-claims/?utm_source=tldrnewsletter",
      "content": "The Cybercab, which won't have pedals, a steering wheel, or side mirrors, will be starting production in April at Tesla's factory in Austin, Texas.",
      "summary": "The Cybercab, which won't have pedals, a steering wheel, or side mirrors, will be starting production in April at Tesla's factory in Austin, Texas.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-11-23T17:37:40.930Z",
      "score": 1.8156398375749756
    },
    {
      "id": "158c1fe092815a223b6428ffd98dadf7",
      "title": "Microsoft Lays Out Ambitious AI Vision, Free From OpenAI (5 minute read)",
      "url": "https://www.wsj.com/tech/ai/microsoft-lays-out-ambitious-ai-vision-free-from-openai-297652ff?st=QXB65W&reflink=desktopwebshare_permalink&mod=tldr&utm_source=tldrnewsletter",
      "content": "Microsoft's new MAI Superintelligence Team, led by Mustafa Suleyman, will put human interests and guardrails first, creating systems that are aligned to human values by default.",
      "summary": "Microsoft's new MAI Superintelligence Team, led by Mustafa Suleyman, will put human interests and guardrails first, creating systems that are aligned to human values by default.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:40.930Z",
      "score": 0.6052132791916586
    },
    {
      "id": "a25cbef12f65a05ab17a22c0a63c8e15",
      "title": "Reflections on my tech career",
      "url": "https://randomascii.wordpress.com/2025/11/10/reflections-on-my-tech-career-part-2/",
      "content": "23 minutes by Bruce Dawson",
      "summary": "23 minutes by Bruce Dawson",
      "publishedAt": "2025-11-23T17:37:40.904Z",
      "author": "Programming Digest",
      "source": "rss",
      "feedName": "Programming Digest",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-11-23T17:37:41.019Z",
      "score": 1.9999998098545064
    },
    {
      "id": "fc236a8af87022919e68da20c726cf41",
      "title": "Reproducing AWS outage with model checker",
      "url": "https://wyounas.github.io/aws/concurrency/2025/10/30/reproducing-the-aws-outage-race-condition-with-model-checker/",
      "content": "8 minutes by Waqas Younas",
      "summary": "8 minutes by Waqas Younas",
      "publishedAt": "2025-11-23T17:37:41.019Z",
      "author": "Programming Digest",
      "source": "rss",
      "feedName": "Programming Digest",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-11-23T17:37:41.019Z",
      "score": 5
    }
  ],
  "lastSync": "2025-11-23T17:37:49.155Z"
}