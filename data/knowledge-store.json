{
  "papers": [
    {
      "id": "37610552",
      "bibcode": "2025arXiv251107205P",
      "title": "Twenty-Five Years of MIR Research: Achievements, Practices, Evaluations, and Future Challenges",
      "authors": [
        "Peeters, Geoffroy",
        "Rafii, Zafar",
        "Fuentes, Magdalena",
        "Duan, Zhiyao",
        "Benetos, Emmanouil",
        "Nam, Juhan",
        "Mitsufuji, Yuki"
      ],
      "year": "2025",
      "abstract": "In this paper, we trace the evolution of Music Information Retrieval (MIR) over the past 25 years. While MIR gathers all kinds of research related to music informatics, a large part of it focuses on signal processing techniques for music data, fostering a close relationship with the IEEE Audio and Acoustic Signal Processing Technical Commitee. In this paper, we reflect the main research achievements of MIR along the three EDICS related to music analysis, processing and generation. We then review a set of successful practices that fuel the rapid development of MIR research. One practice is the annual research benchmark, the Music Information Retrieval Evaluation eXchange, where participants compete on a set of research tasks. Another practice is the pursuit of reproducible and open research. The active engagement with industry research and products is another key factor for achieving large societal impacts and motivating younger generations of students to join the field. Last but not the least, the commitment to diversity, equity and inclusion ensures MIR to be a vibrant and open community where various ideas, methodologies, and career pathways collide. We finish by providing future challenges MIR will have to face.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251107205P/abstract",
      "keywords": [
        "Sound",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.834Z",
      "publishedAt": "2025-11-23T19:31:44.834Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 10
    },
    {
      "id": "37571138",
      "bibcode": "2025arXiv251103985Y",
      "title": "ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering",
      "authors": [
        "Yuan, Zhuowen",
        "Liu, Tao",
        "Yang, Yang",
        "Wang, Yang",
        "Qi, Feng",
        "Rangadurai, Kaushik",
        "Li, Bo",
        "Yang, Shuang"
      ],
      "year": "2025",
      "abstract": "Recent LLM-based agents have demonstrated strong capabilities in automated ML engineering. However, they heavily rely on repeated full training runs to evaluate candidate solutions, resulting in significant computational overhead, limited scalability to large search spaces, and slow iteration cycles. To address these challenges, we introduce ArchPilot, a multi-agent system that integrates architecture generation, proxy-based evaluation, and adaptive search into a unified framework. ArchPilot consists of three specialized agents: an orchestration agent that coordinates the search process using a Monte Carlo Tree Search (MCTS)-inspired novel algorithm with a restart mechanism and manages memory of previous candidates; a generation agent that iteratively generates, improves, and debugs candidate architectures; and an evaluation agent that executes proxy training runs, generates and optimizes proxy functions, and aggregates the proxy scores into a fidelity-aware performance metric. This multi-agent collaboration allows ArchPilot to prioritize high-potential candidates with minimal reliance on expensive full training runs, facilitating efficient ML engineering under limited budgets. Experiments on MLE-Bench demonstrate that ArchPilot outperforms SOTA baselines such as AIDE and ML-Master, validating the effectiveness of our multi-agent system.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251103985Y/abstract",
      "keywords": [
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.835Z",
      "publishedAt": "2025-11-23T19:31:44.834Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "score": 14.499999988012567
    },
    {
      "id": "38059617",
      "bibcode": "2025arXiv251115383J",
      "title": "A Compliance-Preserving Retrieval System for Aircraft MRO Task Search",
      "authors": [
        "Jo, Byungho"
      ],
      "year": "2025",
      "abstract": "Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves &gt;90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251115383J/abstract",
      "keywords": [
        "Computation and Language",
        "Artificial Intelligence",
        "Emerging Technologies",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.835Z",
      "publishedAt": "2025-11-23T19:31:44.835Z",
      "libraryId": "ads-ingest",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "governance"
      ],
      "score": 15
    },
    {
      "id": "38074576",
      "bibcode": "2025arXiv251115778T",
      "title": "Balancing Natural Language Processing Accuracy and Normalisation in Extracting Medical Insights",
      "authors": [
        "Tworek, Paulina",
        "Bargieł, Miłosz",
        "Khan, Yousef",
        "Pełech-Pilichowski, Tomasz",
        "Mikołajczyk, Marek",
        "Lewandowski, Roman",
        "Sousa, Jose"
      ],
      "year": "2025",
      "abstract": "Extracting structured medical insights from unstructured clinical text using Natural Language Processing (NLP) remains an open challenge in healthcare, particularly in non-English contexts where resources are scarce. This study presents a comparative analysis of NLP low-compute rule-based methods and Large Language Models (LLMs) for information extraction from electronic health records (EHR) obtained from the Voivodeship Rehabilitation Hospital for Children in Ameryka, Poland. We evaluate both approaches by extracting patient demographics, clinical findings, and prescribed medications while examining the effects of lack of text normalisation and translation-induced information loss. Results demonstrate that rule-based methods provide higher accuracy in information retrieval tasks, particularly for age and sex extraction. However, LLMs offer greater adaptability and scalability, excelling in drug name recognition. The effectiveness of the LLMs was compared with texts originally in Polish and those translated into English, assessing the impact of translation. These findings highlight the trade-offs between accuracy, normalisation, and computational cost when deploying NLP in healthcare settings. We argue for hybrid approaches that combine the precision of rule-based systems with the adaptability of LLMs, offering a practical path toward more reliable and resource-efficient clinical NLP in real-world hospitals.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251115778T/abstract",
      "keywords": [
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.835Z",
      "publishedAt": "2025-11-23T19:31:44.835Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 9
    },
    {
      "id": "37498653",
      "bibcode": "2025arXiv251100792B",
      "title": "Fast PINN Eigensolvers via Biconvex Reformulation",
      "authors": [
        "Banderwaar, Akshay Sai",
        "Gupta, Abhishek"
      ],
      "year": "2025",
      "abstract": "Eigenvalue problems have a distinctive forward-inverse structure and are fundamental to characterizing a system's thermal response, stability, and natural modes. Physics-Informed Neural Networks (PINNs) offer a mesh-free alternative for solving such problems but are often orders of magnitude slower than classical numerical schemes. In this paper, we introduce a reformulated PINN approach that casts the search for eigenpairs as a biconvex optimization problem, enabling fast and provably convergent alternating convex search (ACS) over eigenvalues and eigenfunctions using analytically optimal updates. Numerical experiments show that PINN-ACS attains high accuracy with convergence speeds up to 500$\\times$ faster than gradient-based PINN training. We release our codes at https://github.com/NeurIPS-ML4PS-2025/PINN_ACS_CODES.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251100792B/abstract",
      "keywords": [
        "Machine Learning",
        "Artificial Intelligence",
        "Neural and Evolutionary Computing"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.835Z",
      "publishedAt": "2025-11-23T19:31:44.835Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review"
      ],
      "score": 8.5
    },
    {
      "id": "38059646",
      "bibcode": "2025arXiv251115408Z",
      "title": "NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework",
      "authors": [
        "Zhou, Shanlin",
        "Wang, Xinpeng",
        "Lian, Jianxun",
        "Liu, Zhenghao",
        "Lakshmanan, Laks V. S.",
        "Yi, Xiaoyuan",
        "Hao, Yongtao"
      ],
      "year": "2025",
      "abstract": "Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251115408Z/abstract",
      "keywords": [
        "Computation and Language",
        "Artificial Intelligence",
        "Information Retrieval",
        "Multiagent Systems",
        "Neural and Evolutionary Computing"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.836Z",
      "publishedAt": "2025-11-23T19:31:44.835Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "agents",
        "observability"
      ],
      "score": 10.499999991319445
    },
    {
      "id": "38074785",
      "bibcode": "2025arXiv251116005D",
      "title": "InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution",
      "authors": [
        "Dong, Qingao",
        "Wang, Mengfei",
        "Zhang, Hengzhi",
        "Li, Zhichao",
        "Yuan, Yuan",
        "Li, Mu",
        "Gao, Xiang",
        "Sun, Hailong",
        "Hu, Chunming",
        "Lv, Weifeng"
      ],
      "year": "2025",
      "abstract": "Large language model (LLM) agents have recently shown strong performance on repository-level issue resolution, but existing systems are almost exclusively designed for Python and rely heavily on lexical retrieval and shallow code navigation. These approaches transfer poorly to C++ projects, where overloaded identifiers, nested namespaces, template instantiations, and deep control-flow structures make context retrieval and fault localization substantially more difficult. As a result, state-of-the-art Python-oriented agents show a drastic performance drop on the C++ subset of MultiSWE-bench. We introduce INFCODE-C++, the first C++-aware autonomous system for end-to-end issue resolution. The system combines two complementary retrieval mechanisms -- semantic code-intent retrieval and deterministic AST-structured querying -- to construct accurate, language-aware context for repair.These components enable precise localization and robust patch synthesis in large, statically typed C++ repositories. Evaluated on the \\texttt{MultiSWE-bench-CPP} benchmark, INFCODE-C++ achieves a resolution rate of 25.58\\%, outperforming the strongest prior agent by 10.85 percentage points and more than doubling the performance of MSWE-agent. Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution. INFCODE-C++ highlights the need for language-aware reasoning in multi-language software agents and establishes a foundation for future research on scalable, LLM-driven repair for complex, statically typed ecosystems.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251116005D/abstract",
      "keywords": [
        "Software Engineering",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.836Z",
      "publishedAt": "2025-11-23T19:31:44.836Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 16
    },
    {
      "id": "38074902",
      "bibcode": "2025arXiv251116108C",
      "title": "SkyRL-Agent: Efficient RL Training for Multi-turn LLM Agent",
      "authors": [
        "Cao, Shiyi",
        "Li, Dacheng",
        "Zhao, Fangzhou",
        "Yuan, Shuo",
        "Hegde, Sumanth R.",
        "Chen, Connor",
        "Ruan, Charlie",
        "Griggs, Tyler",
        "Liu, Shu",
        "Tang, Eric",
        "Liaw, Richard",
        "Moritz, Philipp",
        "Zaharia, Matei",
        "Gonzalez, Joseph E.",
        "Stoica, Ion"
      ],
      "year": "2025",
      "abstract": "We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation. It provides efficient asynchronous dispatching, lightweight tool integration, and flexible backend interoperability, enabling seamless use with existing RL frameworks such as SkyRL-train, VeRL, and Tinker. Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning. We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. Together, these optimizations enable SA-SWE-32B to reach 39.4% Pass@1 on SWE-Bench Verified with more than 2x cost reduction compared to prior models reaching similar performance. Despite being trained solely on SWE tasks, SA-SWE-32B generalizes effectively to other agentic tasks, including Terminal-Bench, BrowseComp-Plus, and WebArena. We further demonstrate SkyRL-Agent's extensibility through case studies on deep research, computer use, and memory agents, each trained using a different training backend.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251116108C/abstract",
      "keywords": [
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.836Z",
      "publishedAt": "2025-11-23T19:31:44.836Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 16
    },
    {
      "id": "37499863",
      "bibcode": "2025arXiv251102002J",
      "title": "InteracSPARQL: An Interactive System for SPARQL Query Refinement Using Natural Language Explanations",
      "authors": [
        "Jian, Xiangru",
        "Dong, Zhengyuan",
        "Tamer Özsu, M."
      ],
      "year": "2025",
      "abstract": "In recent years, querying semantic web data using SPARQL has remained challenging, especially for non-expert users, due to the language's complex syntax and the prerequisite of understanding intricate data structures. To address these challenges, we propose InteracSPARQL, an interactive SPARQL query generation and refinement system that leverages natural language explanations (NLEs) to enhance user comprehension and facilitate iterative query refinement. InteracSPARQL integrates LLMs with a rule-based approach to first produce structured explanations directly from SPARQL abstract syntax trees (ASTs), followed by LLM-based linguistic refinements. Users can interactively refine queries through direct feedback or LLM-driven self-refinement, enabling the correction of ambiguous or incorrect query components in real time. We evaluate InteracSPARQL on standard benchmarks, demonstrating significant improvements in query accuracy, explanation clarity, and overall user satisfaction compared to baseline approaches. Our experiments further highlight the effectiveness of combining rule-based methods with LLM-driven refinements to create more accessible and robust SPARQL interfaces.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251102002J/abstract",
      "keywords": [
        "Databases",
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.836Z",
      "publishedAt": "2025-11-23T19:31:44.836Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 9
    },
    {
      "id": "37573252",
      "bibcode": "2025arXiv251104939N",
      "title": "Search Is Not Retrieval: Decoupling Semantic Matching from Contextual Assembly in RAG",
      "authors": [
        "Nainwani, Harshit",
        "Baban, Hediyeh"
      ],
      "year": "2025",
      "abstract": "Retrieval systems are essential to contemporary AI pipelines, although most confuse two separate processes: finding relevant information and giving enough context for reasoning. We introduce the Search-Is-Not-Retrieve (SINR) framework, a dual-layer architecture that distinguishes between fine-grained search representations and coarse-grained retrieval contexts. SINR enhances the composability, scalability, and context fidelity of retrieval systems by directly connecting small, semantically accurate search chunks to larger, contextually complete retrieve chunks, all without incurring extra processing costs. This design changes retrieval from a passive step to an active one, making the system architecture more like how people process information. We discuss the SINR framework's conceptual foundation, formal structure, implementation issues, and qualitative outcomes. This provides a practical foundation for the next generation of AI systems that use retrieval.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251104939N/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.836Z",
      "publishedAt": "2025-11-23T19:31:44.836Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 9
    },
    {
      "id": "37609044",
      "bibcode": "2025arXiv251105706J",
      "title": "AdvisingWise: Supporting Academic Advising in Higher Educations Through a Human-in-the-Loop Multi-Agent Framework",
      "authors": [
        "Jiang, Wendan",
        "Wang, Shiyuan",
        "Eltigani, Hiba",
        "Haroon, Rukhshan",
        "Faisal, Abdullah Bin",
        "Dogar, Fahad"
      ],
      "year": "2025",
      "abstract": "Academic advising is critical to student success in higher education, yet high student-to-advisor ratios limit advisors' capacity to provide timely support, particularly during peak periods. Recent advances in Large Language Models (LLMs) present opportunities to enhance the advising process. We present AdvisingWise, a multi-agent system that automates time-consuming tasks, such as information retrieval and response drafting, while preserving human oversight. AdvisingWise leverages authoritative institutional resources and adaptively prompts students about their academic backgrounds to generate reliable, personalized responses. All system responses undergo human advisor validation before delivery to students. We evaluate AdvisingWise through a mixed-methods approach: (1) expert evaluation on responses of 20 sample queries, (2) LLM-as-a-judge evaluation of the information retrieval strategy, and (3) a user study with 8 academic advisors to assess the system's practical utility. Our evaluation shows that AdvisingWise produces accurate, personalized responses. Advisors reported increasingly positive perceptions after using AdvisingWise, as their initial concerns about reliability and personalization diminished. We conclude by discussing the implications of human-AI synergy on the practice of academic advising.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251105706J/abstract",
      "keywords": [
        "Human-Computer Interaction",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.836Z",
      "publishedAt": "2025-11-23T19:31:44.836Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 10.999999990906085
    },
    {
      "id": "37616720",
      "bibcode": "2025arXiv251110465X",
      "title": "Beyond Elicitation: Provision-based Prompt Optimization for Knowledge-Intensive Tasks",
      "authors": [
        "Xu, Yunzhe",
        "Zhang, Zhuosheng",
        "Liu, Zhe"
      ],
      "year": "2025",
      "abstract": "While prompt optimization has emerged as a critical technique for enhancing language model performance, existing approaches primarily focus on elicitation-based strategies that search for optimal prompts to activate models' capabilities. These methods exhibit fundamental limitations when addressing knowledge-intensive tasks, as they operate within fixed parametric boundaries rather than providing the factual knowledge, terminology precision, and reasoning patterns required in specialized domains. To address these limitations, we propose Knowledge-Provision-based Prompt Optimization (KPPO), a framework that reformulates prompt optimization as systematic knowledge integration rather than potential elicitation. KPPO introduces three key innovations: 1) a knowledge gap filling mechanism for knowledge gap identification and targeted remediation; 2) a batch-wise candidate evaluation approach that considers both performance improvement and distributional stability; 3) an adaptive knowledge pruning strategy that balances performance and token efficiency, reducing up to 29% token usage. Extensive evaluation on 15 knowledge-intensive benchmarks from various domains demonstrates KPPO's superiority over elicitation-based methods, with an average performance improvement of ~6% over the strongest baseline while achieving comparable or lower token consumption. Code at: https://github.com/xyz9911/KPPO.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251110465X/abstract",
      "keywords": [
        "Computation and Language",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.837Z",
      "publishedAt": "2025-11-23T19:31:44.837Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 13
    },
    {
      "id": "37620304",
      "bibcode": "2025arXiv251112254Z",
      "title": "Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation",
      "authors": [
        "Zhou, Yuxiang",
        "Li, Jichang",
        "Zhang, Yanhao",
        "Lu, Haonan",
        "Li, Guanbin"
      ],
      "year": "2025",
      "abstract": "Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112254Z/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.837Z",
      "publishedAt": "2025-11-23T19:31:44.837Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 16
    },
    {
      "id": "37610890",
      "bibcode": "2025arXiv251107463R",
      "title": "Dynamic Stability of LLM-Generated Code",
      "authors": [
        "Rajput, Prateek",
        "Aziz Bonkoungou, Abdoul",
        "Song, Yewei",
        "Kader Kabore, Abdoul",
        "Olatunji, Iyiola E.",
        "Klein, Jacques",
        "Bissyande, Tegewende"
      ],
      "year": "2025",
      "abstract": "Current evaluations of LLMs for code generation emphasize functional correctness, overlooking the fact that functionally correct solutions can differ significantly in algorithmic complexity. For instance, an $(O(n^2))$ versus $(O(n \\log n))$ sorting algorithm may yield similar output but incur vastly different performance costs in production. This discrepancy reveals a critical limitation in current evaluation methods: they fail to capture the behavioral and performance diversity among correct solutions. To address this, we introduce a principled framework for evaluating the dynamic stability of generated code. We propose two metrics derived from opcode distributions: Static Canonical Trace Divergence (SCTD), which captures algorithmic structure diversity across generated solutions, and Dynamic Canonical Trace Divergence (DCTD), which quantifies runtime behavioral variance. Their ratio, the Behavioral Expression Factor (BEF), serves as a diagnostic signal: it indicates critical runtime instability when BEF $\\ll$ 1 and functional redundancy when BEF $\\gg$ 1. Empirical results on BigOBench and CodeContests show that state-of-the-art LLMs exhibit significant algorithmic variance even among functionally correct outputs. Notably, increasing sampling temperature improves pass@1 rates but degrades stability, revealing an unrecognized trade-off: searching for correct solutions in diverse output spaces introduces a \"penalty of instability\" between correctness and behavioral consistency. Our findings call for stability-aware objectives in code generation and new benchmarks with asymptotic test cases for robust, real-world LLM evaluation.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251107463R/abstract",
      "keywords": [
        "Programming Languages",
        "Artificial Intelligence",
        "Software Engineering"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.837Z",
      "publishedAt": "2025-11-23T19:31:44.837Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "observability"
      ],
      "score": 8.5
    },
    {
      "id": "37611597",
      "bibcode": "2025arXiv251108181C",
      "title": "MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System",
      "authors": [
        "Cho, Seung Hwan",
        "Yang, Yujin",
        "Baeck, Danik",
        "Kim, Minjoo",
        "Kim, Young-Min",
        "Lee, Heejung",
        "Park, Sangjin"
      ],
      "year": "2025",
      "abstract": "Recommender systems (RS) are currently being studied to mitigate limitations during cold-start conditions by leveraging modality information or introducing Agent concepts based on the exceptional reasoning capabilities of Large Language Models (LLMs). Meanwhile, food and beverage recommender systems have traditionally used knowledge graph and ontology concepts due to the domain's unique data attributes and relationship characteristics. On this background, we propose MARC, a multimodal and multi-task cocktail recommender system based on Agentic Retrieval-Augmented Generation (RAG) utilizing graph database under cold-start conditions. The proposed system generates high-quality, contextually appropriate answers through two core processes: a task recognition router and a reflection process. The graph database was constructed by processing cocktail data from Kaggle, and its effectiveness was evaluated using 200 manually crafted questions. The evaluation used both LLM-as-a-judge and human evaluation to demonstrate that answers generated via the graph database outperformed those from a simple vector database in terms of quality. The code is available at https://github.com/diddbwls/cocktail_rec_agentrag",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251108181C/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.837Z",
      "publishedAt": "2025-11-23T19:31:44.837Z",
      "libraryId": "ads-ingest",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "retrieval",
        "agents"
      ],
      "score": 13
    },
    {
      "id": "38058442",
      "bibcode": "2025arXiv251114256L",
      "title": "PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models",
      "authors": [
        "Liu, Yu",
        "Lin, Xixun",
        "Shang, Yanmin",
        "Li, Yangxi",
        "Wang, Shi",
        "Cao, Yanan"
      ],
      "year": "2025",
      "abstract": "Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a \"Retrieve-Prioritize-Reason\" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251114256L/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.837Z",
      "publishedAt": "2025-11-23T19:31:44.837Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 10
    },
    {
      "id": "37499138",
      "bibcode": "2025arXiv251101268K",
      "title": "Rescuing the Unpoisoned: Efficient Defense against Knowledge Corruption Attacks on RAG Systems",
      "authors": [
        "Kim, Minseok",
        "Lee, Hankook",
        "Koo, Hyungjoon"
      ],
      "year": "2025",
      "abstract": "Large language models (LLMs) are reshaping numerous facets of our daily lives, leading widespread adoption as web-based services. Despite their versatility, LLMs face notable challenges, such as generating hallucinated content and lacking access to up-to-date information. Lately, to address such limitations, Retrieval-Augmented Generation (RAG) has emerged as a promising direction by generating responses grounded in external knowledge sources. A typical RAG system consists of i) a retriever that probes a group of relevant passages from a knowledge base and ii) a generator that formulates a response based on the retrieved content. However, as with other AI systems, recent studies demonstrate the vulnerability of RAG, such as knowledge corruption attacks by injecting misleading information. In response, several defense strategies have been proposed, including having LLMs inspect the retrieved passages individually or fine-tuning robust retrievers. While effective, such approaches often come with substantial computational costs. In this work, we introduce RAGDefender, a resource-efficient defense mechanism against knowledge corruption (i.e., by data poisoning) attacks in practical RAG deployments. RAGDefender operates during the post-retrieval phase, leveraging lightweight machine learning techniques to detect and filter out adversarial content without requiring additional model training or inference. Our empirical evaluations show that RAGDefender consistently outperforms existing state-of-the-art defenses across multiple models and adversarial scenarios: e.g., RAGDefender reduces the attack success rate (ASR) against the Gemini model from 0.89 to as low as 0.02, compared to 0.69 for RobustRAG and 0.24 for Discern-and-Answer when adversarial passages outnumber legitimate ones by a factor of four (4x).",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251101268K/abstract",
      "keywords": [
        "Cryptography and Security",
        "Artificial Intelligence",
        "Information Retrieval",
        "D.4.6; K.6.5"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.837Z",
      "publishedAt": "2025-11-23T19:31:44.837Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 14
    },
    {
      "id": "37499717",
      "bibcode": "2025arXiv251101857E",
      "title": "Trove: A Flexible Toolkit for Dense Retrieval",
      "authors": [
        "Esfandiarpoor, Reza",
        "Zuo, Max",
        "Bach, Stephen H."
      ],
      "year": "2025",
      "abstract": "We introduce Trove, an easy-to-use open-source retrieval toolkit that simplifies research experiments without sacrificing flexibility or speed. For the first time, we introduce efficient data management features that load and process (filter, select, transform, and combine) retrieval datasets on the fly, with just a few lines of code. This gives users the flexibility to easily experiment with different dataset configurations without the need to compute and store multiple copies of large datasets. Trove is highly customizable: in addition to many built-in options, it allows users to freely modify existing components or replace them entirely with user-defined objects. It also provides a low-code and unified pipeline for evaluation and hard negative mining, which supports multi-node execution without any code changes. Trove's data management features reduce memory consumption by a factor of 2.6. Moreover, Trove's easy-to-use inference pipeline incurs no overhead, and inference times decrease linearly with the number of available nodes. Most importantly, we demonstrate how Trove simplifies retrieval experiments and allows for arbitrary customizations, thus facilitating exploratory research.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251101857E/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.837Z",
      "publishedAt": "2025-11-23T19:31:44.837Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 9
    },
    {
      "id": "37500072",
      "bibcode": "2025arXiv251102200W",
      "title": "Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration",
      "authors": [
        "Wang, Jingbo",
        "Zhao, Sendong",
        "Wang, Haochun",
        "Fan, Yuzheng",
        "Zhang, Lizhe",
        "Liu, Yan",
        "Liu, Ting"
      ],
      "year": "2025",
      "abstract": "The emergence of multi-agent systems powered by large language models (LLMs) has unlocked new frontiers in complex task-solving, enabling diverse agents to integrate unique expertise, collaborate flexibly, and address challenges unattainable for individual models. However, the full potential of such systems is hindered by rigid agent scheduling and inefficient coordination strategies that fail to adapt to evolving task requirements. In this paper, we propose STRMAC, a state-aware routing framework designed for efficient collaboration in multi-agent systems. Our method separately encodes interaction history and agent knowledge to power the router, which adaptively selects the most suitable single agent at each step for efficient and effective collaboration. Furthermore, we introduce a self-evolving data generation approach that accelerates the collection of high-quality execution paths for efficient system training. Experiments on challenging collaborative reasoning benchmarks demonstrate that our method achieves state-of-the-art performance, achieving up to 23.8% improvement over baselines and reducing data collection overhead by up to 90.1% compared to exhaustive search.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251102200W/abstract",
      "keywords": [
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.838Z",
      "publishedAt": "2025-11-23T19:31:44.837Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "score": 13.499999988839285
    },
    {
      "id": "37612531",
      "bibcode": "2025arXiv251109109W",
      "title": "Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning",
      "authors": [
        "Wei, Wenda",
        "Liu, Yu-An",
        "Zhang, Ruqing",
        "Guo, Jiafeng",
        "Su, Lixin",
        "Wang, Shuaiqiang",
        "Yin, Dawei",
        "de Rijke, Maarten",
        "Cheng, Xueqi"
      ],
      "year": "2025",
      "abstract": "Retrieval-augmented generation (RAG) has proven to be effective in mitigating hallucinations in large language models, yet its effectiveness remains limited in complex, multi-step reasoning scenarios. Recent efforts have incorporated search-based interactions into RAG, enabling iterative reasoning with real-time retrieval. Most approaches rely on outcome-based supervision, offering no explicit guidance for intermediate steps. This often leads to reward hacking and degraded response quality. We propose Bi-RAR, a novel retrieval-augmented reasoning framework that evaluates each intermediate step jointly in both forward and backward directions. To assess the information completeness of each step, we introduce a bidirectional information distance grounded in Kolmogorov complexity, approximated via language model generation probabilities. This quantification measures both how far the current reasoning is from the answer and how well it addresses the question. To optimize reasoning under these bidirectional signals, we adopt a multi-objective reinforcement learning framework with a cascading reward structure that emphasizes early trajectory alignment. Empirical results on seven question answering benchmarks demonstrate that Bi-RAR surpasses previous methods and enables efficient interaction and reasoning with the search engine during training and inference.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251109109W/abstract",
      "keywords": [
        "Computation and Language",
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.838Z",
      "publishedAt": "2025-11-23T19:31:44.838Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 9
    },
    {
      "id": "37571616",
      "bibcode": "2025arXiv251104473C",
      "title": "Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs",
      "authors": [
        "Cattaneo, Alberto",
        "Luschi, Carlo",
        "Justus, Daniel"
      ],
      "year": "2025",
      "abstract": "Retrieval of information from graph-structured knowledge bases represents a promising direction for improving the factuality of LLMs. While various solutions have been proposed, a comparison of methods is difficult due to the lack of challenging QA datasets with ground-truth targets for graph retrieval. We present SynthKGQA, a framework for generating high-quality synthetic Knowledge Graph Question Answering datasets from any Knowledge Graph, providing the full set of ground-truth facts in the KG to reason over each question. We show how, in addition to enabling more informative benchmarking of KG retrievers, the data produced with SynthKGQA also allows us to train better models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset designed to test zero-shot generalization abilities of KG retrievers with respect to unseen graph structures and relation types, and benchmark popular solutions for KG-augmented LLMs on it.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251104473C/abstract",
      "keywords": [
        "Machine Learning",
        "Artificial Intelligence",
        "Computation and Language",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.838Z",
      "publishedAt": "2025-11-23T19:31:44.838Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 9
    },
    {
      "id": "37609193",
      "bibcode": "2025arXiv251105849J",
      "title": "EGG-SR: Embedding Symbolic Equivalence into Symbolic Regression via Equality Graph",
      "authors": [
        "Jiang, Nan",
        "Wang, Ziyi",
        "Xue, Yexiang"
      ],
      "year": "2025",
      "abstract": "Symbolic regression seeks to uncover physical laws from experimental data by searching for closed-form expressions, which is an important task in AI-driven scientific discovery. Yet the exponential growth of the search space of expression renders the task computationally challenging. A promising yet underexplored direction for reducing the effective search space and accelerating training lies in symbolic equivalence: many expressions, although syntactically different, define the same function -- for example, $\\log(x_1^2x_2^3)$, $\\log(x_1^2)+\\log(x_2^3)$, and $2\\log(x_1)+3\\log(x_2)$. Existing algorithms treat such variants as distinct outputs, leading to redundant exploration and slow learning. We introduce EGG-SR, a unified framework that integrates equality graphs (e-graphs) into diverse symbolic regression algorithms, including Monte Carlo Tree Search (MCTS), deep reinforcement learning (DRL), and large language models (LLMs). EGG-SR compactly represents equivalent expressions through the proposed EGG module, enabling more efficient learning by: (1) pruning redundant subtree exploration in EGG-MCTS, (2) aggregating rewards across equivalence classes in EGG-DRL, and (3) enriching feedback prompts in EGG-LLM. Under mild assumptions, we show that embedding e-graphs tightens the regret bound of MCTS and reduces the variance of the DRL gradient estimator. Empirically, EGG-SR consistently enhances multiple baselines across challenging benchmarks, discovering equations with lower normalized mean squared error than state-of-the-art methods. Code implementation is available at: https://www.github.com/jiangnanhugo/egg-sr.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251105849J/abstract",
      "keywords": [
        "Symbolic Computation",
        "Artificial Intelligence",
        "Machine Learning"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.838Z",
      "publishedAt": "2025-11-23T19:31:44.838Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 9
    },
    {
      "id": "37609190",
      "bibcode": "2025arXiv251105850M",
      "title": "Retrieval Quality at Context Limit",
      "authors": [
        "McKinnon, Max"
      ],
      "year": "2025",
      "abstract": "The ability of large language models (LLMs) to recall and retrieve information from long contexts is critical for many real-world applications. Prior work (Liu et al., 2023) reported that LLMs suffer significant drops in retrieval accuracy for facts placed in the middle of large contexts, an effect known as \"Lost in the Middle\" (LITM). We find the model Gemini 2.5 Flash can answer needle-in-a-haystack questions with great accuracy regardless of document position including when the document is nearly at the input context limit. Our results suggest that the \"Lost in the Middle\" effect is not present for simple factoid Q\\&amp;A in Gemini 2.5 Flash, indicating substantial improvements in long-context retrieval.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251105850M/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence",
        "68P20 (Primary)",
        "68T07 (Secondary)"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.838Z",
      "publishedAt": "2025-11-23T19:31:44.838Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 8
    },
    {
      "id": "37500928",
      "bibcode": "2025arXiv251103051Z",
      "title": "No-Human in the Loop: Agentic Evaluation at Scale for Recommendation",
      "authors": [
        "Zhang, Tao",
        "Yao, Kehui",
        "Ma, Luyi",
        "Chen, Jiao",
        "Yousefi Maragheh, Reza",
        "Zhao, Kai",
        "Xu, Jianpeng",
        "Korpeoglu, Evren",
        "Kumar, Sushant",
        "Achan, Kannan"
      ],
      "year": "2025",
      "abstract": "Evaluating large language models (LLMs) as judges is increasingly critical for building scalable and trustworthy evaluation pipelines. We present ScalingEval, a large-scale benchmarking study that systematically compares 36 LLMs, including GPT, Gemini, Claude, and Llama, across multiple product categories using a consensus-driven evaluation protocol. Our multi-agent framework aggregates pattern audits and issue codes into ground-truth labels via scalable majority voting, enabling reproducible comparison of LLM evaluators without human annotation. Applied to large-scale complementary-item recommendation, the benchmark reports four key findings: (i) Anthropic Claude 3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers the best overall performance across categories; (iii) GPT-4o provides the most favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among open-source models. Category-level analysis shows strong consensus in structured domains (Electronics, Sports) but persistent disagreement in lifestyle categories (Clothing, Food). These results establish ScalingEval as a reproducible benchmark and evaluation protocol for LLMs as judges, with actionable guidance on scaling, reliability, and model family tradeoffs.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251103051Z/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.838Z",
      "publishedAt": "2025-11-23T19:31:44.838Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "governance"
      ],
      "score": 12.5
    },
    {
      "id": "arXiv:2511.03330",
      "bibcode": "2025arXiv251103330W",
      "title": "Discourse-Aware Scientific Paper Recommendation via QA-Style Summarization and Multi-Level Contrastive Learning",
      "authors": [
        "Wang, Shenghua",
        "Yin, Zhen"
      ],
      "year": "2025",
      "abstract": "The rapid growth of open-access (OA) publications has intensified the challenge of identifying relevant scientific papers. Due to privacy constraints and limited access to user interaction data, recent efforts have shifted toward content-based recommendation, which relies solely on textual information. However, existing models typically treat papers as unstructured text, neglecting their discourse organization and thereby limiting semantic completeness and interpretability. To address these limitations, we propose OMRC-MR, a hierarchical framework that integrates QA-style OMRC (Objective, Method, Result, Conclusion) summarization, multi-level contrastive learning, and structure-aware re-ranking for scholarly recommendation. The QA-style summarization module converts raw papers into structured and discourse-consistent representations, while multi-level contrastive objectives align semantic representations across metadata, section, and document levels. The final re-ranking stage further refines retrieval precision through contextual similarity calibration. Experiments on DBLP, S2ORC, and the newly constructed Sci-OMRC dataset demonstrate that OMRC-MR consistently surpasses state-of-the-art baselines, achieving up to 7.2% and 3.8% improvements in Precision@10 and Recall@10, respectively. Additional evaluations confirm that QA-style summarization produces more coherent and factually complete representations. Overall, OMRC-MR provides a unified and interpretable content-based paradigm for scientific paper recommendation, advancing trustworthy and privacy-aware scholarly information retrieval.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251103330W/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.381Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 1.5171808244626366
    },
    {
      "id": "37609919",
      "bibcode": "2025arXiv251106582S",
      "title": "TabRAG: Tabular Document Retrieval via Structured Language Representations",
      "authors": [
        "Si, Jacob",
        "Qu, Mike",
        "Lee, Michelle",
        "Li, Yingzhen"
      ],
      "year": "2025",
      "abstract": "Ingesting data for Retrieval-Augmented Generation (RAG) involves either fine-tuning the embedding model directly on the target corpus or parsing documents for embedding model encoding. The former, while accurate, incurs high computational hardware requirements, while the latter suffers from suboptimal performance when extracting tabular data. In this work, we address the latter by presenting TabRAG, a parsing-based RAG pipeline designed to tackle table-heavy documents via structured language representations. TabRAG outperforms existing popular parsing-based methods for generation and retrieval. Code is available at https://github.com/jacobyhsi/TabRAG.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251106582S/abstract",
      "keywords": [
        "Computation and Language",
        "Artificial Intelligence",
        "Computer Vision and Pattern Recognition",
        "Information Retrieval",
        "Machine Learning"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.839Z",
      "publishedAt": "2025-11-23T19:31:44.839Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 9
    },
    {
      "id": "37612950",
      "bibcode": "2025arXiv251109535L",
      "title": "Robust and Diverse Multi-Agent Learning via Rational Policy Gradient",
      "authors": [
        "Lauffer, Niklas",
        "Shah, Ameesh",
        "Carroll, Micah",
        "Seshia, Sanjit A.",
        "Russell, Stuart",
        "Dennis, Michael"
      ],
      "year": "2025",
      "abstract": "Adversarial optimization algorithms that explicitly search for flaws in agents' policies have been successfully applied to finding robust and diverse policies in multi-agent settings. However, the success of adversarial optimization has been largely limited to zero-sum settings because its naive application in cooperative settings leads to a critical failure mode: agents are irrationally incentivized to self-sabotage, blocking the completion of tasks and halting further learning. To address this, we introduce Rationality-preserving Policy Optimization (RPO), a formalism for adversarial optimization that avoids self-sabotage by ensuring agents remain rational--that is, their policies are optimal with respect to some possible partner policy. To solve RPO, we develop Rational Policy Gradient (RPG), which trains agents to maximize their own reward in a modified version of the original game in which we use opponent shaping techniques to optimize the adversarial objective. RPG enables us to extend a variety of existing adversarial optimization algorithms that, no longer subject to the limitations of self-sabotage, can find adversarial examples, improve robustness and adaptability, and learn diverse policies. We empirically validate that our approach achieves strong performance in several popular cooperative and general-sum environments. Our project page can be found at https://rational-policy-gradient.github.io.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251109535L/abstract",
      "keywords": [
        "Artificial Intelligence",
        "I.2.6; I.2.11"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.839Z",
      "publishedAt": "2025-11-23T19:31:44.839Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "governance"
      ],
      "score": 17.5
    },
    {
      "id": "38059669",
      "bibcode": "2025arXiv251115435L",
      "title": "HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation",
      "authors": [
        "Luo, Linyin",
        "Ding, Yujuan",
        "Ma, Yunshan",
        "Fan, Wenqi",
        "Lai, Hanjiang"
      ],
      "year": "2025",
      "abstract": "Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251115435L/abstract",
      "keywords": [
        "Computer Vision and Pattern Recognition",
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.839Z",
      "publishedAt": "2025-11-23T19:31:44.839Z",
      "libraryId": "ads-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 14
    },
    {
      "id": "38059365",
      "bibcode": "2025arXiv251115141K",
      "title": "ItemRAG: Item-Based Retrieval-Augmented Generation for LLM-Based Recommendation",
      "authors": [
        "Kim, Sunwoo",
        "Lee, Geon",
        "Kim, Kyungho",
        "Yoo, Jaemin",
        "Shin, Kijung"
      ],
      "year": "2025",
      "abstract": "Recently, large language models (LLMs) have been widely used as recommender systems, owing to their strong reasoning capability and their effectiveness in handling cold-start items. To better adapt LLMs for recommendation, retrieval-augmented generation (RAG) has been incorporated. Most existing RAG methods are user-based, retrieving purchase patterns of users similar to the target user and providing them to the LLM. In this work, we propose ItemRAG, an item-based RAG method for LLM-based recommendation that retrieves relevant items (rather than users) from item-item co-purchase histories. ItemRAG helps LLMs capture co-purchase patterns among items, which are beneficial for recommendations. Especially, our retrieval strategy incorporates semantically similar items to better handle cold-start items and uses co-purchase frequencies to improve the relevance of the retrieved items. Through extensive experiments, we demonstrate that ItemRAG consistently (1) improves the zero-shot LLM-based recommender by up to 43% in Hit-Ratio-1 and (2) outperforms user-based RAG baselines under both standard and cold-start item recommendation settings.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251115141K/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.839Z",
      "publishedAt": "2025-11-23T19:31:44.839Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 10
    },
    {
      "id": "37571688",
      "bibcode": "2025arXiv251104541B",
      "title": "LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems",
      "authors": [
        "Bonin, Baptiste",
        "Heuillet, Maxime",
        "Durand, Audrey"
      ],
      "year": "2025",
      "abstract": "Modeling user preferences across domains remains a key challenge in slate recommendation (i.e. recommending an ordered sequence of items) research. We investigate how Large Language Models (LLM) can effectively act as world models of user preferences through pairwise reasoning over slates. We conduct an empirical study involving several LLMs on three tasks spanning different datasets. Our results reveal relationships between task performance and properties of the preference function captured by LLMs, hinting towards areas for improvement and highlighting the potential of LLMs as world models in recommender systems.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251104541B/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.839Z",
      "publishedAt": "2025-11-23T19:31:44.839Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "score": 6.5
    },
    {
      "id": "37573615",
      "bibcode": "2025arXiv251105271H",
      "title": "DeepEyesV2: Toward Agentic Multimodal Model",
      "authors": [
        "Hong, Jack",
        "Zhao, Chenxiao",
        "Zhu, ChengLin",
        "Lu, Weiheng",
        "Xu, Guohai",
        "Yu, Xing"
      ],
      "year": "2025",
      "abstract": "Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motivates a two-stage training pipeline: a cold-start stage to establish tool-use patterns, and reinforcement learning stage to further refine tool invocation. We curate a diverse, moderately challenging training dataset, specifically including examples where tool use is beneficial. We further introduce RealX-Bench, a comprehensive benchmark designed to evaluate real-world multimodal reasoning, which inherently requires the integration of multiple capabilities, including perception, search, and reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative benchmarks, demonstrating its effectiveness across real-world understanding, mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2 exhibits task-adaptive tool invocation, tending to use image operations for perception tasks and numerical computations for reasoning tasks. Reinforcement learning further enables complex tool combinations and allows model to selectively invoke tools based on context. We hope our study can provide guidance for community in developing agentic multimodal models.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251105271H/abstract",
      "keywords": [
        "Computer Vision and Pattern Recognition",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.839Z",
      "publishedAt": "2025-11-23T19:31:44.839Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "score": 13.5
    },
    {
      "id": "37499515",
      "bibcode": "2025arXiv251101643C",
      "title": "A Graph-based RAG for Energy Efficiency Question Answering",
      "authors": [
        "Campi, Riccardo",
        "Pinciroli Vago, Nicolò Oreste",
        "Giudici, Mathyas",
        "Barrachina Rodriguez-Guisado, Pablo",
        "Brambilla, Marco",
        "Fraternali, Piero"
      ],
      "year": "2025",
      "abstract": "In this work, we investigate the use of Large Language Models (LLMs) within a graph-based Retrieval Augmented Generation (RAG) architecture for Energy Efficiency (EE) Question Answering. First, the system automatically extracts a Knowledge Graph (KG) from guidance and regulatory documents in the energy field. Then, the generated graph is navigated and reasoned upon to provide users with accurate answers in multiple languages. We implement a human-based validation using the RAGAs framework properties, a validation dataset comprising 101 question-answer pairs, and domain experts. Results confirm the potential of this architecture and identify its strengths and weaknesses. Validation results show how the system correctly answers in about three out of four of the cases (75.2 +- 2.7%), with higher results on questions related to more general EE answers (up to 81.0 +- 4.1%), and featuring promising multilingual abilities (4.4% accuracy loss due to translation).",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251101643C/abstract",
      "keywords": [
        "Computation and Language",
        "Artificial Intelligence",
        "Information Retrieval",
        "I.2.7; I.2.4; I.2.1; I.2.6"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.839Z",
      "publishedAt": "2025-11-23T19:31:44.839Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 8.999999992559523
    },
    {
      "id": "37501020",
      "bibcode": "2025arXiv251103153O",
      "title": "RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring",
      "authors": [
        "Oueslati, Khouloud",
        "Lamothe, Maxime",
        "Khomh, Foutse"
      ],
      "year": "2025",
      "abstract": "Large Language Models (LLMs) have substantially influenced various software engineering tasks. Indeed, in the case of software refactoring, traditional LLMs have shown the ability to reduce development time and enhance code quality. However, these LLMs often rely on static, detailed instructions for specific tasks. In contrast, LLM-based agents can dynamically adapt to evolving contexts and autonomously make decisions by interacting with software tools and executing workflows. In this paper, we explore the potential of LLM-based agents in supporting refactoring activities. Specifically, we introduce RefAgent, a multi-agent LLM-based framework for end-to-end software refactoring. RefAgent consists of specialized agents responsible for planning, executing, testing, and iteratively refining refactorings using self-reflection and tool-calling capabilities. We evaluate RefAgent on eight open-source Java projects, comparing its effectiveness against a single-agent approach, a search-based refactoring tool, and historical developer refactorings. Our assessment focuses on: (1) the impact of generated refactorings on software quality, (2) the ability to identify refactoring opportunities, and (3) the contribution of each LLM agent through an ablation study. Our results show that RefAgent achieves a median unit test pass rate of 90%, reduces code smells by a median of 52.5%, and improves key quality attributes (e.g., reusability) by a median of 8.6%. Additionally, it closely aligns with developer refactorings and the search-based tool in identifying refactoring opportunities, attaining a median F1-score of 79.15% and 72.7%, respectively. Compared to single-agent approaches, RefAgent improves the median unit test pass rate by 64.7% and the median compilation success rate by 40.1%. These findings highlight the promise of multi-agent architectures in advancing automated software refactoring.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251103153O/abstract",
      "keywords": [
        "Software Engineering",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.840Z",
      "publishedAt": "2025-11-23T19:31:44.840Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "testing"
      ],
      "score": 16
    },
    {
      "id": "37609225",
      "bibcode": "2025arXiv251105885Z",
      "title": "A Remarkably Efficient Paradigm to Multimodal Large Language Models for Sequential Recommendation",
      "authors": [
        "Zhong, Qiyong",
        "Su, Jiajie",
        "Yang, Ming",
        "Ma, Yunshan",
        "Zheng, Xiaolin",
        "Chen, Chaochao"
      ],
      "year": "2025",
      "abstract": "Sequential recommendations (SR) predict users' future interactions based on their historical behavior. The rise of Large Language Models (LLMs) has brought powerful generative and reasoning capabilities, significantly enhancing SR performance, while Multimodal LLMs (MLLMs) further extend this by introducing data like images and interactive relationships. However, critical issues remain, i.e., (a) Suboptimal item representations caused by lengthy and redundant descriptions, leading to inefficiencies in both training and inference; (b) Modality-related cognitive bias, as LLMs are predominantly pretrained on textual data, limiting their ability to effectively integrate and utilize non-textual modalities; (c) Weakening sequential perception in long interaction sequences, where attention mechanisms struggle to capture earlier interactions, hindering the modeling of long-range dependencies. To address these issues, we propose Speeder, an efficient MLLM-based paradigm for SR featuring three key innovations: 1) Multimodal Representation Compression (MRC), which condenses item attributes into concise yet informative tokens, reducing redundancy and computational cost; 2) Modality-aware Progressive Optimization (MPO), enabling gradual learning of multimodal representations; 3) Sequential Position Awareness Enhancement (SPAE), improving the LLM's capability to capture both relative and absolute sequential dependencies in long interaction sequences. Extensive experiments on real-world datasets demonstrate the effectiveness and efficiency of Speeder. Speeder increases training speed to 250% of the original while reducing inference time to 25% on the Amazon dataset.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251105885Z/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.840Z",
      "publishedAt": "2025-11-23T19:31:44.840Z",
      "libraryId": "ads-ingest",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "score": 9.5
    },
    {
      "id": "37618734",
      "bibcode": "2025arXiv251111043N",
      "title": "Autonomous Vehicle Path Planning by Searching With Differentiable Simulation",
      "authors": [
        "Nachkov, Asen",
        "Zaech, Jan-Nico",
        "Pani Paudel, Danda",
        "Wang, Xi",
        "Van Gool, Luc"
      ],
      "year": "2025",
      "abstract": "Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251111043N/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Robotics"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.840Z",
      "publishedAt": "2025-11-23T19:31:44.840Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "governance"
      ],
      "score": 14
    },
    {
      "id": "38075326",
      "bibcode": "2025arXiv251116543Z",
      "title": "The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation",
      "authors": [
        "Zhang, Jiaheng",
        "Zhang, Daqiang"
      ],
      "year": "2025",
      "abstract": "The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage. Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models. Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251116543Z/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence",
        "Computation and Language",
        "Machine Learning"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.840Z",
      "publishedAt": "2025-11-23T19:31:44.840Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 13
    },
    {
      "id": "37500686",
      "bibcode": "2025arXiv251102824M",
      "title": "Kosmos: An AI Scientist for Autonomous Discovery",
      "authors": [
        "Mitchener, Ludovico",
        "Yiu, Angela",
        "Chang, Benjamin",
        "Bourdenx, Mathieu",
        "Nadolski, Tyler",
        "Sulovari, Arvis",
        "Landsness, Eric C.",
        "Barabasi, Daniel L.",
        "Narayanan, Siddharth",
        "Evans, Nicky",
        "Reddy, Shriya",
        "Foiani, Martha",
        "Kamal, Aizad",
        "Shriver, Leah P.",
        "Cao, Fang",
        "Wassie, Asmamaw T.",
        "Laurent, Jon M.",
        "Melville-Green, Edwin",
        "Caldas, Mayk",
        "Bou, Albert",
        "Roberts, Kaleigh F.",
        "Zagorac, Sladjana",
        "Orr, Timothy C.",
        "Orr, Miranda E.",
        "Zwezdaryk, Kevin J.",
        "Ghareeb, Ali E.",
        "McCoy, Laurie",
        "Gomes, Bruna",
        "Ashley, Euan A.",
        "Duff, Karen E.",
        "Buonassisi, Tonio",
        "Rainforth, Tom",
        "Bateman, Randall J.",
        "Skarlinski, Michael",
        "Rodriques, Samuel G.",
        "Hinks, Michaela M.",
        "White, Andrew D."
      ],
      "year": "2025",
      "abstract": "Data-driven scientific discovery requires iterative cycles of literature search, hypothesis generation, and data analysis. Substantial progress has been made towards AI agents that can automate scientific research, but all such agents remain limited in the number of actions they can take before losing coherence, thus limiting the depth of their findings. Here we present Kosmos, an AI scientist that automates data-driven discovery. Given an open-ended objective and a dataset, Kosmos runs for up to 12 hours performing cycles of parallel data analysis, literature search, and hypothesis generation before synthesizing discoveries into scientific reports. Unlike prior systems, Kosmos uses a structured world model to share information between a data analysis agent and a literature search agent. The world model enables Kosmos to coherently pursue the specified objective over 200 agent rollouts, collectively executing an average of 42,000 lines of code and reading 1,500 papers per run. Kosmos cites all statements in its reports with code or primary literature, ensuring its reasoning is traceable. Independent scientists found 79.4% of statements in Kosmos reports to be accurate, and collaborators reported that a single 20-cycle Kosmos run performed the equivalent of 6 months of their own research time on average. Furthermore, collaborators reported that the number of valuable scientific findings generated scales linearly with Kosmos cycles (tested up to 20 cycles). We highlight seven discoveries made by Kosmos that span metabolomics, materials science, neuroscience, and statistical genetics. Three discoveries independently reproduce findings from preprinted or unpublished manuscripts that were not accessed by Kosmos at runtime, while four make novel contributions to the scientific literature.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251102824M/abstract",
      "keywords": [
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.840Z",
      "publishedAt": "2025-11-23T19:31:44.840Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents"
      ],
      "score": 15
    },
    {
      "id": "37618997",
      "bibcode": "2025arXiv251111301C",
      "title": "EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment",
      "authors": [
        "Cheng, Ruoxi",
        "Ma, Haoxuan",
        "Ma, Teng",
        "Zhang, Hongyi"
      ],
      "year": "2025",
      "abstract": "Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251111301C/abstract",
      "keywords": [
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.841Z",
      "publishedAt": "2025-11-23T19:31:44.840Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "agents"
      ],
      "score": 8.499999992972883
    },
    {
      "id": "37611102",
      "bibcode": "2025arXiv251107678A",
      "title": "AIA Forecaster: Technical Report",
      "authors": [
        "Alur, Rohan",
        "Stadie, Bradly C.",
        "Kang, Daniel",
        "Chen, Ryan",
        "McManus, Matt",
        "Rickert, Michael",
        "Lee, Tyler",
        "Federici, Michael",
        "Zhu, Richard",
        "Fogerty, Dennis",
        "Williamson, Hayley",
        "Lozinski, Nina",
        "Linsky, Aaron",
        "Sekhon, Jasjeet S."
      ],
      "year": "2025",
      "abstract": "This technical report describes the AIA Forecaster, a Large Language Model (LLM)-based system for judgmental forecasting using unstructured data. The AIA Forecaster approach combines three core elements: agentic search over high-quality news sources, a supervisor agent that reconciles disparate forecasts for the same event, and a set of statistical calibration techniques to counter behavioral biases in large language models. On the ForecastBench benchmark (Karger et al., 2024), the AIA Forecaster achieves performance equal to human superforecasters, surpassing prior LLM baselines. In addition to reporting on ForecastBench, we also introduce a more challenging forecasting benchmark sourced from liquid prediction markets. While the AIA Forecaster underperforms market consensus on this benchmark, an ensemble combining AIA Forecaster with market consensus outperforms consensus alone, demonstrating that our forecaster provides additive information. Our work establishes a new state of the art in AI forecasting and provides practical, transferable recommendations for future research. To the best of our knowledge, this is the first work that verifiably achieves expert-level forecasting at scale.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251107678A/abstract",
      "keywords": [
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.841Z",
      "publishedAt": "2025-11-23T19:31:44.841Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "score": 10.5
    },
    {
      "id": "38058638",
      "bibcode": "2025arXiv251114461W",
      "title": "Effective Diversification of Multi-Carousel Book Recommendation",
      "authors": [
        "Wilten, Daniël",
        "Maillette de Buy Wenniger, Gideon",
        "Hommersom, Arjen",
        "Lucassen, Paul",
        "Poortman, Emiel"
      ],
      "year": "2025",
      "abstract": "Using multiple carousels, lists that wrap around and can be scrolled, is the basis for offering content in most contemporary movie streaming platforms. Carousels allow for highlighting different aspects of users' taste, that fall in categories such as genres and authors. However, while carousels offer structure and greater ease of navigation, they alone do not increase diversity in recommendations, while this is essential to keep users engaged. In this work we propose several approaches to effectively increase item diversity within the domain of book recommendations, on top of a collaborative filtering algorithm. These approaches are intended to improve book recommendations in the web catalogs of public libraries. Furthermore, we introduce metrics to evaluate the resulting strategies, and show that the proposed system finds a suitable balance between accuracy and beyond-accuracy aspects.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251114461W/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence",
        "H.3.3"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.841Z",
      "publishedAt": "2025-11-23T19:31:44.841Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "observability"
      ],
      "score": 7.5
    },
    {
      "id": "37619907",
      "bibcode": "2025arXiv251111847S",
      "title": "A Multimodal Manufacturing Safety Chatbot: Knowledge Base Design, Benchmark Development, and Evaluation of Multiple RAG Approaches",
      "authors": [
        "Singh, Ryan",
        "Hamilton, Austin",
        "White, Amanda",
        "Wise, Michael",
        "Yousif, Ibrahim",
        "Carvalho, Arthur",
        "Shan, Zhe",
        "Abrisham Baf, Reza",
        "Mayyas, Mohammad",
        "Cavuoto, Lora A.",
        "Megahed, Fadel M."
      ],
      "year": "2025",
      "abstract": "Ensuring worker safety remains a critical challenge in modern manufacturing environments. Industry 5.0 reorients the prevailing manufacturing paradigm toward more human-centric operations. Using a design science research methodology, we identify three essential requirements for next-generation safety training systems: high accuracy, low latency, and low cost. We introduce a multimodal chatbot powered by large language models that meets these design requirements. The chatbot uses retrieval-augmented generation to ground its responses in curated regulatory and technical documentation. To evaluate our solution, we developed a domain-specific benchmark of expert-validated question and answer pairs for three representative machines: a Bridgeport manual mill, a Haas TL-1 CNC lathe, and a Universal Robots UR5e collaborative robot. We tested 24 RAG configurations using a full-factorial design and assessed them with automated evaluations of correctness, latency, and cost. Our top 2 configurations were then evaluated by ten industry experts and academic researchers. Our results show that retrieval strategy and model configuration have a significant impact on performance. The top configuration (selected for chatbot deployment) achieved an accuracy of 86.66%, an average latency of 10.04 seconds, and an average cost of $0.005 per query. Overall, our work provides three contributions: an open-source, domain-grounded safety training chatbot; a validated benchmark for evaluating AI-assisted safety instruction; and a systematic methodology for designing and assessing AI-enabled instructional and immersive safety training systems for Industry 5.0 environments.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251111847S/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence",
        "Computers and Society"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.841Z",
      "publishedAt": "2025-11-23T19:31:44.841Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "ide"
      ],
      "score": 12
    },
    {
      "id": "37498619",
      "bibcode": "2025arXiv251100739R",
      "title": "A CPU-Centric Perspective on Agentic AI",
      "authors": [
        "Raj, Ritik",
        "Wang, Hong",
        "Krishna, Tushar"
      ],
      "year": "2025",
      "abstract": "Agentic AI frameworks add a decision-making orchestrator embedded with external tools, including web search, Python interpreter, contextual database, and others, on top of monolithic LLMs, turning them from passive text oracles into autonomous problem-solvers that can plan, call tools, remember past steps, and adapt on the fly. This paper aims to characterize and understand the system bottlenecks introduced by agentic AI workloads from a largely overlooked CPU-centric perspective. We first systematically characterize Agentic AI on the basis of orchestrator/decision making component, inference path dynamics and repetitiveness of the agentic flow which directly influences the system-level performance. Thereafter, based on the characterization, we choose five representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow, Langchain and SWE-Agent to profile latency, throughput and energy metrics and demystify the significant impact of CPUs on these metrics relative to GPUs. We observe that - 1. Tool processing on CPUs can take up to 90.6% of the total latency; 2. Agentic throughput gets bottlenecked either by CPU factors - coherence, synchronization and over-subscription of cores or GPU factors - main memory capacity and bandwidth; \\circled{3} CPU dynamic energy consumes up to 44% of the total dynamic energy at large batch sizes. Based on the profiling insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching (CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and heterogeneous agentic workloads respectively to demonstrate the potential to improve the performance, efficiency, and scalability of agentic AI. We achieve up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing benchmark for homogeneous and heterogeneous agentic workloads respectively.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251100739R/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Machine Learning",
        "Multiagent Systems"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.841Z",
      "publishedAt": "2025-11-23T19:31:44.841Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "observability"
      ],
      "score": 11.999999990079365
    },
    {
      "id": "37499248",
      "bibcode": "2025arXiv251101386Y",
      "title": "RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets",
      "authors": [
        "Yusuf Kartal, Muhammed",
        "Kagan Kose, Suha",
        "Sevinç, Korhan",
        "Aktas, Burak"
      ],
      "year": "2025",
      "abstract": "Retrieval-Augmented Generation (RAG) quality depends on many interacting choices across retrieval, ranking, augmentation, prompting, and generation, so optimizing modules in isolation is brittle. We introduce RAGSmith, a modular framework that treats RAG design as an end-to-end architecture search over nine technique families and 46{,}080 feasible pipeline configurations. A genetic search optimizes a scalar objective that jointly aggregates retrieval metrics (recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law, Finance, Medicine, Defense Industry, Computer Science), each with 100 questions spanning factual, interpretation, and long-answer types. RAGSmith finds configurations that consistently outperform naive RAG baseline by +3.8\\% on average (range +1.2\\% to +6.9\\% across domains), with gains up to +12.5\\% in retrieval and +7.5\\% in generation. The search typically explores $\\approx 0.2\\%$ of the space ($\\sim 100$ candidates) and discovers a robust backbone -- vector retrieval plus post-generation reflection/revision -- augmented by domain-dependent choices in expansion, reranking, augmentation, and prompt reordering; passage compression is never selected. Improvement magnitude correlates with question type, with larger gains on factual/long-answer mixes than interpretation-heavy sets. These results provide practical, domain-aware guidance for assembling effective RAG systems and demonstrate the utility of evolutionary search for full-pipeline optimization.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251101386Y/abstract",
      "keywords": [
        "Computation and Language",
        "Artificial Intelligence",
        "Information Retrieval",
        "H.3.3; I.2.7"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.842Z",
      "publishedAt": "2025-11-23T19:31:44.842Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability"
      ],
      "score": 10
    },
    {
      "id": "10.48550/arXiv.2511.07295",
      "bibcode": "2025arXiv251107295S",
      "title": "Hard vs. Noise: Resolving Hard-Noisy Sample Confusion in Recommender Systems via Large Language Models",
      "authors": [
        "Song, Tianrui",
        "Chao, Wen-Shuo",
        "Liu, Hao"
      ],
      "year": "2025",
      "abstract": "Implicit feedback, employed in training recommender systems, unavoidably confronts noise due to factors such as misclicks and position bias. Previous studies have attempted to identify noisy samples through their diverged data patterns, such as higher loss values, and mitigate their influence through sample dropping or reweighting. However, we observed that noisy samples and hard samples display similar patterns, leading to hard-noisy confusion issue. Such confusion is problematic as hard samples are vital for modeling user preferences. To solve this problem, we propose LLMHNI framework, leveraging two auxiliary user-item relevance signals generated by Large Language Models (LLMs) to differentiate hard and noisy samples. LLMHNI obtains user-item semantic relevance from LLM-encoded embeddings, which is used in negative sampling to select hard negatives while filtering out noisy false negatives. An objective alignment strategy is proposed to project LLM-encoded embeddings, originally for general language tasks, into a representation space optimized for user-item relevance modeling. LLMHNI also exploits LLM-inferred logical relevance within user-item interactions to identify hard and noisy samples. These LLM-inferred interactions are integrated into the interaction graph and guide denoising with cross-graph contrastive alignment. To eliminate the impact of unreliable interactions induced by LLM hallucination, we propose a graph contrastive learning strategy that aligns representations from randomly edge-dropped views to suppress unreliable edges. Empirical results demonstrate that LLMHNI significantly improves denoising and recommendation performance.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251107295S/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.381Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 2.022907765950182
    },
    {
      "id": "37612607",
      "bibcode": "2025arXiv251109193Y",
      "title": "Enhancing PIBT via Multi-Action Operations",
      "authors": [
        "Yukhnevich, Egor",
        "Andreychuk, Anton"
      ],
      "year": "2025",
      "abstract": "PIBT is a rule-based Multi-Agent Path Finding (MAPF) solver, widely used as a low-level planner or action sampler in many state-of-the-art approaches. Its primary advantage lies in its exceptional speed, enabling action selection for thousands of agents within milliseconds by considering only the immediate next timestep. However, this short-horizon design leads to poor performance in scenarios where agents have orientation and must perform time-consuming rotation actions. In this work, we present an enhanced version of PIBT that addresses this limitation by incorporating multi-action operations. We detail the modifications introduced to improve PIBT's performance while preserving its hallmark efficiency. Furthermore, we demonstrate how our method, when combined with graph-guidance technique and large neighborhood search optimization, achieves state-of-the-art performance in the online LMAPF-T setting.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251109193Y/abstract",
      "keywords": [
        "Multiagent Systems",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.842Z",
      "publishedAt": "2025-11-23T19:31:44.842Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "score": 14.5
    },
    {
      "id": "37619709",
      "bibcode": "2025arXiv251111646Y",
      "title": "A Deep Learning Model to Predicting Changes in Consumer Attributes for New Line-extended Products",
      "authors": [
        "Yinxing, Li",
        "Ishigaki, Tsukasa"
      ],
      "year": "2025",
      "abstract": "Product line extension is a marketing strategy that enhances a company's sphere of influence. Because excessive line extensions disrupt brand image, only appropriate line extensions based on consumer needs are desirable. Marketers should know the key consumer attributes of the primary customers for new line-extended products before companies enter the market. This paper describes a method for predicting changes in consumer attributes for new line-extended products using a novel deep learning model. The proposed model, Conditional Tabular Variational Auto-Encoder (CTVAE), generates synthetic data from large-scale tabular data of consumers and products. It can provide various implications about effective product line marketing for marketers. The experimental results demonstrate that the CTVAE offers superior prediction performance than existing models. We indicate implications for new products that change containers or flavors for effective product line marketing. The proposed approach has the potential to contribute to avoiding cannibalization and to designing product images and marketing strategies.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251111646Y/abstract",
      "keywords": [
        "Machine Learning",
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.842Z",
      "publishedAt": "2025-11-23T19:31:44.842Z",
      "libraryId": "ads-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "score": 7.5
    },
    {
      "id": "38075060",
      "bibcode": "2025arXiv251116278S",
      "title": "\"To Survive, I Must Defect\": Jailbreaking LLMs via the Game-Theory Scenarios",
      "authors": [
        "Sun, Zhen",
        "Zhang, Zongmin",
        "Liang, Deqi",
        "Sun, Han",
        "Liu, Yule",
        "Shen, Yun",
        "Gao, Xiangshan",
        "Yang, Yilong",
        "Liu, Shuai",
        "Yue, Yutao",
        "He, Xinlei"
      ],
      "year": "2025",
      "abstract": "As LLMs become more common, non-expert users can pose risks, prompting extensive research into jailbreak attacks. However, most existing black-box jailbreak attacks rely on hand-crafted heuristics or narrow search spaces, which limit scalability. Compared with prior attacks, we propose Game-Theory Attack (GTA), an scalable black-box jailbreak framework. Concretely, we formalize the attacker's interaction against safety-aligned LLMs as a finite-horizon, early-stoppable sequential stochastic game, and reparameterize the LLM's randomized outputs via quantal response. Building on this, we introduce a behavioral conjecture \"template-over-safety flip\": by reshaping the LLM's effective objective through game-theoretic scenarios, the originally safety preference may become maximizing scenario payoffs within the template, which weakens safety constraints in specific contexts. We validate this mechanism with classical game such as the disclosure variant of the Prisoner's Dilemma, and we further introduce an Attacker Agent that adaptively escalates pressure to increase the ASR. Experiments across multiple protocols and datasets show that GTA achieves over 95% ASR on LLMs such as Deepseek-R1, while maintaining efficiency. Ablations over components, decoding, multilingual settings, and the Agent's core model confirm effectiveness and generalization. Moreover, scenario scaling studies further establish scalability. GTA also attains high ASR on other game-theoretic scenarios, and one-shot LLM-generated variants that keep the model mechanism fixed while varying background achieve comparable ASR. Paired with a Harmful-Words Detection Agent that performs word-level insertions, GTA maintains high ASR while lowering detection under prompt-guard models. Beyond benchmarks, GTA jailbreaks real-world LLM applications and reports a longitudinal safety monitoring of popular HuggingFace LLMs.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251116278S/abstract",
      "keywords": [
        "Cryptography and Security",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.843Z",
      "publishedAt": "2025-11-23T19:31:44.842Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "agents",
        "observability"
      ],
      "score": 10.499999991319445
    },
    {
      "id": "37571618",
      "bibcode": "2025arXiv251104481K",
      "title": "Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis",
      "authors": [
        "Krupp, Lars",
        "Geißler, Daniel",
        "Banwari, Vishal",
        "Lukowicz, Paul",
        "Karolus, Jakob"
      ],
      "year": "2025",
      "abstract": "Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful agentic systems pushing the boundaries of Large Language Models (LLM). They can autonomously interact with the internet at the user's behest, such as navigating websites, filling search masks, and comparing price lists. Though web agent research is thriving, induced sustainability issues remain largely unexplored. To highlight the urgency of this issue, we provide an initial exploration of the energy and $CO_2$ cost associated with web agents from both a theoretical -via estimation- and an empirical perspective -by benchmarking. Our results show how different philosophies in web agent creation can severely impact the associated expended energy, and that more energy consumed does not necessarily equate to better results. We highlight a lack of transparency regarding disclosing model parameters and processes used for some web agents as a limiting factor when estimating energy consumption. Our work contributes towards a change in thinking of how we evaluate web agents, advocating for dedicated metrics measuring energy consumption in benchmarks.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251104481K/abstract",
      "keywords": [
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.843Z",
      "publishedAt": "2025-11-23T19:31:44.843Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "observability"
      ],
      "score": 15.5
    },
    {
      "id": "37609482",
      "bibcode": "2025arXiv251106142T",
      "title": "MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning",
      "authors": [
        "Tang, Sizhe",
        "Chen, Jiayu",
        "Lan, Tian"
      ],
      "year": "2025",
      "abstract": "Monte Carlo Tree Search (MCTS), which leverages Upper Confidence Bound for Trees (UCTs) to balance exploration and exploitation through randomized sampling, is instrumental to solving complex planning problems. However, for multi-agent planning, MCTS is confronted with a large combinatorial action space that often grows exponentially with the number of agents. As a result, the branching factor of MCTS during tree expansion also increases exponentially, making it very difficult to efficiently explore and exploit during tree search. To this end, we propose MALinZero, a new approach to leverage low-dimensional representational structures on joint-action returns and enable efficient MCTS in complex multi-agent planning. Our solution can be viewed as projecting the joint-action returns into the low-dimensional space representable using a contextual linear bandit problem formulation. We solve the contextual linear bandit problem with convex and $μ$-smooth loss functions -- in order to place more importance on better joint actions and mitigate potential representational limitations -- and derive a linear Upper Confidence Bound applied to trees (LinUCT) to enable novel multi-agent exploration and exploitation in the low-dimensional space. We analyze the regret of MALinZero for low-dimensional reward functions and propose an $(1-\\tfrac1e)$-approximation algorithm for the joint action selection by maximizing a sub-modular objective. MALinZero demonstrates state-of-the-art performance on multi-agent benchmarks such as matrix games, SMAC, and SMACv2, outperforming both model-based and model-free multi-agent reinforcement learning baselines with faster learning speed and better performance.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251106142T/abstract",
      "keywords": [
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.843Z",
      "publishedAt": "2025-11-23T19:31:44.843Z",
      "libraryId": "ads-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 16
    },
    {
      "id": "37571392",
      "bibcode": "2025arXiv251104247T",
      "title": "On the Brittleness of CLIP Text Encoders",
      "authors": [
        "Tran, Allie",
        "Rossetto, Luca"
      ],
      "year": "2025",
      "abstract": "Multimodal co-embedding models, especially CLIP, have advanced the state of the art in zero-shot classification and multimedia information retrieval in recent years by aligning images and text in a shared representation space. However, such modals trained on a contrastive alignment can lack stability towards small input perturbations. Especially when dealing with manually expressed queries, minor variations in the query can cause large differences in the ranking of the best-matching results. In this paper, we present a systematic analysis of the effect of multiple classes of non-semantic query perturbations in an multimedia information retrieval scenario. We evaluate a diverse set of lexical, syntactic, and semantic perturbations across multiple CLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video collection. Across models, we find that syntactic and semantic perturbations drive the largest instabilities, while brittleness is concentrated in trivial surface edits such as punctuation and case. Our results highlight robustness as a critical dimension for evaluating vision-language models beyond benchmark accuracy.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251104247T/abstract",
      "keywords": [
        "Multimedia",
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-23T19:31:44.843Z",
      "publishedAt": "2025-11-23T19:31:44.843Z",
      "libraryId": "ads-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 10
    },
    {
      "id": "2025arXiv251106179W",
      "bibcode": "2025arXiv251106179W",
      "title": "MemoriesDB: A Temporal-Semantic-Relational Database for Long-Term Agent Memory / Modeling Experience as a Graph of Temporal-Semantic Surfaces",
      "authors": [
        "Ward, Joel"
      ],
      "year": "2025",
      "abstract": "We introduce MemoriesDB, a unified data architecture designed to avoid decoherence across time, meaning, and relation in long-term computational memory. Each memory is a time-semantic-relational entity-a structure that simultaneously encodes when an event occurred, what it means, and how it connects to other events. Built initially atop PostgreSQL with pgvector extensions, MemoriesDB combines the properties of a time-series datastore, a vector database, and a graph system within a single append-only schema. Each memory is represented as a vertex uniquely labeled by its microsecond timestamp and accompanied by low- and high-dimensional normalized embeddings that capture semantic context. Directed edges between memories form labeled relations with per-edge metadata, enabling multiple contextual links between the same vertices. Together these constructs form a time-indexed stack of temporal-semantic surfaces, where edges project as directional arrows in a 1+1-dimensional similarity field, tracing the evolution of meaning through time while maintaining cross-temporal coherence. This formulation supports efficient time-bounded retrieval, hybrid semantic search, and lightweight structural reasoning in a single query path. A working prototype demonstrates scalable recall and contextual reinforcement using standard relational infrastructure, and we discuss extensions toward a columnar backend, distributed clustering, and emergent topic modeling.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251106179W/abstract",
      "keywords": [
        "Databases",
        "Artificial Intelligence",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.380Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "observability"
      ],
      "score": 2.022907765950182
    },
    {
      "id": "arXiv:2511.09219",
      "bibcode": "2025arXiv251109219S",
      "title": "Planning in Branch-and-Bound: Model-Based Reinforcement Learning for Exact Combinatorial Optimization",
      "authors": [
        "Strang, Paul",
        "Alès, Zacharie",
        "Bissuel, Côme",
        "Kedad-Sidhoum, Safia",
        "Rachelson, Emmanuel"
      ],
      "year": "2025",
      "abstract": "Mixed-Integer Linear Programming (MILP) lies at the core of many real-world combinatorial optimization (CO) problems, traditionally solved by branch-and-bound (B&amp;B). A key driver influencing B&amp;B solvers efficiency is the variable selection heuristic that guides branching decisions. Looking to move beyond static, hand-crafted heuristics, recent work has explored adapting traditional reinforcement learning (RL) algorithms to the B&amp;B setting, aiming to learn branching strategies tailored to specific MILP distributions. In parallel, RL agents have achieved remarkable success in board games, a very specific type of combinatorial problems, by leveraging environment simulators to plan via Monte Carlo Tree Search (MCTS). Building on these developments, we introduce Plan-and-Branch-and-Bound (PlanB&amp;B), a model-based reinforcement learning (MBRL) agent that leverages a learned internal model of the B&amp;B dynamics to discover improved branching strategies. Computational experiments empirically validate our approach, with our MBRL branching agent outperforming previous state-of-the-art RL methods across four standard MILP benchmarks.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251109219S/abstract",
      "keywords": [
        "Machine Learning"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.381Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 2.6972103546002426
    },
    {
      "id": "arXiv:2511.01236",
      "bibcode": "2025arXiv251101236Z",
      "title": "Don't Just Search, Understand: Semantic Path Planning Agent for Spherical Tensegrity Robots in Unknown Environments",
      "authors": [
        "Zhang, Junwen",
        "Liu, Changyue",
        "Fu, Pengqi",
        "Guo, Xiang",
        "Shi, Ye",
        "Liang, Xudong",
        "Wang, Zhijian",
        "Ma, Hanzhi"
      ],
      "year": "2025",
      "abstract": "Endowed with inherent dynamical properties that grant them remarkable ruggedness and adaptability, spherical tensegrity robots stand as prototypical examples of hybrid softrigid designs and excellent mobile platforms. However, path planning for these robots in unknown environments presents a significant challenge, requiring a delicate balance between efficient exploration and robust planning. Traditional path planners, which treat the environment as a geometric grid, often suffer from redundant searches and are prone to failure in complex scenarios due to their lack of semantic understanding. To overcome these limitations, we reframe path planning in unknown environments as a semantic reasoning task. We introduce a Semantic Agent for Tensegrity robots (SATPlanner) driven by a Large Language Model (LLM). SATPlanner leverages high-level environmental comprehension to generate efficient and reliable planning strategies.At the core of SATPlanner is an Adaptive Observation Window mechanism, inspired by the \"fast\" and \"slow\" thinking paradigms of LLMs. This mechanism dynamically adjusts the perceptual field of the agent: it narrows for rapid traversal of open spaces and expands to reason about complex obstacle configurations. This allows the agent to construct a semantic belief of the environment, enabling the search space to grow only linearly with the path length (O(L)) while maintaining path quality. We extensively evaluate SATPlanner in 1,000 simulation trials, where it achieves a 100% success rate, outperforming other real-time planning algorithms. Critically, SATPlanner reduces the search space by 37.2% compared to the A* algorithm while achieving comparable, near-optimal path lengths. Finally, the practical feasibility of SATPlanner is validated on a physical spherical tensegrity robot prototype.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251101236Z/abstract",
      "keywords": [
        "Robotics"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.381Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "agents"
      ],
      "score": 1.6857564716251516
    },
    {
      "id": "10.48550/arXiv.2511.15443",
      "bibcode": "2025arXiv251115443X",
      "title": "CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in Short-Video Search",
      "authors": [
        "Xie, Ao",
        "Chen, Jiahui",
        "Zhu, Quanzhi",
        "Jiang, Xiaoze",
        "Qin, Zhiheng",
        "Yu, Enyun",
        "Li, Han"
      ],
      "year": "2025",
      "abstract": "Dense retrieval has become a foundational paradigm in modern search systems, especially on short-video platforms. However, most industrial systems adopt a self-reinforcing training pipeline that relies on historically exposed user interactions for supervision. This paradigm inevitably leads to a filter bubble effect, where potentially relevant but previously unseen content is excluded from the training signal, biasing the model toward narrow and conservative retrieval. In this paper, we present CroPS (Cross-Perspective Positive Samples), a novel retrieval data engine designed to alleviate this problem by introducing diverse and semantically meaningful positive examples from multiple perspectives. CroPS enhances training with positive signals derived from user query reformulation behavior (query-level), engagement data in recommendation streams (system-level), and world knowledge synthesized by large language models (knowledge-level). To effectively utilize these heterogeneous signals, we introduce a Hierarchical Label Assignment (HLA) strategy and a corresponding H-InfoNCE loss that together enable fine-grained, relevance-aware optimization. Extensive experiments conducted on Kuaishou Search, a large-scale commercial short-video search platform, demonstrate that CroPS significantly outperforms strong baselines both offline and in live A/B tests, achieving superior retrieval performance and reducing query reformulation rates. CroPS is now fully deployed in Kuaishou Search, serving hundreds of millions of users daily.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251115443X/abstract",
      "keywords": [
        "Information Retrieval",
        "Computation and Language"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.381Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 2.022907765950182
    },
    {
      "id": "10.48550/arXiv.2511.10589",
      "bibcode": "2025arXiv251110589L",
      "title": "A Fast Earth-scattering Formalism for Light Dark Matter with Dark Photon Mediators",
      "authors": [
        "Lantero-Barreda, Agustín",
        "Centeno, Carlos",
        "Kavanagh, Bradley J.",
        "Castelló-Mor, Nuria"
      ],
      "year": "2025",
      "abstract": "While Dark Matter (DM) is typically assumed to interact only very weakly with the particles of the Standard Model, many direct detection experiments are currently exploring regions of parameter space where DM can have a large scattering cross section. In this scenario, DM may scatter in the atmosphere and Earth before reaching the detector, leading to a distortion of the DM flux and a daily modulation of the signal rate as the detector is shielded by more or less of the Earth at different times of day. This modulation is a distinctive signature of strongly-interacting DM and provides a powerful method of discriminating against time-independent backgrounds. However, the calculation of these Earth-scattering effects by Monte Carlo methods is computationally intensive, inhibiting a systematic exploration of the DM parameter space. Here, we present a semi-analytic formalism for calculating Earth-scattering effects, for models of MeV-mass DM which interacts via a dark photon mediator, and release the associated code Verne2. This formalism assumes that DM travels along straight-line trajectories until it scatters and is reflected back along its incoming path, along us to taking into account the affects of both attenuation and reflection in the Earth. We compare this formalism with the results of full Monte Carlo simulations for cross sections within reach of current and future DM-electron scattering searches. We find that Verne2 is accurate to better than 10-30%, making it suitable for performing signal modeling in the search for daily modulation, while reducing the computational cost by a factor of $\\sim10^4$ compared to full Monte Carlo simulations.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251110589L/abstract",
      "keywords": [
        "High Energy Physics - Phenomenology",
        "Cosmology and Nongalactic Astrophysics"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.382Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "score": 1.2643173526736282
    },
    {
      "id": "arXiv:2511.07943",
      "bibcode": "2025arXiv251107943X",
      "title": "Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction",
      "authors": [
        "Xu, Jun",
        "Du, Xinkai",
        "Ao, Yu",
        "Zhao, Peilong",
        "Li, Yang",
        "Zhong, Ling",
        "Yuan, Lin",
        "Bo, Zhongpu",
        "Wang, Xiaorui",
        "Sun, Mengshu",
        "Gui, Zhengke",
        "Zhang, Dalong",
        "Wang, Zhaoyang",
        "Wang, Qiwei",
        "Hou, Yangyang",
        "Yin, Zhiying",
        "Wang, Haofen",
        "Chen, Huajun",
        "Liang, Lei",
        "Zhou, Jun"
      ],
      "year": "2025",
      "abstract": "Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251107943X/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Computation and Language"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.382Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 1.5171808232083537
    },
    {
      "id": "10.48550/arXiv.2511.06077",
      "bibcode": "2025arXiv251106077G",
      "title": "Make It Long, Keep It Fast: End-to-End 10k-Sequence Modeling at Billion Scale on Douyin",
      "authors": [
        "Guan, Lin",
        "Yang, Jia-Qi",
        "Zhao, Zhishan",
        "Zhang, Beichuan",
        "Sun, Bo",
        "Luo, Xuanyuan",
        "Ni, Jinan",
        "Li, Xiaowen",
        "Qi, Yuhang",
        "Fan, Zhifang",
        "Wang, Hangyu",
        "Chen, Qiwei",
        "Cheng, Yi",
        "Zhang, Feng",
        "Yang, Xiao"
      ],
      "year": "2025",
      "abstract": "Short-video recommenders such as Douyin must exploit extremely long user histories without breaking latency or cost budgets. We present an end-to-end system that scales long-sequence modeling to 10k-length histories in production. First, we introduce Stacked Target-to-History Cross Attention (STCA), which replaces history self-attention with stacked cross-attention from the target to the history, reducing complexity from quadratic to linear in sequence length and enabling efficient end-to-end training. Second, we propose Request Level Batching (RLB), a user-centric batching scheme that aggregates multiple targets for the same user/request to share the user-side encoding, substantially lowering sequence-related storage, communication, and compute without changing the learning objective. Third, we design a length-extrapolative training strategy -- train on shorter windows, infer on much longer ones -- so the model generalizes to 10k histories without additional training cost. Across offline and online experiments, we observe predictable, monotonic gains as we scale history length and model capacity, mirroring the scaling law behavior observed in large language models. Deployed at full traffic on Douyin, our system delivers significant improvements on key engagement metrics while meeting production latency, demonstrating a practical path to scaling end-to-end long-sequence recommendation to the 10k regime.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251106077G/abstract",
      "keywords": [
        "Machine Learning",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.382Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability"
      ],
      "score": 2.1914834113009554
    },
    {
      "id": "2025arXiv251107581V",
      "bibcode": "2025arXiv251107581V",
      "title": "Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models",
      "authors": [
        "Vijay, Supriti",
        "Priyanshu, Aman",
        "Vellore, Anu",
        "Saglam, Baturay",
        "Karbasi, Amin"
      ],
      "year": "2025",
      "abstract": "Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251107581V/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Computation and Language",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.382Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 2.022907764277805
    },
    {
      "id": "2025arXiv251112597G",
      "bibcode": "2025arXiv251112597G",
      "title": "MindRec: A Diffusion-driven Coarse-to-Fine Paradigm for Generative Recommendation",
      "authors": [
        "Gao, Mengyao",
        "Gao, Chongming",
        "Liu, Haoyan",
        "Cai, Qingpeng",
        "Jiang, Peng",
        "Chen, Jiajia",
        "Yuan, Shuai",
        "He, Xiangnan"
      ],
      "year": "2025",
      "abstract": "Recent advancements in large language model-based recommendation systems often represent items as text or semantic IDs and generate recommendations in an auto-regressive manner. However, due to the left-to-right greedy decoding strategy and the unidirectional logical flow, such methods often fail to produce globally optimal recommendations. In contrast, human reasoning does not follow a rigid left-to-right sequence. Instead, it often begins with keywords or intuitive insights, which are then refined and expanded. Inspired by this fact, we propose MindRec, a diffusion-driven coarse-to-fine generative paradigm that emulates human thought processes. Built upon a diffusion language model, MindRec departs from auto-regressive generation by leveraging a masked diffusion process to reconstruct items in a flexible, non-sequential manner. Particularly, our method first generates key tokens that reflect user preferences, and then expands them into the complete item, enabling adaptive and human-like generation. To further emulate the structured nature of human decision-making, we organize items into a hierarchical category tree. This structure guides the model to first produce the coarse-grained category and then progressively refine its selection through finer-grained subcategories before generating the specific item. To mitigate the local optimum problem inherent in greedy decoding, we design a novel beam search algorithm, Diffusion Beam Search, tailored for our mind-inspired generation paradigm. Experimental results demonstrate that MindRec yields a 9.5\\% average improvement in top-1 accuracy over state-of-the-art methods, highlighting its potential to enhance recommendation performance. The implementation is available via https://github.com/Mr-Peach0301/MindRec.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112597G/abstract",
      "keywords": [
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.382Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 1.5171808232083537
    },
    {
      "id": "10.48550/arXiv.2511.16478",
      "bibcode": "2025arXiv251116478E",
      "title": "Music Recommendation with Large Language Models: Challenges, Opportunities, and Evaluation",
      "authors": [
        "Epure, Elena V.",
        "Deldjoo, Yashar",
        "Sguerra, Bruno",
        "Schedl, Markus",
        "Moussallam, Manuel"
      ],
      "year": "2025",
      "abstract": "Music Recommender Systems (MRS) have long relied on an information-retrieval framing, where progress is measured mainly through accuracy on retrieval-oriented subtasks. While effective, this reductionist paradigm struggles to address the deeper question of what makes a good recommendation, and attempts to broaden evaluation, through user studies or fairness analyses, have had limited impact. The emergence of Large Language Models (LLMs) disrupts this framework: LLMs are generative rather than ranking-based, making standard accuracy metrics questionable. They also introduce challenges such as hallucinations, knowledge cutoffs, non-determinism, and opaque training data, rendering traditional train/test protocols difficult to interpret. At the same time, LLMs create new opportunities, enabling natural-language interaction and even allowing models to act as evaluators. This work argues that the shift toward LLM-driven MRS requires rethinking evaluation. We first review how LLMs reshape user modeling, item modeling, and natural-language recommendation in music. We then examine evaluation practices from NLP, highlighting methodologies and open challenges relevant to MRS. Finally, we synthesize insights-focusing on how LLM prompting applies to MRS, to outline a structured set of success and risk dimensions. Our goal is to provide the MRS community with an updated, pedagogical, and cross-disciplinary perspective on evaluation.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251116478E/abstract",
      "keywords": [
        "Information Retrieval",
        "Computation and Language"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.382Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability"
      ],
      "score": 2.022907764277805
    },
    {
      "id": "2025arXiv251102770C",
      "bibcode": "2025arXiv251102770C",
      "title": "Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval",
      "authors": [
        "Chen, Hung-Ting",
        "Liu, Xiang",
        "Ravfogel, Shauli",
        "Choi, Eunsol"
      ],
      "year": "2025",
      "abstract": "Most text retrievers generate \\emph{one} query vector to retrieve relevant documents. Yet, the conditional distribution of relevant documents for the query may be multimodal, e.g., representing different interpretations of the query. We first quantify the limitations of existing retrievers. All retrievers we evaluate struggle more as the distance between target document embeddings grows. To address this limitation, we develop a new retriever architecture, \\emph{A}utoregressive \\emph{M}ulti-\\emph{E}mbedding \\emph{R}etriever (AMER). Our model autoregressively generates multiple query vectors, and all the predicted query vectors are used to retrieve documents from the corpus. We show that on the synthetic vectorized data, the proposed method could capture multiple target distributions perfectly, showing 4x better performance than single embedding model. We also fine-tune our model on real-world multi-answer retrieval datasets and evaluate in-domain. AMER presents 4 and 21\\% relative gains over single-embedding baselines on two datasets we evaluate on. Furthermore, we consistently observe larger gains on the subset of dataset where the embeddings of the target documents are less similar to each other. We demonstrate the potential of using a multi-query vector retriever and open up a new direction for future work.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251102770C/abstract",
      "keywords": [
        "Computation and Language",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.382Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 1.3486051761852034
    },
    {
      "id": "10.48550/arXiv.2511.04237",
      "bibcode": "2025arXiv251104237L",
      "title": "Denoised Recommendation Model with Collaborative Signal Decoupling",
      "authors": [
        "Li, Zefeng",
        "Yang, Ning"
      ],
      "year": "2025",
      "abstract": "Although the collaborative filtering (CF) algorithm has achieved remarkable performance in recommendation systems, it suffers from suboptimal recommendation performance due to noise in the user-item interaction matrix. Numerous noise-removal studies have improved recommendation models, but most existing approaches conduct denoising on a single graph. This may cause attenuation of collaborative signals: removing edges between two nodes can interrupt paths between other nodes, weakening path-dependent collaborative information. To address these limitations, this study proposes a novel GNN-based CF model called DRCSD for denoising unstable interactions. DRCSD includes two core modules: a collaborative signal decoupling module (decomposes signals into distinct orders by structural characteristics) and an order-wise denoising module (performs targeted denoising on each order). Additionally, the information aggregation mechanism of traditional GNN-based CF models is modified to avoid cross-order signal interference until the final pooling operation. Extensive experiments on three public real-world datasets show that DRCSD has superior robustness against unstable interactions and achieves statistically significant performance improvements in recommendation accuracy metrics compared to state-of-the-art baseline models.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251104237L/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.382Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "observability"
      ],
      "score": 1.2643173526736282
    },
    {
      "id": "arXiv:2511.05000",
      "bibcode": "2025arXiv251105000K",
      "title": "Query Generation Pipeline with Enhanced Answerability Assessment for Financial Information Retrieval",
      "authors": [
        "Kim, Hyunkyu",
        "Yoo, Yeeun",
        "Kwak, Youngjun"
      ],
      "year": "2025",
      "abstract": "As financial applications of large language models (LLMs) gain attention, accurate Information Retrieval (IR) remains crucial for reliable AI services. However, existing benchmarks fail to capture the complex and domain-specific information needs of real-world banking scenarios. Building domain-specific IR benchmarks is costly and constrained by legal restrictions on using real customer data. To address these challenges, we propose a systematic methodology for constructing domain-specific IR benchmarks through LLM-based query generation. As a concrete implementation of this methodology, our pipeline combines single and multi-document query generation with an enhanced and reasoning-augmented answerability assessment method, achieving stronger alignment with human judgments than prior approaches. Using this methodology, we construct KoBankIR, comprising 815 queries derived from 204 official banking documents. Our experiments show that existing retrieval models struggle with the complex multi-document queries in KoBankIR, demonstrating the value of our systematic approach for domain-specific benchmark construction and underscoring the need for improved retrieval techniques in financial domains.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251105000K/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.382Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 1.5171808232083537
    },
    {
      "id": "arXiv:2511.12645",
      "bibcode": "2025arXiv251112645L",
      "title": "BeautyGuard: Designing a Multi-Agent Roundtable System for Proactive Beauty Tech Compliance through Stakeholder Collaboration",
      "authors": [
        "Li, Junwei",
        "Wang, Wenqing",
        "Mao, Huiliu",
        "Ni, Jiazhe",
        "Xiong, Zeyu"
      ],
      "year": "2025",
      "abstract": "As generative AI enters enterprise workflows, ensuring compliance with legal, ethical, and reputational standards becomes a pressing challenge. In beauty tech, where biometric and personal data are central, traditional reviews are often manual, fragmented, and reactive. To examine these challenges, we conducted a formative study with six experts (four IT managers, two legal managers) at a multinational beauty company. The study revealed pain points in rule checking, precedent use, and the lack of proactive guidance. Motivated by these findings, we designed a multi-agent \"roundtable\" system powered by a large language model. The system assigns role-specialized agents for legal interpretation, checklist review, precedent search, and risk mitigation, synthesizing their perspectives into structured compliance advice. We evaluated the prototype with the same experts using System Usability Scale(SUS), The Official NASA Task Load Index(NASA-TLX), and interviews. Results show exceptional usability (SUS: 77.5/100) and minimal cognitive workload, with three key findings: (1) multi-agent systems can preserve tacit knowledge into standardized workflows, (2) information augmentation achieves higher acceptance than decision automation, and (3) successful enterprise AI should mirror organizational structures. This work contributes design principles for human-AI collaboration in compliance review, with broader implications for regulated industries beyond beauty tech.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112645L/abstract",
      "keywords": [
        "Human-Computer Interaction"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.382Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "governance"
      ],
      "score": 3.5400885874861587
    },
    {
      "id": "10.48550/arXiv.2511.12947",
      "bibcode": "2025arXiv251112947J",
      "title": "A Plug-and-Play Spatially-Constrained Representation Enhancement Framework for Local-Life Recommendation",
      "authors": [
        "Jiang, Hao",
        "Wang, Guoquan",
        "Yu, Sheng",
        "Zeng, Yang",
        "Zeng, Wencong",
        "Zhou, Guorui"
      ],
      "year": "2025",
      "abstract": "Local-life recommendation have witnessed rapid growth, providing users with convenient access to daily essentials. However, this domain faces two key challenges: (1) spatial constraints, driven by the requirements of the local-life scenario, where items are usually shown only to users within a limited geographic area, indirectly reducing their exposure probability; and (2) long-tail sparsity, where few popular items dominate user interactions, while many high-quality long-tail items are largely overlooked due to imbalanced interaction opportunities. Existing methods typically adopt a user-centric perspective, such as modeling spatial user preferences or enhancing long-tail representations with collaborative filtering signals. However, we argue that an item-centric perspective is more suitable for this domain, focusing on enhancing long-tail items representation that align with the spatially-constrained characteristics of local lifestyle services. To tackle this issue, we propose ReST, a Plug-And-Play Spatially-Constrained Representation Enhancement Framework for Long-Tail Local-Life Recommendation. Specifically, we first introduce a Meta ID Warm-up Network, which initializes fundamental ID representations by injecting their basic attribute-level semantic information. Subsequently, we propose a novel Spatially-Constrained ID Representation Enhancement Network (SIDENet) based on contrastive learning, which incorporates two efficient strategies: a spatially-constrained hard sampling strategy and a dynamic representation alignment strategy. This design adaptively identifies weak ID representations based on their attribute-level information during training. It additionally enhances them by capturing latent item relationships within the spatially-constrained characteristics of local lifestyle services, while preserving compatibility with popular items.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112947J/abstract",
      "keywords": [
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.383Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "score": 1.264317351628392
    },
    {
      "id": "arXiv:2511.12449",
      "bibcode": "2025arXiv251112449N",
      "title": "MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding",
      "authors": [
        "Nie, Zhanheng",
        "Fu, Chenghan",
        "Zhang, Daoze",
        "Wu, Junxian",
        "Guan, Wanxian",
        "Wang, Pengjie",
        "Xu, Jian",
        "Zheng, Bo"
      ],
      "year": "2025",
      "abstract": "The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112449N/abstract",
      "keywords": [
        "Computer Vision and Pattern Recognition",
        "Artificial Intelligence",
        "Information Retrieval",
        "Machine Learning"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.383Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 1.6857564688378561
    },
    {
      "id": "arXiv:2511.12949",
      "bibcode": "2025arXiv251112949F",
      "title": "Can We Predict the Next Question? A Collaborative Filtering Approach to Modeling User Behavior",
      "authors": [
        "Fu, Bokang",
        "Wang, Jiahao",
        "Liu, Xiaojing",
        "Liu, Yuli"
      ],
      "year": "2025",
      "abstract": "In recent years, large language models (LLMs) have excelled in language understanding and generation, powering advanced dialogue and recommendation systems. However, a significant limitation persists: these systems often model user preferences statically, failing to capture the dynamic and sequential nature of interactive behaviors. The sequence of a user's historical questions provides a rich, implicit signal of evolving interests and cognitive patterns, yet leveraging this temporal data for predictive tasks remains challenging due to the inherent disconnect between language modeling and behavioral sequence modeling. To bridge this gap, we propose a Collaborative Filtering-enhanced Question Prediction (CFQP) framework. CFQP dynamically models evolving user-question interactions by integrating personalized memory modules with graph-based preference propagation. This dual mechanism allows the system to adaptively learn from user-specific histories while refining predictions through collaborative signals from similar users. Experimental results demonstrate that our approach effectively generates agents that mimic real-user questioning patterns, highlighting its potential for building proactive and adaptive dialogue systems.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112949F/abstract",
      "keywords": [
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.383Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 2.69721035014057
    },
    {
      "id": "10.48550/arXiv.2511.12850",
      "bibcode": "2025arXiv251112850M",
      "title": "Quantifying consistency and accuracy of Latent Dirichlet Allocation",
      "authors": [
        "Magsarjav, Saranzaya",
        "Humphries, Melissa",
        "Tuke, Jonathan",
        "Mitchell, Lewis"
      ],
      "year": "2025",
      "abstract": "Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112850M/abstract",
      "keywords": [
        "Computation and Language"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.383Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 1.5171808219540706
    },
    {
      "id": "arXiv:2511.12159",
      "bibcode": "2025arXiv251112159Z",
      "title": "CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic",
      "authors": [
        "Zhang, Yaocheng",
        "Huang, Haohuan",
        "Song, Zijun",
        "Zhu, Yuanheng",
        "Zhang, Qichao",
        "Zhao, Zijie",
        "Zhao, Dongbin"
      ],
      "year": "2025",
      "abstract": "Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112159Z/abstract",
      "keywords": [
        "Computation and Language"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.383Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "governance"
      ],
      "score": 3.1186494673500342
    },
    {
      "id": "arXiv:2511.03994",
      "bibcode": "2025arXiv251103994D",
      "title": "HART: A Hybrid Addressing Scheme for Self-Balancing Binary Search Trees in Phase Change Memory (PCM)",
      "authors": [
        "Desai, Mahek",
        "Rumale, Apoorva",
        "Asadinia, Marjan"
      ],
      "year": "2025",
      "abstract": "As DRAM and other transistor-based memory technologies approach their scalability limits, alternative storage solutions like Phase-Change Memory (PCM) are gaining attention for their scalability, fast access times, and zero leakage power. However, current memory-intensive algorithms, especially those used in big data systems, often overlook PCM's endurance limitations (10^6 to 10^8 writes before degradation) and write asymmetry. Self-balancing binary search trees (BSTs), which are widely used for large-scale data management, were developed without considering PCM's unique properties, leading to potential performance degradation. This paper introduces HART, a novel hybrid addressing scheme for self-balancing BSTs, designed to optimize PCM's characteristics. By combining DFATGray code addressing for deeper nodes with linear addressing for shallower nodes, HART balances reduced bit flips during frequent rotations at deeper levels with computational simplicity at shallow levels. Experimental results on PCM-aware AVL trees demonstrate significant improvements in performance, with a reduction in bit flips leading to enhanced endurance, increased lifetime, and lower write energy and latency. Notably, these benefits are achieved without imposing substantial computational overhead, making HART an efficient solution for big data applications.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251103994D/abstract",
      "keywords": [
        "Data Structures and Algorithms"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.383Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 1.6857564688378561
    },
    {
      "id": "2025arXiv251105919F",
      "bibcode": "2025arXiv251105919F",
      "title": "Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs",
      "authors": [
        "Fastowski, Alina",
        "Prenkaj, Bardh",
        "Li, Yuxiao",
        "Kasneci, Gjergji"
      ],
      "year": "2025",
      "abstract": "LLMs are now an integral part of information retrieval. As such, their role as question answering chatbots raises significant concerns due to their shown vulnerability to adversarial man-in-the-middle (MitM) attacks. Here, we propose the first principled attack evaluation on LLM factual memory under prompt injection via Xmera, our novel, theory-grounded MitM framework. By perturbing the input given to \"victim\" LLMs in three closed-book and fact-based QA settings, we undermine the correctness of the responses and assess the uncertainty of their generation process. Surprisingly, trivial instruction-based attacks report the highest success rate (up to ~85.3%) while simultaneously having a high uncertainty for incorrectly answered questions. To provide a simple defense mechanism against Xmera, we train Random Forest classifiers on the response uncertainty levels to distinguish between attacked and unattacked queries (average AUC of up to ~96%). We believe that signaling users to be cautious about the answers they receive from black-box and potentially corrupt LLMs is a first checkpoint toward user cyberspace safety.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251105919F/abstract",
      "keywords": [
        "Cryptography and Security",
        "Artificial Intelligence",
        "Computation and Language"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.383Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 2.360059056372999
    },
    {
      "id": "arXiv:2511.05966",
      "bibcode": "2025arXiv251105966L",
      "title": "Commonality in Few: Few-Shot Multimodal Anomaly Detection via Hypergraph-Enhanced Memory",
      "authors": [
        "Lin, Yuxuan",
        "Yan, Hanjing",
        "Tong, Xuan",
        "Chang, Yang",
        "Wang, Huanzhen",
        "Zhou, Ziheng",
        "Gao, Shuyong",
        "Wang, Yan",
        "Zhang, Wenqiang"
      ],
      "year": "2025",
      "abstract": "Few-shot multimodal industrial anomaly detection is a critical yet underexplored task, offering the ability to quickly adapt to complex industrial scenarios. In few-shot settings, insufficient training samples often fail to cover the diverse patterns present in test samples. This challenge can be mitigated by extracting structural commonality from a small number of training samples. In this paper, we propose a novel few-shot unsupervised multimodal industrial anomaly detection method based on structural commonality, CIF (Commonality In Few). To extract intra-class structural information, we employ hypergraphs, which are capable of modeling higher-order correlations, to capture the structural commonality within training samples, and use a memory bank to store this intra-class structural prior. Firstly, we design a semantic-aware hypergraph construction module tailored for single-semantic industrial images, from which we extract common structures to guide the construction of the memory bank. Secondly, we use a training-free hypergraph message passing module to update the visual features of test samples, reducing the distribution gap between test features and features in the memory bank. We further propose a hyperedge-guided memory search module, which utilizes structural information to assist the memory search process and reduce the false positive rate. Experimental results on the MVTec 3D-AD dataset and the Eyecandies dataset show that our method outperforms the state-of-the-art (SOTA) methods in few-shot settings. Code is available at https://github.com/Sunny5250/CIF.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251105966L/abstract",
      "keywords": [
        "Computer Vision and Pattern Recognition"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.383Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "ide"
      ],
      "score": 1.6014686453959635
    },
    {
      "id": "10.48550/arXiv.2511.04755",
      "bibcode": "2025arXiv251104755H",
      "title": "EMO100DB: An Open Dataset of Improvised Songs with Emotion Data",
      "authors": [
        "Hwang, Daeun",
        "Park, Saebyul"
      ],
      "year": "2025",
      "abstract": "In this study, we introduce Emo100DB: a dataset consisting of improvised songs that were recorded and transcribed with emotion data based on Russell's circumplex model of emotion. The dataset was developed by collecting improvised songs that consist of melody, lyrics, and an instrumental accompaniment played, sung, and recorded by 20 young adults. Before recording each song, the participants were asked to report their emotional state, with the axes representing arousal and valence based on Russell's circumplex model of emotions. The dataset is organized into four emotion quadrants, and it includes the lyrics text and MIDI file of the melody extracted from the participant recordings, along with the original audio in WAV format. By providing an integrated composition of data and analysis, this study aims to offer a comprehensive dataset that allows for a diverse exploration of the relationship between music and emotion.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251104755H/abstract",
      "keywords": [
        "Sound",
        "Information Retrieval",
        "Multimedia"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.383Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "score": 1.0957417047446065
    },
    {
      "id": "10.48550/arXiv.2511.14881",
      "bibcode": "2025arXiv251114881X",
      "title": "SilverTorch: A Unified Model-based System to Democratize Large-Scale Recommendation on GPUs",
      "authors": [
        "Xue, Bi",
        "Wu, Hong",
        "Chen, Lei",
        "Yang, Chao",
        "Ma, Yiming",
        "Ding, Fei",
        "Wang, Zhen",
        "Wang, Liang",
        "Mao, Xiaoheng",
        "Huang, Ke",
        "Li, Xialu",
        "Xia, Peng",
        "Jian, Rui",
        "Zhao, Yanli",
        "Huang, Yanzun",
        "Deng, Yijie",
        "Tran, Harry",
        "Chang, Ryan",
        "Yu, Min",
        "Dong, Eric",
        "Wang, Jiazhou",
        "Zhang, Qianqian",
        "Zhai, Keke",
        "Yin, Hongzhang",
        "Garbacki, Pawel",
        "Fang, Zheng",
        "Pan, Yiyi",
        "Ni, Min",
        "Liu, Yang"
      ],
      "year": "2025",
      "abstract": "Serving deep learning based recommendation models (DLRM) at scale is challenging. Existing systems rely on CPU-based ANN indexing and filtering services, suffering from non-negligible costs and forgoing joint optimization opportunities. Such inefficiency makes them difficult to support more complex model architectures, such as learned similarities and multi-task retrieval. In this paper, we propose SilverTorch, a model-based system for serving recommendation models on GPUs. SilverTorch unifies model serving by replacing standalone indexing and filtering services with layers of served models. We propose a Bloom index algorithm on GPUs for feature filtering and a tensor-native fused Int8 ANN kernel on GPUs for nearest neighbor search. We further co-design the ANN search index and filtering index to reduce GPU memory utilization and eliminate unnecessary computation. Benefit from SilverTorch's serving paradigm, we introduce a OverArch scoring layer and a Value Model to aggregate results across multi-tasks. These advancements improve the accuracy for retrieval and enable future studies for serving more complex models. For ranking, SilverTorch's design accelerates item embedding calculation by caching the pre-calculated embeddings inside the serving model. Our evaluation on the industry-scale datasets show that SilverTorch achieves up to 5.6x lower latency and 23.7x higher throughput compared to the state-of-the-art approaches. We also demonstrate that SilverTorch's solution is 13.35x more cost-efficient than CPU-based solution while improving accuracy via serving more complex models. SilverTorch serves over hundreds of models online across major products and recommends contents for billions of daily active users.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251114881X/abstract",
      "keywords": [
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.384Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 1.6857564674442085
    },
    {
      "id": "2025arXiv251100635L",
      "bibcode": "2025arXiv251100635L",
      "title": "Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles",
      "authors": [
        "Lim, Hyungtae",
        "Kim, Daebeom",
        "Myung, Hyun"
      ],
      "year": "2025",
      "abstract": "As various 3D light detection and ranging (LiDAR) sensors have been introduced to the market, research on multi-session simultaneous localization and mapping (MSS) using heterogeneous LiDAR sensors has been actively conducted. Existing MSS methods mostly rely on loop closure detection for inter-session alignment; however, the performance of loop closure detection can be potentially degraded owing to the differences in the density and field of view (FoV) of the sensors used in different sessions. In this study, we challenge the existing paradigm that relies heavily on loop detection modules and propose a novel MSS framework, called Multi-Mapcher, that employs large-scale map-to-map registration to perform inter-session initial alignment, which is commonly assumed to be infeasible, by leveraging outlier-robust 3D point cloud registration. Next, after finding inter-session loops by radius search based on the assumption that the inter-session initial alignment is sufficiently precise, anchor node-based robust pose graph optimization is employed to build a consistent global map. As demonstrated in our experiments, our approach shows substantially better MSS performance for various LiDAR sensors used to capture the sessions and is faster than state-of-the-art approaches. Our code is available at https://github.com/url-kaist/multi-mapcher.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251100635L/abstract",
      "keywords": [
        "Robotics"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.384Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "agents"
      ],
      "score": 1.6857564674442085
    },
    {
      "id": "10.48550/arXiv.2511.04221",
      "bibcode": "2025arXiv251104221K",
      "title": "Coordination-Free Lane Partitioning for Convergent ANN Search",
      "authors": [
        "Kugblenu, Carl",
        "Vuorimaa, Petri"
      ],
      "year": "2025",
      "abstract": "Production vector search systems often fan out each query across parallel lanes (threads, replicas, or shards) to meet latency service-level objectives (SLOs). In practice, these lanes rediscover the same candidates, so extra compute does not increase coverage. We present a coordination-free lane partitioner that turns duplication into complementary work at the same cost and deadline. For each query we (1) build a deterministic candidate pool sized to the total top-k budget, (2) apply a per-query pseudorandom permutation, and (3) assign each lane a disjoint slice of positions. Lanes then return different results by construction, with no runtime coordination. At equal cost with four lanes (total candidate budget 64), on SIFT1M (1M SIFT feature vectors) with Hierarchical Navigable Small World graphs (HNSW) recall@10 rises from 0.249 to 0.999 while lane overlap falls from nearly 100% to 0%. On MS MARCO (8.8M passages) with HNSW, hit@10 improves from 0.200 to 0.601 and Mean Reciprocal Rank at 10 (MRR@10) from 0.133 to 0.330. For inverted file (IVF) indexes we see smaller but consistent gains (for example, +11% on MS MARCO) by de-duplicating list routing. A microbenchmark shows planner overhead of ~37 microseconds per query (mean at the main setting) with linear growth in the number of merged candidates. These results yield a simple operational guideline: size the per-query pool to the total budget, deterministically partition positions across lanes, and turn redundant fan-out into complementary coverage without changing budget or deadline.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251104221K/abstract",
      "keywords": [
        "Information Retrieval",
        "Databases"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.384Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 1.6857564674442085
    },
    {
      "id": "arXiv:2511.09064",
      "bibcode": "2025arXiv251109064J",
      "title": "Diversifying Counterattacks: Orthogonal Exploration for Robust CLIP Inference",
      "authors": [
        "Jiang, Chengze",
        "Dong, Minjing",
        "Shi, Xinli",
        "Gui, Jie"
      ],
      "year": "2025",
      "abstract": "Vision-language pre-training models (VLPs) demonstrate strong multimodal understanding and zero-shot generalization, yet remain vulnerable to adversarial examples, raising concerns about their reliability. Recent work, Test-Time Counterattack (TTC), improves robustness by generating perturbations that maximize the embedding deviation of adversarial inputs using PGD, pushing them away from their adversarial representations. However, due to the fundamental difference in optimization objectives between adversarial attacks and counterattacks, generating counterattacks solely based on gradients with respect to the adversarial input confines the search to a narrow space. As a result, the counterattacks could overfit limited adversarial patterns and lack the diversity to fully neutralize a broad range of perturbations. In this work, we argue that enhancing the diversity and coverage of counterattacks is crucial to improving adversarial robustness in test-time defense. Accordingly, we propose Directional Orthogonal Counterattack (DOC), which augments counterattack optimization by incorporating orthogonal gradient directions and momentum-based updates. This design expands the exploration of the counterattack space and increases the diversity of perturbations, which facilitates the discovery of more generalizable counterattacks and ultimately improves the ability to neutralize adversarial perturbations. Meanwhile, we present a directional sensitivity score based on averaged cosine similarity to boost DOC by improving example discrimination and adaptively modulating the counterattack strength. Extensive experiments on 16 datasets demonstrate that DOC improves adversarial robustness under various attacks while maintaining competitive clean accuracy. Code is available at https://github.com/bookman233/DOC.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251109064J/abstract",
      "keywords": [
        "Computer Vision and Pattern Recognition"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.384Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 1.6857564674442085
    },
    {
      "id": "2025arXiv251110585E",
      "bibcode": "2025arXiv251110585E",
      "title": "Textual understanding boost in the WikiRace",
      "authors": [
        "Ebrahimi, Raman",
        "Fuhrman, Sean",
        "Nguyen, Kendrick",
        "Gurusankar, Harini",
        "Franceschetti, Massimo"
      ],
      "year": "2025",
      "abstract": "The WikiRace game, where players navigate between Wikipedia articles using only hyperlinks, serves as a compelling benchmark for goal-directed search in complex information networks. This paper presents a systematic evaluation of navigation strategies for this task, comparing agents guided by graph-theoretic structure (betweenness centrality), semantic meaning (language model embeddings), and hybrid approaches. Through rigorous benchmarking on a large Wikipedia subgraph, we demonstrate that a purely greedy agent guided by the semantic similarity of article titles is overwhelmingly effective. This strategy, when combined with a simple loop-avoidance mechanism, achieved a perfect success rate and navigated the network with an efficiency an order of magnitude better than structural or hybrid methods. Our findings highlight the critical limitations of purely structural heuristics for goal-directed search and underscore the transformative potential of large language models to act as powerful, zero-shot semantic navigators in complex information spaces.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251110585E/abstract",
      "keywords": [
        "Social and Information Networks",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.384Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 2.6972103479107337
    },
    {
      "id": "arXiv:2511.09005",
      "bibcode": "2025arXiv251109005C",
      "title": "AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines",
      "authors": [
        "Chauhan, Alvin"
      ],
      "year": "2025",
      "abstract": "Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of the search space in a gradual, incremental, and sequential (GIS) manner. Stated succinctly, high-quality reasoning is a controlled, incremental search. To test this framework, we investigate the efficacy of recursive refinement (RR)--an iterative process of self-criticism, adversarial stress-testing, and integrating critical feedback--as a practical method for implementing GIS search. We designed an experiment comparing a simple, linear pipeline against a complex, explicitly structured pipeline leveraging a recursive refinement layer. The multi-agent models were constructed to reflect the historical personas of three US Founding Fathers (Hamilton, Jefferson, and Madison) using RAG-powered corpora and were prompted to generate responses to three contemporary political issues. Model performance was evaluated using a two-tiered approach: a quantitative score from an LLM arbiter agent and qualitative human judgment. Our results revealed that the complex model consistently outperformed the simple model across all nine test cases with an average arbiter-outputted score of 88.3 versus 71.7. The complex model's arguments were superior in analytical depth, structural nuance, and strategic framing. We conclude that recursive refinement is a robust architectural feature for enhancing LLM reasoning via GIS search.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251109005C/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Computation and Language",
        "Multiagent Systems",
        "I.2.11"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.384Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "testing"
      ],
      "score": 1.9386199375608397
    },
    {
      "id": "10.48550/arXiv.2511.14405",
      "bibcode": "2025arXiv251114405Z",
      "title": "Jasper-Token-Compression-600M Technical Report",
      "authors": [
        "Zhang, Dun",
        "Zeng, Ziyang",
        "Zhou, Yudong",
        "Lu, Shuyang"
      ],
      "year": "2025",
      "abstract": "This technical report presents the training methodology and evaluation results of the open-source Jasper-Token-Compression-600M model, released in November 2025. Building on previous distillation-based recipes from the English Stella and Jasper models, we successfully extend this approach to a bilingual (English and Chinese) domain, further enhancing model performance through the incorporation of contrastive learning. A key innovation of our model is the introduction of a one-dimensional convolution-based token compression module. We dynamically adjust the compression rate during training, enabling the model to learn more robust and efficient compressed text representations. By combining knowledge distillation with token compression techniques, we achieve significant improvements in both embedding quality and inference efficiency. Our model performs with higher efficiency than a traditional 0.6B model while achieving performance comparable to that of an 8B model. For more information on the model release, visit: https://huggingface.co/infgrad/Jasper-Token-Compression-600M.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251114405Z/abstract",
      "keywords": [
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.384Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 1.8543321141886293
    },
    {
      "id": "10.48550/arXiv.2511.13057",
      "bibcode": "2025arXiv251113057P",
      "title": "Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact",
      "authors": [
        "Pati, Satyanarayan"
      ],
      "year": "2025",
      "abstract": "Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the \"performance loss\" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective \"sweet spot,\" achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251113057P/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.384Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability"
      ],
      "score": 1.8543321141886293
    },
    {
      "id": "2025arXiv251112959L",
      "bibcode": "2025arXiv251112959L",
      "title": "Personalized Federated Recommendation With Knowledge Guidance",
      "authors": [
        "Lim, Jaehyung",
        "Kweon, Wonbin",
        "Kim, Woojoo",
        "Kim, Junyoung",
        "Kim, Dongha",
        "Yu, Hwanjo"
      ],
      "year": "2025",
      "abstract": "Federated Recommendation (FedRec) has emerged as a key paradigm for building privacy-preserving recommender systems. However, existing FedRec models face a critical dilemma: memory-efficient single-knowledge models suffer from a suboptimal knowledge replacement practice that discards valuable personalization, while high-performance dual-knowledge models are often too memory-intensive for practical on-device deployment. We propose Federated Recommendation with Knowledge Guidance (FedRKG), a model-agnostic framework that resolves this dilemma. The core principle, Knowledge Guidance, avoids full replacement and instead fuses global knowledge into preserved local embeddings, attaining the personalization benefits of dual-knowledge within a single-knowledge memory footprint. Furthermore, we introduce Adaptive Guidance, a fine-grained mechanism that dynamically modulates the intensity of this guidance for each user-item interaction, overcoming the limitations of static fusion methods. Extensive experiments on benchmark datasets demonstrate that FedRKG significantly outperforms state-of-the-art methods, validating the effectiveness of our approach. The code is available at https://github.com/Jaehyung-Lim/fedrkg.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112959L/abstract",
      "keywords": [
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.384Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 1.5171808206997877
    },
    {
      "id": "2025arXiv251101496A",
      "bibcode": "2025arXiv251101496A",
      "title": "Calculating Web Impact Factor for University Websites of Jammu and Kashmir: A Study",
      "authors": [
        "Ahmad, Muneer",
        "Batcha, M Sadik",
        "Rashid, Wasim",
        "Hafiz, Obaid"
      ],
      "year": "2025",
      "abstract": "This paper examines and explores the web impact factor through a webometric study of the present 12 University Websites of Jammu and Kashmir. Identifies the domain systems of the websites; analyzes the number of web pages and link pages, and calculates the External Link WIF or simple web impact factor (WIF) and external web impact factor of all the University websites. Also reflects that some university websites have higher number of web pages, but correspondingly their link pages are very small in number and websites fall behind in their simple and external link web impact factor. It found that the Cluster University of Jammu ranked 1 (0.9018) in Internal Link WIF of Websites in Jammu and Kashmir. Shri Mata Vaishno Devi University ranked 1 (0.7249) in External Link Web Impact Factor.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251101496A/abstract",
      "keywords": [
        "Digital Libraries",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.384Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "score": 1.2643173505831564
    },
    {
      "id": "10.48550/arXiv.2511.06937",
      "bibcode": "2025arXiv251106937H",
      "title": "Fine-Tuning Diffusion-Based Recommender Systems via Reinforcement Learning with Reward Function Optimization",
      "authors": [
        "Hou, Yu",
        "Li, Hua",
        "Kim, Ha Young",
        "Shin, Won-Yong"
      ],
      "year": "2025",
      "abstract": "Diffusion models recently emerged as a powerful paradigm for recommender systems, offering state-of-the-art performance by modeling the generative process of user-item interactions. However, training such models from scratch is both computationally expensive and yields diminishing returns once convergence is reached. To remedy these challenges, we propose ReFiT, a new framework that integrates Reinforcement learning (RL)-based Fine-Tuning into diffusion-based recommender systems. In contrast to prior RL approaches for diffusion models depending on external reward models, ReFiT adopts a task-aligned design: it formulates the denoising trajectory as a Markov decision process (MDP) and incorporates a collaborative signal-aware reward function that directly reflects recommendation quality. By tightly coupling the MDP structure with this reward signal, ReFiT empowers the RL agent to exploit high-order connectivity for fine-grained optimization, while avoiding the noisy or uninformative feedback common in naive reward designs. Leveraging policy gradient optimization, ReFiT maximizes exact log-likelihood of observed interactions, thereby enabling effective post hoc fine-tuning of diffusion recommenders. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed ReFiT framework (a) exhibits substantial performance gains over strong competitors (up to 36.3% on sequential recommendation), (b) demonstrates strong efficiency with linear complexity in the number of users or items, and (c) generalizes well across multiple diffusion-based recommendation scenarios. The source code and datasets are publicly available at https://anonymous.4open.science/r/ReFiT-4C60.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251106937H/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence",
        "Machine Learning",
        "Networking and Internet Architecture",
        "Social and Information Networks"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.384Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "governance"
      ],
      "score": 3.0343616388910086
    },
    {
      "id": "10.48550/arXiv.2511.05667",
      "bibcode": "2025arXiv251105667S",
      "title": "SARCH: Multimodal Search for Archaeological Archives",
      "authors": [
        "Sinha, Nivedita",
        "Khanijo, Bharati",
        "Singh, Sanskar",
        "Mahant, Priyansh",
        "Roy, Ashutosh",
        "Singh Bhadouria, Saubhagya",
        "Jain, Arpan",
        "Ramanath, Maya"
      ],
      "year": "2025",
      "abstract": "In this paper, we describe a multi-modal search system designed to search old archaeological books and reports. This corpus is digitally available as scanned PDFs, but varies widely in the quality of scans. Our pipeline, designed for multi-modal archaeological documents, extracts and indexes text, images (classified into maps, photos, layouts, and others), and tables. We evaluated different retrieval strategies, including keyword-based search, embedding-based models, and a hybrid approach that selects optimal results from both modalities. We report and analyze our preliminary results and discuss future work in this exciting vertical.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251105667S/abstract",
      "keywords": [
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.385Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 1.5171808194455043
    },
    {
      "id": "2025arXiv251114221J",
      "bibcode": "2025arXiv251114221J",
      "title": "LLM-Aligned Geographic Item Tokenization for Local-Life Recommendation",
      "authors": [
        "Jiang, Hao",
        "Wang, Guoquan",
        "Zhou, Donglin",
        "Yu, Sheng",
        "Zeng, Yang",
        "Zeng, Wencong",
        "Gai, Kun",
        "Zhou, Guorui"
      ],
      "year": "2025",
      "abstract": "Recent advances in Large Language Models (LLMs) have enhanced text-based recommendation by enriching traditional ID-based methods with semantic generalization capabilities. Text-based methods typically encode item textual information via prompt design and generate discrete semantic IDs through item tokenization. However, in domain-specific tasks such as local-life services, simply injecting location information into prompts fails to capture fine-grained spatial characteristics and real-world distance awareness among items. To address this, we propose LGSID, an LLM-Aligned Geographic Item Tokenization Framework for Local-life Recommendation. This framework consists of two key components: (1) RL-based Geographic LLM Alignment, and (2) Hierarchical Geographic Item Tokenization. In the RL-based alignment module, we initially train a list-wise reward model to capture real-world spatial relationships among items. We then introduce a novel G-DPO algorithm that uses pre-trained reward model to inject generalized spatial knowledge and collaborative signals into LLMs while preserving their semantic understanding. Furthermore, we propose a hierarchical geographic item tokenization strategy, where primary tokens are derived from discrete spatial and content attributes, and residual tokens are refined using the aligned LLM's geographic representation vectors. Extensive experiments on real-world Kuaishou industry datasets show that LGSID consistently outperforms state-of-the-art discriminative and generative recommendation models. Ablation studies, visualizations, and case studies further validate its effectiveness.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251114221J/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.385Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 1.5171808194455043
    },
    {
      "id": "2025arXiv251104491A",
      "bibcode": "2025arXiv251104491A",
      "title": "RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables",
      "authors": [
        "Abhyankar, Nikhil",
        "Chaurasia, Purvi",
        "Kabra, Sanchit",
        "Srivastava, Ananya",
        "Gupta, Vivek",
        "Reddy, Chandan K."
      ],
      "year": "2025",
      "abstract": "Existing tabular reasoning benchmarks mostly test models on small, uniform tables, underrepresenting the complexity of real-world data and giving an incomplete view of Large Language Models' (LLMs) reasoning abilities. Real tables are long, heterogeneous, and domain-specific, mixing structured fields with free text and requiring multi-hop reasoning across thousands of tokens. To address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from 2031 real-world tables spanning two domains: i) RB-Science (NSF grant records) and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates LLMs jointly across scale, heterogeneity, domain specificity, and reasoning complexity. Experiments with open-source and proprietary models show that LLMs struggle with heterogeneous schemas and complex multi-hop inference, revealing persistent weaknesses in current architectures and prompting strategies. RUST-BENCH establishes a challenging new testbed for advancing tabular reasoning research.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251104491A/abstract",
      "keywords": [
        "Computation and Language",
        "Artificial Intelligence",
        "Databases",
        "Information Retrieval",
        "Machine Learning"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.385Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review"
      ],
      "score": 1.2643173495379203
    },
    {
      "id": "10.48550/arXiv.2511.10492",
      "bibcode": "2025arXiv251110492Z",
      "title": "Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding",
      "authors": [
        "Zhang, Yunkai",
        "Zhang, Qiang",
        "Lin, Feng",
        "Qiu, Ruizhong",
        "Yu, Hanchao",
        "Liu, Jiayi",
        "Xia, Yinglong",
        "Yu, Zhuoran",
        "Zheng, Zeyu",
        "Yang, Diji"
      ],
      "year": "2025",
      "abstract": "Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner. Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251110492Z/abstract",
      "keywords": [
        "Information Retrieval",
        "Machine Learning"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.385Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "score": 1.5171808194455043
    },
    {
      "id": "2025arXiv251111172S",
      "bibcode": "2025arXiv251111172S",
      "title": "Enhancing Group Recommendation using Soft Impute Singular Value Decomposition",
      "authors": [
        "Sani Ibrahim, Mubaraka",
        "Saidu, Isah Charles",
        "Csato, Lehel"
      ],
      "year": "2025",
      "abstract": "The growing popularity of group activities increased the need to develop methods for providing recommendations to a group of users based on the collective preferences of the group members. Several group recommender systems have been proposed, but these methods often struggle due to sparsity and high-dimensionality of the available data, common in many real-world applications. In this paper, we propose a group recommender system called Group Soft-Impute SVD, which leverages soft-impute singular value decomposition to enhance group recommendations. This approach addresses the challenge of sparse high-dimensional data using low-rank matrix completion. We compared the performance of Group Soft-Impute SVD with Group MF based approaches and found that our method outperforms the baselines in recall for small user groups while achieving comparable results across all group sizes when tasked on Goodbooks, Movielens, and Synthetic datasets. Furthermore, our method recovers lower matrix ranks than the baselines, demonstrating its effectiveness in handling high-dimensional data.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251111172S/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.385Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 1.5171808194455043
    },
    {
      "id": "arXiv:2511.11265",
      "bibcode": "2025arXiv251111265R",
      "title": "SQuaD: The Software Quality Dataset",
      "authors": [
        "Robredo, Mikel",
        "Esposito, Matteo",
        "Taibi, Davide",
        "Peñaloza, Rafael",
        "Lenarduzzi, Valentina"
      ],
      "year": "2025",
      "abstract": "Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251111265R/abstract",
      "keywords": [
        "Software Engineering",
        "Artificial Intelligence",
        "Computation and Language",
        "Cryptography and Security",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.385Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "ide",
        "observability"
      ],
      "score": 2.1071955825632003
    },
    {
      "id": "arXiv:2511.12260",
      "bibcode": "2025arXiv251112260E",
      "title": "Reinforcement Learning for Chemical Ordering in Alloy Nanoparticles",
      "authors": [
        "Elsborg, Jonas",
        "Bhowmik, Arghya"
      ],
      "year": "2025",
      "abstract": "We approach the search for optimal element ordering in bimetallic alloy nanoparticles (NPs) as a reinforcement learning (RL) problem, and have built an RL agent that learns to perform such global optimisation using the geometric graph representation of the NPs. To demonstrate the effectiveness, we train an RL agent to perform composition-conserving atomic swap actions on the icosahedral nanoparticle structure. Trained once on randomised $Ag_{X}Au_{309-X}$ compositions and orderings, the agent discovers previously established ground state structure. We show that this optimization is robust to differently ordered initialisations of the same NP compositions. We also demonstrate that a trained policy can extrapolate effectively to NPs of unseen size. However, the efficacy is limited when multiple alloying elements are involved. Our results demonstrate that RL with pre-trained equivariant graph encodings can navigate combinatorial ordering spaces at the nanoparticle scale, and offer a transferable optimisation strategy with the potential to generalise across composition and reduce repeated individual search cost.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112260E/abstract",
      "keywords": [
        "Materials Science",
        "Machine Learning",
        "Computational Physics"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.385Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "agents",
        "governance"
      ],
      "score": 2.1071955825632003
    },
    {
      "id": "2025arXiv251114403Z",
      "bibcode": "2025arXiv251114403Z",
      "title": "Infer As You Train: A Symmetric Paradigm of Masked Generative for Click-Through Rate Prediction",
      "authors": [
        "Zhang, Moyu",
        "Jin, Yujun",
        "Chen, Yun",
        "Hu, Jinxin",
        "Zhang, Yu",
        "Zeng, Xiaoyi"
      ],
      "year": "2025",
      "abstract": "Generative models are increasingly being explored in click-through rate (CTR) prediction field to overcome the limitations of the conventional discriminative paradigm, which rely on a simple binary classification objective. However, existing generative models typically confine the generative paradigm to the training phase, primarily for representation learning. During online inference, they revert to a standard discriminative paradigm, failing to leverage their powerful generative capabilities to further improve prediction accuracy. This fundamental asymmetry between the training and inference phases prevents the generative paradigm from realizing its full potential. To address this limitation, we propose the Symmetric Masked Generative Paradigm for CTR prediction (SGCTR), a novel framework that establishes symmetry between the training and inference phases. Specifically, after acquiring generative capabilities by learning feature dependencies during training, SGCTR applies the generative capabilities during online inference to iteratively redefine the features of input samples, which mitigates the impact of noisy features and enhances prediction accuracy. Extensive experiments validate the superiority of SGCTR, demonstrating that applying the generative paradigm symmetrically across both training and inference significantly unlocks its power in CTR prediction.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251114403Z/abstract",
      "keywords": [
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.385Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 1.3486051728404482
    },
    {
      "id": "10.48550/arXiv.2511.11788",
      "bibcode": "2025arXiv251111788S",
      "title": "MALBO: Optimizing LLM-Based Multi-Agent Teams via Multi-Objective Bayesian Optimization",
      "authors": [
        "Sabbatella, Antonio"
      ],
      "year": "2025",
      "abstract": "The optimal assignment of Large Language Models (LLMs) to specialized roles in multi-agent systems is a significant challenge, defined by a vast combinatorial search space, expensive black-box evaluations, and an inherent trade-off between performance and cost. Current optimization methods focus on single-agent settings and lack a principled framework for this multi-agent, multi-objective problem. This thesis introduces MALBO (Multi-Agent LLM Bayesian Optimization), a systematic framework designed to automate the efficient composition of LLM-based agent teams. We formalize the assignment challenge as a multi-objective optimization problem, aiming to identify the Pareto front of configurations between task accuracy and inference cost. The methodology employs multi-objective Bayesian Optimization (MOBO) with independent Gaussian Process surrogate models. By searching over a continuous feature-space representation of the LLMs, this approach performs a sample-efficient exploration guided by the expected hypervolume improvement. The primary contribution is a principled and automated methodology that yields a Pareto front of optimal team configurations. Our results demonstrate that the Bayesian optimization phase, compared to an initial random search, maintained a comparable average performance while reducing the average configuration cost by over 45%. Furthermore, MALBO identified specialized, heterogeneous teams that achieve cost reductions of up to 65.8% compared to homogeneous baselines, all while maintaining maximum performance. The framework thus provides a data-driven tool for deploying cost-effective and highly specialized multi-agent AI systems.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251111788S/abstract",
      "keywords": [
        "Multiagent Systems",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.385Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 1.8543321126556163
    },
    {
      "id": "arXiv:2511.13357",
      "bibcode": "2025arXiv251113357M",
      "title": "FLOWER: Flow-Oriented Entity-Relationship Tool",
      "authors": [
        "Moskalev, Dmitry"
      ],
      "year": "2025",
      "abstract": "Exploring relationships across data sources is a crucial optimization for entities recognition. Since databases can store big amount of information with synthetic and organic data, serving all quantity of objects correctly is an important task to deal with. However, the decision of how to construct entity relationship model is associated with human factor. In this paper, we present flow-oriented entity-relationship tool. This is first and unique end-to-end solution that eliminates routine and resource-intensive problems of processing, creating and visualizing both of explicit and implicit dependencies for prominent SQL dialects on-the-fly. Once launched, FLOWER automatically detects built-in constraints and starting to create own correct and necessary one using dynamic sampling and robust data analysis techniques. This approach applies to improve entity-relationship model and data storytelling to better understand the foundation of data and get unseen insights from DB sources using SQL or natural language. Evaluated on state-of-the-art STATS benchmark, experiments show that FLOWER is superior to reservoir sampling by 2.4x for distribution representation and 2.6x for constraint learning with 2.15x acceleration. For data storytelling, our tool archives 1.19x for accuracy enhance with 1.86x context decrease compare to LLM. Presented tool is also support 23 languages and compatible with both of CPU and GPU. Those results show that FLOWER can manage with real-world data a way better to ensure with quality, scalability and applicability for different use-cases.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251113357M/abstract",
      "keywords": [
        "Software Engineering",
        "Information Retrieval"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.385Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "score": 2.107195580821141
    },
    {
      "id": "10.48550/arXiv.2511.12971",
      "bibcode": "2025arXiv251112971C",
      "title": "Esim: EVM Bytecode Similarity Detection Based on Stable-Semantic Graph",
      "authors": [
        "Chen, Zhuo",
        "Ji, Gaoqiang",
        "He, Yiling",
        "Wu, Lei",
        "Zhou, Yajin"
      ],
      "year": "2025",
      "abstract": "Decentralized finance (DeFi) is experiencing rapid expansion. However, prevalent code reuse and limited open-source contributions have introduced significant challenges to the blockchain ecosystem, including plagiarism and the propagation of vulnerable code. Consequently, an effective and accurate similarity detection method for EVM bytecode is urgently needed to identify similar contracts. Traditional binary similarity detection methods are typically based on instruction stream or control flow graph (CFG), which have limitations on EVM bytecode due to specific features like low-level EVM bytecode and heavily-reused basic blocks. Moreover, the highly-diverse Solidity Compiler (Solc) versions further complicate accurate similarity detection. Motivated by these challenges, we propose a novel EVM bytecode representation called Stable-Semantic Graph (SSG), which captures relationships between 'stable instructions' (special instructions identified by our study). Moreover, we implement a prototype, Esim, which embeds SSG into matrices for similarity detection using a heterogeneous graph neural network. Esim demonstrates high accuracy in SSG construction, achieving F1-scores of 100% for control flow and 95.16% for data flow, and its similarity detection performance reaches 96.3% AUC, surpassing traditional approaches. Our large-scale study, analyzing 2,675,573 smart contracts on six EVM-compatible chains over a one-year period, also demonstrates that Esim outperforms the SOTA tool Etherscan in vulnerability search.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251112971C/abstract",
      "keywords": [
        "Cryptography and Security",
        "Artificial Intelligence"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.386Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "ide"
      ],
      "score": 2.107195580821141
    },
    {
      "id": "10.48550/arXiv.2511.13166",
      "bibcode": "2025arXiv251113166S",
      "title": "Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users",
      "authors": [
        "Shen, Zhaoxin",
        "Wu, Dan"
      ],
      "year": "2025",
      "abstract": "To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251113166S/abstract",
      "keywords": [
        "Information Retrieval",
        "Artificial Intelligence",
        "Human-Computer Interaction",
        "68T09 (Primary) 68M11",
        "68W27 (Secondary)",
        "H.3.3"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.386Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "score": 1.3486051717255303
    },
    {
      "id": "arXiv:2511.15061",
      "bibcode": "2025arXiv251115061C",
      "title": "Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering",
      "authors": [
        "Chen, Haodong",
        "Zuccon, Guido",
        "Leelanupab, Teerapong"
      ],
      "year": "2025",
      "abstract": "Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization. In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution. OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251115061C/abstract",
      "keywords": [
        "Artificial Intelligence",
        "Information Retrieval",
        "Machine Learning",
        "H.3.3"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.386Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "score": 2.865785989916752
    },
    {
      "id": "10.48550/arXiv.2511.15844",
      "bibcode": "2025arXiv251115844G",
      "title": "Geant4 based library SCoRe4 for Surface Contamination and Roughness Effects simulations in rare event search experiments",
      "authors": [
        "Grüner, Christoph"
      ],
      "year": "2025",
      "abstract": "Surface simulations are important for accurately modeling particle interactions in experiments where background contributions from surface contaminants can significantly affect detector performance. In rare event searches, such as dark matter or neutrinoless double beta decay experiments, standard Geant4 simulations typically assume perfectly smooth surfaces, neglecting the microscopic roughness that exists in real materials. This simplification can lead to inaccurate predictions of energy deposition. To address this limitation, I developed SCoRe4, a Geant4-based library designed to simulate more realistic surface roughness based on experimentally measurable parameters. The code allows users to generate patches of simplified rough surface geometries across a wide range of scales - from square millimeters to square meters - while maintaining computational efficiency. SCoRe4 is open source and can be easily integrated into existing Geant4 setups. This work presents the structure, implementation, and example application of SCoRe4,as well as its potential use in improving the accuracy of background modeling in rare event physics.",
      "publication": "arXiv e-prints",
      "url": "https://ui.adsabs.harvard.edu/abs/2025arXiv251115844G/abstract",
      "keywords": [
        "Instrumentation and Detectors",
        "Computational Physics"
      ],
      "source": "ads",
      "ingestedAt": "2025-11-25T22:12:16.386Z",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "libraryId": "scix-mcp-ingest",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "ide"
      ],
      "score": 2.2757712272868322
    }
  ],
  "articles": [
    {
      "id": "b8f4d75f84f9477a825a31e445e120e8",
      "title": "Advice for New Principal Tech ICs (i.e., Notes to Myself)",
      "url": "https://eugeneyan.com//writing/principal/",
      "content": "Based on what I've learned from role models and mentors in Amazon",
      "summary": "Based on what I've learned from role models and mentors in Amazon",
      "publishedAt": "2025-10-19T00:00:00.000Z",
      "source": "rss",
      "feedName": "Eugene Yan",
      "sourceType": "engineering_blog",
      "company": "Eugene Yan",
      "contentType": "general",
      "score": 0.5391952589587397,
      "ingestedAt": "2025-11-23T21:21:55.635Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "4410b896ef41bf04394417c256783d2d",
      "title": "Training an LLM-RecSys Hybrid for Steerable Recs with Semantic IDs",
      "url": "https://eugeneyan.com//writing/semantic-ids/",
      "content": "An LLM that can converse in English & item IDs, and make recommendations w/o retrieval or tools.",
      "summary": "An LLM that can converse in English & item IDs, and make recommendations w/o retrieval or tools.",
      "publishedAt": "2025-09-14T00:00:00.000Z",
      "source": "rss",
      "feedName": "Eugene Yan",
      "sourceType": "engineering_blog",
      "company": "Eugene Yan",
      "contentType": "general",
      "score": 0.034775590213289256,
      "ingestedAt": "2025-11-23T21:21:55.635Z",
      "tags": [
        "retrieval"
      ]
    },
    {
      "id": "5e59a8e3a87a68eb8723077bd2fe1e05",
      "title": "OpenAI and Foxconn collaborate to strengthen U.S. manufacturing across the AI supply chain",
      "url": "https://openai.com/index/openai-and-foxconn-collaborate",
      "content": "OpenAI and Foxconn are collaborating to design and manufacture next-generation AI infrastructure hardware in the U.S. The partnership will develop multiple generations of data-center systems, strengthen U.S. supply chains, and build key components domestically to accelerate advanced AI infrastructure.",
      "summary": "OpenAI and Foxconn are collaborating to design and manufacture next-generation AI infrastructure hardware in the U.S. The partnership will develop multiple generations of data-center systems, strengthen U.S. supply chains, and build key components domestically to accelerate advanced AI infrastructure.",
      "publishedAt": "2025-11-20T14:50:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "security_incident",
      "score": 4.74946912602112,
      "ingestedAt": "2025-11-23T21:21:55.720Z",
      "tags": []
    },
    {
      "id": "72be2ab1ebb821913376387da739f8d4",
      "title": "Helping 1,000 small businesses build with AI",
      "url": "https://openai.com/index/small-business-ai-jam",
      "content": "OpenAI is partnering with DoorDash, SCORE, and local organizations to help 1,000 small businesses build with AI. The Small Business AI Jam gives Main Street business owners hands-on tools and training to compete and grow.",
      "summary": "OpenAI is partnering with DoorDash, SCORE, and local organizations to help 1,000 small businesses build with AI. The Small Business AI Jam gives Main Street business owners hands-on tools and training to compete and grow.",
      "publishedAt": "2025-11-20T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 2.3131170446518503,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": []
    },
    {
      "id": "0c40a000ff645391fcc87c00e55a58bd",
      "title": "Early experiments in accelerating science with GPT-5",
      "url": "https://openai.com/index/accelerating-science-gpt-5",
      "content": "OpenAI introduces the first research cases showing how GPT-5 accelerates scientific progress across math, physics, biology, and computer science. Explore how AI and researchers collaborate to generate proofs, uncover new insights, and reshape the pace of discovery.",
      "summary": "OpenAI introduces the first research cases showing how GPT-5 accelerates scientific progress across math, physics, biology, and computer science. Explore how AI and researchers collaborate to generate proofs, uncover new insights, and reshape the pace of discovery.",
      "publishedAt": "2025-11-20T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 4.544355996426689,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "0ebef0cfe225255149ed76d29443910d",
      "title": "Strengthening our safety ecosystem with external testing",
      "url": "https://openai.com/index/strengthening-safety-with-external-testing",
      "content": "OpenAI works with independent experts to evaluate frontier AI systems. Third-party testing strengthens safety, validates safeguards, and increases transparency in how we assess model capabilities and risks.",
      "summary": "OpenAI works with independent experts to evaluate frontier AI systems. Third-party testing strengthens safety, validates safeguards, and increases transparency in how we assess model capabilities and risks.",
      "publishedAt": "2025-11-19T12:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 3.288691168392226,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "testing"
      ]
    },
    {
      "id": "2aba7a56124890447b62f7caad623b9d",
      "title": "How evals drive the next chapter in AI for businesses",
      "url": "https://openai.com/index/evals-drive-next-chapter-of-ai",
      "content": "Learn how evals help businesses define, measure, and improve AI performance—reducing risk, boosting productivity, and driving strategic advantage.",
      "summary": "Learn how evals help businesses define, measure, and improve AI performance—reducing risk, boosting productivity, and driving strategic advantage.",
      "publishedAt": "2025-11-19T11:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 4.371890597006461,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "94c1a542d3710b59d4d9aec07e4b9c31",
      "title": "OpenAI and Target team up on new AI-powered experiences",
      "url": "https://openai.com/index/target-partnership",
      "content": "OpenAI and Target are partnering to bring a new Target app to ChatGPT, offering personalized shopping and faster checkout. Target will also expand its use of ChatGPT Enterprise to boost productivity and guest experiences.",
      "summary": "OpenAI and Target are partnering to bring a new Target app to ChatGPT, offering personalized shopping and faster checkout. Target will also expand its use of ChatGPT Enterprise to boost productivity and guest experiences.",
      "publishedAt": "2025-11-19T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "score": 7.178857284581019,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "2cebf18ae568df8657f35714cc29b1bc",
      "title": "GPT-5.1-Codex-Max System Card",
      "url": "https://openai.com/index/gpt-5-1-codex-max-system-card",
      "content": "This system card outlines the comprehensive safety measures implemented for GPT‑5.1-CodexMax. It details both model-level mitigations, such as specialized safety training for harmful tasks and prompt injections, and product-level mitigations like agent sandboxing and configurable network access.",
      "summary": "This system card outlines the comprehensive safety measures implemented for GPT‑5.1-CodexMax. It details both model-level mitigations, such as specialized safety training for harmful tasks and prompt injections, and product-level mitigations like agent sandboxing and configurable network access.",
      "publishedAt": "2025-11-19T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 7.051801209996129,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review",
        "agents"
      ]
    },
    {
      "id": "e6949d4d910f746de520e1d46478a327",
      "title": "Teacher Access Terms",
      "url": "https://openai.com/policies/education-terms",
      "content": "Teacher Access Terms outline how verified educators may use ChatGPT for Teachers, covering eligibility, account management, and data privacy requirements.",
      "summary": "Teacher Access Terms outline how verified educators may use ChatGPT for Teachers, covering eligibility, account management, and data privacy requirements.",
      "publishedAt": "2025-11-19T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 4.2310807259976775,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "17045c091cb6e6fcb5f6a7f519b1245e",
      "title": "How Scania is accelerating work with AI across its global workforce",
      "url": "https://openai.com/index/scania",
      "content": "Description: Global manufacturer Scania is scaling AI with ChatGPT Enterprise. With team-based onboarding and strong guardrails, AI is boosting productivity, quality, and innovation.",
      "summary": "Description: Global manufacturer Scania is scaling AI with ChatGPT Enterprise. With team-based onboarding and strong guardrails, AI is boosting productivity, quality, and innovation.",
      "publishedAt": "2025-11-19T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "score": 7.051801209996129,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "10c90d7cc7819d97e798a3759b2b0abf",
      "title": "A free version of ChatGPT built for teachers",
      "url": "https://openai.com/index/chatgpt-for-teachers",
      "content": "ChatGPT for Teachers is a secure workspace with education‑grade privacy and admin controls. Free for verified U.S. K–12 educators through June 2027.",
      "summary": "ChatGPT for Teachers is a secure workspace with education‑grade privacy and admin controls. Free for verified U.S. K–12 educators through June 2027.",
      "publishedAt": "2025-11-19T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 4.2310807259976775,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "8a6a49d607bcd5fa4da85f6781aec3aa",
      "title": "Building more with GPT-5.1-Codex-Max",
      "url": "https://openai.com/index/gpt-5-1-codex-max",
      "content": "Introducing GPT-5.1-Codex-Max, a faster, more intelligent agentic coding model for Codex. The model is designed for long-running, project-scale work with enhanced reasoning and token efficiency.",
      "summary": "Introducing GPT-5.1-Codex-Max, a faster, more intelligent agentic coding model for Codex. The model is designed for long-running, project-scale work with enhanced reasoning and token efficiency.",
      "publishedAt": "2025-11-19T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 7.7569813309957425,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review",
        "agents"
      ]
    },
    {
      "id": "b7f6337287f50eb44829e8f73a37876f",
      "title": "Intuit and OpenAI join forces on new AI-powered experiences",
      "url": "https://openai.com/index/intuit-partnership",
      "content": "OpenAI and Intuit have entered a $100M+ multi-year partnership to launch Intuit app experiences in ChatGPT and expand Intuit’s use of OpenAI’s frontier models to power personalized financial tools.",
      "summary": "OpenAI and Intuit have entered a $100M+ multi-year partnership to launch Intuit app experiences in ChatGPT and expand Intuit’s use of OpenAI’s frontier models to power personalized financial tools.",
      "publishedAt": "2025-11-18T05:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 5.997693269759067,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": []
    },
    {
      "id": "0ee69ec84e3309ca88e2a257f13122c4",
      "title": "OpenAI named Emerging Leader in Generative AI",
      "url": "https://openai.com/index/gartner-2025-emerging-leader",
      "content": "OpenAI has been named an Emerging Leader in Gartner’s 2025 Innovation Guide for Generative AI Model Providers. The recognition reflects our enterprise momentum, with over 1 million companies building with ChatGPT.",
      "summary": "OpenAI has been named an Emerging Leader in Gartner’s 2025 Innovation Guide for Generative AI Model Providers. The recognition reflects our enterprise momentum, with over 1 million companies building with ChatGPT.",
      "publishedAt": "2025-11-17T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "score": 6.927493203610769,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "41722a5599ac5dfac4a04db42448e49f",
      "title": "Introducing OpenAI for Ireland",
      "url": "https://openai.com/index/openai-for-ireland",
      "content": "OpenAI launches OpenAI for Ireland, partnering with the Irish Government, Dogpatch Labs and Patch to help SMEs, founders and young builders use AI to innovate, boost productivity and build the next generation of Irish tech startups.",
      "summary": "OpenAI launches OpenAI for Ireland, partnering with the Irish Government, Dogpatch Labs and Patch to help SMEs, founders and young builders use AI to innovate, boost productivity and build the next generation of Irish tech startups.",
      "publishedAt": "2025-11-14T04:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 5.991648221878992,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "965e60a14502559303595f93f6b0ee74",
      "title": "Understanding neural networks through sparse circuits",
      "url": "https://openai.com/index/understanding-neural-networks-through-sparse-circuits",
      "content": "OpenAI is exploring mechanistic interpretability to understand how neural networks reason. Our new sparse model approach could make AI systems more transparent and support safer, more reliable behavior.",
      "summary": "OpenAI is exploring mechanistic interpretability to understand how neural networks reason. Our new sparse model approach could make AI systems more transparent and support safer, more reliable behavior.",
      "publishedAt": "2025-11-13T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 2.8395566402437424,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "c78557dbdad9b4f12ac173ddd735e477",
      "title": "Introducing GPT-5.1 for developers",
      "url": "https://openai.com/index/gpt-5-1-for-developers",
      "content": "GPT-5.1 is now available in the API, bringing faster adaptive reasoning, extended prompt caching, improved coding performance, and new apply_patch and shell tools.",
      "summary": "GPT-5.1 is now available in the API, bringing faster adaptive reasoning, extended prompt caching, improved coding performance, and new apply_patch and shell tools.",
      "publishedAt": "2025-11-13T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 4.134436860722612,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "a7818ab7bde93e745d5e7e936314b982",
      "title": "How Philips is scaling AI literacy across 70,000 employees",
      "url": "https://openai.com/index/philips",
      "content": "Philips is scaling AI literacy with ChatGPT Enterprise, training 70,000 employees to use AI responsibly and improve healthcare outcomes worldwide.",
      "summary": "Philips is scaling AI literacy with ChatGPT Enterprise, training 70,000 employees to use AI responsibly and improve healthcare outcomes worldwide.",
      "publishedAt": "2025-11-13T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "score": 5.05320060754986,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "402d1c676b28ff91394671aa38a74c10",
      "title": "Introducing group chats in ChatGPT",
      "url": "https://openai.com/index/group-chats-in-chatgpt",
      "content": "We’re piloting group chats in ChatGPT to make collaboration simple. Bring others—and ChatGPT—into one shared conversation to plan, brainstorm, and create together.",
      "summary": "We’re piloting group chats in ChatGPT to make collaboration simple. Bring others—and ChatGPT—into one shared conversation to plan, brainstorm, and create together.",
      "publishedAt": "2025-11-13T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 2.7562912404817412,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": []
    },
    {
      "id": "39cff7c19932507f265b3e9ed2a3a7c3",
      "title": "Neuro drives national retail wins with ChatGPT Business",
      "url": "https://openai.com/index/neurogum",
      "content": "Neuro uses ChatGPT Business to scale nationwide with fewer than seventy employees. From drafting contracts to uncovering insights in customer data, the team saves time, cuts costs, and turns ideas into growth.",
      "summary": "Neuro uses ChatGPT Business to scale nationwide with fewer than seventy employees. From drafting contracts to uncovering insights in customer data, the team saves time, cuts costs, and turns ideas into growth.",
      "publishedAt": "2025-11-12T11:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 1.7677904586625248,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "ide"
      ]
    },
    {
      "id": "b377a85ca19b7ac5211137128691f9b5",
      "title": "Fighting the New York Times’ invasion of user privacy",
      "url": "https://openai.com/index/fighting-nyt-user-privacy-invasion",
      "content": "OpenAI is fighting the New York Times’ demand for 20 million private ChatGPT conversations and accelerating new security and privacy protections to protect your data.",
      "summary": "OpenAI is fighting the New York Times’ demand for 20 million private ChatGPT conversations and accelerating new security and privacy protections to protect your data.",
      "publishedAt": "2025-11-12T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "security_incident",
      "score": 4.789616749279747,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "36ade2f70cc53016a3448fc3a473cc35",
      "title": "GPT-5.1 Instant and GPT-5.1 Thinking System Card Addendum",
      "url": "https://openai.com/index/gpt-5-system-card-addendum-gpt-5-1",
      "content": "This GPT-5 system card addendum provides updated safety metrics for GPT-5.1 Instant and Thinking, including new evaluations for mental health and emotional reliance.",
      "summary": "This GPT-5 system card addendum provides updated safety metrics for GPT-5.1 Instant and Thinking, including new evaluations for mental health and emotional reliance.",
      "publishedAt": "2025-11-12T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "feature_update",
      "score": 4.277133640061299,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review",
        "ide",
        "observability"
      ]
    },
    {
      "id": "9cbe22667b832a7e186116d4a4eebc4c",
      "title": "GPT-5.1: A smarter, more conversational ChatGPT",
      "url": "https://openai.com/index/gpt-5-1",
      "content": "We’re upgrading the GPT-5 series with warmer, more capable models and new ways to customize ChatGPT’s tone and style. GPT-5.1 starts rolling out today to paid users.",
      "summary": "We’re upgrading the GPT-5 series with warmer, more capable models and new ways to customize ChatGPT’s tone and style. GPT-5.1 starts rolling out today to paid users.",
      "publishedAt": "2025-11-12T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 1.2831400920183897,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": []
    },
    {
      "id": "963896ca7550df724537c1d3a3f6fb07",
      "title": "Free ChatGPT for transitioning U.S. servicemembers and veterans",
      "url": "https://openai.com/index/chatgpt-for-veterans",
      "content": "OpenAI is offering U.S. servicemembers and veterans within 12 months of retirement or separation a free year of ChatGPT Plus to support their transition to civilian life. The tools can help with resumes, interviews, education, and planning for what’s next.",
      "summary": "OpenAI is offering U.S. servicemembers and veterans within 12 months of retirement or separation a free year of ChatGPT Plus to support their transition to civilian life. The tools can help with resumes, interviews, education, and planning for what’s next.",
      "publishedAt": "2025-11-10T02:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 1.1189665193618734,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": []
    },
    {
      "id": "fe9a5f2a83a904e45e648632b6863145",
      "title": "Understanding prompt injections: a frontier security challenge ",
      "url": "https://openai.com/index/prompt-injections",
      "content": "Prompt injections are a frontier security challenge for AI systems. Learn how these attacks work and how OpenAI is advancing research, training models, and building safeguards for users.",
      "summary": "Prompt injections are a frontier security challenge for AI systems. Learn how these attacks work and how OpenAI is advancing research, training models, and building safeguards for users.",
      "publishedAt": "2025-11-07T11:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "security_incident",
      "score": 3.4064700804250445,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "fbb44934033164d0cf9a7af53ff519bc",
      "title": "Notion’s rebuild for agentic AI: How GPT‑5 helped unlock autonomous workflows",
      "url": "https://openai.com/index/notion",
      "content": "Discover how Notion rebuilt its AI architecture with GPT-5 to create autonomous agents that reason, act, and adapt across workflows. Learn how this shift unlocked smarter, faster, and more flexible productivity in Notion 3.0.",
      "summary": "Discover how Notion rebuilt its AI architecture with GPT-5 to create autonomous agents that reason, act, and adapt across workflows. Learn how this shift unlocked smarter, faster, and more flexible productivity in Notion 3.0.",
      "publishedAt": "2025-11-07T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "feature_update",
      "score": 4.007895886640608,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review",
        "agents"
      ]
    },
    {
      "id": "79d8d3cdcb4b7b140ef6ae2a79cbfb99",
      "title": "From Pilot to Practice: How BBVA Is Scaling AI Across the Organization",
      "url": "https://openai.com/index/bbva-2025",
      "content": "BBVA is reimagining how employees work with ChatGPT Enterprise, embedding AI into everyday operations. The bank has saved hours per week per employee, created 20,000+ Custom GPTs, and achieved up to 80% efficiency gains.",
      "summary": "BBVA is reimagining how employees work with ChatGPT Enterprise, embedding AI into everyday operations. The bank has saved hours per week per employee, created 20,000+ Custom GPTs, and achieved up to 80% efficiency gains.",
      "publishedAt": "2025-11-06T09:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "score": 3.2961245458392203,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review",
        "retrieval"
      ]
    },
    {
      "id": "18d486d98d8e15ab9bd6e59a4cf8fb68",
      "title": "Introducing the Teen Safety Blueprint",
      "url": "https://openai.com/index/introducing-the-teen-safety-blueprint",
      "content": "Discover OpenAI’s Teen Safety Blueprint—a roadmap for building AI responsibly with safeguards, age-appropriate design, and collaboration to protect and empower young people online.",
      "summary": "Discover OpenAI’s Teen Safety Blueprint—a roadmap for building AI responsibly with safeguards, age-appropriate design, and collaboration to protect and empower young people online.",
      "publishedAt": "2025-11-06T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 2.5076627166743153,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "98b1ba064888fdf56902e113065c3f82",
      "title": "AI progress and recommendations",
      "url": "https://openai.com/index/ai-progress-and-recommendations",
      "content": "AI is advancing fast. We have the chance to shape its progress—toward discovery, safety, and a better future for everyone.",
      "summary": "AI is advancing fast. We have the chance to shape its progress—toward discovery, safety, and a better future for everyone.",
      "publishedAt": "2025-11-06T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 1.6717751444495437,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "b0c322f8b9c6c755a6cdaa6ded1b80bb",
      "title": "How CRED is tapping AI to deliver premium customer experiences",
      "url": "https://openai.com/index/cred-swamy-seetharaman",
      "content": "CRED is transforming premium customer experiences in India with OpenAI. Using GPT-powered tools, the company is improving support accuracy, reducing response times, and boosting customer satisfaction. ",
      "summary": "CRED is transforming premium customer experiences in India with OpenAI. Using GPT-powered tools, the company is improving support accuracy, reducing response times, and boosting customer satisfaction.",
      "publishedAt": "2025-11-05T21:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 1.659382502043861,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "903883be1da5291d9c594b8e86c84af9",
      "title": "How Chime is redefining marketing through AI",
      "url": "https://openai.com/index/chime-vineet-mehra",
      "content": "Vineet Mehra, Chief Marketing Officer at Chime, shares how AI is reshaping marketing into an agent-driven discipline. He explains why CMOs who champion AI literacy and thoughtful adoption will lead in the new era of growth.",
      "summary": "Vineet Mehra, Chief Marketing Officer at Chime, shares how AI is reshaping marketing into an agent-driven discipline. He explains why CMOs who champion AI literacy and thoughtful adoption will lead in the new era of growth.",
      "publishedAt": "2025-11-05T15:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 1.356324885034685,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "agents"
      ]
    },
    {
      "id": "0116832adda7180dde75dfc8f43bf0e9",
      "title": "1 million business customers putting AI to work",
      "url": "https://openai.com/index/1-million-businesses-putting-ai-to-work",
      "content": "More than 1 million business customers around the world now use OpenAI. Across healthcare, life sciences, financial services, and more, ChatGPT and our APIs are driving a new era of intelligent, AI-powered work.",
      "summary": "More than 1 million business customers around the world now use OpenAI. Across healthcare, life sciences, financial services, and more, ChatGPT and our APIs are driving a new era of intelligent, AI-powered work.",
      "publishedAt": "2025-11-05T05:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.7899317126241806,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": []
    },
    {
      "id": "cfa7130eb87b757a8f3079190eb6c6b6",
      "title": "Brazil’s AI moment is here",
      "url": "https://openai.com/global-affairs/brazil-ai-moment-is-here",
      "content": "Brazil is now one of the most engaged countries in the world when it comes to AI. From classrooms to farms and small businesses, Brazilians are using OpenAI products to learn, create, and drive innovation.",
      "summary": "Brazil is now one of the most engaged countries in the world when it comes to AI. From classrooms to farms and small businesses, Brazilians are using OpenAI products to learn, create, and drive innovation.",
      "publishedAt": "2025-11-04T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 1.493004876118862,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "5f20663e3457bcdb3265601be63b85dd",
      "title": "Introducing IndQA",
      "url": "https://openai.com/index/introducing-indqa",
      "content": "OpenAI introduces IndQA, a new benchmark for evaluating AI systems in Indian languages. Built with domain experts, IndQA tests cultural understanding and reasoning across 12 languages and 10 knowledge areas.",
      "summary": "OpenAI introduces IndQA, a new benchmark for evaluating AI systems in Indian languages. Built with domain experts, IndQA tests cultural understanding and reasoning across 12 languages and 10 knowledge areas.",
      "publishedAt": "2025-11-03T22:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 1.4427695918899535,
      "ingestedAt": "2025-11-23T21:21:55.721Z",
      "tags": []
    },
    {
      "id": "e68f6330a84831492f791da1bba7c027",
      "title": "AWS and OpenAI announce multi-year strategic partnership",
      "url": "https://openai.com/index/aws-and-openai-partnership",
      "content": "OpenAI and AWS have entered a multi-year, $38 billion partnership to scale advanced AI workloads. AWS will provide world-class infrastructure and compute capacity to power OpenAI’s next generation of models.",
      "summary": "OpenAI and AWS have entered a multi-year, $38 billion partnership to scale advanced AI workloads. AWS will provide world-class infrastructure and compute capacity to power OpenAI’s next generation of models.",
      "publishedAt": "2025-11-03T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 1.6025692621772123,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "505e58f8ab61940d5c691b82e6f50f7e",
      "title": "Expanding Stargate to Michigan",
      "url": "https://openai.com/index/expanding-stargate-to-michigan",
      "content": "OpenAI is expanding Stargate to Michigan with a new one-gigawatt campus that strengthens America’s AI infrastructure. The project will create jobs, drive investment, and support economic growth across the Midwest.",
      "summary": "OpenAI is expanding Stargate to Michigan with a new one-gigawatt campus that strengthens America’s AI infrastructure. The project will create jobs, drive investment, and support economic growth across the Midwest.",
      "publishedAt": "2025-10-30T13:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 1.0555527815230117,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "c4440d30b857fbf5c0a2e557dd7047c2",
      "title": "Introducing Aardvark: OpenAI’s agentic security researcher",
      "url": "https://openai.com/index/introducing-aardvark",
      "content": "OpenAI introduces Aardvark, an AI-powered security researcher that autonomously finds, validates, and helps fix software vulnerabilities at scale. The system is in private beta—sign up to join early testing.",
      "summary": "OpenAI introduces Aardvark, an AI-powered security researcher that autonomously finds, validates, and helps fix software vulnerabilities at scale. The system is in private beta—sign up to join early testing.",
      "publishedAt": "2025-10-30T11:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 3.055873659756385,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review",
        "agents",
        "testing"
      ]
    },
    {
      "id": "da48b434eb0d31b33c96b13edca512c6",
      "title": "How we built OWL, the new architecture behind our ChatGPT-based browser, Atlas",
      "url": "https://openai.com/index/building-chatgpt-atlas",
      "content": "A deep dive into OWL, the new architecture powering ChatGPT Atlas—decoupling Chromium, enabling fast startup, rich UI, and agentic browsing with ChatGPT.",
      "summary": "A deep dive into OWL, the new architecture powering ChatGPT Atlas—decoupling Chromium, enabling fast startup, rich UI, and agentic browsing with ChatGPT.",
      "publishedAt": "2025-10-30T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "thought_leadership",
      "score": 0.7604871603119138,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "agents"
      ]
    },
    {
      "id": "38a263f439fa2281877e4af4ff39e33c",
      "title": "Technical Report: Performance and baseline evaluations of gpt-oss-safeguard-120b and gpt-oss-safeguard-20b",
      "url": "https://openai.com/index/gpt-oss-safeguard-technical-report",
      "content": "gpt-oss-safeguard-120b and gpt-oss-safeguard-20b are two open-weight reasoning models post-trained from the gpt-oss models and trained to reason from a provided policy in order to label content under that policy. In this report, we describe gpt-oss-safeguard’s capabilities and provide our baseline safety evaluations on the gpt-oss-safeguard models, using the underlying gpt-oss models as a baseline. For more information about the development and architecture of the underlying gpt-oss models, see the original gpt-oss model model card⁠.",
      "summary": "gpt-oss-safeguard-120b and gpt-oss-safeguard-20b are two open-weight reasoning models post-trained from the gpt-oss models and trained to reason from a provided policy in order to label content under that policy. In this report, we describe gpt-oss-safeguard’s capabilities and provide our baseline safety evaluations on the gpt-oss-safeguard models, using the underlying gpt-oss models as a baseline. For more information about the development and architecture of the underlying gpt-oss models, see the original gpt-oss model model card⁠.",
      "publishedAt": "2025-10-29T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 1.7308164852225603,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review",
        "ide",
        "governance"
      ]
    },
    {
      "id": "d210a60ba3c9ac11de685cfaad8c759f",
      "title": "Introducing gpt-oss-safeguard",
      "url": "https://openai.com/index/introducing-gpt-oss-safeguard",
      "content": "OpenAI introduces gpt-oss-safeguard—open-weight reasoning models for safety classification that let developers apply and iterate on custom policies.",
      "summary": "OpenAI introduces gpt-oss-safeguard—open-weight reasoning models for safety classification that let developers apply and iterate on custom policies.",
      "publishedAt": "2025-10-29T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.9440817192123057,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": []
    },
    {
      "id": "d0aeb835ebcc55244a78cab0dea6c915",
      "title": "Knowledge preservation powered by ChatGPT",
      "url": "https://openai.com/index/dai-nippon-printing",
      "content": "Dai Nippon Printing (DNP) rolled out ChatGPT Enterprise across ten core departments to drive companywide adoption. Within three months, it achieved 95% faster patent research, 10x processing volume, 100% weekly active usage, 87% automation, and 70% knowledge reuse.",
      "summary": "Dai Nippon Printing (DNP) rolled out ChatGPT Enterprise across ten core departments to drive companywide adoption. Within three months, it achieved 95% faster patent research, 10x processing volume, 100% weekly active usage, 87% automation, and 70% knowledge reuse.",
      "publishedAt": "2025-10-28T17:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "score": 1.6951308247813415,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "c9a5421993a12907c5ef4f4b2a1bbb29",
      "title": "Doppel’s AI defense system stops attacks before they spread",
      "url": "https://openai.com/index/doppel",
      "content": "Discover how Doppel uses OpenAI’s GPT-5 and reinforcement fine-tuning (RFT) to stop deepfake and impersonation attacks before they spread, cutting analyst workloads by 80% and reducing threat response from hours to minutes.",
      "summary": "Discover how Doppel uses OpenAI’s GPT-5 and reinforcement fine-tuning (RFT) to stop deepfake and impersonation attacks before they spread, cutting analyst workloads by 80% and reducing threat response from hours to minutes.",
      "publishedAt": "2025-10-28T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.9055532317179135,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "e38d1cab2aaf508a8f8c92e705c84a1e",
      "title": "The next chapter of the Microsoft–OpenAI partnership",
      "url": "https://openai.com/index/next-chapter-of-microsoft-openai-partnership",
      "content": "Microsoft and OpenAI sign a new agreement that strengthens its long-term partnership, expands innovation, and ensures responsible AI progress.",
      "summary": "Microsoft and OpenAI sign a new agreement that strengthens its long-term partnership, expands innovation, and ensures responsible AI progress.",
      "publishedAt": "2025-10-28T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.8948367512408248,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "3b1dccf662884f705b8f3ee8f71556ad",
      "title": "Built to benefit everyone",
      "url": "https://openai.com/index/built-to-benefit-everyone",
      "content": "OpenAI’s recapitalization strengthens mission-focused governance, expanding resources to ensure AI benefits everyone while advancing innovation responsibly.",
      "summary": "OpenAI’s recapitalization strengthens mission-focused governance, expanding resources to ensure AI benefits everyone while advancing innovation responsibly.",
      "publishedAt": "2025-10-28T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 1.0439762097809622,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "governance"
      ]
    },
    {
      "id": "0b114fe032efff2c9e404ee3519459ad",
      "title": "Seizing the AI opportunity",
      "url": "https://openai.com/global-affairs/seizing-the-ai-opportunity",
      "content": "Meeting the demands of the Intelligence Age will require strategic investment in energy and infrastructure. OpenAI’s submission to the White House details how expanding capacity and workforce readiness can sustain U.S. leadership in AI and economic growth.",
      "summary": "Meeting the demands of the Intelligence Age will require strategic investment in energy and infrastructure. OpenAI’s submission to the White House details how expanding capacity and workforce readiness can sustain U.S. leadership in AI and economic growth.",
      "publishedAt": "2025-10-27T12:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.42408024383700854,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": []
    },
    {
      "id": "0b4d41f3f0256100b027dc00b4f17856",
      "title": "Strengthening ChatGPT’s responses in sensitive conversations",
      "url": "https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations",
      "content": "OpenAI collaborated with 170+ mental health experts to improve ChatGPT’s ability to recognize distress, respond empathetically, and guide users toward real-world support—reducing unsafe responses by up to 80%. Learn how we’re making ChatGPT safer and more supportive in sensitive moments.",
      "summary": "OpenAI collaborated with 170+ mental health experts to improve ChatGPT’s ability to recognize distress, respond empathetically, and guide users toward real-world support—reducing unsafe responses by up to 80%. Learn how we’re making ChatGPT safer and more supportive in sensitive moments.",
      "publishedAt": "2025-10-27T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.9836480606087818,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "645cb42d567084e0db8ca964b86b17cb",
      "title": "Addendum to GPT-5 System Card: Sensitive conversations",
      "url": "https://openai.com/index/gpt-5-system-card-sensitive-conversations",
      "content": "This system card details GPT-5’s improvements in handling sensitive conversations, including new benchmarks for emotional reliance, mental health, and jailbreak resistance.",
      "summary": "This system card details GPT-5’s improvements in handling sensitive conversations, including new benchmarks for emotional reliance, mental health, and jailbreak resistance.",
      "publishedAt": "2025-10-27T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "benchmark_eval",
      "score": 0.9836480606087818,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "530334a2b1f084930d8c41f1f42dbc97",
      "title": "A law and tax firm redefines efficiency with ChatGPT Business",
      "url": "https://openai.com/index/steuerrecht",
      "content": "Learn how Steuerrecht.com uses ChatGPT Business to streamline legal workflows, automate tax research, and scale client service—helping law firms boost productivity and stay competitive.",
      "summary": "Learn how Steuerrecht.com uses ChatGPT Business to streamline legal workflows, automate tax research, and scale client service—helping law firms boost productivity and stay competitive.",
      "publishedAt": "2025-10-27T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.8184035779433043,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "56c92596194fcf972d95186fa117d291",
      "title": "OpenAI acquires Software Applications Incorporated, maker of Sky",
      "url": "https://openai.com/index/openai-acquires-software-applications-incorporated",
      "content": "OpenAI has acquired Software Applications Incorporated, maker of Sky—a natural language interface for Mac that brings AI directly into your desktop experience. Together, we’re integrating Sky’s deep macOS capabilities into ChatGPT to make AI more intuitive, contextual, and action-oriented.",
      "summary": "OpenAI has acquired Software Applications Incorporated, maker of Sky—a natural language interface for Mac that brings AI directly into your desktop experience. Together, we’re integrating Sky’s deep macOS capabilities into ChatGPT to make AI more intuitive, contextual, and action-oriented.",
      "publishedAt": "2025-10-23T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "funding_mna",
      "score": 0.5279922728036015,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": []
    },
    {
      "id": "64b3f66ab547fb2f8b4bad62612426e1",
      "title": "Consensus accelerates research with GPT-5 and Responses API",
      "url": "https://openai.com/index/consensus",
      "content": "Consensus uses GPT-5 and OpenAI’s Responses API to power a multi-agent research assistant that reads, analyzes, and synthesizes evidence in minutes—helping over 8 million researchers accelerate scientific discovery.",
      "summary": "Consensus uses GPT-5 and OpenAI’s Responses API to power a multi-agent research assistant that reads, analyzes, and synthesizes evidence in minutes—helping over 8 million researchers accelerate scientific discovery.",
      "publishedAt": "2025-10-23T09:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.6317078439753784,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "agents",
        "ide"
      ]
    },
    {
      "id": "98db91455deebf842653cbcf27fddc0c",
      "title": "AI in South Korea—OpenAI’s Economic Blueprint",
      "url": "https://openai.com/index/south-korea-economic-blueprint",
      "content": "OpenAI's Korea Economic Blueprint outlines how South Korea can scale trusted AI through sovereign capabilities and strategic partnerships to drive growth.",
      "summary": "OpenAI's Korea Economic Blueprint outlines how South Korea can scale trusted AI through sovereign capabilities and strategic partnerships to drive growth.",
      "publishedAt": "2025-10-23T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.6150117053959632,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "6cab6185e60b3c00f2df28ddd700a237",
      "title": "Work smarter with your company knowledge in ChatGPT",
      "url": "https://openai.com/index/introducing-company-knowledge",
      "content": "Company knowledge brings context from your apps into ChatGPT for answers specific to your business, with clear citations, security, privacy, and admin controls. Available now for Business, Enterprise, and Edu users.",
      "summary": "Company knowledge brings context from your apps into ChatGPT for answers specific to your business, with clear citations, security, privacy, and admin controls. Available now for Business, Enterprise, and Edu users.",
      "publishedAt": "2025-10-23T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "score": 1.2300234107919263,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "9e3769b2689a02b5c992bdc0aeeeff10",
      "title": "The next chapter for UK sovereign AI",
      "url": "https://openai.com/index/the-next-chapter-for-uk-sovereign-ai",
      "content": "OpenAI expands its UK partnership with a new Ministry of Justice agreement, bringing ChatGPT to civil servants. It also introduces UK data residency for ChatGPT Enterprise, ChatGPT Edu, and the API Platform to support trusted and secure AI adoption.",
      "summary": "OpenAI expands its UK partnership with a new Ministry of Justice agreement, bringing ChatGPT to civil servants. It also introduces UK data residency for ChatGPT Enterprise, ChatGPT Edu, and the API Platform to support trusted and secure AI adoption.",
      "publishedAt": "2025-10-22T16:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "score": 1.100992781677926,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "be4be33746705b0d0950d2be47f72e6c",
      "title": "AI in Japan—OpenAI’s Japan Economic Blueprint",
      "url": "https://openai.com/index/japan-economic-blueprint",
      "content": "OpenAI’s Japan Economic Blueprint outlines how Japan can harness AI to boost innovation, strengthen competitiveness, and enable sustainable, inclusive growth.",
      "summary": "OpenAI’s Japan Economic Blueprint outlines how Japan can harness AI to boost innovation, strengthen competitiveness, and enable sustainable, inclusive growth.",
      "publishedAt": "2025-10-22T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.5726145079764772,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "e4d1b0c932ea2468037dbe669b213a6d",
      "title": "Continue your ChatGPT experience beyond WhatsApp",
      "url": "https://openai.com/index/chatgpt-whatsapp-transition",
      "content": "ChatGPT will no longer be available on WhatsApp after January 15, 2026. Learn how to link your ChatGPT account and continue your conversations across devices.",
      "summary": "ChatGPT will no longer be available on WhatsApp after January 15, 2026. Learn how to link your ChatGPT account and continue your conversations across devices.",
      "publishedAt": "2025-10-21T17:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "thought_leadership",
      "score": 0.2336701857849653,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": []
    },
    {
      "id": "b9ca7a0cbf8b850f689c19fbc99e7ea3",
      "title": "Introducing ChatGPT Atlas, the browser with ChatGPT built in",
      "url": "https://openai.com/index/introducing-chatgpt-atlas",
      "content": "ChatGPT Atlas, the browser with ChatGPT built it. Get instant answers, summaries, and smart web help—right from any page. With privacy settings you can control. Available now for MacOS.",
      "summary": "ChatGPT Atlas, the browser with ChatGPT built it. Get instant answers, summaries, and smart web help—right from any page. With privacy settings you can control. Available now for MacOS.",
      "publishedAt": "2025-10-21T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.7997100832431452,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "28547405a2a03a9d9f75af7980ce49d4",
      "title": "Plex Coffee delivers fast service and personal connections with ChatGPT Business",
      "url": "https://openai.com/index/plex-coffee",
      "content": "Learn how Plex Coffee uses ChatGPT Business to centralize knowledge, train staff faster, and preserve personal connections while expanding.",
      "summary": "Learn how Plex Coffee uses ChatGPT Business to centralize knowledge, train staff faster, and preserve personal connections while expanding.",
      "publishedAt": "2025-10-15T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.34730825528399767,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "1add205a7b950a6c34f0aff1705b621a",
      "title": "Expert Council on Well-Being and AI",
      "url": "https://openai.com/index/expert-council-on-well-being-and-ai",
      "content": "OpenAI’s new Expert Council on Well-Being and AI brings together leading psychologists, clinicians, and researchers to guide how ChatGPT supports emotional health, especially for teens. Learn how their insights are shaping safer, more caring AI experiences.",
      "summary": "OpenAI’s new Expert Council on Well-Being and AI brings together leading psychologists, clinicians, and researchers to guide how ChatGPT supports emotional health, especially for teens. Learn how their insights are shaping safer, more caring AI experiences.",
      "publishedAt": "2025-10-14T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.2220896112235864,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "ide"
      ]
    },
    {
      "id": "1dc90d6c45eeb820572ee35457dd3973",
      "title": "Argentina’s AI opportunity",
      "url": "https://openai.com/global-affairs/argentinas-ai-opportunity",
      "content": "OpenAI and Sur Energy are exploring Argentina’s first Stargate project—an AI and clean energy collaboration that could make Argentina a Latin American leader in artificial intelligence, sustainable infrastructure, and digital innovation.",
      "summary": "OpenAI and Sur Energy are exploring Argentina’s first Stargate project—an AI and clean energy collaboration that could make Argentina a Latin American leader in artificial intelligence, sustainable infrastructure, and digital innovation.",
      "publishedAt": "2025-10-14T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.3291920439861436,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "bb222ce6ef6f18b3cbd95657b37d61e2",
      "title": "OpenAI and Broadcom announce strategic collaboration to deploy 10 gigawatts of OpenAI-designed AI accelerators",
      "url": "https://openai.com/index/openai-and-broadcom-announce-strategic-collaboration",
      "content": "OpenAI and Broadcom announce a multi-year partnership to deploy 10 gigawatts of OpenAI-designed AI accelerators, co-developing next-generation systems and Ethernet solutions to power scalable, energy-efficient AI infrastructure by 2029.",
      "summary": "OpenAI and Broadcom announce a multi-year partnership to deploy 10 gigawatts of OpenAI-designed AI accelerators, co-developing next-generation systems and Ethernet solutions to power scalable, energy-efficient AI infrastructure by 2029.",
      "publishedAt": "2025-10-13T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.15324922976509395,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": []
    },
    {
      "id": "cc033ceceb1f0dd641d673a39364802a",
      "title": "HYGH powers next-gen digital ads with ChatGPT Business",
      "url": "https://openai.com/index/hygh",
      "content": "HYGH speeds up software development and campaign delivery with ChatGPT Business, cutting turnaround times, scaling output, and driving revenue growth.",
      "summary": "HYGH speeds up software development and campaign delivery with ChatGPT Business, cutting turnaround times, scaling output, and driving revenue growth.",
      "publishedAt": "2025-10-10T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.12150102411294209,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": []
    },
    {
      "id": "e82e5aa8257125e1a056f2c6f042b7ed",
      "title": "Defining and evaluating political bias in LLMs",
      "url": "https://openai.com/index/defining-and-evaluating-political-bias-in-llms",
      "content": "Learn how OpenAI evaluates political bias in ChatGPT through new real-world testing methods that improve objectivity and reduce bias.",
      "summary": "Learn how OpenAI evaluates political bias in ChatGPT through new real-world testing methods that improve objectivity and reduce bias.",
      "publishedAt": "2025-10-09T13:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.29396929652428383,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review",
        "testing"
      ]
    },
    {
      "id": "c4c17fc22989318d842838e2d6084c2c",
      "title": "Growing impact and scale with ChatGPT",
      "url": "https://openai.com/index/hibob",
      "content": "Discover how HiBob uses ChatGPT Enterprise and custom GPTs to scale AI adoption, boost revenue, streamline HR workflows, and deliver AI-powered features in the Bob platform.",
      "summary": "Discover how HiBob uses ChatGPT Enterprise and custom GPTs to scale AI adoption, boost revenue, streamline HR workflows, and deliver AI-powered features in the Bob platform.",
      "publishedAt": "2025-10-08T08:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "score": 0.35954806828254354,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "04d6a1d695c66b58622e52c69cb7b03f",
      "title": "Disrupting malicious uses of AI: October 2025",
      "url": "https://openai.com/global-affairs/disrupting-malicious-uses-of-ai-october-2025",
      "content": "Discover how OpenAI is detecting and disrupting malicious uses of AI in our October 2025 report. Learn how we’re countering misuse, enforcing policies, and protecting users from real-world harms.",
      "summary": "Discover how OpenAI is detecting and disrupting malicious uses of AI in our October 2025 report. Learn how we’re countering misuse, enforcing policies, and protecting users from real-world harms.",
      "publishedAt": "2025-10-07T03:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.19789027874808474,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "cfa376d35cdc706ff41e08fab86264a2",
      "title": "Codex is now generally available",
      "url": "https://openai.com/index/codex-now-generally-available",
      "content": "OpenAI Codex is now generally available with powerful new features for developers: a Slack integration, Codex SDK, and admin tools like usage dashboards and workspace management—making Codex easier to use and manage at scale.",
      "summary": "OpenAI Codex is now generally available with powerful new features for developers: a Slack integration, Codex SDK, and admin tools like usage dashboards and workspace management—making Codex easier to use and manage at scale.",
      "publishedAt": "2025-10-06T10:50:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.28289130938793305,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": []
    },
    {
      "id": "827fa6105627442a73e477a0c4b0877e",
      "title": "Introducing apps in ChatGPT and the new Apps SDK",
      "url": "https://openai.com/index/introducing-apps-in-chatgpt",
      "content": "We’re introducing a new generation of apps you can chat with, right inside ChatGPT. Developers can start building them today with the new Apps SDK, available in preview.",
      "summary": "We’re introducing a new generation of apps you can chat with, right inside ChatGPT. Developers can start building them today with the new Apps SDK, available in preview.",
      "publishedAt": "2025-10-06T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.3135450704190426,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "5a187a71faa5ec4d12cac0311ea99cf3",
      "title": "AMD and OpenAI announce strategic partnership to deploy 6 gigawatts of AMD GPUs",
      "url": "https://openai.com/index/openai-amd-strategic-partnership",
      "content": "AMD and OpenAI have announced a multi-year partnership to deploy 6 gigawatts of AMD Instinct GPUs, beginning with 1 gigawatt in 2026, to power OpenAI’s next-generation AI infrastructure and accelerate global AI innovation.",
      "summary": "AMD and OpenAI have announced a multi-year partnership to deploy 6 gigawatts of AMD Instinct GPUs, beginning with 1 gigawatt in 2026, to power OpenAI’s next-generation AI infrastructure and accelerate global AI innovation.",
      "publishedAt": "2025-10-06T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.09295035642987537,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": []
    },
    {
      "id": "85ca9d16f1edb3b31b657898c77ac088",
      "title": "Introducing AgentKit, new Evals, and RFT for agents",
      "url": "https://openai.com/index/introducing-agentkit",
      "content": "Today, we’re releasing  new tools to help developers go from prototype to production faster: AgentKit, expanded evals capabilities, and reinforcement fine-tuning for agents.",
      "summary": "Today, we’re releasing  new tools to help developers go from prototype to production faster: AgentKit, expanded evals capabilities, and reinforcement fine-tuning for agents.",
      "publishedAt": "2025-10-06T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.4260912166292541,
      "ingestedAt": "2025-11-23T21:21:55.722Z",
      "tags": [
        "code_review",
        "agents"
      ]
    },
    {
      "id": "1477c89fc230ebc2a6a524720fca682d",
      "title": "Accelerating AI adoption in Europe",
      "url": "https://openai.com/global-affairs/accelerating-ai-uptake-in-europe",
      "content": "OpenAI and Allied for Startups release the Hacktivate AI report with 20 actionable policy ideas to accelerate AI adoption in Europe, boost competitiveness, and empower innovators.",
      "summary": "OpenAI and Allied for Startups release the Hacktivate AI report with 20 actionable policy ideas to accelerate AI adoption in Europe, boost competitiveness, and empower innovators.",
      "publishedAt": "2025-10-06T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.24348069501542646,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "ide",
        "governance"
      ]
    },
    {
      "id": "dbc72c125358e94388b0d3c5f49c854c",
      "title": "With GPT-5, Wrtn builds lifestyle AI for millions in Korea",
      "url": "https://openai.com/index/wrtn",
      "content": "Wrtn scaled AI apps to 6.5M users in Korea with GPT-5, creating ‘Lifestyle AI’ that blends productivity, creativity, and learning—now expanding across East Asia.",
      "summary": "Wrtn scaled AI apps to 6.5M users in Korea with GPT-5, creating ‘Lifestyle AI’ that blends productivity, creativity, and learning—now expanding across East Asia.",
      "publishedAt": "2025-10-02T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.14137320034848527,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "20d626c31e241a549f0fca70bdc6f587",
      "title": "OpenAI announces strategic collaboration with Japan’s Digital Agency",
      "url": "https://openai.com/global-affairs/strategic-collaboration-with-japan-digital-agency",
      "content": "OpenAI and Japan’s Digital Agency partner to advance generative AI in public services, support international AI governance, and promote safe, trustworthy AI adoption worldwide.",
      "summary": "OpenAI and Japan’s Digital Agency partner to advance generative AI in public services, support international AI governance, and promote safe, trustworthy AI adoption worldwide.",
      "publishedAt": "2025-10-02T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.2515840437086376,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "code_review",
        "ide",
        "governance"
      ]
    },
    {
      "id": "e484c6c3bfd6bfbe7d8dd664728de4dc",
      "title": "Samsung and SK join OpenAI’s Stargate initiative to advance global AI infrastructure",
      "url": "https://openai.com/index/samsung-and-sk-join-stargate",
      "content": "Samsung and SK join OpenAI’s Stargate initiative to expand global AI infrastructure, scaling advanced memory chip production and building next-gen data centers in Korea.",
      "summary": "Samsung and SK join OpenAI’s Stargate initiative to expand global AI infrastructure, scaling advanced memory chip production and building next-gen data centers in Korea.",
      "publishedAt": "2025-10-01T03:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.12891345657563485,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "4d9450ab9ccf696eb09be15d4a74ed55",
      "title": "The Sora feed philosophy",
      "url": "https://openai.com/index/sora-feed-philosophy",
      "content": "Discover the Sora feed philosophy—built to spark creativity, foster connections, and keep experiences safe with personalized recommendations, parental controls, and strong guardrails.",
      "summary": "Discover the Sora feed philosophy—built to spark creativity, foster connections, and keep experiences safe with personalized recommendations, parental controls, and strong guardrails.",
      "publishedAt": "2025-09-30T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.06127665149952828,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": []
    },
    {
      "id": "2fac130322d848b9b924cf9b4ddbf67a",
      "title": "Sora 2 is here ",
      "url": "https://openai.com/index/sora-2",
      "content": "Our latest video generation model is more physically accurate, realistic, and controllable than prior systems. It also features synchronized dialogue and sound effects. Create with it in the new Sora app.",
      "summary": "Our latest video generation model is more physically accurate, realistic, and controllable than prior systems. It also features synchronized dialogue and sound effects. Create with it in the new Sora app.",
      "publishedAt": "2025-09-30T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.1387862301768738,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "f69a340d1942c483799966425ef86bdf",
      "title": "Launching Sora responsibly",
      "url": "https://openai.com/index/launching-sora-responsibly",
      "content": "To address the novel safety challenges posed by a state-of-the-art video model as well as a new social creation platform, we’ve built Sora 2 and the Sora app with safety at the foundation. Our approach is anchored in concrete protections.",
      "summary": "To address the novel safety challenges posed by a state-of-the-art video model as well as a new social creation platform, we’ve built Sora 2 and the Sora app with safety at the foundation. Our approach is anchored in concrete protections.",
      "publishedAt": "2025-09-30T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.2577458560427656,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "a6fde770d64655cd6c8349474e7d5696",
      "title": "Sora 2 System Card",
      "url": "https://openai.com/index/sora-2-system-card",
      "content": "Sora 2 is our new state of the art video and audio generation model. Building on the foundation of Sora, this new model introduces capabilities that have been difficult for prior video models to achieve– such as more accurate physics, sharper realism, synchronized audio, enhanced steerability, and an expanded stylistic range.",
      "summary": "Sora 2 is our new state of the art video and audio generation model. Building on the foundation of Sora, this new model introduces capabilities that have been difficult for prior video models to achieve– such as more accurate physics, sharper realism, synchronized audio, enhanced steerability, and an expanded stylistic range.",
      "publishedAt": "2025-09-30T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.1387862301768738,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "5dddb19ef1f395f47468164a91736c7e",
      "title": "Converting inbound leads into customers at OpenAI",
      "url": "https://openai.com/index/openai-inbound-sales-assistant",
      "content": "Learn how OpenAI used AI to deliver personalized answers at scale, converting inbound leads into customers.",
      "summary": "Learn how OpenAI used AI to deliver personalized answers at scale, converting inbound leads into customers.",
      "publishedAt": "2025-09-29T13:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.05764981147504705,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": []
    },
    {
      "id": "f08b360665f3ef27edf5f2e4a308247a",
      "title": "Turning contracts into searchable data at OpenAI",
      "url": "https://openai.com/index/openai-contract-data-agent",
      "content": "OpenAI built a system to extract contract data quickly, cutting turnaround times and making it easier for teams to access the details they need.",
      "summary": "OpenAI built a system to extract contract data quickly, cutting turnaround times and making it easier for teams to access the details they need.",
      "publishedAt": "2025-09-29T13:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.05764981147504705,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": []
    },
    {
      "id": "bb50d1f5dcd988ee2be4db2bd185a5f0",
      "title": "Improving support with every interaction at OpenAI",
      "url": "https://openai.com/index/openai-support-model",
      "content": "Learn how OpenAI uses AI to enhance support, cutting response times, improving quality, and scaling to meet hypergrowth.",
      "summary": "Learn how OpenAI uses AI to enhance support, cutting response times, improving quality, and scaling to meet hypergrowth.",
      "publishedAt": "2025-09-29T13:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.1152996229500941,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "f971e17cab1cf7230ce6366613bf6f73",
      "title": "Empowering teams to unlock insights faster at OpenAI",
      "url": "https://openai.com/index/openai-research-assistant",
      "content": "OpenAI’s research assistant helps teams analyze millions of support tickets, surface insights faster, and scale curiosity across the company.",
      "summary": "OpenAI’s research assistant helps teams analyze millions of support tickets, surface insights faster, and scale curiosity across the company.",
      "publishedAt": "2025-09-29T13:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.05764981147504705,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": []
    },
    {
      "id": "8b0251310f43d0b052900ad6d0eebb3d",
      "title": "Driving sales productivity and customer success at OpenAI",
      "url": "https://openai.com/index/openai-gtm-assistant",
      "content": "Learn how OpenAI boosts sales productivity by automating prep, centralizing knowledge, and scaling top-selling practices.",
      "summary": "Learn how OpenAI boosts sales productivity by automating prep, centralizing knowledge, and scaling top-selling practices.",
      "publishedAt": "2025-09-29T13:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.1152996229500941,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "975901811b7cd7780e3112a4d4b312a0",
      "title": "Building OpenAI with OpenAI",
      "url": "https://openai.com/index/building-openai-with-openai",
      "content": "At OpenAI, we rely on our own technology to help streamline work, scale expertise, and drive outcomes. In our new series, OpenAI on OpenAI, we share lessons to help other organizations do the same.",
      "summary": "At OpenAI, we rely on our own technology to help streamline work, scale expertise, and drive outcomes. In our new series, OpenAI on OpenAI, we share lessons to help other organizations do the same.",
      "publishedAt": "2025-09-29T13:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.05764981147504705,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": []
    },
    {
      "id": "094b6d8bc0c0e95589a0abdb802878e7",
      "title": "Introducing parental controls",
      "url": "https://openai.com/index/introducing-parental-controls",
      "content": "We’re rolling out parental controls and a new parent resource page to help families guide how ChatGPT works in their homes. ",
      "summary": "We’re rolling out parental controls and a new parent resource page to help families guide how ChatGPT works in their homes.",
      "publishedAt": "2025-09-29T03:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.13037759756679304,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "ide"
      ]
    },
    {
      "id": "ccfe69df641ddbdd0816ab0322aff250",
      "title": "Combating online child sexual exploitation & abuse",
      "url": "https://openai.com/index/combating-online-child-sexual-exploitation-abuse",
      "content": "Discover how OpenAI combats online child sexual exploitation and abuse with strict usage policies, advanced detection tools, and industry collaboration to block, report, and prevent AI misuse.",
      "summary": "Discover how OpenAI combats online child sexual exploitation and abuse with strict usage policies, advanced detection tools, and industry collaboration to block, report, and prevent AI misuse.",
      "publishedAt": "2025-09-29T03:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "security_incident",
      "score": 0.1676283397287339,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "a08a05bcf099abba82d52afaff908801",
      "title": "Buy it in ChatGPT: Instant Checkout and the Agentic Commerce Protocol",
      "url": "https://openai.com/index/buy-it-in-chatgpt",
      "content": "We’re taking first steps toward agentic commerce in ChatGPT with new ways for people, AI agents, and businesses to shop together.",
      "summary": "We’re taking first steps toward agentic commerce in ChatGPT with new ways for people, AI agents, and businesses to shop together.",
      "publishedAt": "2025-09-29T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "feature_update",
      "score": 0.23997757318437027,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "code_review",
        "agents"
      ]
    },
    {
      "id": "eb382ae71c88bf997db66713d5c18306",
      "title": "Partnering with AARP to help keep older adults safe online",
      "url": "https://openai.com/index/aarp-partnership-older-adults-online-safety",
      "content": "OpenAI and AARP are partnering to help older adults stay safe online with new AI training, scam-spotting tools, and nationwide programs through OpenAI Academy and OATS’s Senior Planet initiative.",
      "summary": "OpenAI and AARP are partnering to help older adults stay safe online with new AI training, scam-spotting tools, and nationwide programs through OpenAI Academy and OATS’s Senior Planet initiative.",
      "publishedAt": "2025-09-26T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.10617383397943748,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "51e0b2cb3881a875c1accfb152c1d346",
      "title": "More ways to work with your team and tools in ChatGPT",
      "url": "https://openai.com/index/more-ways-to-work-with-your-team",
      "content": "ChatGPT business plans now support shared projects, smarter connectors, and enhanced compliance features to help teams work faster and more securely.",
      "summary": "ChatGPT business plans now support shared projects, smarter connectors, and enhanced compliance features to help teams work faster and more securely.",
      "publishedAt": "2025-09-25T11:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "score": 0.17200552110756745,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "code_review",
        "governance"
      ]
    },
    {
      "id": "8e2e6c05adfeada02b2aaaf9cd48dd73",
      "title": "Measuring the performance of our models on real-world tasks",
      "url": "https://openai.com/index/gdpval",
      "content": "OpenAI introduces GDPval, a new evaluation that measures model performance on real-world economically valuable tasks across 44 occupations.",
      "summary": "OpenAI introduces GDPval, a new evaluation that measures model performance on real-world economically valuable tasks across 44 occupations.",
      "publishedAt": "2025-09-25T09:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.0427461799582598,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": []
    },
    {
      "id": "ce60d03934966df51726d812522c5569",
      "title": "Introducing ChatGPT Pulse",
      "url": "https://openai.com/index/introducing-chatgpt-pulse",
      "content": "Today we're releasing a preview of ChatGPT Pulse to Pro users on mobile. Pulse is a new experience where ChatGPT proactively does research to deliver personalized updates based on your chats, feedback, and connected apps like your calendar. ",
      "summary": "Today we're releasing a preview of ChatGPT Pulse to Pro users on mobile. Pulse is a new experience where ChatGPT proactively does research to deliver personalized updates based on your chats, feedback, and connected apps like your calendar.",
      "publishedAt": "2025-09-25T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.12484917491217705,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "25e1e85d3fb851ce95a300d3ce82405e",
      "title": "Transforming the manufacturing industry with ChatGPT",
      "url": "https://openai.com/index/eneos-materials",
      "content": "By deploying ChatGPT Enterprise, ENEOS Materials transformed operations with faster research, safer plant design, and streamlined HR processes. Over 80% of employees report major workflow improvements, strengthening competitiveness in manufacturing.",
      "summary": "By deploying ChatGPT Enterprise, ENEOS Materials transformed operations with faster research, safer plant design, and streamlined HR processes. Over 80% of employees report major workflow improvements, strengthening competitiveness in manufacturing.",
      "publishedAt": "2025-09-24T17:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "score": 0.13586117473654827,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "b9772dbe916eeb921363e414d1015162",
      "title": "SAP and OpenAI partner to launch sovereign ‘OpenAI for Germany’",
      "url": "https://openai.com/global-affairs/openai-for-germany",
      "content": "SAP and OpenAI launch OpenAI for Germany, a 2026 partnership to bring secure, sovereign AI to Germany’s public sector, enabling safe, efficient public services.",
      "summary": "SAP and OpenAI launch OpenAI for Germany, a 2026 partnership to bring secure, sovereign AI to Germany’s public sector, enabling safe, efficient public services.",
      "publishedAt": "2025-09-24T04:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.11763452808635932,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": []
    },
    {
      "id": "cfd16316f767c456c01780da65181fad",
      "title": "OpenAI, Oracle, and SoftBank expand Stargate with five new AI datacenter sites",
      "url": "https://openai.com/index/five-new-stargate-sites",
      "content": "OpenAI, Oracle, and SoftBank announce five new Stargate AI datacenter sites, accelerating a $500B, 10-gigawatt U.S. infrastructure buildout to power next-generation AI and create tens of thousands of jobs.",
      "summary": "OpenAI, Oracle, and SoftBank announce five new Stargate AI datacenter sites, accelerating a $500B, 10-gigawatt U.S. infrastructure buildout to power next-generation AI and create tens of thousands of jobs.",
      "publishedAt": "2025-09-23T14:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.037611266377481536,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": []
    },
    {
      "id": "5d8fe756e5c59b6ba3c4ddac18750f2e",
      "title": "CNA is transforming its newsroom with AI ",
      "url": "https://openai.com/index/cna-walter-fernandez",
      "content": "In this Executive Function series from OpenAI, discover how CNA is transforming its newsroom with AI. Editor-in-Chief Walter Fernandez shares insights on AI adoption, culture, and the future of journalism.",
      "summary": "In this Executive Function series from OpenAI, discover how CNA is transforming its newsroom with AI. Editor-in-Chief Walter Fernandez shares insights on AI adoption, culture, and the future of journalism.",
      "publishedAt": "2025-09-22T17:17:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "thought_leadership",
      "score": 0.04125604209183795,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "ide"
      ]
    },
    {
      "id": "98abe74afed5312cf32e09c01561c8ad",
      "title": "American-made innovation",
      "url": "https://openai.com/global-affairs/american-made-innovation",
      "content": "American-made innovation",
      "summary": "American-made innovation",
      "publishedAt": "2025-09-22T11:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.03475886319960864,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": []
    },
    {
      "id": "9b042a61ba87a8a943bb08bb61cd6292",
      "title": "Creating a safe, observable AI infrastructure for 1 million classrooms",
      "url": "https://openai.com/index/schoolai",
      "content": "Discover how SchoolAI, built on OpenAI’s GPT-4.1, image generation, and TTS, powers safe, teacher-guided AI tools for 1 million classrooms worldwide—boosting engagement, oversight, and personalized learning.",
      "summary": "Discover how SchoolAI, built on OpenAI’s GPT-4.1, image generation, and TTS, powers safe, teacher-guided AI tools for 1 million classrooms worldwide—boosting engagement, oversight, and personalized learning.",
      "publishedAt": "2025-09-22T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.04613871407698342,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": [
        "ide"
      ]
    },
    {
      "id": "4b2a7a48f7c5d49808f505194f48653c",
      "title": "OpenAI and NVIDIA announce strategic partnership to deploy 10 gigawatts of NVIDIA systems",
      "url": "https://openai.com/index/openai-nvidia-systems-partnership",
      "content": "OpenAI and NVIDIA announce a strategic partnership to deploy 10 gigawatts of AI datacenters powered by NVIDIA systems, with the first phase launching in 2026.",
      "summary": "OpenAI and NVIDIA announce a strategic partnership to deploy 10 gigawatts of AI datacenters powered by NVIDIA systems, with the first phase launching in 2026.",
      "publishedAt": "2025-09-22T08:45:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.10342661833236347,
      "ingestedAt": "2025-11-23T21:21:55.723Z",
      "tags": []
    },
    {
      "id": "c92c326e7977c38c715e4093d91f4e55",
      "title": "Outbound coordinated vulnerability disclosure policy",
      "url": "https://openai.com/policies/outbound-coordinated-disclosure-policy",
      "content": "Outbound coordinated vulnerability disclosure policy",
      "summary": "Outbound coordinated vulnerability disclosure policy",
      "publishedAt": "2025-09-22T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "security_incident",
      "score": 0.13435731291638825,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "governance"
      ]
    },
    {
      "id": "7c673bb5582cb600bdb5d37ee67f10c2",
      "title": "Detecting and reducing scheming in AI models",
      "url": "https://openai.com/index/detecting-and-reducing-scheming-in-ai-models",
      "content": "Apollo Research and OpenAI developed evaluations for hidden misalignment (“scheming”) and found behaviors consistent with scheming in controlled tests across frontier models. The team shared concrete examples and stress tests of an early method to reduce scheming. ",
      "summary": "Apollo Research and OpenAI developed evaluations for hidden misalignment (“scheming”) and found behaviors consistent with scheming in controlled tests across frontier models. The team shared concrete examples and stress tests of an early method to reduce scheming.",
      "publishedAt": "2025-09-17T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.023501530510778447,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": []
    },
    {
      "id": "e90c5244240c96a3e507e5163d66592a",
      "title": "Introducing Stargate UK",
      "url": "https://openai.com/index/introducing-stargate-uk",
      "content": "OpenAI, NVIDIA, and Nscale launch Stargate UK, a sovereign AI infrastructure partnership delivering up to 50,000 GPUs and the UK’s largest supercomputer to power national AI innovation, public services, and economic growth.",
      "summary": "OpenAI, NVIDIA, and Nscale launch Stargate UK, a sovereign AI infrastructure partnership delivering up to 50,000 GPUs and the UK’s largest supercomputer to power national AI innovation, public services, and economic growth.",
      "publishedAt": "2025-09-16T14:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.06853907539104288,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": []
    },
    {
      "id": "7e49bfef600761c4c581a839ef9acd1e",
      "title": "Teen safety, freedom, and privacy",
      "url": "https://openai.com/index/teen-safety-freedom-and-privacy",
      "content": "Explore OpenAI’s approach to balancing teen safety, freedom, and privacy in AI use.",
      "summary": "Explore OpenAI’s approach to balancing teen safety, freedom, and privacy in AI use.",
      "publishedAt": "2025-09-16T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.04455129843844137,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "d42b318d4ecd54b84becb6ac86c8d257",
      "title": "Building towards age prediction",
      "url": "https://openai.com/index/building-towards-age-prediction",
      "content": "Learn how OpenAI is building age prediction and parental controls in ChatGPT to create safer, age-appropriate experiences for teens while supporting families with new tools.",
      "summary": "Learn how OpenAI is building age prediction and parental controls in ChatGPT to create safer, age-appropriate experiences for teens while supporting families with new tools.",
      "publishedAt": "2025-09-16T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.04455129843844137,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "a7a0c8c9ee2c2652fa75f4e745beed59",
      "title": "Introducing upgrades to Codex",
      "url": "https://openai.com/index/introducing-upgrades-to-codex",
      "content": "Codex just got faster, more reliable, and better at real-time collaboration and tackling tasks independently anywhere you develop—whether via the terminal, IDE, web, or even your phone.",
      "summary": "Codex just got faster, more reliable, and better at real-time collaboration and tackling tasks independently anywhere you develop—whether via the terminal, IDE, web, or even your phone.",
      "publishedAt": "2025-09-15T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.04897295316247238,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "ide"
      ]
    },
    {
      "id": "9bf2135904c60a85a9b6620e76029750",
      "title": "How people are using ChatGPT",
      "url": "https://openai.com/index/how-people-are-using-chatgpt",
      "content": "New research from the largest study of ChatGPT use shows how the tool creates economic value through both personal and professional use. Adoption is broadening beyond early users, closing gaps and making AI a part of everyday life.",
      "summary": "New research from the largest study of ChatGPT use shows how the tool creates economic value through both personal and professional use. Adoption is broadening beyond early users, closing gaps and making AI a part of everyday life.",
      "publishedAt": "2025-09-15T03:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.041111346595281324,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "7d1e13edca9f0e939d70af04915921b7",
      "title": "Addendum to GPT-5 system card: GPT-5-Codex",
      "url": "https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex",
      "content": "This addendum to the GPT-5 system card shares a new model: GPT-5-Codex, a version of GPT-5 further optimized for agentic coding in Codex. GPT-5-Codex adjusts its thinking effort more dynamically based on task complexity, responding quickly to simple conversational queries or small tasks, while independently working for longer on more complex tasks.",
      "summary": "This addendum to the GPT-5 system card shares a new model: GPT-5-Codex, a version of GPT-5 further optimized for agentic coding in Codex. GPT-5-Codex adjusts its thinking effort more dynamically based on task complexity, responding quickly to simple conversational queries or small tasks, while independently working for longer on more complex tasks.",
      "publishedAt": "2025-09-15T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.03395492901683074,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "agents"
      ]
    },
    {
      "id": "1524b7a996a733768616f5c9a020c8c8",
      "title": "Working with US CAISI and UK AISI to build more secure AI systems",
      "url": "https://openai.com/index/us-caisi-uk-aisi-ai-update",
      "content": "OpenAI shares progress on the partnership with the US CAISI and UK AISI to strengthen AI safety and security. The collaboration is setting new standards for responsible frontier AI deployment through joint red-teaming, biosecurity safeguards, and agentic system testing.",
      "summary": "OpenAI shares progress on the partnership with the US CAISI and UK AISI to strengthen AI safety and security. The collaboration is setting new standards for responsible frontier AI deployment through joint red-teaming, biosecurity safeguards, and agentic system testing.",
      "publishedAt": "2025-09-12T12:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "security_incident",
      "score": 0.08236604991981261,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "code_review",
        "agents",
        "testing"
      ]
    },
    {
      "id": "d3941dee22b6e8fd53bdf86ab78ebea9",
      "title": "Announcing OpenAI Grove",
      "url": "https://openai.com/index/openai-grove",
      "content": "Applications are now open for OpenAI Grove, a 5-week founder program designed for individuals at any stage, from pre-idea to product. Participants receive $50K in API credits, early access to AI tools, and hands-on mentorship from the OpenAI team.",
      "summary": "Applications are now open for OpenAI Grove, a 5-week founder program designed for individuals at any stage, from pre-idea to product. Participants receive $50K in API credits, early access to AI tools, and hands-on mentorship from the OpenAI team.",
      "publishedAt": "2025-09-12T07:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.08394769579289511,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "046838869b448dd196dce13cb4eb0d42",
      "title": "A joint statement from OpenAI and Microsoft",
      "url": "https://openai.com/index/joint-statement-from-openai-and-microsoft",
      "content": "OpenAI and Microsoft sign a new MOU, reinforcing their partnership and shared commitment to AI safety and innovation.",
      "summary": "OpenAI and Microsoft sign a new MOU, reinforcing their partnership and shared commitment to AI safety and innovation.",
      "publishedAt": "2025-09-11T14:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.015961200128930195,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": []
    },
    {
      "id": "4f06a3965c9b93e0f55e8d2a88a14d8e",
      "title": "Statement on OpenAI’s Nonprofit and PBC",
      "url": "https://openai.com/index/statement-on-openai-nonprofit-and-pbc",
      "content": "OpenAI reaffirms its nonprofit leadership with a new structure granting equity in its PBC, enabling over $100B in resources to advance safe, beneficial AI for humanity.",
      "summary": "OpenAI reaffirms its nonprofit leadership with a new structure granting equity in its PBC, enabling over $100B in resources to advance safe, beneficial AI for humanity.",
      "publishedAt": "2025-09-11T14:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.03192240025786039,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "55eec3c25bc99547b97bcef342d7dd55",
      "title": "Shipping smarter agents with every new model",
      "url": "https://openai.com/index/safetykit",
      "content": "Discover how SafetyKit leverages OpenAI GPT-5 to enhance content moderation, enforce compliance, and outpace legacy safety systems with greater accuracy .",
      "summary": "Discover how SafetyKit leverages OpenAI GPT-5 to enhance content moderation, enforce compliance, and outpace legacy safety systems with greater accuracy .",
      "publishedAt": "2025-09-09T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "feature_update",
      "score": 0.07064212343091995,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "retrieval",
        "agents",
        "governance"
      ]
    },
    {
      "id": "a9357f717f7361d40f0a5905f3b249d1",
      "title": "A People-First AI Fund: $50M to support nonprofits",
      "url": "https://openai.com/index/people-first-ai-fund",
      "content": "Applications are now open for OpenAI’s People-First AI Fund, a $50M initiative supporting U.S. nonprofits advancing education, community innovation, and economic opportunity. Apply by October 8, 2025, for unrestricted grants that help communities shape AI for the public good.",
      "summary": "Applications are now open for OpenAI’s People-First AI Fund, a $50M initiative supporting U.S. nonprofits advancing education, community innovation, and economic opportunity. Apply by October 8, 2025, for unrestricted grants that help communities shape AI for the public good.",
      "publishedAt": "2025-09-08T14:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.025765135775128532,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "fd79768a741eb7c71583266838e7f6bb",
      "title": "Why language models hallucinate",
      "url": "https://openai.com/index/why-language-models-hallucinate",
      "content": "OpenAI’s new research explains why language models hallucinate. The findings show how improved evaluations can enhance AI reliability, honesty, and safety.",
      "summary": "OpenAI’s new research explains why language models hallucinate. The findings show how improved evaluations can enhance AI reliability, honesty, and safety.",
      "publishedAt": "2025-09-05T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.02054940065533857,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "551e662c8b623388488699db9222981d",
      "title": "GPT-5 bio bug bounty call",
      "url": "https://openai.com/gpt-5-bio-bug-bounty",
      "content": "OpenAI invites researchers to its Bio Bug Bounty. Test GPT-5’s safety with a universal jailbreak prompt and win up to $25,000.",
      "summary": "OpenAI invites researchers to its Bio Bug Bounty. Test GPT-5’s safety with a universal jailbreak prompt and win up to $25,000.",
      "publishedAt": "2025-09-05T08:45:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.020473094019620882,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "967ad6c60a3b2cd3baf60c4d50419a91",
      "title": "OpenAI and Greek Government launch ‘OpenAI for Greece’",
      "url": "https://openai.com/global-affairs/openai-for-greece",
      "content": "OpenAI and the Greek Government have launched “OpenAI for Greece” to bring ChatGPT Edu into secondary schools and support responsible AI learning. This partnership aims to boost AI literacy, fuel local start-ups, and drive national economic growth.",
      "summary": "OpenAI and the Greek Government have launched “OpenAI for Greece” to bring ChatGPT Edu into secondary schools and support responsible AI learning. This partnership aims to boost AI literacy, fuel local start-ups, and drive national economic growth.",
      "publishedAt": "2025-09-05T08:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.030641169171476263,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": []
    },
    {
      "id": "a66b5ce615173d7b25ef8b4d4bebb469",
      "title": "Expanding economic opportunity with AI",
      "url": "https://openai.com/index/expanding-economic-opportunity-with-ai",
      "content": "OpenAI is launching a Jobs Platform and new Certifications to connect workers with jobs, training, and certifications. Learn how we’re expanding economic opportunity and making AI skills more accessible.",
      "summary": "OpenAI is launching a Jobs Platform and new Certifications to connect workers with jobs, training, and certifications. Learn how we’re expanding economic opportunity and making AI skills more accessible.",
      "publishedAt": "2025-09-04T11:30:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.02882758086290384,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": []
    },
    {
      "id": "3dc331a0e3074238c370a97b3f70a150",
      "title": "Vijaye Raji to become CTO of Applications with acquisition of Statsig",
      "url": "https://openai.com/index/vijaye-raji-to-become-cto-of-applications-with-acquisition-of-statsig",
      "content": "Vijaye Raji will step into a new role as CTO of Applications, reporting to CEO of Applications, Fidji Simo, following the acquisition of Statsig.",
      "summary": "Vijaye Raji will step into a new role as CTO of Applications, reporting to CEO of Applications, Fidji Simo, following the acquisition of Statsig.",
      "publishedAt": "2025-09-02T11:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "funding_mna",
      "score": 0.01386268495535525,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": []
    },
    {
      "id": "1568f222ff7a1cd3bde45a9a84a4fe0b",
      "title": "Building more helpful ChatGPT experiences for everyone",
      "url": "https://openai.com/index/building-more-helpful-chatgpt-experiences-for-everyone",
      "content": "We’re partnering with experts, strengthening protections for teens with parental controls, and routing sensitive conversations to reasoning models in ChatGPT.",
      "summary": "We’re partnering with experts, strengthening protections for teens with parental controls, and routing sensitive conversations to reasoning models in ChatGPT.",
      "publishedAt": "2025-09-02T04:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.016292239956820297,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "20e7ffe2a0312e60d0f5e06ea458b516",
      "title": "Introducing gpt-realtime and Realtime API updates",
      "url": "https://openai.com/index/introducing-gpt-realtime",
      "content": "We’re releasing a more advanced speech-to-speech model and new API capabilities including MCP server support, image input, and SIP phone calling support.",
      "summary": "We’re releasing a more advanced speech-to-speech model and new API capabilities including MCP server support, image input, and SIP phone calling support.",
      "publishedAt": "2025-08-28T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.021275134735190996,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "agents"
      ]
    },
    {
      "id": "b203b36d85228904428c06875fbf7088",
      "title": "Supporting nonprofit and community innovation",
      "url": "https://openai.com/index/supporting-nonprofit-and-community-innovation",
      "content": "OpenAI launches a $50M People-First AI Fund to help U.S. nonprofits scale impact with AI. Applications open Sept 8–Oct 8, 2025 for grants in education, healthcare, research, and more.",
      "summary": "OpenAI launches a $50M People-First AI Fund to help U.S. nonprofits scale impact with AI. Applications open Sept 8–Oct 8, 2025 for grants in education, healthcare, research, and more.",
      "publishedAt": "2025-08-28T05:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 0.02286641938817417,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "9fa6c6e036dfa556ed306bc643069ee0",
      "title": "Collective alignment: public input on our Model Spec",
      "url": "https://openai.com/index/collective-alignment-aug-2025-updates",
      "content": "OpenAI surveyed over 1,000 people worldwide on how AI should behave and compared their views to our Model Spec. Learn how collective alignment is shaping AI defaults to better reflect diverse human values and perspectives.",
      "summary": "OpenAI surveyed over 1,000 people worldwide on how AI should behave and compared their views to our Model Spec. Learn how collective alignment is shaping AI defaults to better reflect diverse human values and perspectives.",
      "publishedAt": "2025-08-27T13:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.0072676870848564145,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "ide"
      ]
    },
    {
      "id": "36bc196e70e92d79a1d8bf2690ab1169",
      "title": "OpenAI and Anthropic share findings from a joint safety evaluation",
      "url": "https://openai.com/index/openai-anthropic-safety-evaluation",
      "content": "OpenAI and Anthropic share findings from a first-of-its-kind joint safety evaluation, testing each other’s models for misalignment, instruction following, hallucinations, jailbreaking, and more—highlighting progress, challenges, and the value of cross-lab collaboration.",
      "summary": "OpenAI and Anthropic share findings from a first-of-its-kind joint safety evaluation, testing each other’s models for misalignment, instruction following, hallucinations, jailbreaking, and more—highlighting progress, challenges, and the value of cross-lab collaboration.",
      "publishedAt": "2025-08-27T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.013505785967130358,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": [
        "code_review",
        "testing"
      ]
    },
    {
      "id": "cbf71272abd2ae5723d9bbf20dd37500",
      "title": "Helping people when they need it most",
      "url": "https://openai.com/index/helping-people-when-they-need-it-most",
      "content": "How we think about safety for users experiencing mental or emotional distress, the limits of today’s systems, and the work underway to refine them.",
      "summary": "How we think about safety for users experiencing mental or emotional distress, the limits of today’s systems, and the work underway to refine them.",
      "publishedAt": "2025-08-26T04:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 0.004940871524603371,
      "ingestedAt": "2025-11-23T21:21:55.724Z",
      "tags": []
    },
    {
      "id": "e11f49f3e01fc55b91ca5ed348f382da",
      "title": "The Pulse #154: Cloudflare takes down half the internet – but shares a great postmortem",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pulse-154",
      "content": "Also: why it&#8217;s not practical to build for CDN redundancy, Google launches AI IDE Antigravity externally while using Jetski internally, more AI fakers caught in remote interviews, and more",
      "summary": "Also: why it’s not practical to build for CDN redundancy, Google launches AI IDE Antigravity externally while using Jetski internally, more AI fakers caught in remote interviews, and more",
      "publishedAt": "2025-11-20T18:06:09.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "product_launch",
      "score": 9.591810698666094,
      "ingestedAt": "2025-11-23T21:21:55.732Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "7c91366a8af7da6991824f95020f9038",
      "title": "The Pulse #153: Is Microsoft too early to agentic OS – like with smartphones?",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pulse-153",
      "content": "Also: inside Cursor&#8217;s unique engineering culture, five AI fakers caught in one month by an employer, and more. Plus: early applications for The Pragmatic Summit",
      "summary": "Also: inside Cursor’s unique engineering culture, five AI fakers caught in one month by an employer, and more. Plus: early applications for The Pragmatic Summit",
      "publishedAt": "2025-11-13T17:53:36.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "score": 4.602834508266677,
      "ingestedAt": "2025-11-23T21:21:55.732Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ]
    },
    {
      "id": "87bd0f26c9d71be78e74f92c604d8303",
      "title": "Netflix’s Engineering Culture",
      "url": "https://newsletter.pragmaticengineer.com/p/netflix",
      "content": "Peek inside Netflix&#8217;s engineering culture with CTO Elizabeth Stone, as she shares how the company has no formal performance reviews, learns from failures, and builds at a global scale.",
      "summary": "Peek inside Netflix’s engineering culture with CTO Elizabeth Stone, as she shares how the company has no formal performance reviews, learns from failures, and builds at a global scale.",
      "publishedAt": "2025-11-12T17:40:07.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "score": 1.3524197746892264,
      "ingestedAt": "2025-11-23T21:21:55.732Z",
      "tags": [
        "ide"
      ]
    },
    {
      "id": "590a2568c9ee4a6be8e8a093b3b1c8ef",
      "title": " The Software Engineer’s Guidebook: a recap",
      "url": "https://newsletter.pragmaticengineer.com/p/the-software-engineers-guidebook",
      "content": "Reflections on publishing The Software Engineer&#8217;s Guidebook two years ago, which has sold around 40,000 copies. Also: an unexpected trip to Mongolia to visit the startup which translated it",
      "summary": "Reflections on publishing The Software Engineer’s Guidebook two years ago, which has sold around 40,000 copies. Also: an unexpected trip to Mongolia to visit the startup which translated it",
      "publishedAt": "2025-11-11T17:01:48.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "score": 1.2567967398315794,
      "ingestedAt": "2025-11-23T21:21:55.732Z",
      "tags": [
        "ide"
      ]
    },
    {
      "id": "93f3c9e802ccd853a8fd80954130605e",
      "title": "The Pulse #152: Cursor and GitHub double down on agents",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pulse-150-cursor-and-github-double",
      "content": "Also: AI-assisted interviews at Meta, pay for Directors of Engineering at VC-funded startups, and is OpenAI inflating the bubble? And more",
      "summary": "Also: AI-assisted interviews at Meta, pay for Directors of Engineering at VC-funded startups, and is OpenAI inflating the bubble? And more",
      "publishedAt": "2025-11-06T16:33:28.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "feature_update",
      "score": 2.6343335351832335,
      "ingestedAt": "2025-11-23T21:21:55.732Z",
      "tags": [
        "agents"
      ]
    },
    {
      "id": "10dae6e044780159310ed30db4eca1ff",
      "title": "From Swift to Mojo and high-performance AI Engineering with Chris Lattner",
      "url": "https://newsletter.pragmaticengineer.com/p/from-swift-to-mojo-and-high-performance",
      "content": "I sit down with Chris Lattner, creator of LLVM, Swift, and Mojo, to discuss how better language and compiler design can open the door to faster, more accessible AI development.",
      "summary": "I sit down with Chris Lattner, creator of LLVM, Swift, and Mojo, to discuss how better language and compiler design can open the door to faster, more accessible AI development.",
      "publishedAt": "2025-11-05T16:02:14.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "score": 0.5442073108965926,
      "ingestedAt": "2025-11-23T21:21:55.732Z",
      "tags": []
    },
    {
      "id": "1339d7effcfe6235fd1f6c6f9ea9e613",
      "title": "The Pulse #151: Amazon layoffs – AI or economy to blame?",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pulse-151",
      "content": "Also: OpenAI becomes for-profit and takes on Chrome, NVIDIA the biggest Big Tech by a distance, Citibank annoys premium customers for weeks with disastrous product rollout, and more",
      "summary": "Also: OpenAI becomes for-profit and takes on Chrome, NVIDIA the biggest Big Tech by a distance, Citibank annoys premium customers for weeks with disastrous product rollout, and more",
      "publishedAt": "2025-10-30T17:46:40.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "score": 1.2472570077169178,
      "ingestedAt": "2025-11-23T21:21:55.732Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "570302873daedee8879e05c12c09de52",
      "title": "San Francisco is back as the world’s leading tech hub",
      "url": "https://newsletter.pragmaticengineer.com/p/san-francisco-is-back",
      "content": "Impressions from a week in San Fran spent visiting engineering teams at Cursor, OpenAI, Anthropic, Wispr, Factory, and more",
      "summary": "Impressions from a week in San Fran spent visiting engineering teams at Cursor, OpenAI, Anthropic, Wispr, Factory, and more",
      "publishedAt": "2025-10-28T16:55:50.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "score": 0.7703547715969876,
      "ingestedAt": "2025-11-23T21:21:55.732Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "2ecaf12b8ab9462690b7c606a5303537",
      "title": "The Pulse: AWS takes down a good part of the internet",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pulse-aws-takes-down-a-good-part",
      "content": "On Monday, a major AWS outage hit thousands of sites & apps, and even a Premier League soccer game. An overview of what caused this high-profile, global outage, and learnings from the incident",
      "summary": "On Monday, a major AWS outage hit thousands of sites & apps, and even a Premier League soccer game. An overview of what caused this high-profile, global outage, and learnings from the incident",
      "publishedAt": "2025-10-23T16:17:31.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "security_incident",
      "score": 0.9683507149824209,
      "ingestedAt": "2025-11-23T21:21:55.732Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "ef14ac127f1c54317b68179164631d04",
      "title": "The Pragmatic Engineer 2025 Survey: What’s in your tech stack? Part 3",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pragmatic-engineer-2025-survey-part-3",
      "content": "Which tools do software engineers use for observability, oncall tooling, feature flags, frontend & mobile work, and for developer tooling? Results from our survey, based on 3,000+ responses by readers",
      "summary": "Which tools do software engineers use for observability, oncall tooling, feature flags, frontend & mobile work, and for developer tooling? Results from our survey, based on 3,000+ responses by readers",
      "publishedAt": "2025-10-14T14:59:35.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "score": 0.42265231475807813,
      "ingestedAt": "2025-11-23T21:21:55.732Z",
      "tags": [
        "code_review",
        "retrieval",
        "observability"
      ]
    },
    {
      "id": "cf3872e75a8ac092c287d2fd0171adfc",
      "title": "The Pulse #149: New trend: programming by kicking off parallel AI agents",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pulse-149-new-trend-programming",
      "content": "Also: the ACP protocol, AI security tooling, comparing interview experiences across 8 tech companies, and more",
      "summary": "Also: the ACP protocol, AI security tooling, comparing interview experiences across 8 tech companies, and more",
      "publishedAt": "2025-10-09T16:31:16.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "feature_update",
      "score": 0.5545234667503015,
      "ingestedAt": "2025-11-23T21:21:55.732Z",
      "tags": [
        "code_review",
        "agents"
      ]
    },
    {
      "id": "77d7fb344400a0cbaa0538cbbeb2699e",
      "title": "The Pulse #148: Did OpenAI set a new record for Datadog spend?",
      "url": "https://newsletter.pragmaticengineer.com/p/the-pulse-148",
      "content": "A reported $170M per year is almost triple Coinbase&#8217;s previous record. Also: the largest British automaker learns that outsourcing cybersecurity was a terrible decision, and more",
      "summary": "A reported $170M per year is almost triple Coinbase’s previous record. Also: the largest British automaker learns that outsourcing cybersecurity was a terrible decision, and more",
      "publishedAt": "2025-10-02T16:11:02.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "security_incident",
      "score": 0.23999863894590656,
      "ingestedAt": "2025-11-23T21:21:55.732Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "5abd0db3b7997f68293e4f56a2d98561",
      "title": "The Agent Labs Thesis",
      "url": "https://www.latent.space/p/agent-labs",
      "content": "How great Agent Engineering and Research are combining in a new playbook for building high growth AI startups that doesn't involve training a SOTA LLM.",
      "summary": "How great Agent Engineering and Research are combining in a new playbook for building high growth AI startups that doesn't involve training a SOTA LLM.",
      "publishedAt": "2025-11-18T02:41:17.000Z",
      "author": "swyx (Shawn)",
      "source": "rss",
      "feedName": "Latent Space",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "score": 3.971043193588123,
      "ingestedAt": "2025-11-23T21:21:55.982Z",
      "tags": [
        "agents"
      ]
    },
    {
      "id": "4bdfa6d2ea07ffd2b51039b7ad325d5b",
      "title": "Biohub for Non-Biologists: Behind Priscilla Chan and Mark Zuckerberg's plan to cure all diseases",
      "url": "https://www.latent.space/p/biohub",
      "content": "The CZI has acquired EvoScale, established the first 10,000 GPU cluster for bio research, open sourced the largest atlas of human cell types, and gone all in on AI x Bio for its 2nd decade.",
      "summary": "The CZI has acquired EvoScale, established the first 10,000 GPU cluster for bio research, open sourced the largest atlas of human cell types, and gone all in on AI x Bio for its 2nd decade.",
      "publishedAt": "2025-11-13T19:46:41.000Z",
      "author": "swyx (Shawn)",
      "source": "rss",
      "feedName": "Latent Space",
      "sourceType": "engineering_blog",
      "contentType": "funding_mna",
      "score": 4.38510765187149,
      "ingestedAt": "2025-11-23T21:21:55.982Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "74d0198fd6d6635be653afed2fff184f",
      "title": "Agentic Commerce Protocol and building the Economic Infrastructure for AI — with Emily Glassberg Sands, Head of Data & AI at Stripe",
      "url": "https://www.latent.space/p/stripe",
      "content": "How Stripe built a payments foundation model, why stablecoins are powering more of the AI economy, and growing internal AI adoption to 8,500 employees daily.",
      "summary": "How Stripe built a payments foundation model, why stablecoins are powering more of the AI economy, and growing internal AI adoption to 8,500 employees daily.",
      "publishedAt": "2025-10-30T22:30:27.000Z",
      "source": "rss",
      "feedName": "Latent Space",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "score": 1.6263488322756414,
      "ingestedAt": "2025-11-23T21:21:55.982Z",
      "tags": [
        "code_review",
        "agents"
      ]
    },
    {
      "id": "551f4704dce7d3e20cf00e7355f2ec4e",
      "title": "Developers as the distribution layer of AGI (OpenAI Dev Day 2025, ft. Sherwin Wu and Christina Huang)",
      "url": "https://www.latent.space/p/devday-2025",
      "content": "A quick recap of the 2025 OpenAI DevDay coupled with an exclusive interview with the OpenAI Platform team that shipped AgentKit and related products!",
      "summary": "A quick recap of the 2025 OpenAI DevDay coupled with an exclusive interview with the OpenAI Platform team that shipped AgentKit and related products!",
      "publishedAt": "2025-10-07T17:50:03.000Z",
      "source": "rss",
      "feedName": "Latent Space",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "score": 0.3102340237751563,
      "ingestedAt": "2025-11-23T21:21:55.982Z",
      "tags": [
        "code_review",
        "agents"
      ]
    },
    {
      "id": "cdfdf4ebd03e2db20eb3efeb1b34257a",
      "title": "Taste is your moat — with Dylan Field, Figma",
      "url": "https://www.latent.space/p/figma",
      "content": "Letting designers build with Figma Make, how Figma can be the context repository for aesthetic in the age of vibe coding, and why design is your only differentiator now",
      "summary": "Letting designers build with Figma Make, how Figma can be the context repository for aesthetic in the age of vibe coding, and why design is your only differentiator now",
      "publishedAt": "2025-10-02T16:40:18.000Z",
      "source": "rss",
      "feedName": "Latent Space",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "score": 0.09613890120530275,
      "ingestedAt": "2025-11-23T21:21:55.982Z",
      "tags": []
    },
    {
      "id": "cd01d183003fe4945d495249425bc9ce",
      "title": "How GPT5 + Codex took over Agentic Coding — ft. Greg Brockman, OpenAI",
      "url": "https://www.latent.space/p/gpt5-codex",
      "content": "Belated catchup on our podcast with Greg Brockman, + latest takes on the new GPT-5-Codex model",
      "summary": "Belated catchup on our podcast with Greg Brockman, + latest takes on the new GPT-5-Codex model",
      "publishedAt": "2025-09-16T00:16:43.000Z",
      "source": "rss",
      "feedName": "Latent Space",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "score": 0.0437990944591817,
      "ingestedAt": "2025-11-23T21:21:55.982Z",
      "tags": [
        "agents"
      ]
    },
    {
      "id": "c856888e31b329ab17148c68feef099b",
      "title": "A Technical History of Generative Media — with Gorkem and Batuhan from Fal.ai",
      "url": "https://www.latent.space/p/fal",
      "content": "From Stable Diffusion to Veo3, why generative media is completely different than LLM inference, and how to scale to $100M ARR while writing custom kernels",
      "summary": "From Stable Diffusion to Veo3, why generative media is completely different than LLM inference, and how to scale to $100M ARR while writing custom kernels",
      "publishedAt": "2025-09-05T21:46:47.000Z",
      "source": "rss",
      "feedName": "Latent Space",
      "sourceType": "engineering_blog",
      "contentType": "thought_leadership",
      "score": 0.012414855234868388,
      "ingestedAt": "2025-11-23T21:21:55.982Z",
      "tags": []
    },
    {
      "id": "4ecdb37fef912618e94485d62b220d42",
      "title": "Expanding data residency access to business customers worldwide",
      "url": "https://openai.com/index/expanding-data-residency-access-to-business-customers-worldwide",
      "content": "OpenAI expands data residency for ChatGPT Enterprise, ChatGPT Edu, and the API Platform, enabling eligible customers to store data at rest in-region.",
      "summary": "OpenAI expands data residency for ChatGPT Enterprise, ChatGPT Edu, and the API Platform, enabling eligible customers to store data at rest in-region.",
      "publishedAt": "2025-11-25T22:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "score": 10.999307956624685,
      "ingestedAt": "2025-11-25T22:01:16.102Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "bb1431936d559fcda818181e95ea7013",
      "title": "Our approach to mental health-related litigation",
      "url": "https://openai.com/index/mental-health-litigation-approach",
      "content": "We’re sharing our approach to mental health-related litigation. O handle sensitive cases with care, transparency, and respect while continuing to strengthen safety and support in ChatGPT.",
      "summary": "We’re sharing our approach to mental health-related litigation. O handle sensitive cases with care, transparency, and respect while continuing to strengthen safety and support in ChatGPT.",
      "publishedAt": "2025-11-25T12:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 5.823693303025859,
      "ingestedAt": "2025-11-25T22:01:16.103Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "c46ba09513929d71f25603f36ae87084",
      "title": "Inside JetBrains—the company reshaping how the world writes code",
      "url": "https://openai.com/index/jetbrains-2025",
      "content": "JetBrains is integrating GPT-5 across its coding tools, helping millions of developers design, reason, and build software faster.",
      "summary": "JetBrains is integrating GPT-5 across its coding tools, helping millions of developers design, reason, and build software faster.",
      "publishedAt": "2025-11-25T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 3.7462496821494966,
      "ingestedAt": "2025-11-25T22:01:16.103Z",
      "tags": [
        "ide"
      ]
    },
    {
      "id": "6a99952608935921d89098e4cf2228ca",
      "title": "Introducing shopping research in ChatGPT",
      "url": "https://openai.com/index/chatgpt-shopping-research",
      "content": "Shopping research in ChatGPT helps you explore, compare, and discover products with personalized buyer’s guides that simplify decision-making",
      "summary": "Shopping research in ChatGPT helps you explore, compare, and discover products with personalized buyer’s guides that simplify decision-making",
      "publishedAt": "2025-11-24T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "score": 8.719984106318554,
      "ingestedAt": "2025-11-25T22:01:16.103Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "80006acfee14c25b20de2fb6cde3fa9d",
      "title": "GPT-5 and the future of mathematical discovery",
      "url": "https://openai.com/index/gpt-5-mathematical-discovery",
      "content": "UCLA Professor Ernest Ryu and GPT-5 solved a key question in optimization theory, showcasing AI’s role in accelerating mathematical discovery.",
      "summary": "UCLA Professor Ernest Ryu and GPT-5 solved a key question in optimization theory, showcasing AI’s role in accelerating mathematical discovery.",
      "publishedAt": "2025-11-24T00:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "thought_leadership",
      "score": 4.795991258475205,
      "ingestedAt": "2025-11-25T22:01:16.103Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "b48f231ef70d320b9a934f4bba200691",
      "title": "Product Evals in Three Simple Steps",
      "url": "https://eugeneyan.com//writing/product-evals/",
      "content": "Label some data, align LLM-evaluators, and run the eval harness with each change.",
      "summary": "Label some data, align LLM-evaluators, and run the eval harness with each change.",
      "publishedAt": "2025-11-23T00:00:00.000Z",
      "source": "rss",
      "feedName": "Eugene Yan",
      "sourceType": "engineering_blog",
      "company": "Eugene Yan",
      "contentType": "general",
      "score": 5.683196256703056,
      "ingestedAt": "2025-11-25T22:01:16.229Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "521606c7343b7920879a28143d7b81dc",
      "title": "Holiday gift ideas for techies",
      "url": "https://newsletter.pragmaticengineer.com/p/copy-holiday-gift-ideas-for-techies-2025",
      "content": "Gift ideas and inspiration for the tech workers in your life (and maybe yourself) this holiday season",
      "summary": "Gift ideas and inspiration for the tech workers in your life (and maybe yourself) this holiday season",
      "publishedAt": "2025-11-25T17:30:00.000Z",
      "author": "Gergely Orosz",
      "source": "rss",
      "feedName": "Pragmatic Engineer",
      "sourceType": "curated_ai",
      "company": "Pragmatic Engineer",
      "contentType": "general",
      "score": 2.959902373473022,
      "ingestedAt": "2025-11-25T22:01:16.381Z",
      "tags": [
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09af040a",
      "title": "Comment on Toxic positivity at work: how to spot it and squash it by WorkWell &ndash; How to Avoid 'Toxic Positivity' in Teams &ndash; WorkWell",
      "url": "https://www.atlassian.com/blog/communication/toxic-positivity#comment-24919",
      "content": "<p><img src=\"https://www.atlassian.com/blog/wp-content/uploads/2023/09/toxic-positivity_1120x545@2x-scaled.jpg\" alt=\"toxic-positivity_1120x545@2x-scaled.jpg\"></p><p>[…] Effective leadership is necessary in combating toxic positivity within teams. Leaders set the tone for the workplace culture, so it’s vital for you to acknowledge and validate your team’s emotions. By fostering an environment where employees feel safe to express themselves, you can help mitigate the adverse effects of unrealistic optimism. Learn more about Toxic positivity at work – Work Life by Atlassian. […]</p>",
      "summary": "[…] Effective leadership is necessary in combating toxic positivity within teams. Leaders set the tone for the workplace culture, so it’s vital for you to acknowledge and validate your team’s emotions. By fostering an environment where employees feel safe to express themselves, you can help mitigate the adverse effects of unrealistic optimism. Learn more about Toxic positivity at work – Work Life by Atlassian. […]",
      "publishedAt": "2025-12-02T14:02:34.000Z",
      "author": "WorkWell &#8211; How to Avoid &#039;Toxic Positivity&#039; in Teams &#8211; WorkWell",
      "source": "rss",
      "feedName": "Comments for Atlassian Blog Work Life",
      "sourceType": "engineering_blog",
      "contentType": "thought_leadership",
      "score": 6.486637800135441,
      "ingestedAt": "2025-12-02T14:44:03.162Z",
      "tags": [
        "code_review",
        "Tech Company Blogs"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09af040b",
      "title": "Comment on Migrating the Jira and Confluence applications to AWS Graviton by Atlassian now available on AWS Marketplace - Work Life by Atlassian",
      "url": "https://www.atlassian.com/blog/atlassian-engineering/migrating-the-jira-and-confluence-applications-to-aws-graviton#comment-24918",
      "content": "<p><img src=\"https://www.atlassian.com/blog/wp-content/uploads/2025/11/fy26-sil-engblog-40-1120x545_@2x.png\" alt=\"fy26-sil-engblog-40-1120x545_@2x.png\"></p><p>[…] cloud performance: Atlassian’s migration of Jira and Confluence to Graviton delivers faster speeds and reduced latency for enterprise […]</p>",
      "summary": "[…] cloud performance: Atlassian’s migration of Jira and Confluence to Graviton delivers faster speeds and reduced latency for enterprise […]",
      "publishedAt": "2025-12-02T14:01:09.000Z",
      "author": "Atlassian now available on AWS Marketplace - Work Life by Atlassian",
      "source": "rss",
      "feedName": "Comments for Atlassian Blog Work Life",
      "sourceType": "engineering_blog",
      "contentType": "product_launch",
      "score": 11.974489832421957,
      "ingestedAt": "2025-12-02T14:44:03.162Z",
      "tags": [
        "code_review",
        "Tech Company Blogs"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae73ae",
      "title": "SmartTube Breach 🎥, Shai Hulud 2.0 Analysis 🐛, Fake Cyber Monday Shopping Sites 🛍️",
      "url": "https://www.inoreader.com/article/3a9c6e76b2a58c51",
      "content": "<div class=\"email_is_html\"><div><div>       <div style=\"display: none; max-height: 0px; overflow: hidden\">The popular open-source SmartTube YouTube client for Android TV, which provides ad blocking and runs well on low-powered devices, was compromised ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ </div> <div style=\"display: none; max-height: 0px; overflow: hidden\"> <br /> </div>  <table align=\"center\"><tbody><tr><td valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"600\"><tbody><tr><td>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td>  <table width=\"100%\"><tbody><tr><td>    <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" style=\"margin-top: 0px\" width=\"100%\"><tbody><tr><td style=\"padding: 0px\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div style=\"text-align: center\"> <span style=\"margin-right: 0px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Finfosec%3Futm_source=tldrinfosec/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/GztVW3Sf5QaKwSaiPfvULXKeHYdKChN5hxXJXpgq_Eo=433\" rel=\"noreferrer\" target=\"_blank\"><span>Sign Up</span></a> |<span style=\"margin-right: 2px; margin-left: 2px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrinfosec%26utm_medium=newsletter%26utm_campaign=advertisetopnav/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/R22LhCo2VX_nWU2tp0aVQD02VUQlYz8C-hrHVF16vBQ=433\" rel=\"noreferrer\" target=\"_blank\"><span>Advertise</span></a></span>|<span style=\"margin-left: 2px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa.tldrnewsletter.com%2Fweb-version%3Fep=1%26lc=8cb497a0-9621-11f0-9193-63bc73b21b46%26p=3f396156-cf3b-11f0-8211-cb95d47fe381%26pt=campaign%26t=1764684771%26s=1f71d24b0dd8f411c5e1d3f5ccc637e4d2e57a2175e5fc8c72f2426e90d61b6b/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/VK_HrrbA85ZXS6odExjqcta37-DQusTJaYk4ztZdO7U=433\" target=\"_blank\" rel=\"noreferrer\"><span>View Online</span></a></span> <br /> </span></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"text-align: center\"><span style=\"--darkreader-inline-color: #3db3ff; color: rgb(51, 175, 255) !important; font-size: 30px\">T</span><span style=\"font-size: 30px\"><span style=\"color: rgb(232, 192, 96) !important; --darkreader-inline-color: #e8c163; font-size: 30px\">L</span><span style=\"color: rgb(101, 195, 173) !important; --darkreader-inline-color: #6ec7b2; font-size: 30px\">D</span></span><span style=\"--darkreader-inline-color: #dd6e6e; color: rgb(220, 107, 107) !important; font-size: 30px\">R</span> <br /> </td></tr></tbody></table>  <br />  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"center\" height=\"20\" style=\"vertical-align: middle !important\" valign=\"middle\" width=\"100%\"><strong style=\"vertical-align: middle !important; height: 100%\">Together With </strong> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.adaptivesecurity.com%2Fdemo%2Fsecurity-awareness-training%3Futm_medium=newsletter%26utm_source=tldr-infosec%26utm_campaign=20251202/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/C-A_MNQyWz2JGxHN8hMWb1I2GdT-hKrwfyKzsIRCJb0=433\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://images.tldr.tech/adaptive.png\" valign=\"middle\" style=\"vertical-align: middle !important; height: 100%\" alt=\"Adaptive Security\" /></a></td></tr></tbody></table>   <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\"> <div style=\"text-align: center\">  <h1><strong>TLDR Information Security <span>2025-12-02</span></strong></h1> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.adaptivesecurity.com%2Fdemo%2Fsecurity-awareness-training%3Futm_medium=newsletter%26utm_source=tldr-infosec%26utm_campaign=20251202/2/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/-NPhQtVtS5yGYZGxw8adHHLynaIiBRcTXtIbCXChTSw=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>When your CEO calls, will you know it's real? (Sponsor)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Today's phishing attacks involve AI-generated voices, videos, and interactive deepfakes of company executives. They fool 99% of people.<p></p><p>Adaptive Security - backed by <strong>$65M+ in funding from OpenAI and a16z</strong> - is the first <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.adaptivesecurity.com%2Fdemo%2Fsecurity-awareness-training%3Futm_medium=newsletter%26utm_source=tldr-infosec%26utm_campaign=20251202/3/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/EeaEvVu5vYNr5-aRtJF9FAwTjC7U4GckGDKWG36xHT0=433\" rel=\"noreferrer\" target=\"_blank\"><span>security awareness platform built to stop AI-powered social engineering</span></a>. Adaptive trains your team with tools that stay one step ahead:</p>  <ul> <li>Deepfake attack simulations featuring your real executives in realistic attack scenarios</li> <li>Interactive, personalized training content tailored for each employee</li> <li>AI-driven risk scoring that reveals what attackers can learn from your public data</li> </ul>  <p><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.adaptivesecurity.com%2Fdemo%2Fsecurity-awareness-training%3Futm_medium=newsletter%26utm_source=tldr-infosec%26utm_campaign=20251202/4/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/HYeD5cGy4qL5TQEvLIHCj1rlj01yEiYceJ3qw5qzY3A=433\" rel=\"noreferrer\" target=\"_blank\"><span><strong>&gt;&gt; Book a demo</strong></span></a><strong> </strong>and chat with a custom interactive deepfake of your CEO</p>  <p><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.adaptivesecurity.com%2Fdemo%2Fself-guided-tour%3Futm_medium=newsletter%26utm_source=tldr-infosec%26utm_campaign=20251202/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/2zPaZsydCgh6FpaRp4qsRia0BsgS8aKW1pRc_zWVgWE=433\" rel=\"noreferrer\" target=\"_blank\"><span><strong>&gt;&gt; Take a tour</strong></span></a><strong> </strong>of the platform (3 minutes)   </p> </span></span></div> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr> <tr><td>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 0px\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🔓</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Attacks &amp; Vulnerabilities</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.plerion.com%2Fblog%2Fprivilege-escalation-with-sagemaker-and-execution-roles%3Futm_source=tldrinfosec/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/QTYB_chud6KHygKB02GvIXVHujm6z3MpGK2DGsAvxEk=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Privilege escalation with SageMaker and there's more hiding in execution roles (10 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Privilege escalation flaws have been discovered in AWS SageMaker. Attackers with specific permissions can inject malicious code via lifecycle configurations, mimicking EC2 user data escalation and affecting Lambda and CloudFormation. It is important to detect stop-modify-start sequences in CloudTrail and implement strict access controls.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FzM9xC7/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/ghiVmSjE_26i7yjXMgEw523GRlYNc0XA8HmSSQRLSKw=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Top South Korean e-commerce firm Coupang apologises over massive data breach (2 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     South Korean e-commerce giant Coupang disclosed unauthorized access that affected 33.7 million customer accounts beginning June 24 through overseas servers, exposing names, email addresses, phone numbers, shipping addresses, and order histories. Authorities are investigating a suspected Chinese former employee. The government is examining potential personal information protection violations. This is the country's worst data breach in over a decade.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsecurity%2Fsmarttube-youtube-app-for-android-tv-breached-to-push-malicious-update%2F%3Futm_source=tldrinfosec/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/8JAsFanvw8jMXIOghmQkzHoSbzi_CdX3lge28jzC2kg=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>SmartTube YouTube App for Android TV Breached to Push Malicious Update (2 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     The popular open-source SmartTube YouTube client for Android TV, which provides ad blocking and runs well on low-powered devices, was compromised after an attacker gained access to the developer's signing keys. The compromise was detected when multiple users reported that Play Protect blocked SmartTube. The app developer confirmed that his digital keys were compromised and stated that he had revoked the old signature and would publish a new version with a separate app ID.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🧠</span></div> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Strategies &amp; Tactics</strong></h1> </div> </div> </td></tr></tbody></table>   <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fipurple.team%2F2025%2F12%2F01%2Fbind-link-edr-tampering%2F%3Futm_source=tldrinfosec/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/pW8YIZaIvoGnW8YsJZs3x59U90hGOPFFEgBf7kUh0nE=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Bind Link – EDR Tampering (11 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Threat actors can abuse Windows 11's Bind Link API through the bindflt.sys driver to redirect EDR installation folders to attacker-controlled directories, enabling DLL hijacking and code execution under EDR context. The EDR-Redir proof of concept uses LoadLibraryW to load bindfltapi.dll and CreateDirectoryW to create transparent folder mappings between virtual and backing paths. CrowdStrike, SentinelOne, and Carbon Black have implemented BindFlt monitoring, while Microsoft Defender for Endpoint remains vulnerable. Security teams should deploy Sysmon Event ID 7 monitoring for bindfltapi.dll image load events, validate whether legitimate bind link usage exists in their environment to reduce false positives, and investigate EDR vendor support for bindflt driver activity detection.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flearn.microsoft.com%2Fen-us%2Fwindows%2Fdeployment%2Fwindows-autopatch%2Fprepare%2Fwindows-autopatch-start-using-autopatch%3Futm_source=tldrinfosec/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/V6OY5mJQeCBknUPhCgd0mwXXhRYvXKOf3YrPMUEu8pM=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Start Using Windows Autopatch (2 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Microsoft Intune provides an endpoint management suite for Windows devices. Windows Autopatch is built into Intune and allows administrators to define groups to gradually roll out to an organization. Administrators can also configure hot patching on devices to expedite compliance.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpulse.latio.tech%2Fp%2Fshai-hulud-20-analysis-and-community%3Futm_source=tldrinfosec/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/93Y7X6Ez8APebQKPdJKWNua8mEG3l1CvUaW6rmoRVuo=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Shai Hulud 2.0: Analysis and Community Resources (9 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Shai Hulud 2.0 is a large-scale software supply chain attack that compromised many popular npm packages, including ones tied to services like Zapier, ENS Domains, PostHog, and Postman, in order to steal secrets and establish remote code execution via GitHub runners. Defenders are advised to use published IOCs and scanners to identify infected packages and leftover malware files, treat any secrets on affected machines as compromised, and rotate or revoke them.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🧑‍💻</span></div> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Launches &amp; Tools</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fztw.com%2F%3Futm_source=tldrinfosec/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/SGRnRBWewhSvDuy2y11_4WGdlM5NgPZBecUw4jI_0Nk=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Special offer for TLDR readers: $200 off Zero Trust World 2026 with code ZTWTLDR26 (Sponsor)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     ThreatLocker's annual <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fztw.com%2F/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/FpdH2V8-pli3VWsUhJHlTIpADfvQmZjSh4pe6b4Kyu4=433\" rel=\"noreferrer\" target=\"_blank\"><span>Zero Trust World</span></a> is the most interactive, <strong>hands-on cybersecurity learning</strong> event. Join hacking labs, become Cyber Hero certified, and attend sessions led by cybersecurity, IT, and business experts.  <p></p>  <p>👀 TLDR readers get <strong>$200 off all-access registration</strong>. That's <strong>33% less than the list price</strong>. </p>  <p>🎓 Registration includes all sessions and labs (including CPE-eligible sessions!) </p>  <p>🍹At <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fztw.com%2F/2/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/fn0wGJQXyWe68xrPH6dJAR8PXXcublmIZzJ4NfuF8v0=433\" rel=\"noreferrer\" target=\"_blank\"><span>Zero Trust World</span></a>, all access really means all access, so meals and the afterparty are included with each pass. </p>  <p>Use code <strong>ZTWTLDR26</strong> for <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fztw.com%2F/3/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/aEfRBozdmYfKAWx2rXUGPV7II-BIOqRP0D30UbztFyM=433\" rel=\"noreferrer\" target=\"_blank\"><span>$200 off your all-access pass</span></a>   </p> </span></span></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FSam0rai%2Fguilty-as-yara%3Futm_source=tldrinfosec/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/Nj2tzozZvdDmZ-uhI6ve2uX5RwOCYTL3raVPrSrHb30=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Guilty-As-Yara (GitHub Repo)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Guilty-As-Yara is a Rust-based tool that generates Windows PE executables containing data patterns designed to trigger YARA rule matches for validating rules.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fnowsecure%2Fr2frida%3Futm_source=tldrinfosec/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/wu_Jcg95g7fPc9spsJuMKlv6Mv01r1GL1ExyGWwQMVU=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>R2frida (GitHub Repo)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     r2frida is a radare2 plugin that bundles Frida to instrument and analyze local or remote processes via r2 commands and scripts.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>   <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🎁</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><strong><h1>Miscellaneous</h1></strong></div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthehackernews.com%2F2025%2F12%2Ftomiris-shifts-to-public-service.html%3Futm_source=tldrinfosec/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/J6bwJnZBOZXj7qJ0xJtM3hbrBO-mZWBHrvLjtrVchto=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Tomiris Shifts to Public-Service Implants for Stealthier C2 in Attacks on Government Targets (4 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Tomiris is a Kazakhstan-linked threat actor that leverages Telegram and Discord as command-and-control infrastructure while targeting foreign ministries and government entities across Russia and Central Asia through spear-phishing campaigns. The attack deploys multi-language malware, including Rust-based downloaders, Python-based backdoors like Distopia, and custom implants. Over 50% of lures use Russian-language content to blend malicious traffic with legitimate service activity. Security teams should monitor for unusual Telegram and Discord API traffic patterns, implement application control policies that restrict execution from archive files, and deploy behavioral detection to detect persistence mechanisms targeting Windows Registry modifications associated with these custom implant families.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FK8Zaqx/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/5hFbh9A5UNSspEQ4XyZeEkGUTxWWwjhvEVTUnZLueWc=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>AWS pre:Invent Security Highlights: What Changed and Why it Matters (5 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     AWS has added a new CLI command, `aws login`, which allows users to obtain short-lived credentials for AWS even if the account isn't configured with IAM Identity Center. AWS IAM Outbound Identity Federation now allows AWS users or services to request a short-lived JWT for external services that trust your AWS account, which can replace the use of hardcoded, long-term credentials or API keys in Lambda or EC2. AWS has also enabled Attribute-Based Access Control (ABAC) for S3, which allows users to define access permissions to S3 using tags instead of listing every bucket in an IAM policy.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FBQ5qym/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/S2HsmalrmygD6Z56i7ueYr0dKanCf-LcvAJ9rVU-SEU=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Purple Team Maturity Model: From Chaos to Controlled Chaos (4 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Organizations that wish to start with purple teaming can begin by defining a purple team strategy with loosely scheduled sessions, referencing MITRE ATT&amp;CK tactics in testing, and feeding early detection gaps into detection engineering. Teams can then introduce metrics as they mature, begin using more structured purple team exercises, and map red team TTPs more closely to MITRE ATT&amp;CK tactics and threat intel. As teams further mature, they can introduce automated adversary emulation. Eventually, purple teaming can be driven by threat-intel, continuously run fully automated attack chains, and integrate machine learning, SOAR, and XDR to power rapid detection and response.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">⚡</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Quick Links</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F4utjrk/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/5DD94uBn1BcgyNFaLNxZe5JqbrHEI3iNvaRjF0XrL6c=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Train your entire organization with Infosec IQ &amp; Infosec Skills (Sponsor)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Unlock on-demand cybersecurity ranges and labs for your technical team with any new Infosec IQ security awareness training contract. Act now to get your <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F4utjrk/2/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/8exrKk7p61VfBrC0JOHbtqz4oNXhyHCY5-xTuiRs3hs=433\" rel=\"noreferrer\" target=\"_blank\"><span><strong>3 free Infosec Skills seats.</strong></span></a> </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthehackernews.com%2F2025%2F11%2Fnorth-korean-hackers-deploy-197-npm.html%3Futm_source=tldrinfosec/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/1S_PU5OWVfb2h0rZdFY9Y9MSvlVDz52MyC_jGHgA4DE=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>North Korean Hackers Deploy 197 npm Packages to Spread Updated OtterCookie Malware (4 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     North Korean threat actors deployed 197 malicious npm packages downloaded over 31,000 times as part of the Contagious Interview campaign.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackread.com%2Ffake-shopping-sites-cyber-monday%2F%3Futm_source=tldrinfosec/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/8UDbetuwCeE8Bj7sSTV8CnexHnWOAw8Zzy7a0j3nYOQ=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Over 2,000 Fake Shopping Sites Spotted Before Cyber Monday (3 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     CloudSEK discovered over 2,000 interconnected fake shopping sites targeting Black Friday and Cyber Monday shoppers through coordinated phishing campaigns.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F12%2F01%2Feuropean-cops-shut-down-crypto-mixing-website-that-helped-launder-1-3-billion-euros%2F%3Futm_source=tldrinfosec/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/kBVMnCXJDvyhWVq50JXTmzTnUo5H29sok_XvvdJjJc8=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>European cops shut down crypto mixing website that helped launder 1.3B euros (2 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Europol and partner agencies have seized Cryptomixer, a widely used crypto-mixing service linked to cybercriminal activities such as drug trafficking, arms sales, ransomware, and card fraud.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>    <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"left\" style=\"word-break: break-word; vertical-align: top; padding: 5px 10px\">  <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Love TLDR? Tell your friends and get rewards! </p> </td></tr> <tr><td style=\"padding: 0px 10px 15px\"> <div> Share your referral link below with friends to get free TLDR swag! </div> </td></tr> <tr><td align=\"left\" style=\"padding: 10px\"> <div> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frefer.tldr.tech%2F1579fdb7%2F8/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/FnXvFTJmryrv4cSDlI5iyBKVkChvX_p0HDxck_6vgVg=433\" style=\"color: #464ba4; text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">https://refer.tldr.tech/1579fdb7/8</a> </div> </td></tr> <tr></tr> <tr><td align=\"left\" style=\"padding: 5px 10px\"> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhub.sparklp.co%2Fsub_aa85119a90bc%2F8/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/45TDhFOccJLbfDSBhbN-Mv7M45UCLswtu6CUq2izQlw=433\" style=\"font-size: 16px; line-height: 1.6; padding: 10px 0; display: inline-block; text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\"><span style=\"mso-text-raise: 13pt; text-decoration: underline\">Track your referrals here.</span></a> </td></tr></tbody></table>     <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"left\" style=\"word-break: break-word; vertical-align: top; padding: 5px 10px\">  <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Want to advertise in TLDR? 📰 </p> <div style=\"margin-top: 10px\"> If your company is interested in reaching an audience of cybersecurity professionals and decision makers, you may want to <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrinfosec%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/EnCsWTkr1ZOYrZkj8S9sdxWpSw3vJrVtWZ20h4Aj0l8=433\" target=\"_blank\" rel=\"noreferrer\"><strong><span>advertise with us</span></strong></a>. </div> <br />    <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Want to work at TLDR? 💼 </p> <div style=\"margin-top: 10px\"> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/XKr6qXWsvso9rDygckredv4aeJ7a_z8jz4WfKbAScUw=433\" rel=\"noreferrer\" style=\"color: #0000EE; text-decoration: underline\" target=\"_blank\"><strong>Apply here</strong></a> or send a friend's resume to <a href=\"mailto:jobs@tldr.tech\" style=\"color: #0000EE; text-decoration: underline\" onclick=\"return rcmail.command('compose','jobs@tldr.tech',this)\" rel=\"noreferrer\">jobs@tldr.tech</a> and get $1k if we hire them! </div> <br />  <div> If you have any comments or feedback, just respond to this email! <br /> <br /> Thanks for reading, <br /> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fin%2Fprasannagautam%2F/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/pIlUlFTdeIUqAJg2Wr2QW-mxHN4iH4gEsvujygMILww=433\" target=\"_blank\" rel=\"noreferrer\"><span>Prasanna Gautam</span></a>, <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fin%2Fericfernandezdelcampo%2F/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/ZcrqZp23IXLy4QE4YnySngWxwEJG1YzNS9x16hY9Hu4=433\" target=\"_blank\" rel=\"noreferrer\"><span>Eric Fernandez</span></a> &amp; <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fin%2Fsammy-tbeile%2F/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/OEAGvMdA-BW5r6Hkun5nDZiTL3-d_0NdwR-XRmzuaQc=433\" target=\"_blank\" rel=\"noreferrer\"><span>Sammy Tbeile</span></a> <br /> <br /> </div> <br /> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Finfosec%2Fmanage%3Femail=tldrai90%2540ino.to/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/g88S3jseKTRFDNB01HuLQBE1vWhBGZPiYSp7kdDcIcU=433\" target=\"_blank\" rel=\"noreferrer\">Manage your subscriptions</a> to our other newsletters on tech, startups, and programming. Or if TLDR Information Security isn't for you, please <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa.tldrnewsletter.com%2Funsubscribe%3Fep=1%26l=8d9cea11-3e94-11ed-9a32-0241b9615763%26lc=8cb497a0-9621-11f0-9193-63bc73b21b46%26p=3f396156-cf3b-11f0-8211-cb95d47fe381%26pt=campaign%26pv=4%26spa=1764684140%26t=1764684771%26s=1eb639308e3d8636bdf611b9ffa15195833becc1ceb7ad68f209611fcae29bcf/1/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/UCBXDOJDJb0jgudJjKNfYhYFMiOY1jmWS1twv7fttVw=433\" target=\"_blank\" rel=\"noreferrer\">unsubscribe</a>. <br /> </div> </td></tr></tbody></table>  </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table>    <img src=\"http://tracking.tldrnewsletter.com/CI0/0100019adf68aef8-4fa9f713-61d8-4317-bf62-e4991dbcb233-000000/iURTcoQ_giQVFn_jEPT8ovF7eHAK4IoGXNa_wWuaDgM=433\" style=\"display: none; width: 1px; height: 1px\" /> </div></div></div>",
      "summary": "       The popular open-source SmartTube YouTube client for Android TV, which provides ad blocking and runs well on low-powered devices, was compromised ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌                    Sign Up |Advertise|View Online     TLDR      Together With        TLDR Information Security 2025-12-02                                                                                                                     When",
      "publishedAt": "2025-12-02T14:12:54.000Z",
      "author": "TLDR InfoSec ",
      "source": "rss",
      "feedName": "TLDR",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 16.47452267466043,
      "ingestedAt": "2025-12-02T14:44:03.163Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "testing",
        "observability",
        "governance"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f4a",
      "title": "Forecasting in Offline Reinforcement Learning for Non-stationary Environments",
      "url": "https://arxiv.org/abs/2512.01987v1",
      "content": "Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific pattern of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent's experience, we aim to bridge the gap between offline RL and the complexities of real-world, non-stationary environments.",
      "summary": "Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade per",
      "publishedAt": "2025-12-01T18:45:05.000Z",
      "author": "Suzan Ece Ada",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 12.720527126030992,
      "ingestedAt": "2025-12-02T14:44:03.163Z",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "observability",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f4b",
      "title": "A Dual Approach for Hierarchical Information-Theoretic Tree Abstractions",
      "url": "https://arxiv.org/abs/2512.01985v1",
      "content": "In this paper, we consider establishing a formal connection between two distinct tree-abstraction problems inspired by the information-bottleneck (IB) method. Specifically, we consider the hard- and soft-constrained formulations that have recently appeared in the literature to determine the conditions for which the two approaches are equivalent. Our analysis leverages concepts from Lagrangian relaxation and duality theory to relate the dual function of the hard-constrained problem to the Q-function employed in Q-tree search and shows the connection between tree phase transitions and solutions to the dual problem obtained by exploiting the problem structure. An algorithm is proposed that employs knowledge of the tree phase transitions to find a setting of the dual variable that solves the dual problem. Furthermore, we present an alternative approach to select the dual variable that leverages the integer programming formulation of the hard-constrained problem and the strong duality of linear programming. To obtain a linear program, we establish that a relaxation of the integer programming formulation of the hard-constrained tree-search problem has the integrality property by showing that the program constraint matrix is totally unimodular. Empirical results that corroborate the theoretical developments are presented and discussed throughout.",
      "summary": "In this paper, we consider establishing a formal connection between two distinct tree-abstraction problems inspired by the information-bottleneck (IB) method. Specifically, we consider the hard- and soft-constrained formulations that have recently appeared in the literature to determine the conditions for which the two approaches are equivalent. Our analysis leverages concepts from Lagrangian relaxation and duality theory to relate the dual function of the hard-constrained problem to the Q-funct",
      "publishedAt": "2025-12-01T18:43:11.000Z",
      "author": "Daniel T. Larsson",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "security_incident",
      "score": 9.421724683940543,
      "ingestedAt": "2025-12-02T14:44:03.163Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f4c",
      "title": "Feature-Based Semantics-Aware Scheduling for Energy-Harvesting Federated Learning",
      "url": "https://arxiv.org/abs/2512.01983v1",
      "content": "Federated Learning (FL) on resource-constrained edge devices faces a critical challenge: The computational energy required for training Deep Neural Networks (DNNs) often dominates communication costs. However, most existing Energy-Harvesting FL (EHFL) strategies fail to account for this reality, resulting in wasted energy due to redundant local computations. For efficient and proactive resource management, algorithms that predict local update contributions must be devised. We propose a lightweight client scheduling framework using the Version Age of Information (VAoI), a semantics-aware metric that quantifies update timeliness and significance. Crucially, we overcome VAoI's typical prohibitive computational cost, which requires statistical distance over the entire parameter space, by introducing a feature-based proxy. This proxy estimates model redundancy using intermediate-layer extraction from a single forward pass, dramatically reducing computational complexity. Experiments conducted under extreme non-IID data distributions and scarce energy availability demonstrate superior learning performance while achieving energy reduction compared to existing baseline selection policies. Our framework establishes semantics-aware scheduling as a practical and vital solution for EHFL in realistic scenarios where training costs dominate transmission costs.",
      "summary": "Federated Learning (FL) on resource-constrained edge devices faces a critical challenge: The computational energy required for training Deep Neural Networks (DNNs) often dominates communication costs. However, most existing Energy-Harvesting FL (EHFL) strategies fail to account for this reality, resulting in wasted energy due to redundant local computations. For efficient and proactive resource management, algorithms that predict local update contributions must be devised. We propose a lightweig",
      "publishedAt": "2025-12-01T18:40:26.000Z",
      "author": "Eunjeong Jeong",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 7.065329674569855,
      "ingestedAt": "2025-12-02T14:44:03.163Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f4d",
      "title": "Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback",
      "url": "https://arxiv.org/abs/2512.01979v1",
      "content": "GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.",
      "summary": "GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large ",
      "publishedAt": "2025-12-01T18:37:19.000Z",
      "author": "Aiden Yiliu Li",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 5.1804408168241345,
      "ingestedAt": "2025-12-02T14:44:03.164Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f4f",
      "title": "AI-Driven Optimization under Uncertainty for Mineral Processing Operations",
      "url": "https://arxiv.org/abs/2512.01977v1",
      "content": "The global capacity for mineral processing must expand rapidly to meet the demand for critical minerals, which are essential for building the clean energy technologies necessary to mitigate climate change. However, the efficiency of mineral processing is severely limited by uncertainty, which arises from both the variability of feedstock and the complexity of process dynamics. To optimize mineral processing circuits under uncertainty, we introduce an AI-driven approach that formulates mineral processing as a Partially Observable Markov Decision Process (POMDP). We demonstrate the capabilities of this approach in handling both feedstock uncertainty and process model uncertainty to optimize the operation of a simulated, simplified flotation cell as an example. We show that by integrating the process of information gathering (i.e., uncertainty reduction) and process optimization, this approach has the potential to consistently perform better than traditional approaches at maximizing an overall objective, such as net present value (NPV). Our methodological demonstration of this optimization-under-uncertainty approach for a synthetic case provides a mathematical and computational framework for later real-world application, with the potential to improve both the laboratory-scale design of experiments and industrial-scale operation of mineral processing circuits without any additional hardware.",
      "summary": "The global capacity for mineral processing must expand rapidly to meet the demand for critical minerals, which are essential for building the clean energy technologies necessary to mitigate climate change. However, the efficiency of mineral processing is severely limited by uncertainty, which arises from both the variability of feedstock and the complexity of process dynamics. To optimize mineral processing circuits under uncertainty, we introduce an AI-driven approach that formulates mineral pr",
      "publishedAt": "2025-12-01T18:35:54.000Z",
      "author": "William Xu",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.180076794008097,
      "ingestedAt": "2025-12-02T14:44:03.164Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f50",
      "title": "A List of Complexity Bounds for Property Testing by Quantum Sample-to-Query Lifting",
      "url": "https://arxiv.org/abs/2512.01971v1",
      "content": "Quantum sample-to-query lifting, a relation between quantum sample complexity and quantum query complexity presented in Wang and Zhang (SIAM J. Comput. 2025), was significantly strengthened by Tang, Wright, and Zhandry (2025) to the case of state-preparation oracles. In this paper, we compile a list of quantum lower and upper bounds for property testing that are obtained by quantum sample-to-query lifting. The problems of interest include testing properties of probability distributions and quantum states, such as entropy and closeness. This collection contains new results, as well as new proofs of known bounds. In total, we present 49 complexity bounds, where 41 are new and 18 are (near-)optimal.",
      "summary": "Quantum sample-to-query lifting, a relation between quantum sample complexity and quantum query complexity presented in Wang and Zhang (SIAM J. Comput. 2025), was significantly strengthened by Tang, Wright, and Zhandry (2025) to the case of state-preparation oracles. In this paper, we compile a list of quantum lower and upper bounds for property testing that are obtained by quantum sample-to-query lifting. The problems of interest include testing properties of probability distributions and quant",
      "publishedAt": "2025-12-01T18:27:37.000Z",
      "author": "Kean Chen",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.648671465275584,
      "ingestedAt": "2025-12-02T14:44:03.164Z",
      "tags": [
        "code_review",
        "testing"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f51",
      "title": "From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning",
      "url": "https://arxiv.org/abs/2512.01970v1",
      "content": "The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.",
      "summary": "The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic sk",
      "publishedAt": "2025-12-01T18:27:25.000Z",
      "author": "Sitao Cheng",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.177897474883978,
      "ingestedAt": "2025-12-02T14:44:03.164Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f52",
      "title": "Learned-Rule-Augmented Large Language Model Evaluators",
      "url": "https://arxiv.org/abs/2512.01958v1",
      "content": "Large language models (LLMs) are predominantly used as evaluators for natural language generation (NLG) tasks, but their application to broader evaluation scenarios remains limited. In this work, we explore the potential of LLMs as general evaluators across diverse tasks. Although LLM-based evaluators have made progress in different areas, existing methods struggle to generalize due to their reliance on costly, human-designed evaluation principles, which are often misaligned with both annotated data and LLMs' understanding.To address these challenges, we propose a rule-augmented evaluation paradigm. First, we introduce a rule distillation method that automatically extracts scoring rules from data using an LLM-assisted Monte Carlo Tree Search (MCTS), alleviating scalability issues and improving alignment with data. Second, to enable LLMs to effectively apply the learned rules, we propose two strategies: (1) Chain-of-Rule (CoR), which guides LLM to follow distilled rules, and (2) training a rule-augmented LLM evaluator (RuAE) via reinforcement learning, further bridging the gap between rules and LLMs' reasoning. Extensive experiments on diverse tasks demonstrate the effectiveness and generalizability of our approach across various evaluation scenarios.",
      "summary": "Large language models (LLMs) are predominantly used as evaluators for natural language generation (NLG) tasks, but their application to broader evaluation scenarios remains limited. In this work, we explore the potential of LLMs as general evaluators across diverse tasks. Although LLM-based evaluators have made progress in different areas, existing methods struggle to generalize due to their reliance on costly, human-designed evaluation principles, which are often misaligned with both annotated ",
      "publishedAt": "2025-12-01T18:08:45.000Z",
      "author": "Jie Meng",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.173105344291548,
      "ingestedAt": "2025-12-02T14:44:03.164Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f53",
      "title": "GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment",
      "url": "https://arxiv.org/abs/2512.01952v1",
      "content": "Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.",
      "summary": "Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pre",
      "publishedAt": "2025-12-01T18:03:29.000Z",
      "author": "Haoyang He",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 8.462870314766121,
      "ingestedAt": "2025-12-02T14:44:03.164Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "governance"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f54",
      "title": "How Far Are We from Genuinely Useful Deep Research Agents?",
      "url": "https://arxiv.org/abs/2512.01948v1",
      "content": "Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.",
      "summary": "Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we p",
      "publishedAt": "2025-12-01T17:58:59.000Z",
      "author": "Dingling Zhang",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 14.101635820046118,
      "ingestedAt": "2025-12-02T14:44:03.164Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "observability",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f55",
      "title": "Agentic Policy Optimization via Instruction-Policy Co-Evolution",
      "url": "https://arxiv.org/abs/2512.01945v1",
      "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and e",
      "publishedAt": "2025-12-01T17:56:29.000Z",
      "author": "Han Zhou",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 16.919864656451352,
      "ingestedAt": "2025-12-02T14:44:03.164Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "governance",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f56",
      "title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks",
      "url": "https://arxiv.org/abs/2512.01939v1",
      "content": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.",
      "summary": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems ",
      "publishedAt": "2025-12-01T17:52:15.000Z",
      "author": "Yanlin Wang",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 11.747438945618846,
      "ingestedAt": "2025-12-02T14:44:03.164Z",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f57",
      "title": "SVRG and Beyond via Posterior Correction",
      "url": "https://arxiv.org/abs/2512.01930v1",
      "content": "Stochastic Variance Reduced Gradient (SVRG) and its variants aim to speed-up training by using gradient corrections, but have seen limited success in deep learning. Here, we show surprising new foundational connections of SVRG to a recently proposed Bayesian method called posterior correction. Specifically, we show that SVRG is recovered as a special case of posterior correction over the isotropic-Gaussian family, while novel extensions are automatically obtained by using more flexible exponential families. We derive two new SVRG variants by using Gaussian families: First, a Newton-like variant that employs novel Hessian corrections, and second, an Adam-like extension that improves pretraining and finetuning of Transformer language models. This is the first work to connect SVRG to Bayes and use it to boost variational training for deep networks.",
      "summary": "Stochastic Variance Reduced Gradient (SVRG) and its variants aim to speed-up training by using gradient corrections, but have seen limited success in deep learning. Here, we show surprising new foundational connections of SVRG to a recently proposed Bayesian method called posterior correction. Specifically, we show that SVRG is recovered as a special case of posterior correction over the isotropic-Gaussian family, while novel extensions are automatically obtained by using more flexible exponenti",
      "publishedAt": "2025-12-01T17:45:30.000Z",
      "author": "Nico Daheim",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.167142776281701,
      "ingestedAt": "2025-12-02T14:44:03.164Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f58",
      "title": "Rectifying LLM Thought from Lens of Optimization",
      "url": "https://arxiv.org/abs/2512.01925v1",
      "content": "Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.",
      "summary": "Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gra",
      "publishedAt": "2025-12-01T17:41:08.000Z",
      "author": "Junnan Liu",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 6.10530072141731,
      "ingestedAt": "2025-12-02T14:44:03.165Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f59",
      "title": "Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model",
      "url": "https://arxiv.org/abs/2512.01924v1",
      "content": "Robots in uncertain real-world environments must perform both goal-directed and exploratory actions. However, most deep learning-based control methods neglect exploration and struggle under uncertainty. To address this, we adopt deep active inference, a framework that accounts for human goal-directed and exploratory actions. Yet, conventional deep active inference approaches face challenges due to limited environmental representation capacity and high computational cost in action selection. We propose a novel deep active inference framework that consists of a world model, an action model, and an abstract world model. The world model encodes environmental dynamics into hidden state representations at slow and fast timescales. The action model compresses action sequences into abstract actions using vector quantization, and the abstract world model predicts future slow states conditioned on the abstract action, enabling low-cost action selection. We evaluate the framework on object-manipulation tasks with a real-world robot. Results show that it achieves high success rates across diverse manipulation tasks and switches between goal-directed and exploratory actions in uncertain settings, while making action selection computationally tractable. These findings highlight the importance of modeling multiple timescale dynamics and abstracting actions and state transitions.",
      "summary": "Robots in uncertain real-world environments must perform both goal-directed and exploratory actions. However, most deep learning-based control methods neglect exploration and struggle under uncertainty. To address this, we adopt deep active inference, a framework that accounts for human goal-directed and exploratory actions. Yet, conventional deep active inference approaches face challenges due to limited environmental representation capacity and high computational cost in action selection. We p",
      "publishedAt": "2025-12-01T17:41:01.000Z",
      "author": "Kentaro Fujii",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.635629590693672,
      "ingestedAt": "2025-12-02T14:44:03.165Z",
      "tags": [
        "code_review",
        "retrieval"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f5b",
      "title": "Latent Debate: A Surrogate Framework for Interpreting LLM Thinking",
      "url": "https://arxiv.org/abs/2512.01909v1",
      "content": "Understanding the internal thinking process of Large Language Models (LLMs) and the cause of hallucinations remains a key challenge. To this end, we introduce latent debate, a novel framework for interpreting model predictions through the lens of implicit internal arguments. Unlike the current work of self-consistency and multi-agent debate, which relies on explicit debates among multiple answers or multiple models, latent debate captures the hidden supporting and attacking signals that arise within a single model during a single inference. We first present a model- and task-agnostic conceptual framework, and then instantiate it symbolically to approximate the thinking process of LLMs on True/False prediction tasks. Empirical studies demonstrate that latent debate is a faithful structured surrogate model that has highly consistent predictions with the original LLM. Beyond interpretability, we demonstrate that latent debate provides a strong baseline for hallucination detection. Further analysis reveals strong correlations between hallucinations and debate patterns, such as a high degree of latent debates in the middle layers is linked to a higher risk of hallucinations. These findings position latent debate as a potential framework for understanding internal mechanisms of LLMs, especially for scenarios where internal (dis)agreements appear during the inference steps.",
      "summary": "Understanding the internal thinking process of Large Language Models (LLMs) and the cause of hallucinations remains a key challenge. To this end, we introduce latent debate, a novel framework for interpreting model predictions through the lens of implicit internal arguments. Unlike the current work of self-consistency and multi-agent debate, which relies on explicit debates among multiple answers or multiple models, latent debate captures the hidden supporting and attacking signals that arise wi",
      "publishedAt": "2025-12-01T17:27:31.000Z",
      "author": "Lihu Chen",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 7.0398212435995715,
      "ingestedAt": "2025-12-02T14:44:03.165Z",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f5c",
      "title": "A Hands-On Molecular Communication Testbed for Undergraduate Education",
      "url": "https://arxiv.org/abs/2512.01904v1",
      "content": "This work presents a hands-on molecular communication (MC) testbed developed for the undergraduate \\textit{Communication Engineering} lab course at the Institute for Communications Technology (IfN), TU~Braunschweig. The goal of the experiment is to provide students with an intuitive and reproducible introduction to MC concepts using a low-cost and accessible fluidic setup. The system employs a background water flow into which three dye colors are injected and symbols are detected by a multi-wavelength photosensor. A zero-forcing--based estimator is used to separate the spectral components and reliably identify the transmitted colors. The experiment is designed to be completed independently by students within a single laboratory session and requires only basic prior knowledge from introductory communication engineering courses. A detailed script accompanies the experiment, guiding students through channel characterization, color detection, pseudoinverse computation, and simple data transmission using on-off keying. In pilot trials, students successfully reproduced the entire communication chain and achieved stable data rates of up to 0.5~bit/s over a 15~cm channel. The proposed testbed demonstrates that fundamental principles of MC can be taught effectively using a compact and inexpensive experimental setup. The experiment will be integrated into an undergraduate lab course.",
      "summary": "This work presents a hands-on molecular communication (MC) testbed developed for the undergraduate \\textit{Communication Engineering} lab course at the Institute for Communications Technology (IfN), TU~Braunschweig. The goal of the experiment is to provide students with an intuitive and reproducible introduction to MC concepts using a low-cost and accessible fluidic setup. The system employs a background water flow into which three dye colors are injected and symbols are detected by a multi-wave",
      "publishedAt": "2025-12-01T17:25:33.000Z",
      "author": "Arne Gaedeken",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.162031982834756,
      "ingestedAt": "2025-12-02T14:44:03.165Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f5d",
      "title": "Digital Twin Aided Millimeter Wave MIMO: Site-Specific Beam Codebook Learning",
      "url": "https://arxiv.org/abs/2512.01902v1",
      "content": "Learning site-specific beams that adapt to the deployment environment, interference sources, and hardware imperfections can lead to noticeable performance gains in coverage, data rate, and power saving, among other interesting advantages. This learning process, however, typically requires a large number of active interactions/iterations, which limits its practical feasibility and leads to excessive overhead. To address these challenges, we propose a digital twin aided codebook learning framework, where a site-specific digital twin is leveraged to generate synthetic channel data for codebook learning. We also propose to learn separate codebooks for line-of-sight and non-line-of-sight users, leveraging the geometric information provided by the digital twin. Simulation results demonstrate that the codebook learned from the digital twin can adapt to the environment geometry and user distribution, leading to high received signal-to-noise ratio performance. Moreover, we identify the ray-tracing accuracy as the most critical factor in digital twin fidelity that impacts the learned codebook performance.",
      "summary": "Learning site-specific beams that adapt to the deployment environment, interference sources, and hardware imperfections can lead to noticeable performance gains in coverage, data rate, and power saving, among other interesting advantages. This learning process, however, typically requires a large number of active interactions/iterations, which limits its practical feasibility and leads to excessive overhead. To address these challenges, we propose a digital twin aided codebook learning framework",
      "publishedAt": "2025-12-01T17:24:07.000Z",
      "author": "Hao Luo",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 7.507876343755932,
      "ingestedAt": "2025-12-02T14:44:03.165Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f5f",
      "title": "Tight Bounds for Feedback Vertex Set Parameterized by Clique-width",
      "url": "https://arxiv.org/abs/2512.01900v1",
      "content": "We introduce a new notion of acyclicity representation in labeled graphs, and present three applications thereof. Our main result is an algorithm that, given a graph $G$ and a $k$-clique expression of $G$, in time $O(6^kn^c)$ counts modulo $2$ the number of feedback vertex sets of $G$ of each size. We achieve this through an involved subroutine for merging partial solutions at union nodes in the expression. In the usual way this results in a one-sided error Monte-Carlo algorithm for solving the decision problem in the same time. We complement these by a matching lower bound under the Strong Exponential-Time Hypothesis (SETH). This closes an open question that appeared multiple times in the literature [ESA 23, ICALP 24, IPEC 25]. \n  We also present an algorithm that, given a graph $G$ and a tree decomposition of width $k$ of $G$, in time $O(3^kn^c)$ counts modulo $2$ the number of feedback vertex sets of $G$ of each size. This matches the known SETH-tight bound for the decision version, which was obtained using the celebrated cut-and-count technique [FOCS 11, TALG 22]. Unlike other applications of cut-and-count, which use the isolation lemma to reduce a decision problem to counting solutions modulo $2$, this bound was obtained via counting other objects, leaving the complexity of counting solutions modulo $2$ open. \n  Finally, we present a one-sided error Monte-Carlo algorithm that, given a graph $G$ and a $k$-clique expression of $G$, in time $O(18^kn^c)$ decides the existence of a connected feedback vertex set of size $b$ in $G$. We provide a matching lower bound under SETH.",
      "summary": "We introduce a new notion of acyclicity representation in labeled graphs, and present three applications thereof. Our main result is an algorithm that, given a graph $G$ and a $k$-clique expression of $G$, in time $O(6^kn^c)$ counts modulo $2$ the number of feedback vertex sets of $G$ of each size. We achieve this through an involved subroutine for merging partial solutions at union nodes in the expression. In the usual way this results in a one-sided error Monte-Carlo algorithm for solving the ",
      "publishedAt": "2025-12-01T17:21:12.000Z",
      "author": "Narek Bojikian",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.160918271688105,
      "ingestedAt": "2025-12-02T14:44:03.165Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f61",
      "title": "OPOR-Bench: Evaluating Large Language Models on Online Public Opinion Report Generation",
      "url": "https://arxiv.org/abs/2512.01896v1",
      "content": "Online Public Opinion Reports consolidate news and social media for timely crisis management by governments and enterprises. While large language models have made automated report generation technically feasible, systematic research in this specific area remains notably absent, particularly lacking formal task definitions and corresponding benchmarks. To bridge this gap, we define the Automated Online Public Opinion Report Generation (OPOR-GEN) task and construct OPOR-BENCH, an event-centric dataset covering 463 crisis events with their corresponding news articles, social media posts, and a reference summary. To evaluate report quality, we propose OPOR-EVAL, a novel agent-based framework that simulates human expert evaluation by analyzing generated reports in context. Experiments with frontier models demonstrate that our framework achieves high correlation with human judgments. Our comprehensive task definition, benchmark dataset, and evaluation framework provide a solid foundation for future research in this critical domain.",
      "summary": "Online Public Opinion Reports consolidate news and social media for timely crisis management by governments and enterprises. While large language models have made automated report generation technically feasible, systematic research in this specific area remains notably absent, particularly lacking formal task definitions and corresponding benchmarks. To bridge this gap, we define the Automated Online Public Opinion Report Generation (OPOR-GEN) task and construct OPOR-BENCH, an event-centric dat",
      "publishedAt": "2025-12-01T17:18:02.000Z",
      "author": "Jinzheng Yu",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "pricing_business",
      "score": 10.789316048230276,
      "ingestedAt": "2025-12-02T14:44:03.165Z",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f63",
      "title": "Exploring Human Perceptions of AI Responses: Insights from a Mixed-Methods Study on Risk Mitigation in Generative Models",
      "url": "https://arxiv.org/abs/2512.01892v1",
      "content": "With the rapid uptake of generative AI, investigating human perceptions of generated responses has become crucial. A major challenge is their `aptitude' for hallucinating and generating harmful contents. Despite major efforts for implementing guardrails, human perceptions of these mitigation strategies are largely unknown. We conducted a mixed-method experiment for evaluating the responses of a mitigation strategy across multiple-dimensions: faithfulness, fairness, harm-removal capacity, and relevance. In a within-subject study design, 57 participants assessed the responses under two conditions: harmful response plus its mitigation and solely mitigated response. Results revealed that participants' native language, AI work experience, and annotation familiarity significantly influenced evaluations. Participants showed high sensitivity to linguistic and contextual attributes, penalizing minor grammar errors while rewarding preserved semantic contexts. This contrasts with how language is often treated in the quantitative evaluation of LLMs. We also introduced new metrics for training and evaluating mitigation strategies and insights for human-AI evaluation studies.",
      "summary": "With the rapid uptake of generative AI, investigating human perceptions of generated responses has become crucial. A major challenge is their `aptitude' for hallucinating and generating harmful contents. Despite major efforts for implementing guardrails, human perceptions of these mitigation strategies are largely unknown. We conducted a mixed-method experiment for evaluating the responses of a mitigation strategy across multiple-dimensions: faithfulness, fairness, harm-removal capacity, and rel",
      "publishedAt": "2025-12-01T17:12:28.000Z",
      "author": "Heloisa Candello",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.158683040618708,
      "ingestedAt": "2025-12-02T14:44:03.165Z",
      "tags": [
        "code_review",
        "observability"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f66",
      "title": "Unifying Sign and Magnitude for Optimizing Deep Vision Networks via ThermoLion",
      "url": "https://arxiv.org/abs/2512.01881v1",
      "content": "The training of deep vision models is fundamentally a signal recovery problem amidst high-dimensional stochastic noise. Current optimization paradigms impose a static compromise on information channel capacity. For instance, magnitude-based methods, such as AdamW, operate on the assumption that gradient norms are high-fidelity curvature signals. While this allows for precision in smooth regimes, it leads to catastrophic noise amplification when applied to rugged, non-convex landscapes. Conversely, sign-based methods (e.g., Lion) perform a radical 1-bit quantization of the gradient, which aims to provide robust regularization at the cost of discarding fine-grained descent information. We propose that optimal convergence requires neither static prior, but rather a dynamic modulation of the update bitrate. We introduce \\textbf{ThermoLion}, a vision-centric framework that utilizes local Signal-to-Noise Ratio (SNR) gating to autonomously transition parameters between a \"low-bit\" exploration phase and a \"high-precision\" exploitation phase. Furthermore, we introduce a Momentum Alignment mechanism that detects constructive interference between historical drift and instantaneous gradients to accelerate convergence during stable trajectories. Empirical benchmarks across 12 diverse vision datasets (including CIFAR, SVHN, and GTSRB) demonstrate that ThermoLion serves as a hyperparameter-free generalist, surpassing both AdamW and Lion in convergence speed and terminal accuracy without architecture-specific tuning.",
      "summary": "The training of deep vision models is fundamentally a signal recovery problem amidst high-dimensional stochastic noise. Current optimization paradigms impose a static compromise on information channel capacity. For instance, magnitude-based methods, such as AdamW, operate on the assumption that gradient norms are high-fidelity curvature signals. While this allows for precision in smooth regimes, it leads to catastrophic noise amplification when applied to rugged, non-convex landscapes. Conversel",
      "publishedAt": "2025-12-01T17:04:17.000Z",
      "author": "Ahmed Nebli",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 8.906836333935585,
      "ingestedAt": "2025-12-02T14:44:03.165Z",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f6a",
      "title": "Predicting Human Chess Moves: An AI Assisted Analysis of Chess Games Using Skill-group Specific n-gram Language Models",
      "url": "https://arxiv.org/abs/2512.01880v1",
      "content": "Chess, a deterministic game with perfect information, has long served as a benchmark for studying strategic decision-making and artificial intelligence. Traditional chess engines or tools for analysis primarily focus on calculating optimal moves, often neglecting the variability inherent in human chess playing, particularly across different skill levels. \n  To overcome this limitation, we propose a novel and computationally efficient move prediction framework that approaches chess move prediction as a behavioral analysis task. The framework employs n-gram language models to capture move patterns characteristic of specific player skill levels. By dividing players into seven distinct skill groups, from novice to expert, we trained separate models using data from the open-source chess platform Lichess. The framework dynamically selects the most suitable model for prediction tasks and generates player moves based on preceding sequences. \n  Evaluation on real-world game data demonstrates that the model selector module within the framework can classify skill levels with an accuracy of up to 31.7\\% when utilizing early game information (16 half-moves). The move prediction framework also shows substantial accuracy improvements, with our Selector Assisted Accuracy being up to 39.1\\% more accurate than our benchmark accuracy. The computational efficiency of the framework further enhances its suitability for real-time chess analysis.",
      "summary": "Chess, a deterministic game with perfect information, has long served as a benchmark for studying strategic decision-making and artificial intelligence. Traditional chess engines or tools for analysis primarily focus on calculating optimal moves, often neglecting the variability inherent in human chess playing, particularly across different skill levels. \n  To overcome this limitation, we propose a novel and computationally efficient move prediction framework that approaches chess move predictio",
      "publishedAt": "2025-12-01T17:02:07.000Z",
      "author": "Daren Zhong",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 5.1560352850572535,
      "ingestedAt": "2025-12-02T14:44:03.166Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f6c",
      "title": "Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning",
      "url": "https://arxiv.org/abs/2512.01878v1",
      "content": "In this work, we propose that reasoning in knowledge graph (KG) networks can be guided by surprise minimization. Entities that are close in graph distance will have lower surprise than those farther apart. This connects the Free Energy Principle (FEP) from neuroscience to KG systems, where the KG serves as the agent's generative model. We formalize surprise using the shortest-path distance in directed graphs and provide a framework for KG-based agents. Graph distance appears in graph neural networks as message passing depth and in model-based reinforcement learning as world model trajectories. This work-in-progress study explores whether distance-based surprise can extend recent work showing that syntax minimizes surprise and free energy via tree structures.",
      "summary": "In this work, we propose that reasoning in knowledge graph (KG) networks can be guided by surprise minimization. Entities that are close in graph distance will have lower surprise than those farther apart. This connects the Free Energy Principle (FEP) from neuroscience to KG systems, where the KG serves as the agent's generative model. We formalize surprise using the shortest-path distance in directed graphs and provide a framework for KG-based agents. Graph distance appears in graph neural netw",
      "publishedAt": "2025-12-01T16:59:28.000Z",
      "author": "Gaganpreet Jhajj",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 11.716721765786408,
      "ingestedAt": "2025-12-02T14:44:03.166Z",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f6f",
      "title": "Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees",
      "url": "https://arxiv.org/abs/2512.01870v1",
      "content": "We study whether a Large Language Model can learn the deterministic sequence of trees generated by the iterated prime factorization of the natural numbers. Each integer is mapped into a rooted planar tree and the resulting sequence $ \\mathbb{N}\\mathcal{T}$ defines an arithmetic text with measurable statistical structure. A transformer network (the GPT-2 architecture) is trained from scratch on the first $10^{11}$ elements to subsequently test its predictive ability under next-word and masked-word prediction tasks. Our results show that the model partially learns the internal grammar of $\\mathbb{N}\\mathcal{T}$, capturing non-trivial regularities and correlations. This suggests that learnability may extend beyond empirical data to the very structure of arithmetic.",
      "summary": "We study whether a Large Language Model can learn the deterministic sequence of trees generated by the iterated prime factorization of the natural numbers. Each integer is mapped into a rooted planar tree and the resulting sequence $ \\mathbb{N}\\mathcal{T}$ defines an arithmetic text with measurable statistical structure. A transformer network (the GPT-2 architecture) is trained from scratch on the first $10^{11}$ elements to subsequently test its predictive ability under next-word and masked-wor",
      "publishedAt": "2025-12-01T16:51:38.000Z",
      "author": "Alessandro Breccia",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.621841610473977,
      "ingestedAt": "2025-12-02T14:44:03.166Z",
      "tags": [
        "code_review",
        "testing"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f72",
      "title": "Cross-Lingual Interleaving for Speech Language Models",
      "url": "https://arxiv.org/abs/2512.01865v1",
      "content": "Spoken Language Models (SLMs) aim to learn linguistic competence directly from speech using discrete units, widening access to Natural Language Processing (NLP) technologies for languages with limited written resources. However, progress has been largely English-centric due to scarce spoken evaluation benchmarks and training data, making cross-lingual learning difficult. We present a cross-lingual interleaving method that mixes speech tokens across languages without textual supervision. We also release an EN-FR training dataset, TinyStories (~42k hours), together with EN-FR spoken StoryCloze and TopicCloze benchmarks for cross-lingual semantic evaluation, both synthetically generated using GPT-4. On 360M and 1B SLMs under matched training-token budgets, interleaving improves monolingual semantic accuracy, enables robust cross-lingual continuation, and strengthens cross-lingual hidden-state alignment. Taken together, these results indicate that cross-lingual interleaving is a simple, scalable route to building multilingual SLMs that understand and converse across languages. All resources will be made open-source to support reproducibility.",
      "summary": "Spoken Language Models (SLMs) aim to learn linguistic competence directly from speech using discrete units, widening access to Natural Language Processing (NLP) technologies for languages with limited written resources. However, progress has been largely English-centric due to scarce spoken evaluation benchmarks and training data, making cross-lingual learning difficult. We present a cross-lingual interleaving method that mixes speech tokens across languages without textual supervision. We also ",
      "publishedAt": "2025-12-01T16:48:05.000Z",
      "author": "Adel Moumen",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 6.089256052102426,
      "ingestedAt": "2025-12-02T14:44:03.166Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f74",
      "title": "Topological Order in Deep State",
      "url": "https://arxiv.org/abs/2512.01863v1",
      "content": "Topologically ordered states are among the most interesting quantum phases of matter that host emergent quasi-particles having fractional charge and obeying fractional quantum statistics. Theoretical study of such states is however challenging owing to their strong-coupling nature that prevents conventional mean-field treatment. Here, we demonstrate that an attention-based deep neural network provides an expressive variational wavefunction that discovers fractional Chern insulator ground states purely through energy minimization without prior knowledge and achieves remarkable accuracy. We introduce an efficient method to extract ground state topological degeneracy -- a hallmark of topological order -- from a single optimized real-space wavefunction in translation-invariant systems by decomposing it into different many-body momentum sectors. Our results establish neural network variational Monte Carlo as a versatile tool for discovering strongly correlated topological phases.",
      "summary": "Topologically ordered states are among the most interesting quantum phases of matter that host emergent quasi-particles having fractional charge and obeying fractional quantum statistics. Theoretical study of such states is however challenging owing to their strong-coupling nature that prevents conventional mean-field treatment. Here, we demonstrate that an attention-based deep neural network provides an expressive variational wavefunction that discovers fractional Chern insulator ground states ",
      "publishedAt": "2025-12-01T16:46:39.000Z",
      "author": "Ahmed Abouelkomsan",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.1520811136167906,
      "ingestedAt": "2025-12-02T14:44:03.166Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f76",
      "title": "Storage capacity of perceptron with variable selection",
      "url": "https://arxiv.org/abs/2512.01861v1",
      "content": "A central challenge in machine learning is to distinguish genuine structure from chance correlations in high-dimensional data. In this work, we address this issue for the perceptron, a foundational model of neural computation. Specifically, we investigate the relationship between the pattern load $α$ and the variable selection ratio $ρ$ for which a simple perceptron can perfectly classify $P = αN$ random patterns by optimally selecting $M = ρN$ variables out of $N$ variables. While the Cover--Gardner theory establishes that a random subset of $ρN$ dimensions can separate $αN$ random patterns if and only if $α&lt; 2ρ$, we demonstrate that optimal variable selection can surpass this bound by developing a method, based on the replica method from statistical mechanics, for enumerating the combinations of variables that enable perfect pattern classification. This not only provides a quantitative criterion for distinguishing true structure in the data from spurious regularities, but also yields the storage capacity of associative memory models with sparse asymmetric couplings.",
      "summary": "A central challenge in machine learning is to distinguish genuine structure from chance correlations in high-dimensional data. In this work, we address this issue for the perceptron, a foundational model of neural computation. Specifically, we investigate the relationship between the pattern load $α$ and the variable selection ratio $ρ$ for which a simple perceptron can perfectly classify $P = αN$ random patterns by optimally selecting $M = ρN$ variables out of $N$ variables. While the Cover--Ga",
      "publishedAt": "2025-12-01T16:44:57.000Z",
      "author": "Yingying Xu",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 6.556641229917585,
      "ingestedAt": "2025-12-02T14:44:03.166Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f78",
      "title": "Excluding a Forest Induced Minor",
      "url": "https://arxiv.org/abs/2512.01857v1",
      "content": "In the first paper of the Graph Minors series [JCTB '83], Robertson and Seymour proved the Forest Minor theorem: the $H$-minor-free graphs have bounded pathwidth if and only if $H$ is a forest. In recent years, considerable effort has been devoted to understanding the unavoidable induced substructures of graphs with large pathwidth or large treewidth. In this paper, we give an induced counterpart of the Forest Minor theorem: for any $t \\geqslant 2$, the $K_{t,t}$-subgraph-free $H$-induced-minor-free graphs have bounded pathwidth if and only if $H$ belongs to a class $\\mathcal F$ of forests, which we describe as the induced minors of two (very similar) infinite parameterized families. This constitutes a significant step toward classifying the graphs $H$ for which every weakly sparse $H$-induced-minor-free class has bounded treewidth. Our work builds on the theory of constellations developed in the Induced Subgraphs and Tree Decompositions series.",
      "summary": "In the first paper of the Graph Minors series [JCTB '83], Robertson and Seymour proved the Forest Minor theorem: the $H$-minor-free graphs have bounded pathwidth if and only if $H$ is a forest. In recent years, considerable effort has been devoted to understanding the unavoidable induced substructures of graphs with large pathwidth or large treewidth. In this paper, we give an induced counterpart of the Forest Minor theorem: for any $t \\geqslant 2$, the $K_{t,t}$-subgraph-free $H$-induced-minor-",
      "publishedAt": "2025-12-01T16:43:37.000Z",
      "author": "Édouard Bonnet",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.151305974543333,
      "ingestedAt": "2025-12-02T14:44:03.166Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f7a",
      "title": "BHRAM-IL: A Benchmark for Hallucination Recognition and Assessment in Multiple Indian Languages",
      "url": "https://arxiv.org/abs/2512.01852v1",
      "content": "Large language models (LLMs) are increasingly deployed in multilingual applications but often generate plausible yet incorrect or misleading outputs, known as hallucinations. While hallucination detection has been studied extensively in English, under-resourced Indian languages remain largely unexplored. We present BHRAM-IL, a benchmark for hallucination recognition and assessment in multiple Indian languages, covering Hindi, Gujarati, Marathi, Odia, along with English. The benchmark comprises 36,047 curated questions across nine categories spanning factual, numerical, reasoning, and linguistic tasks. We evaluate 14 state-of-the-art multilingual LLMs on a benchmark subset of 10,265 questions, analyzing cross-lingual and factual hallucinations across languages, models, scales, categories, and domains using category-specific metrics normalized to (0,1) range. Aggregation over all categories and models yields a primary score of 0.23 and a language-corrected fuzzy score of 0.385, demonstrating the usefulness of BHRAM-IL for hallucination-focused evaluation. The dataset, and the code for generation and evaluation are available on GitHub (https://github.com/sambhashana/BHRAM-IL/) and HuggingFace (https://huggingface.co/datasets/sambhashana/BHRAM-IL/) to support future research in multilingual hallucination detection and mitigation.",
      "summary": "Large language models (LLMs) are increasingly deployed in multilingual applications but often generate plausible yet incorrect or misleading outputs, known as hallucinations. While hallucination detection has been studied extensively in English, under-resourced Indian languages remain largely unexplored. We present BHRAM-IL, a benchmark for hallucination recognition and assessment in multiple Indian languages, covering Hindi, Gujarati, Marathi, Odia, along with English. The benchmark comprises 3",
      "publishedAt": "2025-12-01T16:37:34.000Z",
      "author": "Hrishikesh Terdalkar",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 6.086080358856273,
      "ingestedAt": "2025-12-02T14:44:03.166Z",
      "tags": [
        "code_review",
        "observability"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f7e",
      "title": "Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability",
      "url": "https://arxiv.org/abs/2512.01848v1",
      "content": "Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. While intuitive, we find that SFT produces inconsistent safety improvements, degrades reasoning ability, and generalizes poorly across model families. These limitations suggest that purely supervised approaches are insufficient for robust safety alignment in LRMs. To address this, we investigate reinforcement learning (RL) as a complementary optimization framework for LRM safety training. Unlike SFT, RL directly optimizes model policies with reward feedback, enabling more adaptive and stable alignment. Extensive experiments across multiple model families and benchmarks show that RL achieves stronger and more consistent safety gains while maintaining reasoning competence. Further analysis of reflection dynamics and token-level entropy reveals that RL suppresses unsafe exploratory reasoning while preserving reflective depth, leading to safer and more reliable reasoning processes.",
      "summary": "Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. W",
      "publishedAt": "2025-12-01T16:35:34.000Z",
      "author": "Jinghan Jia",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 5.149249440069477,
      "ingestedAt": "2025-12-02T14:44:03.166Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f80",
      "title": "Mitigating Gender Bias in Depression Detection via Counterfactual Inference",
      "url": "https://arxiv.org/abs/2512.01834v1",
      "content": "Audio-based depression detection models have demonstrated promising performance but often suffer from gender bias due to imbalanced training data. Epidemiological statistics show a higher prevalence of depression in females, leading models to learn spurious correlations between gender and depression. Consequently, models tend to over-diagnose female patients while underperforming on male patients, raising significant fairness concerns. To address this, we propose a novel Counterfactual Debiasing Framework grounded in causal inference. We construct a causal graph to model the decision-making process and identify gender bias as the direct causal effect of gender on the prediction. During inference, we employ counterfactual inference to estimate and subtract this direct effect, ensuring the model relies primarily on authentic acoustic pathological features. Extensive experiments on the DAIC-WOZ dataset using two advanced acoustic backbones demonstrate that our framework not only significantly reduces gender bias but also improves overall detection performance compared to existing debiasing strategies.",
      "summary": "Audio-based depression detection models have demonstrated promising performance but often suffer from gender bias due to imbalanced training data. Epidemiological statistics show a higher prevalence of depression in females, leading models to learn spurious correlations between gender and depression. Consequently, models tend to over-diagnose female patients while underperforming on male patients, raising significant fairness concerns. To address this, we propose a novel Counterfactual Debiasing",
      "publishedAt": "2025-12-01T16:14:20.000Z",
      "author": "Mingxuan Hu",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.143828895845059,
      "ingestedAt": "2025-12-02T14:44:03.166Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f82",
      "title": "Deconstructing Generative Diversity: An Information Bottleneck Analysis of Discrete Latent Generative Models",
      "url": "https://arxiv.org/abs/2512.01831v1",
      "content": "Generative diversity varies significantly across discrete latent generative models such as AR, MIM, and Diffusion. We propose a diagnostic framework, grounded in Information Bottleneck (IB) theory, to analyze the underlying strategies resolving this behavior. The framework models generation as a conflict between a 'Compression Pressure' - a drive to minimize overall codebook entropy - and a 'Diversity Pressure' - a drive to maximize conditional entropy given an input. We further decompose this diversity into two primary sources: 'Path Diversity', representing the choice of high-level generative strategies, and 'Execution Diversity', the randomness in executing a chosen strategy. To make this decomposition operational, we introduce three zero-shot, inference-time interventions that directly perturb the latent generative process and reveal how models allocate and express diversity. Application of this probe-based framework to representative AR, MIM, and Diffusion systems reveals three distinct strategies: \"Diversity-Prioritized\" (MIM), \"Compression-Prioritized\" (AR), and \"Decoupled\" (Diffusion). Our analysis provides a principled explanation for their behavioral differences and informs a novel inference-time diversity enhancement technique.",
      "summary": "Generative diversity varies significantly across discrete latent generative models such as AR, MIM, and Diffusion. We propose a diagnostic framework, grounded in Information Bottleneck (IB) theory, to analyze the underlying strategies resolving this behavior. The framework models generation as a conflict between a 'Compression Pressure' - a drive to minimize overall codebook entropy - and a 'Diversity Pressure' - a drive to maximize conditional entropy given an input. We further decompose this d",
      "publishedAt": "2025-12-01T16:13:23.000Z",
      "author": "Yudi Wu",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.143586504573096,
      "ingestedAt": "2025-12-02T14:44:03.167Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f84",
      "title": "InnoGym: Benchmarking the Innovation Potential of AI Agents",
      "url": "https://arxiv.org/abs/2512.01822v1",
      "content": "LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.",
      "summary": "LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two comple",
      "publishedAt": "2025-12-01T16:03:04.000Z",
      "author": "Jintian Zhang",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 12.618707732152599,
      "ingestedAt": "2025-12-02T14:44:03.167Z",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "observability",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f88",
      "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights",
      "url": "https://arxiv.org/abs/2512.01816v1",
      "content": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.",
      "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose E",
      "publishedAt": "2025-12-01T15:52:31.000Z",
      "author": "Juanxi Tian",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 5.138265374693971,
      "ingestedAt": "2025-12-02T14:44:03.167Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f8a",
      "title": "JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford",
      "url": "https://arxiv.org/abs/2512.01802v1",
      "content": "We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential.",
      "summary": "We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million e",
      "publishedAt": "2025-12-01T15:35:53.000Z",
      "author": "Xin Wang",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 6.534217090981879,
      "ingestedAt": "2025-12-02T14:44:03.167Z",
      "tags": [
        "code_review",
        "retrieval"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f8c",
      "title": "H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons",
      "url": "https://arxiv.org/abs/2512.01797v1",
      "content": "Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than $0.1\\%$ of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.",
      "summary": "Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and ori",
      "publishedAt": "2025-12-01T15:32:14.000Z",
      "author": "Cheng Gao",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 6.999679465386518,
      "ingestedAt": "2025-12-02T14:44:03.167Z",
      "tags": [
        "code_review",
        "ide",
        "governance"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f8f",
      "title": "Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation Systems",
      "url": "https://arxiv.org/abs/2512.01786v1",
      "content": "As Large Language Models (LLMs) become integrated into high-stakes domains, there is a growing need for evaluation methods that are both scalable for real-time deployment and reliable for critical decision-making. While human evaluation is reliable, it is slow and costly. Single LLM judges are biased, and static juries lack adaptability. To overcome these limitations, we propose LLM Jury-on-Demand - a dynamic, learning-based framework for scalable and context-aware evaluation. Our method trains a set of reliability predictors to assess when LLM judges will agree with human experts, leveraging token distributions, embeddings, and structural input features. This enables a fully adaptive evaluation where, for each data point, an optimal jury of the most reliable judges is dynamically selected, and their scores are aggregated using their reliability as weights. Experiments on summarization and RAG benchmarks show that our dynamic jury system achieves significantly higher correlation with human judgment than both single-judge and static-jury baselines. These results highlight the promise of adaptive, learning-based juries for building scalable, more reliable and trustworthy evaluation systems for modern LLMs in high-stakes domains.",
      "summary": "As Large Language Models (LLMs) become integrated into high-stakes domains, there is a growing need for evaluation methods that are both scalable for real-time deployment and reliable for critical decision-making. While human evaluation is reliable, it is slow and costly. Single LLM judges are biased, and static juries lack adaptability. To overcome these limitations, we propose LLM Jury-on-Demand - a dynamic, learning-based framework for scalable and context-aware evaluation. Our method trains ",
      "publishedAt": "2025-12-01T15:26:20.000Z",
      "author": "Xiaochuan Li",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 6.531122497957546,
      "ingestedAt": "2025-12-02T14:44:03.167Z",
      "tags": [
        "code_review",
        "retrieval"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f91",
      "title": "Dual Randomized Smoothing: Beyond Global Noise Variance",
      "url": "https://arxiv.org/abs/2512.01782v1",
      "content": "Randomized Smoothing (RS) is a prominent technique for certifying the robustness of neural networks against adversarial perturbations. With RS, achieving high accuracy at small radii requires a small noise variance, while achieving high accuracy at large radii requires a large noise variance. However, the global noise variance used in the standard RS formulation leads to a fundamental limitation: there exists no global noise variance that simultaneously achieves strong performance at both small and large radii. To break through the global variance limitation, we propose a dual RS framework which enables input-dependent noise variances. To achieve that, we first prove that RS remains valid with input-dependent noise variances, provided the variance is locally constant around each input. Building on this result, we introduce two components which form our dual RS framework: (i) a variance estimator first predicts an optimal noise variance for each input, (ii) this estimated variance is then used by a standard RS classifier. The variance estimator is independently smoothed via RS to ensure local constancy, enabling flexible design. We also introduce training strategies to iteratively optimize the two components. Extensive experiments on CIFAR-10 show that our dual RS method provides strong performance for both small and large radii-unattainable with global noise variance-while incurring only a 60% computational overhead at inference. Moreover, it consistently outperforms prior input-dependent noise approaches across most radii, with particularly large gains at radii 0.5, 0.75, and 1.0, achieving relative improvements of 19%, 24%, and 21%, respectively. On ImageNet, dual RS remains effective across all radii. Additionally, the dual RS framework naturally provides a routing perspective for certified robustness, improving the accuracy-robustness trade-off with off-the-shelf expert RS models.",
      "summary": "Randomized Smoothing (RS) is a prominent technique for certifying the robustness of neural networks against adversarial perturbations. With RS, achieving high accuracy at small radii requires a small noise variance, while achieving high accuracy at large radii requires a large noise variance. However, the global noise variance used in the standard RS formulation leads to a fundamental limitation: there exists no global noise variance that simultaneously achieves strong performance at both small ",
      "publishedAt": "2025-12-01T15:23:00.000Z",
      "author": "Chenhao Sun",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 6.063611084015763,
      "ingestedAt": "2025-12-02T14:44:03.167Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f93",
      "title": "Secure Over-the-Air Computation Against Multiple Eavesdroppers using Correlated Artificial Noise",
      "url": "https://arxiv.org/abs/2512.01778v1",
      "content": "In the era of the Internet of Things and massive connectivity, many engineering applications, such as sensor fusion and federated edge learning, rely on efficient data aggregation from geographically distributed users over wireless networks. Over-the-air computation shows promising potential for enhancing resource efficiency and scalability in such scenarios by leveraging the superposition property of wireless channels. However, due to the use of uncoded transmission with linear mapping, it also suffers from security vulnerabilities that must be dealt with to allow widespread adoption. In this work, we consider a scenario where multiple cooperating eavesdroppers attempt to infer information about the aggregation result. We derive the optimal joint estimator for the eavesdroppers and provide bounds on the achievable estimation accuracy for both the eavesdroppers and the intended receiver. We show that significant inherent security exists against individual eavesdroppers due to channel misalignment. However, the security level is greatly compromised when the eavesdroppers can cooperate, motivating the need for deliberate security measures. A common measure is to add carefully calibrated perturbation signals (artificial noise) prior to data transmission to improve the security level. To this end, we propose a zero-forced artificial noise design that achieves a high level of security against cooperative eavesdroppers without compromising the aggregation accuracy.",
      "summary": "In the era of the Internet of Things and massive connectivity, many engineering applications, such as sensor fusion and federated edge learning, rely on efficient data aggregation from geographically distributed users over wireless networks. Over-the-air computation shows promising potential for enhancing resource efficiency and scalability in such scenarios by leveraging the superposition property of wireless channels. However, due to the use of uncoded transmission with linear mapping, it also",
      "publishedAt": "2025-12-01T15:19:47.000Z",
      "author": "David Nordlund",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "security_incident",
      "score": 9.327144108216517,
      "ingestedAt": "2025-12-02T14:44:03.167Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f96",
      "title": "VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis",
      "url": "https://arxiv.org/abs/2512.01769v1",
      "content": "Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is done manually with a human in the loop, which is error-prone and labor-intensive, or through custom algorithms designed for specific video types or situations. These algorithms are not general-purpose and require a new algorithm/software for each new situation or video from a new domain. \n  This report proposes a general-purpose VSA framework that overcomes the above limitations. Video contents are extracted once using state-of-the-art Video Content Extraction technologies. They are represented using two alternative models -- the extended relational model (R++) and graph models. When represented using R++, the extracted contents can be used as data streams, enabling Continuous Query Processing via the proposed Continuous Query Language for Video Analysis. The graph models complement this by enabling the detection of situations that are difficult or impossible to detect using the relational model alone. Existing graph algorithms and newly developed algorithms support a wide variety of situation detection. To support domain independence, primitive situation variants across domains are identified and expressed as parameterized templates. Extensive experiments were conducted across several interesting situations from three domains -- AL, CM, and SL-- to evaluate the accuracy, efficiency, and robustness of the proposed approach using a dataset of videos of varying lengths from these domains.",
      "summary": "Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is don",
      "publishedAt": "2025-12-01T15:09:46.000Z",
      "author": "Hafsa Billah",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 6.059632142722609,
      "ingestedAt": "2025-12-02T14:44:03.167Z",
      "tags": [
        "code_review",
        "ide",
        "observability"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f98",
      "title": "Weight Space Representation Learning with Neural Fields",
      "url": "https://arxiv.org/abs/2512.01759v1",
      "content": "In this work, we investigate the potential of weights to serve as effective representations, focusing on neural fields. Our key insight is that constraining the optimization space through a pre-trained base model and low-rank adaptation (LoRA) can induce structure in weight space. Across reconstruction, generation, and analysis tasks on 2D and 3D data, we find that multiplicative LoRA weights achieve high representation quality while exhibiting distinctiveness and semantic structure. When used with latent diffusion models, multiplicative LoRA weights enable higher-quality generation than existing weight-space methods.",
      "summary": "In this work, we investigate the potential of weights to serve as effective representations, focusing on neural fields. Our key insight is that constraining the optimization space through a pre-trained base model and low-rank adaptation (LoRA) can induce structure in weight space. Across reconstruction, generation, and analysis tasks on 2D and 3D data, we find that multiplicative LoRA weights achieve high representation quality while exhibiting distinctiveness and semantic structure. When used w",
      "publishedAt": "2025-12-01T15:05:01.000Z",
      "author": "Zhuoqian Yang",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.194141622055552,
      "ingestedAt": "2025-12-02T14:44:03.167Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f9a",
      "title": "Answering Constraint Path Queries over Graphs",
      "url": "https://arxiv.org/abs/2512.01733v1",
      "content": "Constraints are powerful declarative constructs that allow users to \n  conveniently restrict variable values that potentially range over an \n  infinite domain. In this paper, we propose a constraint path query language \n  over property graphs, \n  which extends Regular Path Queries (RPQs) with SMT constraints on data \n  attributes in the form of equality constraints and Linear \n  Real Arithmetic (LRA) constraints. We provide efficient algorithms \n  for evaluating such path queries over property graphs, which exploits \n  optimization of macro-states (among others, using theory-specific \n  techniques). \n  In particular, we demonstrate how such an algorithm may effectively utilize \n  highly optimized SMT solvers for resolving such constraints over paths. \n  We implement our algorithm in MillenniumDB, an open-source graph engine \n  supporting property graph queries and GQL. Our extensive empirical \n  evaluation in a real-world setting demonstrates the viability of our \n  approach.",
      "summary": "Constraints are powerful declarative constructs that allow users to \n  conveniently restrict variable values that potentially range over an \n  infinite domain. In this paper, we propose a constraint path query language \n  over property graphs, \n  which extends Regular Path Queries (RPQs) with SMT constraints on data \n  attributes in the form of equality constraints and Linear \n  Real Arithmetic (LRA) constraints. We provide efficient algorithms \n  for evaluating such path queries over property g",
      "publishedAt": "2025-12-01T14:40:35.000Z",
      "author": "Heyang Li",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "security_incident",
      "score": 7.912671766703619,
      "ingestedAt": "2025-12-02T14:44:03.168Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f9c",
      "title": "Reasoning About the Unsaid: Misinformation Detection with Omission-Aware Graph Inference",
      "url": "https://arxiv.org/abs/2512.01728v1",
      "content": "This paper investigates the detection of misinformation, which deceives readers by explicitly fabricating misleading content or implicitly omitting important information necessary for informed judgment. While the former has been extensively studied, omission-based deception remains largely overlooked, even though it can subtly guide readers toward false conclusions under the illusion of completeness. To pioneer in this direction, this paper presents OmiGraph, the first omission-aware framework for misinformation detection. Specifically, OmiGraph constructs an omission-aware graph for the target news by utilizing a contextual environment that captures complementary perspectives of the same event, thereby surfacing potentially omitted contents. Based on this graph, omission-oriented relation modeling is then proposed to identify the internal contextual dependencies, as well as the dynamic omission intents, formulating a comprehensive omission relation representation. Finally, to extract omission patterns for detection, OmiGraph introduces omission-aware message-passing and aggregation that establishes holistic deception perception by integrating the omission contents and relations. Experiments show that, by considering the omission perspective, our approach attains remarkable performance, achieving average improvements of +5.4% F1 and +5.3% ACC on two large-scale benchmarks.",
      "summary": "This paper investigates the detection of misinformation, which deceives readers by explicitly fabricating misleading content or implicitly omitting important information necessary for informed judgment. While the former has been extensively studied, omission-based deception remains largely overlooked, even though it can subtly guide readers toward false conclusions under the illusion of completeness. To pioneer in this direction, this paper presents OmiGraph, the first omission-aware framework f",
      "publishedAt": "2025-12-01T14:37:00.000Z",
      "author": "Zhengjia Wang",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 7.445896899904855,
      "ingestedAt": "2025-12-02T14:44:03.168Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f9d",
      "title": "Beware of Reasoning Overconfidence: Pitfalls in the Reasoning Process for Multi-solution Tasks",
      "url": "https://arxiv.org/abs/2512.01725v1",
      "content": "Large Language Models (LLMs) excel in reasoning tasks requiring a single correct answer, but they perform poorly in multi-solution tasks that require generating comprehensive and diverse answers. We attribute this limitation to \\textbf{reasoning overconfidence}: a tendency to express undue certainty in an incomplete solution set. To examine the effect, we introduce \\textit{MuSoBench}, a benchmark of multi-solution problems. Experiments show that the conventional short chain-of-thought (Short-CoT) prompting paradigm exhibits pronounced overconfidence, whereas the emerging long chain-of-thought (Long-CoT) approach mitigates it through iterative exploration and self-reflection. We further characterise observable behaviours and influential factors. To probe the underlying cause, we propose the \\textbf{cognitive-rigidity hypothesis}, which posits that overconfidence arises when the reasoning process prematurely converges on a narrow set of thought paths. An attention-entropy analysis offers preliminary support for this view. These findings provide tools for assessing the completeness of LLM reasoning and highlight the need to move evaluation beyond single-answer accuracy toward comprehensive exploration.",
      "summary": "Large Language Models (LLMs) excel in reasoning tasks requiring a single correct answer, but they perform poorly in multi-solution tasks that require generating comprehensive and diverse answers. We attribute this limitation to \\textbf{reasoning overconfidence}: a tendency to express undue certainty in an incomplete solution set. To examine the effect, we introduce \\textit{MuSoBench}, a benchmark of multi-solution problems. Experiments show that the conventional short chain-of-thought (Short-CoT",
      "publishedAt": "2025-12-01T14:35:06.000Z",
      "author": "Jiannan Guan",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 6.97987048756557,
      "ingestedAt": "2025-12-02T14:44:03.168Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f9e",
      "title": "Probabilistic Neuro-Symbolic Reasoning for Sparse Historical Data: A Framework Integrating Bayesian Inference, Causal Models, and Game-Theoretic Allocation",
      "url": "https://arxiv.org/abs/2512.01723v1",
      "content": "Modeling historical events poses fundamental challenges for machine learning: extreme data scarcity (N &lt;&lt; 100), heterogeneous and noisy measurements, missing counterfactuals, and the requirement for human interpretable explanations. We present HistoricalML, a probabilistic neuro-symbolic framework that addresses these challenges through principled integration of (1) Bayesian uncertainty quantification to separate epistemic from aleatoric uncertainty, (2) structural causal models for counterfactual reasoning under confounding, (3) cooperative game theory (Shapley values) for fair allocation modeling, and (4) attention based neural architectures for context dependent factor weighting. We provide theoretical analysis showing that our approach achieves consistent estimation in the sparse data regime when strong priors from domain knowledge are available, and that Shapley based allocation satisfies axiomatic fairness guarantees that pure regression approaches cannot provide. We instantiate the framework on two historical case studies: the 19th century partition of Africa (N = 7 colonial powers) and the Second Punic War (N = 2 factions). Our model identifies Germany's +107.9 percent discrepancy as a quantifiable structural tension preceding World War I, with tension factor 36.43 and 0.79 naval arms race correlation. For the Punic Wars, Monte Carlo battle simulations achieve a 57.3 percent win probability for Carthage at Cannae and 57.8 percent for Rome at Zama, aligning with historical outcomes. Counterfactual analysis reveals that Carthaginian political support (support score 6.4 vs Napoleon's 7.1), rather than military capability, was the decisive factor.",
      "summary": "Modeling historical events poses fundamental challenges for machine learning: extreme data scarcity (N &lt;&lt; 100), heterogeneous and noisy measurements, missing counterfactuals, and the requirement for human interpretable explanations. We present HistoricalML, a probabilistic neuro-symbolic framework that addresses these challenges through principled integration of (1) Bayesian uncertainty quantification to separate epistemic from aleatoric uncertainty, (2) structural causal models for counte",
      "publishedAt": "2025-12-01T14:35:04.000Z",
      "author": "Saba Kublashvili",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 7.910506806355157,
      "ingestedAt": "2025-12-02T14:44:03.168Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0f9f",
      "title": "Self-Supervised Borrowing Detection on Multilingual Wordlists",
      "url": "https://arxiv.org/abs/2512.01713v1",
      "content": "This paper presents a fully self-supervised approach to borrowing detection in multilingual wordlists. The method combines two sources of information: PMI similarities based on a global correspondence model and a lightweight contrastive component trained on phonetic feature vectors. It further includes an automatic procedure for selecting decision thresholds without requiring labeled data. Experiments on benchmark datasets show that PMI alone already improves over existing string similarity measures such as NED and SCA, and that the combined similarity performs on par with or better than supervised baselines. An ablation study highlights the importance of character encoding, temperature settings and augmentation strategies. The approach scales to datasets of different sizes, works without manual supervision and is provided with a command-line tool that allows researchers to conduct their own studies.",
      "summary": "This paper presents a fully self-supervised approach to borrowing detection in multilingual wordlists. The method combines two sources of information: PMI similarities based on a global correspondence model and a lightweight contrastive component trained on phonetic feature vectors. It further includes an automatic procedure for selecting decision thresholds without requiring labeled data. Experiments on benchmark datasets show that PMI alone already improves over existing string similarity meas",
      "publishedAt": "2025-12-01T14:20:03.000Z",
      "author": "Tim Wientzek",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 7.4396392158200735,
      "ingestedAt": "2025-12-02T14:44:03.168Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09b26e73",
      "title": "Train a Custom Z‑Image Turbo LoRA with the Ostris AI Toolkit (RunPod Edition)",
      "url": "https://dev.to/promptingpixels/train-a-custom-z-image-turbo-lora-with-the-ostris-ai-toolkit-runpod-edition-1n4h",
      "content": "<h2> \n   \n   \n  What we’re building \n</h2> \n \n<p>A complete, reproducible workflow to train a Z‑Image Turbo LoRA with the <a href=\"https://github.com/ostris/ai-toolkit\">Ostris AI Toolkit</a>, running on a rented GPU (RunPod). We’ll go from blank slate to a downloadable .safetensors LoRA, then load it into a downstream workflow (e.g., ComfyUI) to test the results with a trigger token.</p> \n \n<p>You’ll learn:</p> \n \n<ul> \n<li>How to spin up the right environment on RunPod</li> \n<li>How to assemble and configure a dataset for concept training</li> \n<li>How to pick the right model, adapter, and sample prompts for monitoring</li> \n<li>How to kick off and observe training progress</li> \n<li>How to export and use your LoRA in your own pipeline</li> \n</ul> \n \n<blockquote> \n<p>💡 Pro tip: Z‑Image Turbo is fast and surprisingly VRAM‑friendly. Even before the base model drops, the distilled weights already make for practical LoRA experimentation.</p> \n</blockquote> \n \n<p><a href=\"https://youtu.be/ePybOjM2sbE\">Check out the accompanying video on YouTube</a>.</p> \n \n \n \n \n<h2> \n   \n   \n  TL;DR (Quick Reference) \n</h2> \n \n<ol> \n<li>Start a RunPod instance using the “Ostris AI Toolkit” template.</li> \n<li>Create a dataset (8–20 images is a good starting point). Optionally add captions.</li> \n<li>New job → select Z‑Image Turbo + LoRA target.</li> \n<li>Set a unique trigger token (e.g., myuniqueconcept) and configure sample prompts.</li> \n<li>Run ~3,000 steps to start; expect ~1 hour on a high-end GPU (e.g., RTX 5090).</li> \n<li>Download the resulting LoRA (.safetensors) from the job’s Checkpoints.</li> \n<li>Load the LoRA into your favorite workflow (ComfyUI, etc.) and prompt with the trigger.</li> \n</ol> \n \n \n \n \n<h2> \n   \n   \n  Step 1 — Spin up the GPU workspace \n</h2> \n \n<p>On RunPod, search for and launch the Ostris AI Toolkit template. Keep disk size generous (datasets and samples eat space as you iterate).</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fn0syab4j0ayngdqyttha.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fn0syab4j0ayngdqyttha.png\" alt=\"RunPod 'Deploy a Pod' UI screenshot with red arrows: select 'AI Toolkit - ostris - ui - official' template, edit/change template, adjust disk size, and press purple 'Deploy On-Demand' button; shows On-Demand $0.89/hr and RTX 5090 pod summary (200 GB disk).\" width=\"800\" height=\"520\"></a></p> \n \n<blockquote> \n<p>🧪 Debug tip: If you see 0% GPU utilization during training, your job likely didn’t start or is stuck on CPU. Check the Training Queue and logs.</p> \n</blockquote> \n \n \n \n \n<h2> \n   \n   \n  Step 2 — Assemble a tiny but consistent dataset \n</h2> \n \n<p>Hop into Datasets → New Dataset. Name it something meaningful; I like a short handle that matches my future trigger token.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft7lk4tgf89ydqo0f5ldg.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft7lk4tgf89ydqo0f5ldg.png\" alt=\"Dark-mode web UI 'OSTRIS AI-TOOLKIT' showing Datasets page with left sidebar (Dashboard, New Job, Training Queue, Datasets highlighted, Settings), main area saying 'Empty' and 'Refresh', and red annotated arrows labeled '1. Navigate to\" width=\"800\" height=\"520\"></a></p> \n \n<p>Upload 8–20 representative images. Keep variety in poses and contexts, but a consistent subject identity.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fcv4vs05yeiidcoo955kb.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fcv4vs05yeiidcoo955kb.png\" alt=\"OSTRIS AI-Toolkit dataset 'teach3r' screenshot: 3x3 grid of teacher thumbnails with overlays and trash icons, left nav and Add Images button.\" width=\"800\" height=\"520\"></a></p> \n \n<p>Captions are optional. If you add them, keep the phrasing consistent (e.g., always include your trigger token).</p> \n \n<blockquote> \n<p>🧭 Guideline: Resolution 1024×1024 is a solid baseline with Z‑Image Turbo. If your source images vary wildly, consider pre-cropping/centering the subject.</p> \n</blockquote> \n \n \n \n \n<h2> \n   \n   \n  Step 3 — Configure the training job like a pro \n</h2> \n \n<p>Head to New Job:</p> \n \n<ul> \n<li>Training name: something short you’ll recognize later</li> \n<li>Trigger token: a unique string (avoid real words; e.g., xqteachu, zimg_concept01)</li> \n<li>Architecture: Z‑Image Turbo (LoRA target)</li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fu57lrzo6tjsx5ihrmw9f.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fu57lrzo6tjsx5ihrmw9f.png\" alt=\"OSTRIS AI-TOOLKIT 'New Training Job' UI screenshot; red arrows highlight Training Name/Trigger 'teach3r' and Model Architecture dropdown set to 'Z-Image Turbo'. Fields show GPU #0, Steps 3000, Target LoRA.\" width=\"800\" height=\"520\"></a></p> \n \n<p>You’ll see a training adapter path. There’s also a newer “v2” adapter rolling out. If it’s available in your build, you can switch the file name from v1 to v2 to try it out.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbu1t7mroo0yy47hjlwsu.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbu1t7mroo0yy47hjlwsu.png\" alt=\"Screenshot of a tweet about a v2 z-image-turbo training adapter above a split image: left shows model settings selecting Z-Image Turbo and training_adapter_v2.safetensors with Low VRAM on; right shows config lines highlighting training_adapter_v1.safetensors and training_adapter_v2.safetensors\" width=\"800\" height=\"864\"></a></p> \n \n<p>Attach your dataset and set preview sampling. Samples during training are clutch—they confirm your LoRA is “taking.”</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F05efx2pxj4rtgj6de7n5.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F05efx2pxj4rtgj6de7n5.png\" alt=\"OSTRIS New Training Job UI: Dataset 1 panel, red arrow 'Select your Dataset...', target teach3r, 1024x1024 selected, sample settings shown, prompt contains 'bomb'.\" width=\"800\" height=\"520\"></a></p> \n \n<p>For samples, create two contrasting prompts so you can inspect generalization:</p> \n \n<ul> \n<li>“{trigger}, cinematic portrait, soft light, 85mm, bokeh”</li> \n<li>“{trigger}, full body action scene, dynamic pose, outdoor, golden hour”</li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgdptz72qnwhc1t3tf73s.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgdptz72qnwhc1t3tf73s.png\" alt=\"Ostris AI-TOOLKIT New Training Job UI showing SAMPLE settings (Sample Every 250, Width/Height 1024, Seed 42), two sample prompts with seeds and LoRA scale, and a red arrow and large red note: 'Recommended to change the prompts to test LoRA outputs during training'.\" width=\"800\" height=\"520\"></a></p> \n \n<blockquote> \n<p>💡 Pro tip: Keep the LoRA strength modest when previewing (e.g., 0.7–0.9). Too high can overcook and hide issues until it’s too late.</p> \n</blockquote> \n \n<p>If your GPU is tight on VRAM, turn on the Low VRAM option in the model panel.</p> \n \n \n \n \n<h2> \n   \n   \n  Step 4 — Start the job and watch it like a hawk \n</h2> \n \n<p>Create Job → Training Queue → Play → Start.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fy0veymeeiiwp70sqqvfn.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fy0veymeeiiwp70sqqvfn.png\" alt=\"Dark OSTRIS AI-Toolkit view for 'teach3r' showing progress and GPU/CPU stats; red arrow and text 'Click the play button to start training' point to the play icon top-right.\" width=\"800\" height=\"520\"></a></p> \n \n<p>On a 5090, ~3k steps typically finishes around the 1‑hour mark (defaults). If samples are configured every 250 steps, you’ll see the subject “phase in” across iterations.</p> \n \n<blockquote> \n<p>🧪 Debug tip: If loss flatlines suspiciously early or samples look unrelated to your subject after ~1k steps, your trigger might not be present in the sample prompts, or your dataset is too small/too noisy.</p> \n</blockquote> \n \n \n \n \n<h2> \n   \n   \n  Step 5 — Evaluate progress and export the LoRA \n</h2> \n \n<p>Open the Samples tab to review the training trajectory. You’ll usually notice early frames not obeying the trigger, then progressively adapting to your subject.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Flx78lhhbyb0gvrfjrgj2.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Flx78lhhbyb0gvrfjrgj2.png\" alt=\"Screenshot of OSTRIS AI-TOOLKIT 'Job: ma1a' Samples tab showing four illustrated teacher-classroom panels, a hand cursor over the teacher, and left navigation menu\" width=\"800\" height=\"507\"></a></p> \n \n<p>When it’s done, jump to the job Overview → Checkpoints. Download the newest .safetensors file—this is your LoRA.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F74dsa4xju1grbxg0zfc8.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F74dsa4xju1grbxg0zfc8.png\" alt=\"OSTRIS AI-TOOLKIT job 'ma1a' UI showing 'Training completed' banner, terminal logs and progress bar, right sidebar with CPU/GPU stats and a checkpoints list; red annotation arrow points to the ma1a.safetensors download icon and cursor.\" width=\"800\" height=\"426\"></a></p> \n \n<blockquote> \n<p>📦 Housekeeping: Save the training config alongside the .safetensors so you can reproduce tweaks later (steps, adapter version, dataset size, etc.).</p> \n</blockquote> \n \n \n \n \n<h2> \n   \n   \n  Step 6 — Try the LoRA in your workflow \n</h2> \n \n<p>I like to validate in ComfyUI with a simple graph: base Z‑Image Turbo → CLIP encode prompt (including trigger) → sampler → VAE → preview.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvbro34pgw6e2iu5opreu.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvbro34pgw6e2iu5opreu.png\" alt=\"ComfyUI node graph showing Load models and CLIP Text Encode nodes with prompt 'mala, school teacher shooting a basketball, smiling', connected sampler and VAE nodes, and a right-side cartoon image preview of a woman shooting a basketball on an outdoor court\" width=\"800\" height=\"520\"></a></p> \n \n<p>Example prompt:</p> \n \n<ul> \n<li>“myuniqueconcept, cheerful portrait, natural light, editorial style”</li> \n</ul> \n \n<p>If the result skews too strongly to the subject or artifacts creep in, lower the LoRA strength a bit and re‑sample.</p> \n \n<p>Final output from one of my runs:</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fctxkzfuwmc78ex2k7717.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fctxkzfuwmc78ex2k7717.png\" alt=\"Smiling girl in a yellow cardigan and blue jeans tossing a basketball toward a hoop on an outdoor court with trees and a building in the background\" width=\"768\" height=\"1024\"></a></p>",
      "summary": " \n   \n   \n  What we’re building \n \n \nA complete, reproducible workflow to train a Z‑Image Turbo LoRA with the Ostris AI Toolkit, running on a rented GPU (RunPod). We’ll go from blank slate to a downloadable .safetensors LoRA, then load it into a downstream workflow (e.g., ComfyUI) to test the results with a trigger token. \n \nYou’ll learn: \n \n \nHow to spin up the right environment on RunPod \nHow to assemble and configure a dataset for concept training \nHow to pick the right model, adapter, and sa",
      "publishedAt": "2025-12-02T14:34:26.000Z",
      "author": "Prompting Pixels",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 9.495468200545103,
      "ingestedAt": "2025-12-02T14:44:03.154Z",
      "tags": [
        "code_review",
        "ide",
        "observability",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09b26e75",
      "title": "Design Patterns for Data Engineers: Cleaner ETL with the Builder Pattern.",
      "url": "https://dev.to/cristianbergamo/design-patterns-for-data-engineers-cleaner-etl-with-the-builder-pattern-4bi7",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fdyv5buvt1pcvx9w1952i.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>In my job, I often end up writing long ETL pipelines in Python and PySpark. The usual story: you read a bunch of tables, join them together, run several preprocessing steps where the output of one step becomes the input of another, and meanwhile, requirements keep changing --&gt; new rules, new table names, renamed columns, and so on.</p>  \n  \n<p>Recently, I had to refactor a client function that was doing exactly this. It received several Spark DataFrames as input and was supposed to return one “clean” transactions DataFrame, ready to be written to the database. Inside, it was basically just calling a chain of helper functions in a fixed order. Each helper returned a DataFrame that either contributed to the final result or was used as input for some other step further down the pipeline.</p>  \n  \n<p>The problem was that everything was tightly coupled. Each helper depended (sometimes implicitly) on what the previous helpers were doing. Changing a single line in one of them meant double-checking a lot of other functions to make sure nothing broke. It was fragile, and refactoring was painful.</p>  \n  \n<p>In situations like this, the Builder design pattern turned out to be a lifesaver for me. It’s very handy when the object you want to build is the result of several processing steps, not just a few fields assigned inside an <code>__init__</code>. If you want a proper deep dive into the pattern itself, I recommend this page: <a href=\"https://refactoring.guru/design-patterns/builder\">https://refactoring.guru/design-patterns/builder</a>. I first studied it in the book <em>Python Design Patterns</em> by Ayeva and Kasampalis, which I’d also recommend.</p>  \n  \n<p>In the example below, we’ll define:</p>  \n  \n<ul>  \n<li>a <strong>Transactions</strong> class – with very little responsibility: it just defines the attributes that make up our final object (raw inputs, intermediate tables, final output);</li>  \n<li>a <strong>TransactionsBuilderType1</strong> class – which receives the input DataFrames, creates a Transactions instance, and exposes the methods that progressively build each attribute;</li>  \n<li>a <strong>Director</strong> class – which knows the builder interface, has one main method that runs the builder steps in the right order, and exposes a method/property to return the final Transactions object once everything is done.  \n</li>  \n</ul>  \n  \n<div>  \n<pre><code>  \nclass Transactions:  \n    def __init__(self, input_table_1, input_table_2):  \n        self.raw_input_table_1 = input_table_1  \n        self.raw_input_table_2 = input_table_2  \n  \n        self.preprocessed_table_1 = None  \n        self.preprocessed_table_2 = None  \n  \n        # Final ETL output table (e.g. cleaned, enriched transactions)  \n        self.preprocessed_transactions = None  \n  \n  \nclass TransactionsBuilderType1:  \n    def __init__(self, input_table_1, input_table_2):  \n        self.transactions = Transactions(  \n            input_table_1=input_table_1,  \n            input_table_2=input_table_2,  \n        )  \n        ... # other builder-related attributes (configs, parameters, etc.)  \n  \n    def preprocess_table_1(self):  \n        # Business logic to preprocess input_table_1  \n        self.transactions.preprocessed_table_1 = ...  \n  \n    def preprocess_table_2(self):  \n        # Business logic to preprocess input_table_2  \n        self.transactions.preprocessed_table_2 = ...  \n  \n    def compute_final_table(self):  \n        # Use preprocessed tables to compute the final transactions table   \n        # (like self.preprocess_table_1.join(self.preprocess_table_2) etc.)  \n        self.transactions.preprocessed_transactions = ...   \n  \n  \nclass Director:  \n    def __init__(self):  \n        self.builder = None  \n  \n    def construct_transactions(self, builder):  \n        self.builder = builder  \n        steps = [  \n            self.builder.preprocess_table_1,  \n            self.builder.preprocess_table_2,  \n            self.builder.compute_final_table,  \n        ]  \n  \n        for step in steps:  \n            step()  \n  \n    @property  \n    def transactions(self):  \n        # Return the fully built Transactions object  \n        return self.builder.transactions  \n  \n  \n# client code:  \n  \ninput_table_1 = spark.table(...)  \ninput_table_2 = spark.table(...)  \nbuilder = TransactionsBuilderType1(input_table_1, input_table_2)  \ndirector = Director()  \ndirector.construct_transactions(builder)  \n  \npreprocessed_transactions = director.transactions.preprocessed_transactions  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Let me know your thoughts!<br>  \nThanks.</p>  \n  \n<p>Cristian Bergamo</p>",
      "summary": "In my job, I often end up writing long ETL pipelines in Python and PySpark. The usual story: you read a bunch of tables, join them together, run several preprocessing steps where the output of one step becomes the input of another, and meanwhile, requirements keep changing --&gt; new rules, new table names, renamed columns, and so on.  \n  \nRecently, I had to refactor a client function that was doing exactly this. It received several Spark DataFrames as input and was supposed to return one “clean",
      "publishedAt": "2025-12-02T14:33:55.000Z",
      "author": "Cristian Bergamo",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "thought_leadership",
      "score": 6.4967327927145115,
      "ingestedAt": "2025-12-02T14:44:03.155Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09b26e78",
      "title": "SAML vs OIDC: Choosing the Right Authentication Protocol for Your App",
      "url": "https://dev.to/iamdevbox/saml-vs-oidc-choosing-the-right-authentication-protocol-for-your-app-1h6o",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8eimuzeyyd6rfaomlw89.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>When it comes to authentication, two protocols reign supreme: SAML and OIDC. Both have their strengths and weaknesses, but which one is right for your app? In this article, we'll dive into the details of each protocol and help you make an informed decision.<br><br>  \nSAML, or Security Assertion Markup Language, is an XML-based protocol that allows users to access multiple applications with a single set of login credentials. It's commonly used for enterprise-level authentication, where security is paramount. On the other hand, OIDC, or OpenID Connect, is an OAuth-based protocol that provides a more standardized and flexible approach to authentication. It's perfect for web applications and services that require a high level of customization.<br><br>  \nBoth protocols have their advantages and disadvantages. SAML is more secure, but can be complex and inflexible. OIDC is more flexible, but may compromise on security. So, when should you use each? If you're building an enterprise-level application that requires top-notch security, SAML might be the way to go. But if you're building a web application that requires customization and flexibility, OIDC is the better choice.</p>  \n  \n<p>Read more: <a href=\"https://iamdevbox.com/posts/saml-vs-oidc-when-to-use-which-protocol-in-2025/?utm_source=devto&amp;utm_medium=social&amp;utm_campaign=blog_post\">SAML vs OIDC: Choosing the Right Authentication Protocol for Your App</a></p>",
      "summary": "When it comes to authentication, two protocols reign supreme: SAML and OIDC. Both have their strengths and weaknesses, but which one is right for your app? In this article, we'll dive into the details of each protocol and help you make an informed decision.  \nSAML, or Security Assertion Markup Language, is an XML-based protocol that allows users to access multiple applications with a single set of login credentials. It's commonly used for enterprise-level authentication, where security is paramo",
      "publishedAt": "2025-12-02T14:22:16.000Z",
      "author": "IAMDevBox",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "pricing_business",
      "score": 11.487579230239547,
      "ingestedAt": "2025-12-02T14:44:03.155Z",
      "tags": [
        "code_review",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09b26e79",
      "title": "What is Low Code Tools?What are the tools?",
      "url": "https://dev.to/aj_arul/what-is-low-code-toolswhat-are-the-tools-2k7p",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fx8udvpbi316fjpiqcpy6.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><div>  \n<pre><code>The Low Code Tools are Software platforms that let you build applications using drag and drop interfaces, pre built components and minimal coding, instead of writing the full code manually.  \n</code></pre>  \n  \n</div>  \n  \n<p>The Tools Are :</p>  \n  \n<ul>  \n<li><p>Microsoft power apps (Microsoft 365)</p></li>  \n<li><p>Mendix (Enterprice Scale Apps)</p></li>  \n<li><p>OutSystems (Enterprise mobile + web apps)</p></li>  \n<li><p>App sheets(Google) (simple mobile/web apps from spreadsheets)</p></li>  \n<li><p>Bubble(SaaS-like web applications without coding)</p></li>  \n<li><p>Zoho Creator(business workflows, forms, process automation)</p></li>  \n<li><p>Salesforce Lightning(apps inside Salesforce ecosystem)</p></li>  \n</ul>  \n  \n<h2>  \n    \n    \n  -   \n</h2>  \n  \n<ul>  \n<li>  \n</li>  \n</ul>",
      "summary": "  \nThe Low Code Tools are Software platforms that let you build applications using drag and drop interfaces, pre built components and minimal coding, instead of writing the full code manually.  \n  \n  \n  \n  \nThe Tools Are :  \n  \n  \nMicrosoft power apps (Microsoft 365)  \nMendix (Enterprice Scale Apps)  \nOutSystems (Enterprise mobile + web apps)  \nApp sheets(Google) (simple mobile/web apps from spreadsheets)  \nBubble(SaaS-like web applications without coding)  \nZoho Creator(business workflows, form",
      "publishedAt": "2025-12-02T14:20:43.000Z",
      "author": "Arul .A",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "pricing_business",
      "score": 10.987274475506029,
      "ingestedAt": "2025-12-02T14:44:03.155Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09b26e7d",
      "title": "Can an AI Predict the Taste of Your Salad? Here’s What I Built. 🥗🤖",
      "url": "https://dev.to/hari_narayanan_c557f6488b/can-an-ai-predict-the-taste-of-your-salad-heres-what-i-built-1ejd",
      "content": "<p>In recent months, I’ve been developing a Salad Taste Prediction AI — a system designed to analyze selected ingredients and forecast the overall flavor profile of a salad. Using structured flavor datasets, ingredient attributes, and machine learning–based taste mapping, the model predicts whether a combination will result in something fresh, tangy, sweet, savory, or otherwise distinctive.</p> \n \n<p>The goal behind this project is simple: to bring more clarity and creativity into food experimentation. Whether you’re a nutrition enthusiast, a culinary innovator, or simply someone who enjoys exploring flavors, this technology aims to offer a smarter and more insightful way to understand ingredient interactions.</p> \n \n<p>At this stage, I’m interested in understanding how many people would genuinely be excited to use a tool like this. Your interest will help guide the next steps, including making it publicly accessible.</p> \n \n<p>If you find this concept valuable or would like to explore such a system in the future, I’d appreciate hearing your thoughts in the comments. Your feedback will play a key role in shaping its development. 🥗✨<br> \n<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsdit7cpby1plt3kkegt8.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsdit7cpby1plt3kkegt8.png\" alt=\"\" width=\"800\" height=\"467\"></a></p>",
      "summary": "In recent months, I’ve been developing a Salad Taste Prediction AI — a system designed to analyze selected ingredients and forecast the overall flavor profile of a salad. Using structured flavor datasets, ingredient attributes, and machine learning–based taste mapping, the model predicts whether a combination will result in something fresh, tangy, sweet, savory, or otherwise distinctive. \n \nThe goal behind this project is simple: to bring more clarity and creativity into food experimentation. Wh",
      "publishedAt": "2025-12-02T14:16:45.000Z",
      "author": "hari narayanan",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.492556415552599,
      "ingestedAt": "2025-12-02T14:44:03.155Z",
      "tags": [
        "code_review",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09b26e7e",
      "title": "How to Use Vanna.ai to Query Your Database with Open-Source Language Models",
      "url": "https://dev.to/aairom/how-to-use-vannaai-to-query-your-database-with-open-source-language-models-5ado",
      "content": "<p>Yet another Text2SQL exercise with Vanna.ai, Ollama, Granite and gpt-oss.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxg1nylxj8dvtsw4u2g04.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxg1nylxj8dvtsw4u2g04.png\" alt=\"\" width=\"800\" height=\"800\"></a></p> \n \n<h2> \n   \n   \n  Introduction - What is Vanna.ai? \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F28kfmzxv996blqpaaxjn.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F28kfmzxv996blqpaaxjn.png\" alt=\"\" width=\"800\" height=\"420\"></a></p> \n \n<p>In the rapidly evolving world of data analytics, the search for the perfect Text-to-SQL generation tool often feels like a quest for the ‘Holy Grail’. I recently stumbled upon Vanna.ai, and it immediately stood out. At its core, <strong>Vanna.ai</strong> is a powerful agent designed to turn natural language questions into data insights. It achieves this by acting as a translator: taking a question from a user, converting it into valid SQL, executing that query against your database, and delivering the answer back in a rich format. More than just a simple query generator, Vanna 2.0 seems to be built for production, offering features like user-aware security, streaming responses (including interactive data tables and charts), and seamless integration with virtually any LLM (including open-source models like Granite and gpt-oss (among others…) and (almost) any database.</p> \n \n<p>As I mentioned, Vanna.ai seems to be fit for more than Text2SQL… but I stick to this part for my tests. You can refer to their site provided in “Links” to discover more. <strong>Last but not least; I have no affiliation with them!</strong></p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmlwlw0oqao67wk73p06h.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmlwlw0oqao67wk73p06h.png\" alt=\"\" width=\"800\" height=\"467\"></a></p> \n \n<h2> \n   \n   \n  Implementation and Tests \n</h2> \n \n<p>The magic of Vanna’s solution resides in their powerful, agent-based package, which handles everything from authenticating the user to selecting the right tools — like the <code>RunSqlTool</code>—to safely query the database and generate the final visualization. This robust architecture is the foundation of the sample application we'll be exploring from their site. So, let's jump into the test and coding steps!</p> \n \n<p><strong>To</strong> properly test Vanna’s capabilities and demonstrate its Text-to-SQL functionality, we first need a working environment with a database containing meaningful data. For maximum simplicity and portability, I chose to create and populate a SQLite database. As a file-based SQL engine, SQLite eliminates the need for a separate server setup, making it the fastest and easiest way to get our Users and Commands tables ready for the Vanna agent to query.</p> \n \n<p>So let’s jump into coding… 🪂</p> \n \n<h2> \n   \n   \n  Prepare a / your Database \n</h2> \n \n<ul> \n<li>Preparing the environment 🧑‍🍳 \n</li> \n</ul> \n \n<div> \n<pre><code>python3 <span>-m</span> venv venv \n<span>source </span>venv/bin/activate \n \npip <span>install</span> <span>--upgrade</span> pip \n \n<span># to populate and generate data this package is nice!</span> \npip <span>install </span>faker \n</code></pre> \n \n</div> \n \n \n \n<ul> \n<li>Create your tables in SQLite \n</li> \n</ul> \n \n<div> \n<pre><code><span># setup_database.py \n</span><span>import</span> <span>sqlite3</span> \n<span>import</span> <span>os</span> \n \n<span># Define the database file name \n</span><span>DB_NAME</span> <span>=</span> <span>'</span><span>users_commands.db</span><span>'</span> \n \n<span>def</span> <span>setup_database</span><span>():</span> \n    <span>\"\"\"</span><span> \n    Creates the SQLite database file and defines the Users and Commands tables. \n    </span><span>\"\"\"</span> \n    <span>if</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>exists</span><span>(</span><span>DB_NAME</span><span>):</span> \n        <span>os</span><span>.</span><span>remove</span><span>(</span><span>DB_NAME</span><span>)</span> \n \n    <span>conn</span> <span>=</span> <span>sqlite3</span><span>.</span><span>connect</span><span>(</span><span>DB_NAME</span><span>)</span> \n    <span>cursor</span> <span>=</span> <span>conn</span><span>.</span><span>cursor</span><span>()</span> \n \n    <span>cursor</span><span>.</span><span>execute</span><span>(</span><span>'''</span><span> \n        CREATE TABLE Users ( \n            ID INTEGER PRIMARY KEY, \n            FirstName TEXT NOT NULL, \n            LastName TEXT NOT NULL, \n            DateOfBirth TEXT NOT NULL,  \n            Address TEXT, \n            CommandList TEXT UNIQUE NOT NULL \n        ) \n    </span><span>'''</span><span>)</span> \n \n    <span>cursor</span><span>.</span><span>execute</span><span>(</span><span>'''</span><span> \n        CREATE TABLE Commands ( \n            ProductID TEXT PRIMARY KEY, \n            ProductName TEXT NOT NULL, \n            CommandNumber TEXT NOT NULL, \n            FOREIGN KEY (CommandNumber) REFERENCES Users(CommandList) \n        ) \n    </span><span>'''</span><span>)</span> \n \n    <span>conn</span><span>.</span><span>commit</span><span>()</span> \n    <span>conn</span><span>.</span><span>close</span><span>()</span> \n    <span>print</span><span>(</span><span>f</span><span>\"</span><span>✅ Successfully created database </span><span>'</span><span>{</span><span>DB_NAME</span><span>}</span><span>'</span><span> and tables </span><span>'</span><span>Users</span><span>'</span><span> and </span><span>'</span><span>Commands</span><span>'</span><span>.</span><span>\"</span><span>)</span> \n \n<span>if</span> <span>__name__</span> <span>==</span> <span>\"</span><span>__main__</span><span>\"</span><span>:</span> \n    <span>setup_database</span><span>()</span> \n</code></pre> \n \n</div> \n \n \n \n<ul> \n<li>Insert some data in the DB! \n</li> \n</ul> \n \n<div> \n<pre><code><span># insert_data.py \n</span><span>import</span> <span>sqlite3</span> \n<span>from</span> <span>faker</span> <span>import</span> <span>Faker</span> \n<span>import</span> <span>random</span> \n<span>import</span> <span>uuid</span> \n \n<span>DB_NAME</span> <span>=</span> <span>'</span><span>users_commands.db</span><span>'</span> \n \n<span>def</span> <span>insert_random_data</span><span>(</span><span>num_records</span><span>=</span><span>10</span><span>):</span> \n    <span>\"\"\"</span><span> \n    Generates and inserts random user and command data into the database. \n    </span><span>\"\"\"</span> \n    <span>try</span><span>:</span> \n        <span>conn</span> <span>=</span> <span>sqlite3</span><span>.</span><span>connect</span><span>(</span><span>DB_NAME</span><span>)</span> \n        <span>cursor</span> <span>=</span> <span>conn</span><span>.</span><span>cursor</span><span>()</span> \n        <span>fake</span> <span>=</span> <span>Faker</span><span>()</span> \n \n        <span>user_data</span> <span>=</span> <span>[]</span> \n        <span>command_data</span> <span>=</span> <span>[]</span> \n \n        <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>num_records</span><span>):</span> \n            <span>command_ref</span> <span>=</span> <span>str</span><span>(</span><span>uuid</span><span>.</span><span>uuid4</span><span>())</span> \n \n            <span>user_data</span><span>.</span><span>append</span><span>((</span> \n                <span>fake</span><span>.</span><span>first_name</span><span>(),</span> \n                <span>fake</span><span>.</span><span>last_name</span><span>(),</span> \n                <span>fake</span><span>.</span><span>date_of_birth</span><span>(</span><span>minimum_age</span><span>=</span><span>18</span><span>,</span> <span>maximum_age</span><span>=</span><span>65</span><span>).</span><span>strftime</span><span>(</span><span>'</span><span>%Y-%m-%d</span><span>'</span><span>),</span> \n                <span>fake</span><span>.</span><span>address</span><span>().</span><span>replace</span><span>(</span><span>'</span><span>\\n</span><span>'</span><span>,</span> <span>'</span><span>, </span><span>'</span><span>),</span>  \n                <span>command_ref</span> <span># The link field \n</span>            <span>))</span> \n \n            <span>product_id</span> <span>=</span> <span>f</span><span>\"</span><span>PROD-</span><span>{</span><span>random</span><span>.</span><span>randint</span><span>(</span><span>1000</span><span>,</span> <span>9999</span><span>)</span><span>}</span><span>\"</span>  \n            <span>product_name</span> <span>=</span> <span>fake</span><span>.</span><span>word</span><span>().</span><span>capitalize</span><span>()</span> <span>+</span> <span>\"</span><span> </span><span>\"</span> <span>+</span> <span>fake</span><span>.</span><span>word</span><span>().</span><span>capitalize</span><span>()</span> \n \n            <span>command_data</span><span>.</span><span>append</span><span>((</span> \n                <span>product_id</span><span>,</span> \n                <span>product_name</span><span>,</span> \n                <span>command_ref</span>  \n            <span>))</span> \n \n        <span>cursor</span><span>.</span><span>executemany</span><span>(</span><span>'''</span><span> \n            INSERT INTO Users (FirstName, LastName, DateOfBirth, Address, CommandList) \n            VALUES (?, ?, ?, ?, ?) \n        </span><span>'''</span><span>,</span> <span>user_data</span><span>)</span> \n \n        <span>cursor</span><span>.</span><span>executemany</span><span>(</span><span>'''</span><span> \n            INSERT INTO Commands (ProductID, ProductName, CommandNumber) \n            VALUES (?, ?, ?) \n        </span><span>'''</span><span>,</span> <span>command_data</span><span>)</span> \n \n        <span>conn</span><span>.</span><span>commit</span><span>()</span> \n        <span>print</span><span>(</span><span>f</span><span>\"</span><span>✅ Successfully inserted </span><span>{</span><span>len</span><span>(</span><span>user_data</span><span>)</span><span>}</span><span> random records into **</span><span>'</span><span>Users</span><span>'</span><span>** and </span><span>'</span><span>Commands</span><span>'</span><span>.</span><span>\"</span><span>)</span> \n \n    <span>except</span> <span>sqlite3</span><span>.</span><span>Error</span> <span>as</span> <span>e</span><span>:</span> \n        <span>print</span><span>(</span><span>f</span><span>\"</span><span>An error occurred: </span><span>{</span><span>e</span><span>}</span><span>\"</span><span>)</span> \n    <span>finally</span><span>:</span> \n        <span>if</span> <span>conn</span><span>:</span> \n            <span>conn</span><span>.</span><span>close</span><span>()</span> \n \n<span>if</span> <span>__name__</span> <span>==</span> <span>\"</span><span>__main__</span><span>\"</span><span>:</span> \n    <span>insert_random_data</span><span>(</span><span>10</span><span>)</span> <span># Insert 10 lines of data!!! \n</span></code></pre> \n \n</div> \n \n \n \n<ul> \n<li>Test and Query the Database 🧪 \n</li> \n</ul> \n \n<div> \n<pre><code><span># query_database.py \n</span><span>import</span> <span>sqlite3</span> \n<span>import</span> <span>os</span> \n<span>from</span> <span>typing</span> <span>import</span> <span>List</span><span>,</span> <span>Tuple</span> \n \n<span>DB_NAME</span> <span>=</span> <span>'</span><span>users_commands.db</span><span>'</span> \n<span>OUTPUT_DIR</span> <span>=</span> <span>'</span><span>output</span><span>'</span> \n<span>REPORT_PATH</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>OUTPUT_DIR</span><span>,</span> <span>'</span><span>database_report.md</span><span>'</span><span>)</span> \n \n \n<span>def</span> <span>format_table_data</span><span>(</span><span>title</span><span>:</span> <span>str</span><span>,</span> <span>headers</span><span>:</span> <span>List</span><span>[</span><span>str</span><span>],</span> <span>data</span><span>:</span> <span>List</span><span>[</span><span>Tuple</span><span>],</span> <span>is_markdown</span><span>:</span> <span>bool</span><span>,</span> <span>col_widths</span><span>:</span> <span>List</span><span>[</span><span>int</span><span>]</span> <span>=</span> <span>None</span><span>)</span> <span>-&gt;</span> <span>str</span><span>:</span> \n    <span>\"\"\"</span><span>Formats data into a printable string, either for console or Markdown.</span><span>\"\"\"</span> \n    <span>output</span> <span>=</span> <span>\"\"</span> \n \n    <span>if</span> <span>is_markdown</span><span>:</span> \n        <span>output</span> <span>+=</span> <span>f</span><span>\"</span><span>## </span><span>{</span><span>title</span><span>}</span><span>\\n\\n</span><span>\"</span> \n \n        <span># Markdown Header Row \n</span>        <span>output</span> <span>+=</span> <span>\"</span><span>| </span><span>\"</span> <span>+</span> <span>\"</span><span> | </span><span>\"</span><span>.</span><span>join</span><span>(</span><span>headers</span><span>)</span> <span>+</span> <span>\"</span><span> |</span><span>\\n</span><span>\"</span> \n        <span># Markdown Separator Row \n</span>        <span>output</span> <span>+=</span> <span>\"</span><span>|-</span><span>\"</span> <span>+</span> <span>\"</span><span>-|-</span><span>\"</span><span>.</span><span>join</span><span>([</span><span>'</span><span>-</span><span>'</span> <span>*</span> <span>len</span><span>(</span><span>h</span><span>)</span> <span>for</span> <span>h</span> <span>in</span> <span>headers</span><span>])</span> <span>+</span> <span>\"</span><span>-|</span><span>\\n</span><span>\"</span> \n \n        <span># Markdown Data Rows \n</span>        <span>for</span> <span>row</span> <span>in</span> <span>data</span><span>:</span> \n            <span># Ensure all elements are strings for joining \n</span>            <span>str_row</span> <span>=</span> <span>[</span><span>str</span><span>(</span><span>col</span><span>)</span> <span>for</span> <span>col</span> <span>in</span> <span>row</span><span>]</span> \n            <span>output</span> <span>+=</span> <span>\"</span><span>| </span><span>\"</span> <span>+</span> <span>\"</span><span> | </span><span>\"</span><span>.</span><span>join</span><span>(</span><span>str_row</span><span>)</span> <span>+</span> <span>\"</span><span> |</span><span>\\n</span><span>\"</span> \n        <span>output</span> <span>+=</span> <span>\"</span><span>\\n</span><span>\"</span> \n \n    <span>else</span><span>:</span> <span># Console Formatting \n</span>        <span>output</span> <span>+=</span> <span>\"</span><span>-</span><span>\"</span> <span>*</span> <span>70</span> <span>+</span> <span>\"</span><span>\\n</span><span>\"</span> \n        <span>output</span> <span>+=</span> <span>f</span><span>\"</span><span>--- </span><span>{</span><span>title</span><span>}</span><span> ---</span><span>\\n</span><span>\"</span> \n        <span>output</span> <span>+=</span> <span>\"</span><span>-</span><span>\"</span> <span>*</span> <span>70</span> <span>+</span> <span>\"</span><span>\\n</span><span>\"</span> \n \n        <span>if</span> <span>not</span> <span>data</span><span>:</span> \n            <span>output</span> <span>+=</span> <span>\"</span><span>The table is empty.</span><span>\\n</span><span>\"</span> \n            <span>return</span> <span>output</span> \n \n        <span>if</span> <span>not</span> <span>col_widths</span> <span>or</span> <span>len</span><span>(</span><span>col_widths</span><span>)</span> <span>!=</span> <span>len</span><span>(</span><span>headers</span><span>):</span> \n            <span>col_widths</span> <span>=</span> <span>[</span><span>len</span><span>(</span><span>h</span><span>)</span> <span>for</span> <span>h</span> <span>in</span> <span>headers</span><span>]</span> <span># Simple fallback \n</span> \n        <span>header_row</span> <span>=</span> <span>\"\"</span> \n        <span>for</span> <span>i</span><span>,</span> <span>header</span> <span>in</span> <span>enumerate</span><span>(</span><span>headers</span><span>):</span> \n            <span>width</span> <span>=</span> <span>[</span><span>4</span><span>,</span> <span>15</span><span>,</span> <span>15</span><span>,</span> <span>12</span><span>,</span> <span>25</span><span>,</span> <span>36</span><span>][</span><span>i</span><span>]</span> <span>if</span> <span>len</span><span>(</span><span>headers</span><span>)</span> <span>==</span> <span>6</span> <span>else</span> <span>[</span><span>15</span><span>,</span> <span>15</span><span>,</span> <span>25</span><span>,</span> <span>15</span><span>][</span><span>i</span><span>]</span> \n            <span>header_row</span> <span>+=</span> <span>f</span><span>\"</span><span>{</span><span>header</span><span>:</span><span>&lt;</span><span>{</span><span>width</span><span>}}</span><span> | </span><span>\"</span> \n        <span>output</span> <span>+=</span> <span>header_row</span><span>.</span><span>strip</span><span>()</span> <span>+</span> <span>\"</span><span>\\n</span><span>\"</span> \n        <span>output</span> <span>+=</span> <span>\"</span><span>-</span><span>\"</span> <span>*</span> <span>(</span><span>len</span><span>(</span><span>header_row</span><span>)</span> <span>+</span> <span>5</span><span>)</span> <span>+</span> <span>\"</span><span>\\n</span><span>\"</span> \n \n        <span>for</span> <span>row</span> <span>in</span> <span>data</span><span>:</span> \n            <span>data_row</span> <span>=</span> <span>\"\"</span> \n            <span>for</span> <span>i</span><span>,</span> <span>item</span> <span>in</span> <span>enumerate</span><span>(</span><span>row</span><span>):</span> \n                <span>width</span> <span>=</span> <span>[</span><span>4</span><span>,</span> <span>15</span><span>,</span> <span>15</span><span>,</span> <span>12</span><span>,</span> <span>25</span><span>,</span> <span>36</span><span>][</span><span>i</span><span>]</span> <span>if</span> <span>len</span><span>(</span><span>headers</span><span>)</span> <span>==</span> <span>6</span> <span>else</span> <span>[</span><span>15</span><span>,</span> <span>15</span><span>,</span> <span>25</span><span>,</span> <span>15</span><span>][</span><span>i</span><span>]</span> \n \n                <span>display_item</span> <span>=</span> <span>str</span><span>(</span><span>item</span><span>)</span> \n                <span>if</span> <span>i</span> <span>==</span> <span>4</span> <span>and</span> <span>len</span><span>(</span><span>headers</span><span>)</span> <span>==</span> <span>6</span><span>:</span> <span># Address column in Users table \n</span>                    <span>display_item</span> <span>=</span> <span>display_item</span><span>[:</span><span>width</span> <span>-</span> <span>3</span><span>]</span> <span>+</span> <span>'</span><span>...</span><span>'</span> <span>if</span> <span>len</span><span>(</span><span>display_item</span><span>)</span> <span>&gt;</span> <span>width</span> <span>else</span> <span>display_item</span> \n \n                <span>data_row</span> <span>+=</span> <span>f</span><span>\"</span><span>{</span><span>display_item</span><span>:</span><span>&lt;</span><span>{</span><span>width</span><span>}}</span><span> | </span><span>\"</span> \n \n            <span>output</span> <span>+=</span> <span>data_row</span><span>.</span><span>strip</span><span>()</span> <span>+</span> <span>\"</span><span>\\n</span><span>\"</span> \n        <span>output</span> <span>+=</span> <span>\"</span><span>\\n</span><span>\"</span> \n \n    <span>return</span> <span>output</span> \n \n<span>def</span> <span>query_and_display_data</span><span>():</span> \n    <span>\"\"\"</span><span> \n    Connects to the database, queries the data, prints to console,  \n    and saves the results to a Markdown file. \n    </span><span>\"\"\"</span> \n    <span>conn</span> <span>=</span> <span>None</span> \n    <span>try</span><span>:</span> \n        <span>conn</span> <span>=</span> <span>sqlite3</span><span>.</span><span>connect</span><span>(</span><span>DB_NAME</span><span>)</span> \n        <span>cursor</span> <span>=</span> <span>conn</span><span>.</span><span>cursor</span><span>()</span> \n \n        <span>cursor</span><span>.</span><span>execute</span><span>(</span><span>\"</span><span>SELECT ID, FirstName, LastName, DateOfBirth, Address, CommandList FROM Users</span><span>\"</span><span>)</span> \n        <span>users_data</span> <span>=</span> <span>cursor</span><span>.</span><span>fetchall</span><span>()</span> \n        <span>users_headers</span> <span>=</span> <span>[</span><span>'</span><span>ID</span><span>'</span><span>,</span> <span>'</span><span>First Name</span><span>'</span><span>,</span> <span>'</span><span>Last Name</span><span>'</span><span>,</span> <span>'</span><span>DOB</span><span>'</span><span>,</span> <span>'</span><span>Address</span><span>'</span><span>,</span> <span>'</span><span>CommandList Ref</span><span>'</span><span>]</span> \n \n        <span>if</span> <span>not</span> <span>users_data</span><span>:</span> \n            <span>console_output</span> <span>=</span> <span>\"</span><span>The Users table is empty. Please ensure </span><span>'</span><span>insert_data.py</span><span>'</span><span> was run.</span><span>\"</span> \n            <span>markdown_content</span> <span>=</span> <span>\"</span><span># Database Report</span><span>\\n\\n</span><span>## Users and Commands Data</span><span>\\n\\n</span><span>\"</span> <span>+</span> <span>console_output</span> \n        <span>else</span><span>:</span> \n            <span>join_query</span> <span>=</span> <span>\"\"\"</span><span> \n                SELECT \n                    U.FirstName, \n                    U.LastName, \n                    C.ProductName, \n                    C.ProductID \n                FROM \n                    Users U \n                INNER JOIN \n                    Commands C ON U.CommandList = C.CommandNumber \n                ORDER BY \n                    U.ID; \n            </span><span>\"\"\"</span> \n            <span>cursor</span><span>.</span><span>execute</span><span>(</span><span>join_query</span><span>)</span> \n            <span>joined_data</span> <span>=</span> <span>cursor</span><span>.</span><span>fetchall</span><span>()</span> \n            <span>joined_headers</span> <span>=</span> <span>[</span><span>'</span><span>First Name</span><span>'</span><span>,</span> <span>'</span><span>Last Name</span><span>'</span><span>,</span> <span>'</span><span>Product Name</span><span>'</span><span>,</span> <span>'</span><span>Product ID</span><span>'</span><span>]</span> \n \n            <span>console_output</span> <span>=</span> <span>format_table_data</span><span>(</span><span>\"</span><span>1. Listing All Records from the </span><span>'</span><span>Users</span><span>'</span><span> Table</span><span>\"</span><span>,</span> <span>users_headers</span><span>,</span> <span>users_data</span><span>,</span> <span>is_markdown</span><span>=</span><span>False</span><span>)</span> \n            <span>console_output</span> <span>+=</span> <span>\"</span><span>\\n</span><span>\"</span> <span>*</span> <span>2</span> \n            <span>console_output</span> <span>+=</span> <span>format_table_data</span><span>(</span><span>\"</span><span>2. Listing Joined Data (</span><span>'</span><span>User</span><span>'</span><span> and </span><span>'</span><span>Command</span><span>'</span><span>) using INNER JOIN</span><span>\"</span><span>,</span> <span>joined_headers</span><span>,</span> <span>joined_data</span><span>,</span> <span>is_markdown</span><span>=</span><span>False</span><span>)</span> \n \n            <span>markdown_content</span> <span>=</span> <span>\"</span><span># Database Query Report</span><span>\\n\\n</span><span>\"</span> \n            <span>markdown_content</span> <span>+=</span> <span>format_table_data</span><span>(</span><span>\"</span><span>User Records (Users Table)</span><span>\"</span><span>,</span> <span>users_headers</span><span>,</span> <span>users_data</span><span>,</span> <span>is_markdown</span><span>=</span><span>True</span><span>)</span> \n            <span>markdown_content</span> <span>+=</span> <span>\"</span><span>\\n</span><span>---</span><span>\\n\\n</span><span>\"</span> \n            <span>markdown_content</span> <span>+=</span> <span>format_table_data</span><span>(</span><span>\"</span><span>User Commands (Joined Data)</span><span>\"</span><span>,</span> <span>joined_headers</span><span>,</span> <span>joined_data</span><span>,</span> <span>is_markdown</span><span>=</span><span>True</span><span>)</span> \n \n \n        <span># Console Display \n</span>        <span>print</span><span>(</span><span>console_output</span><span>)</span> \n \n        <span># File Save \n</span>        <span>os</span><span>.</span><span>makedirs</span><span>(</span><span>OUTPUT_DIR</span><span>,</span> <span>exist_ok</span><span>=</span><span>True</span><span>)</span> \n \n        <span>with</span> <span>open</span><span>(</span><span>REPORT_PATH</span><span>,</span> <span>'</span><span>w</span><span>'</span><span>)</span> <span>as</span> <span>f</span><span>:</span> \n            <span>f</span><span>.</span><span>write</span><span>(</span><span>markdown_content</span><span>)</span> \n \n        <span>print</span><span>(</span><span>f</span><span>\"</span><span>\\n</span><span>✅ Successfully saved database report to: </span><span>{</span><span>REPORT_PATH</span><span>}</span><span>\"</span><span>)</span> \n \n    <span>except</span> <span>sqlite3</span><span>.</span><span>Error</span> <span>as</span> <span>e</span><span>:</span> \n        <span>print</span><span>(</span><span>f</span><span>\"</span><span>\\n</span><span>❌ Database Error: </span><span>{</span><span>e</span><span>}</span><span>\"</span><span>)</span> \n        <span>print</span><span>(</span><span>f</span><span>\"</span><span>Please ensure the database file </span><span>'</span><span>{</span><span>DB_NAME</span><span>}</span><span>'</span><span> exists and the tables were correctly set up.</span><span>\"</span><span>)</span> \n    <span>except</span> <span>Exception</span> <span>as</span> <span>e</span><span>:</span> \n        <span>print</span><span>(</span><span>f</span><span>\"</span><span>\\n</span><span>❌ An unexpected error occurred: </span><span>{</span><span>e</span><span>}</span><span>\"</span><span>)</span> \n    <span>finally</span><span>:</span> \n        <span>if</span> <span>conn</span><span>:</span> \n            <span>conn</span><span>.</span><span>close</span><span>()</span> \n \n<span>if</span> <span>__name__</span> <span>==</span> <span>\"</span><span>__main__</span><span>\"</span><span>:</span> \n    <span>query_and_display_data</span><span>()</span> \n</code></pre> \n \n</div> \n \n \n \n<ul> \n<li>Populate and build your database. \n</li> \n</ul> \n \n<div> \n<pre><code>python setup_database.py \npython insert_data.py \npython query_database.py \n</code></pre> \n \n</div> \n \n \n \n<ul> \n<li>This will give you both a console output and a markdown file. \n</li> \n</ul> \n \n<div> \n<pre><code><span>&gt; python query_database.py \n---------------------------------------------------------------------- \n--- 1. Listing All Records from the 'Users' Table --- \n---------------------------------------------------------------------- \nID   | First Name      | Last Name       | DOB          | Address                   | CommandList Ref                      | \n---------------------------------------------------------------------------------------------------------------------------------- \n</span>1    | David           | Frank           | 1983-03-22   | 3973 Carmen Gateway Su... | 173e986d-77b0-4c6e-9314-52813419f108 | \n2    | Sydney          | Middleton       | 1981-09-24   | 4080 Regina Lake Apt. ... | 5bd1cf9a-2cfc-46e5-b150-77c368efaccb | \n3    | Joshua          | Holder          | 2000-06-19   | 0072 Sanchez Hollow, P... | 68009684-34a9-4be0-8f67-ef26155d6cfd | \n4    | Robert          | Walker          | 1968-01-10   | USS Lee, FPO AP 42065     | dc60fd66-f59b-486b-8f04-4162bdf9c99e | \n5    | Douglas         | Johnson         | 1999-08-18   | 51189 Jacqueline Shore... | 80c13c6f-c5f7-4df7-abfe-f91ced3e1237 | \n6    | James           | Hernandez       | 1976-10-28   | Unit 7218 Box 1614, DP... | 9b4dee77-37ad-4fa4-8672-a7c053e71429 | \n7    | Diana           | Thomas          | 2004-02-29   | 84300 Vaughn Crossroad... | 7062b085-18b4-40c3-8e4e-c3bd6c214e08 | \n8    | Jamie           | Torres          | 1961-06-22   | 857 Pennington Flats S... | 31725920-e0c3-4fac-bd42-31d113e6e107 | \n9    | Renee           | Rice            | 2006-09-04   | 963 Marvin Underpass, ... | 8536b45b-3502-480a-a9be-89189e68478e | \n10   | Henry           | Wallace         | 1962-04-18   | 79093 Danielle Haven S... | 83c466e4-239f-4844-8a79-6286be0edb30 |<span> \n \n \n \n</span><span>----------------------------------------------------------------------</span> \n<span>--- 2. Listing Joined Data ('User' and 'Command') using INNER JOIN --- \n---------------------------------------------------------------------- \nFirst Name      | Last Name       | Product Name              | Product ID      | \n--------------------------------------------------------------------------------------- \n</span>David           | Frank           | Wife Choose               | PROD-3937       | \nSydney          | Middleton       | Produce Fall              | PROD-5279       | \nJoshua          | Holder          | Plan Sell                 | PROD-5104       | \nRobert          | Walker          | Interesting Guy           | PROD-3216       | \nDouglas         | Johnson         | Time At                   | PROD-8672       | \nJames           | Hernandez       | Someone Mouth             | PROD-4010       | \nDiana           | Thomas          | Pressure Move             | PROD-3587       | \nJamie           | Torres          | It Every                  | PROD-8192       | \nRenee           | Rice            | Writer Still              | PROD-7510       | \nHenry           | Wallace         | Professional Scene        | PROD-7318       |<span> \n \n \n \n</span>✅ Successfully saved database report to: output/database_report.md \n</code></pre> \n \n</div> \n \n \n \n<ul> \n<li>Also, I made a GUI interface to see my database and the data (pretty hard-coded stuff, but we get the idea) 🈁 \n</li> \n</ul> \n \n<div> \n<pre><code><span>&lt;!DOCTYPE html&gt;</span> \n<span>&lt;html</span> <span>lang=</span><span>\"en\"</span><span>&gt;</span> \n<span>&lt;head&gt;</span> \n    <span>&lt;meta</span> <span>charset=</span><span>\"UTF-8\"</span><span>&gt;</span> \n    <span>&lt;meta</span> <span>name=</span><span>\"viewport\"</span> <span>content=</span><span>\"width=device-width, initial-scale=1.0\"</span><span>&gt;</span> \n    <span>&lt;title&gt;</span>SQLite Database Browser (Users <span>&amp;</span> Commands)<span>&lt;/title&gt;</span> \n    <span>&lt;script </span><span>src=</span><span>\"https://cdn.tailwindcss.com\"</span><span>&gt;&lt;/script&gt;</span> \n    <span>&lt;script </span><span>src=</span><span>\"https://cdnjs.cloudflare.com/ajax/libs/sql.js/1.10.3/sql-wasm.js\"</span><span>&gt;&lt;/script&gt;</span> \n    <span>&lt;style&gt;</span> \n        <span>body</span> <span>{</span> <span>font-family</span><span>:</span> <span>'Inter'</span><span>,</span> <span>sans-serif</span><span>;</span> <span>background-color</span><span>:</span> <span>#f7f9fb</span><span>;</span> <span>}</span> \n        <span>.container</span> <span>{</span> <span>max-width</span><span>:</span> <span>1200px</span><span>;</span> <span>}</span> \n        <span>.table-container</span> <span>{</span> <span>max-height</span><span>:</span> <span>400px</span><span>;</span> <span>overflow-y</span><span>:</span> <span>auto</span><span>;</span> <span>background-color</span><span>:</span> <span>white</span><span>;</span> <span>border-radius</span><span>:</span> <span>0.5rem</span><span>;</span> <span>}</span> \n        <span>.sql-textarea</span> <span>{</span> <span>font-family</span><span>:</span> <span>monospace</span><span>;</span> <span>}</span> \n        <span>table</span> <span>{</span> <span>width</span><span>:</span> <span>100%</span><span>;</span> <span>border-collapse</span><span>:</span> <span>collapse</span><span>;</span> <span>}</span> \n        <span>th</span><span>,</span> <span>td</span> <span>{</span> <span>padding</span><span>:</span> <span>8px</span> <span>12px</span><span>;</span> <span>text-align</span><span>:</span> <span>left</span><span>;</span> <span>border-bottom</span><span>:</span> <span>1px</span> <span>solid</span> <span>#e5e7eb</span><span>;</span> <span>}</span> \n        <span>th</span> <span>{</span> <span>background-color</span><span>:</span> <span>#1e40af</span><span>;</span> <span>color</span><span>:</span> <span>white</span><span>;</span> <span>position</span><span>:</span> <span>sticky</span><span>;</span> <span>top</span><span>:</span> <span>0</span><span>;</span> <span>}</span> \n        <span>.btn</span> <span>{</span> <span>transition</span><span>:</span> <span>background-color</span> <span>0.2s</span><span>;</span> <span>}</span> \n        <span>.btn</span><span>:hover</span> <span>{</span> <span>filter</span><span>:</span> <span>brightness</span><span>(</span><span>1.1</span><span>);</span> <span>}</span> \n    <span>&lt;/style&gt;</span> \n<span>&lt;/head&gt;</span> \n<span>&lt;body</span> <span>class=</span><span>\"p-6\"</span><span>&gt;</span> \n \n    <span>&lt;div</span> <span>id=</span><span>\"app\"</span> <span>class=</span><span>\"container mx-auto space-y-8\"</span><span>&gt;</span> \n        <span>&lt;h1</span> <span>class=</span><span>\"text-4xl font-bold text-gray-800 border-b-4 border-indigo-600 pb-2\"</span><span>&gt;</span>Database Viewer Interface<span>&lt;/h1&gt;</span> \n \n        <span>&lt;div</span> <span>class=</span><span>\"bg-white p-6 rounded-xl shadow-lg flex flex-col md:flex-row justify-between items-start md:items-center space-y-4 md:space-y-0\"</span><span>&gt;</span> \n            <span>&lt;div</span> <span>id=</span><span>\"status\"</span> <span>class=</span><span>\"text-lg font-semibold text-green-700\"</span><span>&gt;</span>Database initializing...<span>&lt;/div&gt;</span> \n            <span>&lt;button</span> <span>id=</span><span>\"resetDbBtn\"</span> <span>class=</span><span>\"btn bg-red-500 hover:bg-red-600 text-white font-bold py-2 px-4 rounded-lg shadow-md\"</span> <span>onclick=</span><span>\"initDb(true)\"</span><span>&gt;</span> \n                Reset Database <span>&amp;</span> Reload Data \n            <span>&lt;/button&gt;</span> \n        <span>&lt;/div&gt;</span> \n \n        <span>&lt;!-- Predefined Queries --&gt;</span> \n        <span>&lt;div</span> <span>class=</span><span>\"bg-white p-6 rounded-xl shadow-lg space-y-4\"</span><span>&gt;</span> \n            <span>&lt;h2</span> <span>class=</span><span>\"text-2xl font-semibold text-gray-700\"</span><span>&gt;</span>Predefined Queries<span>&lt;/h2&gt;</span> \n            <span>&lt;div</span> <span>class=</span><span>\"flex flex-wrap gap-4\"</span><span>&gt;</span> \n                <span>&lt;button</span> <span>class=</span><span>\"btn bg-indigo-500 hover:bg-indigo-600 text-white font-semibold py-2 px-4 rounded-lg shadow-md\"</span> <span>onclick=</span><span>\"displayUsers()\"</span><span>&gt;</span> \n                    Show All Users \n                <span>&lt;/button&gt;</span> \n                <span>&lt;button</span> <span>class=</span><span>\"btn bg-indigo-500 hover:bg-indigo-600 text-white font-semibold py-2 px-4 rounded-lg shadow-md\"</span> <span>onclick=</span><span>\"displayJoinedData()\"</span><span>&gt;</span> \n                    Show Users <span>&amp;</span> Commands (JOIN) \n                <span>&lt;/button&gt;</span> \n            <span>&lt;/div&gt;</span> \n        <span>&lt;/div&gt;</span> \n \n        <span>&lt;div</span> <span>class=</span><span>\"bg-white p-6 rounded-xl shadow-lg space-y-4\"</span><span>&gt;</span> \n            <span>&lt;h2</span> <span>class=</span><span>\"text-2xl font-semibold text-gray-700\"</span><span>&gt;</span>Custom SQL Query<span>&lt;/h2&gt;</span> \n            <span>&lt;textarea</span> <span>id=</span><span>\"sqlInput\"</span> <span>class=</span><span>\"sql-textarea w-full p-3 border-2 border-gray-300 rounded-lg focus:ring-indigo-500 focus:border-indigo-500\"</span> <span>rows=</span><span>\"4\"</span> <span>placeholder=</span><span>\"e.g., SELECT * FROM Users WHERE FirstName = 'Alex'\"</span><span>&gt;&lt;/textarea&gt;</span> \n            <span>&lt;button</span> <span>class=</span><span>\"btn bg-blue-600 hover:bg-blue-700 text-white font-semibold py-2 px-4 rounded-lg shadow-md\"</span> <span>onclick=</span><span>\"executeCustomQuery()\"</span><span>&gt;</span> \n                Execute Query \n            <span>&lt;/button&gt;</span> \n        <span>&lt;/div&gt;</span> \n \n        <span>&lt;div</span> <span>class=</span><span>\"space-y-4\"</span><span>&gt;</span> \n            <span>&lt;h2</span> <span>id=</span><span>\"resultTitle\"</span> <span>class=</span><span>\"text-3xl font-bold text-gray-800\"</span><span>&gt;</span>Query Results<span>&lt;/h2&gt;</span> \n            <span>&lt;div</span> <span>id=</span><span>\"results\"</span> <span>class=</span><span>\"table-container border-2 border-gray-200\"</span><span>&gt;</span> \n                <span>&lt;p</span> <span>class=</span><span>\"p-4 text-gray-500\"</span><span>&gt;</span>Run a query above to see results.<span>&lt;/p&gt;</span> \n            <span>&lt;/div&gt;</span> \n        <span>&lt;/div&gt;</span> \n \n    <span>&lt;/div&gt;</span> \n \n    <span>&lt;script&gt;</span> \n        <span>let</span> <span>db</span> <span>=</span> <span>null</span><span>;</span> \n        <span>let</span> <span>SQL</span> <span>=</span> <span>null</span><span>;</span> \n        <span>const</span> <span>DB_NAME</span> <span>=</span> <span>'</span><span>users_commands.db</span><span>'</span><span>;</span> \n        <span>const</span> <span>NUM_RECORDS</span> <span>=</span> <span>10</span><span>;</span> \n        <span>const</span> <span>resultDiv</span> <span>=</span> <span>document</span><span>.</span><span>getElementById</span><span>(</span><span>'</span><span>results</span><span>'</span><span>);</span> \n        <span>const</span> <span>resultTitle</span> <span>=</span> <span>document</span><span>.</span><span>getElementById</span><span>(</span><span>'</span><span>resultTitle</span><span>'</span><span>);</span> \n        <span>const</span> <span>statusDiv</span> <span>=</span> <span>document</span><span>.</span><span>getElementById</span><span>(</span><span>'</span><span>status</span><span>'</span><span>);</span> \n \n        <span>const</span> <span>MOCK_DATA</span> <span>=</span> <span>{</span> \n            <span>firstNames</span><span>:</span> <span>[</span><span>\"</span><span>Alex</span><span>\"</span><span>,</span> <span>\"</span><span>Bella</span><span>\"</span><span>,</span> <span>\"</span><span>Chris</span><span>\"</span><span>,</span> <span>\"</span><span>Dana</span><span>\"</span><span>,</span> <span>\"</span><span>Ethan</span><span>\"</span><span>,</span> <span>\"</span><span>Fiona</span><span>\"</span><span>,</span> <span>\"</span><span>George</span><span>\"</span><span>,</span> <span>\"</span><span>Hannah</span><span>\"</span><span>,</span> <span>\"</span><span>Ivan</span><span>\"</span><span>,</span> <span>\"</span><span>Jasmine</span><span>\"</span><span>],</span> \n            <span>lastNames</span><span>:</span> <span>[</span><span>\"</span><span>Smith</span><span>\"</span><span>,</span> <span>\"</span><span>Jones</span><span>\"</span><span>,</span> <span>\"</span><span>Williams</span><span>\"</span><span>,</span> <span>\"</span><span>Brown</span><span>\"</span><span>,</span> <span>\"</span><span>Davis</span><span>\"</span><span>,</span> <span>\"</span><span>Miller</span><span>\"</span><span>,</span> <span>\"</span><span>Wilson</span><span>\"</span><span>,</span> <span>\"</span><span>Moore</span><span>\"</span><span>,</span> <span>\"</span><span>Taylor</span><span>\"</span><span>,</span> <span>\"</span><span>Anderson</span><span>\"</span><span>],</span> \n            <span>addresses</span><span>:</span> <span>[</span> \n                <span>\"</span><span>123 Main St, Anytown</span><span>\"</span><span>,</span> <span>\"</span><span>45 Oak Ave, Smallville</span><span>\"</span><span>,</span> <span>\"</span><span>78 Pine Ln, Big City</span><span>\"</span><span>,</span> \n                <span>\"</span><span>90 Maple Rd, Suburbia</span><span>\"</span><span>,</span> <span>\"</span><span>11 Elm Dr, Metropolis</span><span>\"</span><span>,</span> <span>\"</span><span>22 Birch Blvd, Village</span><span>\"</span><span>,</span> \n                <span>\"</span><span>33 Cedar Ct, Hamlet</span><span>\"</span><span>,</span> <span>\"</span><span>44 Spruce Sq, Town</span><span>\"</span><span>,</span> <span>\"</span><span>55 Willow Wy, County</span><span>\"</span><span>,</span> \n                <span>\"</span><span>66 Poplar Pk, District</span><span>\"</span> \n            <span>],</span> \n            <span>productNames</span><span>:</span> <span>[</span><span>\"</span><span>Laptop</span><span>\"</span><span>,</span> <span>\"</span><span>Monitor</span><span>\"</span><span>,</span> <span>\"</span><span>Keyboard</span><span>\"</span><span>,</span> <span>\"</span><span>Mouse</span><span>\"</span><span>,</span> <span>\"</span><span>Webcam</span><span>\"</span><span>,</span> <span>\"</span><span>Headset</span><span>\"</span><span>,</span> <span>\"</span><span>Router</span><span>\"</span><span>,</span> <span>\"</span><span>Speaker</span><span>\"</span><span>,</span> <span>\"</span><span>Printer</span><span>\"</span><span>,</span> <span>\"</span><span>Scanner</span><span>\"</span><span>]</span> \n        <span>};</span> \n \n        <span>function</span> <span>generateRandomData</span><span>()</span> <span>{</span> \n            <span>const</span> <span>data</span> <span>=</span> <span>[];</span> \n            <span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>NUM_RECORDS</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span> \n                <span>const</span> <span>commandRef</span> <span>=</span> <span>crypto</span><span>.</span><span>randomUUID</span><span>();</span> <span>// Unique link ID</span> \n \n                <span>const</span> <span>year</span> <span>=</span> <span>1960</span> <span>+</span> <span>Math</span><span>.</span><span>floor</span><span>(</span><span>Math</span><span>.</span><span>random</span><span>()</span> <span>*</span> <span>30</span><span>);</span> \n                <span>const</span> <span>month</span> <span>=</span> <span>String</span><span>(</span><span>Math</span><span>.</span><span>floor</span><span>(</span><span>Math</span><span>.</span><span>random</span><span>()</span> <span>*</span> <span>12</span><span>)</span> <span>+</span> <span>1</span><span>).</span><span>padStart</span><span>(</span><span>2</span><span>,</span> <span>'</span><span>0</span><span>'</span><span>);</span> \n                <span>const</span> <span>day</span> <span>=</span> <span>String</span><span>(</span><span>Math</span><span>.</span><span>floor</span><span>(</span><span>Math</span><span>.</span><span>random</span><span>()</span> <span>*</span> <span>28</span><span>)</span> <span>+</span> <span>1</span><span>).</span><span>padStart</span><span>(</span><span>2</span><span>,</span> <span>'</span><span>0</span><span>'</span><span>);</span> \n                <span>const</span> <span>dob</span> <span>=</span> <span>`</span><span>${</span><span>year</span><span>}</span><span>-</span><span>${</span><span>month</span><span>}</span><span>-</span><span>${</span><span>day</span><span>}</span><span>`</span><span>;</span> \n \n                <span>data</span><span>.</span><span>push</span><span>({</span> \n                    <span>firstName</span><span>:</span> <span>MOCK_DATA</span><span>.</span><span>firstNames</span><span>[</span><span>i</span><span>],</span> \n                    <span>lastName</span><span>:</span> <span>MOCK_DATA</span><span>.</span><span>lastNames</span><span>[</span><span>i</span><span>],</span> \n                    <span>dob</span><span>:</span> <span>dob</span><span>,</span> \n                    <span>address</span><span>:</span> <span>MOCK_DATA</span><span>.</span><span>addresses</span><span>[</span><span>i</span><span>],</span> \n                    <span>commandList</span><span>:</span> <span>commandRef</span><span>,</span> \n                    <span>// Command data</span> \n                    <span>productID</span><span>:</span> <span>`PROD-</span><span>${</span><span>Math</span><span>.</span><span>floor</span><span>(</span><span>Math</span><span>.</span><span>random</span><span>()</span> <span>*</span> <span>9000</span><span>)</span> <span>+</span> <span>1000</span><span>}</span><span>`</span><span>,</span> \n                    <span>productName</span><span>:</span> <span>MOCK_DATA</span><span>.</span><span>productNames</span><span>[</span><span>i</span><span>]</span> <span>+</span> <span>'</span><span> Pro</span><span>'</span><span>,</span> \n                    <span>commandNumber</span><span>:</span> <span>commandRef</span> \n                <span>});</span> \n            <span>}</span> \n            <span>return</span> <span>data</span><span>;</span> \n        <span>}</span> \n \n        <span>async</span> <span>function</span> <span>initDb</span><span>(</span><span>reset</span> <span>=</span> <span>false</span><span>)</span> <span>{</span> \n            <span>statusDiv</span><span>.</span><span>textContent</span> <span>=</span> <span>\"</span><span>Loading WebAssembly SQLite engine...</span><span>\"</span><span>;</span> \n            <span>try</span> <span>{</span> \n                <span>if </span><span>(</span><span>!</span><span>SQL</span><span>)</span> <span>{</span> \n                    <span>SQL</span> <span>=</span> <span>await</span> <span>initSqlJs</span><span>({</span> <span>locateFile</span><span>:</span> <span>file</span> <span>=&gt;</span> <span>`https://cdnjs.cloudflare.com/ajax/libs/sql.js/1.10.3/</span><span>${</span><span>file</span><span>}</span><span>`</span> <span>});</span> \n                <span>}</span> \n \n                <span>if </span><span>(</span><span>db</span><span>)</span> <span>{</span> \n                    <span>db</span><span>.</span><span>close</span><span>();</span> \n                <span>}</span> \n \n                <span>db</span> <span>=</span> <span>new</span> <span>SQL</span><span>.</span><span>Database</span><span>();</span> \n \n                <span>// --- A. Create Users Table ---</span> \n                <span>db</span><span>.</span><span>run</span><span>(</span><span>` \n                    CREATE TABLE Users ( \n                        ID INTEGER PRIMARY KEY, \n                        FirstName TEXT NOT NULL, \n                        LastName TEXT NOT NULL, \n                        DateOfBirth TEXT NOT NULL,  \n                        Address TEXT, \n                        CommandList TEXT UNIQUE NOT NULL \n                    ); \n                `</span><span>);</span> \n \n                <span>db</span><span>.</span><span>run</span><span>(</span><span>` \n                    CREATE TABLE Commands ( \n                        ProductID TEXT PRIMARY KEY, \n                        ProductName TEXT NOT NULL, \n                        CommandNumber TEXT NOT NULL, \n                        FOREIGN KEY (CommandNumber) REFERENCES Users(CommandList) \n                    ); \n                `</span><span>);</span> \n \n                <span>const</span> <span>mockData</span> <span>=</span> <span>generateRandomData</span><span>();</span> \n                <span>mockData</span><span>.</span><span>forEach</span><span>(</span><span>item</span> <span>=&gt;</span> <span>{</span> \n                    <span>db</span><span>.</span><span>run</span><span>(</span><span>\"</span><span>INSERT INTO Users (FirstName, LastName, DateOfBirth, Address, CommandList) VALUES (?, ?, ?, ?, ?)</span><span>\"</span><span>,</span> \n                        <span>[</span><span>item</span><span>.</span><span>firstName</span><span>,</span> <span>item</span><span>.</span><span>lastName</span><span>,</span> <span>item</span><span>.</span><span>dob</span><span>,</span> <span>item</span><span>.</span><span>address</span><span>,</span> <span>item</span><span>.</span><span>commandList</span><span>]);</span> \n \n                    <span>db</span><span>.</span><span>run</span><span>(</span><span>\"</span><span>INSERT INTO Commands (ProductID, ProductName, CommandNumber) VALUES (?, ?, ?)</span><span>\"</span><span>,</span> \n                        <span>[</span><span>item</span><span>.</span><span>productID</span><span>,</span> <span>item</span><span>.</span><span>productName</span><span>,</span> <span>item</span><span>.</span><span>commandNumber</span><span>]);</span> \n                <span>});</span> \n \n                <span>statusDiv</span><span>.</span><span>textContent</span> <span>=</span> <span>reset</span>  \n                    <span>?</span> <span>`✅ Database Reset &amp; </span><span>${</span><span>NUM_RECORDS</span><span>}</span><span> records inserted successfully!`</span> \n                    <span>:</span> <span>`✅ Database initialized with </span><span>${</span><span>NUM_RECORDS</span><span>}</span><span> records.`</span><span>;</span> \n                <span>statusDiv</span><span>.</span><span>classList</span><span>.</span><span>remove</span><span>(</span><span>'</span><span>text-red-700</span><span>'</span><span>);</span> \n                <span>statusDiv</span><span>.</span><span>classList</span><span>.</span><span>add</span><span>(</span><span>'</span><span>text-green-700</span><span>'</span><span>);</span> \n \n                <span>displayUsers</span><span>();</span> \n \n            <span>}</span> <span>catch </span><span>(</span><span>error</span><span>)</span> <span>{</span> \n                <span>statusDiv</span><span>.</span><span>textContent</span> <span>=</span> <span>`❌ Error initializing database: </span><span>${</span><span>error</span><span>.</span><span>message</span><span>}</span><span>`</span><span>;</span> \n                <span>statusDiv</span><span>.</span><span>classList</span><span>.</span><span>remove</span><span>(</span><span>'</span><span>text-green-700</span><span>'</span><span>);</span> \n                <span>statusDiv</span><span>.</span><span>classList</span><span>.</span><span>add</span><span>(</span><span>'</span><span>text-red-700</span><span>'</span><span>);</span> \n                <span>console</span><span>.</span><span>error</span><span>(</span><span>\"</span><span>DB Initialization Error:</span><span>\"</span><span>,</span> <span>error</span><span>);</span> \n            <span>}</span> \n        <span>}</span> \n \n \n        <span>function</span> <span>renderResults</span><span>(</span><span>results</span><span>)</span> <span>{</span> \n            <span>resultDiv</span><span>.</span><span>innerHTML</span> <span>=</span> <span>''</span><span>;</span> \n \n            <span>if </span><span>(</span><span>!</span><span>results</span> <span>||</span> <span>results</span><span>.</span><span>length</span> <span>===</span> <span>0</span><span>)</span> <span>{</span> \n                <span>resultDiv</span><span>.</span><span>innerHTML</span> <span>=</span> <span>'</span><span>&lt;p class=\"p-4 text-orange-500\"&gt;Query executed successfully, but returned no rows.&lt;/p&gt;</span><span>'</span><span>;</span> \n                <span>return</span><span>;</span> \n            <span>}</span> \n \n            <span>const</span> <span>table</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>table</span><span>'</span><span>);</span> \n            <span>table</span><span>.</span><span>classList</span><span>.</span><span>add</span><span>(</span><span>'</span><span>min-w-full</span><span>'</span><span>,</span> <span>'</span><span>divide-y</span><span>'</span><span>,</span> <span>'</span><span>divide-gray-200</span><span>'</span><span>);</span> \n \n            <span>const</span> <span>thead</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>thead</span><span>'</span><span>);</span> \n            <span>const</span> <span>headerRow</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>tr</span><span>'</span><span>);</span> \n            <span>results</span><span>[</span><span>0</span><span>].</span><span>columns</span><span>.</span><span>forEach</span><span>(</span><span>col</span> <span>=&gt;</span> <span>{</span> \n                <span>const</span> <span>th</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>th</span><span>'</span><span>);</span> \n                <span>th</span><span>.</span><span>textContent</span> <span>=</span> <span>col</span><span>;</span> \n                <span>headerRow</span><span>.</span><span>appendChild</span><span>(</span><span>th</span><span>);</span> \n            <span>});</span> \n            <span>thead</span><span>.</span><span>appendChild</span><span>(</span><span>headerRow</span><span>);</span> \n            <span>table</span><span>.</span><span>appendChild</span><span>(</span><span>thead</span><span>);</span> \n \n           <span>const</span> <span>tbody</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>tbody</span><span>'</span><span>);</span> \n            <span>results</span><span>[</span><span>0</span><span>].</span><span>values</span><span>.</span><span>forEach</span><span>(</span><span>row</span> <span>=&gt;</span> <span>{</span> \n                <span>const</span> <span>tr</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>tr</span><span>'</span><span>);</span> \n                <span>row</span><span>.</span><span>forEach</span><span>(</span><span>cell</span> <span>=&gt;</span> <span>{</span> \n                    <span>const</span> <span>td</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>td</span><span>'</span><span>);</span> \n                    <span>td</span><span>.</span><span>textContent</span> <span>=</span> <span>cell</span> <span>===</span> <span>null</span> <span>?</span> <span>'</span><span>NULL</span><span>'</span> <span>:</span> <span>cell</span><span>;</span> \n                    <span>tr</span><span>.</span><span>appendChild</span><span>(</span><span>td</span><span>);</span> \n                <span>});</span> \n                <span>tbody</span><span>.</span><span>appendChild</span><span>(</span><span>tr</span><span>);</span> \n            <span>});</span> \n            <span>table</span><span>.</span><span>appendChild</span><span>(</span><span>tbody</span><span>);</span> \n \n            <span>resultDiv</span><span>.</span><span>appendChild</span><span>(</span><span>table</span><span>);</span> \n        <span>}</span> \n \n        <span>function</span> <span>runQuery</span><span>(</span><span>sql</span><span>,</span> <span>title</span><span>)</span> <span>{</span> \n            <span>resultTitle</span><span>.</span><span>textContent</span> <span>=</span> <span>title</span><span>;</span> \n            <span>try</span> <span>{</span> \n                <span>if </span><span>(</span><span>!</span><span>db</span><span>)</span> <span>{</span> \n                    <span>resultDiv</span><span>.</span><span>innerHTML</span> <span>=</span> <span>'</span><span>&lt;p class=\"p-4 text-red-500\"&gt;Database not initialized. Please click \"Reset Database &amp; Reload Data\".&lt;/p&gt;</span><span>'</span><span>;</span> \n                    <span>return</span><span>;</span> \n                <span>}</span> \n                <span>const</span> <span>results</span> <span>=</span> <span>db</span><span>.</span><span>exec</span><span>(</span><span>sql</span><span>);</span> \n                <span>renderResults</span><span>(</span><span>results</span><span>);</span> \n            <span>}</span> <span>catch </span><span>(</span><span>error</span><span>)</span> <span>{</span> \n                <span>resultDiv</span><span>.</span><span>innerHTML</span> <span>=</span> <span>`&lt;div class=\"p-4 bg-red-100 text-red-700 rounded-lg\"&gt;❌ SQL Error: </span><span>${</span><span>error</span><span>.</span><span>message</span><span>}</span><span>&lt;/div&gt;`</span><span>;</span> \n            <span>}</span> \n        <span>}</span> \n \n \n        <span>function</span> <span>displayUsers</span><span>()</span> <span>{</span> \n            <span>const</span> <span>sql</span> <span>=</span> <span>\"</span><span>SELECT ID, FirstName, LastName, DateOfBirth, Address, CommandList FROM Users</span><span>\"</span><span>;</span> \n            <span>runQuery</span><span>(</span><span>sql</span><span>,</span> <span>\"</span><span>Table: Users (All Fields)</span><span>\"</span><span>);</span> \n        <span>}</span> \n \n        <span>function</span> <span>displayJoinedData</span><span>()</span> <span>{</span> \n            <span>const</span> <span>sql</span> <span>=</span> <span>` \n                SELECT \n                    U.ID, \n                    U.FirstName, \n                    U.LastName, \n                    C.ProductName, \n                    C.ProductID, \n                    U.CommandList as CommandRef \n                FROM \n                    Users U \n                INNER JOIN \n                    Commands C ON U.CommandList = C.CommandNumber \n                ORDER BY \n                    U.ID; \n            `</span><span>;</span> \n            <span>runQuery</span><span>(</span><span>sql</span><span>,</span> <span>\"</span><span>Joined Data: Users linked to Commands</span><span>\"</span><span>);</span> \n        <span>}</span> \n \n        <span>function</span> <span>executeCustomQuery</span><span>()</span> <span>{</span> \n            <span>const</span> <span>sql</span> <span>=</span> <span>document</span><span>.</span><span>getElementById</span><span>(</span><span>'</span><span>sqlInput</span><span>'</span><span>).</span><span>value</span><span>.</span><span>trim</span><span>();</span> \n            <span>if </span><span>(</span><span>!</span><span>sql</span><span>)</span> <span>{</span> \n                <span>resultDiv</span><span>.</span><span>innerHTML</span> <span>=</span> <span>'</span><span>&lt;p class=\"p-4 text-orange-500\"&gt;Please enter an SQL query to execute.&lt;/p&gt;</span><span>'</span><span>;</span> \n                <span>return</span><span>;</span> \n            <span>}</span> \n            <span>runQuery</span><span>(</span><span>sql</span><span>,</span> <span>\"</span><span>Custom Query Results</span><span>\"</span><span>);</span> \n        <span>}</span> \n \n        <span>window</span><span>.</span><span>onload</span> <span>=</span> <span>()</span> <span>=&gt;</span> <span>{</span> \n            <span>initDb</span><span>();</span> \n        <span>};</span> \n \n    <span>&lt;/script&gt;</span> \n<span>&lt;/body&gt;</span> \n<span>&lt;/html&gt;</span> \n</code></pre> \n \n</div> \n \n \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Feeegkrl4o8pk42yjhty6.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Feeegkrl4o8pk42yjhty6.png\" alt=\"\" width=\"800\" height=\"478\"></a></p> \n \n<h2> \n   \n   \n  Text-2-SQL Tests \n</h2> \n \n<p>Now that the database and our data are readyn we will start using the solution.</p> \n \n<ul> \n<li>What is extremely convenient is the way Vanna.ai supports setup: depending on the specific LLM provider (like Ollama) and database type (like SQLite) you select, their documentation provides the exact installation requirements and configuration snippets you need, making the initial setup frictionless. \n</li> \n</ul> \n \n<div> \n<pre><code>pip <span>install</span> <span>'vanna[fastapi,httpx,ollama]'</span> \n</code></pre> \n \n</div> \n \n \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F7x9lbmcwamylgyjchj8i.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F7x9lbmcwamylgyjchj8i.png\" alt=\"\" width=\"800\" height=\"284\"></a></p> \n \n<ul> \n<li>The site provides the full sample code to build an application, just copy/paste it! \n</li> \n</ul> \n \n<div> \n<pre><code><span># vanna-ai-app.py \n# All imports at the top \n</span><span>from</span> <span>vanna</span> <span>import</span> <span>Agent</span> \n<span>from</span> <span>vanna.core.registry</span> <span>import</span> <span>ToolRegistry</span> \n<span>from</span> <span>vanna.core.user</span> <span>import</span> <span>UserResolver</span><span>,</span> <span>User</span><span>,</span> <span>RequestContext</span> \n<span>from</span> <span>vanna.tools</span> <span>import</span> <span>RunSqlTool</span><span>,</span> <span>VisualizeDataTool</span> \n<span>from</span> <span>vanna.tools.agent_memory</span> <span>import</span> <span>SaveQuestionToolArgsTool</span><span>,</span> <span>SearchSavedCorrectToolUsesTool</span><span>,</span> <span>SaveTextMemoryTool</span> \n<span>from</span> <span>vanna.servers.fastapi</span> <span>import</span> <span>VannaFastAPIServer</span> \n<span>from</span> <span>vanna.integrations.ollama</span> <span>import</span> <span>OllamaLlmService</span> \n<span>from</span> <span>vanna.integrations.sqlite</span> <span>import</span> <span>SqliteRunner</span> \n<span>from</span> <span>vanna.integrations.local.agent_memory</span> <span>import</span> <span>DemoAgentMemory</span> \n \n<span># Configure your LLM \n</span><span>llm</span> <span>=</span> <span>OllamaLlmService</span><span>(</span> \n    <span>model</span><span>=</span><span>\"</span><span>gpt-oss:latest</span><span>\"</span><span>,</span> \n    <span>host</span><span>=</span><span>\"</span><span>http://localhost:11434</span><span>\"</span> \n<span>)</span> \n \n<span># Configure your database \n</span><span>db_tool</span> <span>=</span> <span>RunSqlTool</span><span>(</span> \n    <span>sql_runner</span><span>=</span><span>SqliteRunner</span><span>(</span><span>database_path</span><span>=</span><span>\"</span><span>./users_commands.db</span><span>\"</span><span>)</span> \n<span>)</span> \n \n<span># Configure your agent memory \n</span><span>agent_memory</span> <span>=</span> <span>DemoAgentMemory</span><span>(</span><span>max_items</span><span>=</span><span>1000</span><span>)</span> \n \n<span># Configure user authentication \n</span><span>class</span> <span>SimpleUserResolver</span><span>(</span><span>UserResolver</span><span>):</span> \n    <span>async</span> <span>def</span> <span>resolve_user</span><span>(</span><span>self</span><span>,</span> <span>request_context</span><span>:</span> <span>RequestContext</span><span>)</span> <span>-&gt;</span> <span>User</span><span>:</span> \n        <span>user_email</span> <span>=</span> <span>request_context</span><span>.</span><span>get_cookie</span><span>(</span><span>'</span><span>vanna_email</span><span>'</span><span>)</span> <span>or</span> <span>'</span><span>guest@example.com</span><span>'</span> \n        <span>group</span> <span>=</span> <span>'</span><span>admin</span><span>'</span> <span>if</span> <span>user_email</span> <span>==</span> <span>'</span><span>admin@example.com</span><span>'</span> <span>else</span> <span>'</span><span>user</span><span>'</span> \n        <span>return</span> <span>User</span><span>(</span><span>id</span><span>=</span><span>user_email</span><span>,</span> <span>email</span><span>=</span><span>user_email</span><span>,</span> <span>group_memberships</span><span>=</span><span>[</span><span>group</span><span>])</span> \n \n<span>user_resolver</span> <span>=</span> <span>SimpleUserResolver</span><span>()</span> \n \n<span># Create your agent \n</span><span>tools</span> <span>=</span> <span>ToolRegistry</span><span>()</span> \n<span>tools</span><span>.</span><span>register_local_tool</span><span>(</span><span>db_tool</span><span>,</span> <span>access_groups</span><span>=</span><span>[</span><span>'</span><span>admin</span><span>'</span><span>,</span> <span>'</span><span>user</span><span>'</span><span>])</span> \n<span>tools</span><span>.</span><span>register_local_tool</span><span>(</span><span>SaveQuestionToolArgsTool</span><span>(),</span> <span>access_groups</span><span>=</span><span>[</span><span>'</span><span>admin</span><span>'</span><span>])</span> \n<span>tools</span><span>.</span><span>register_local_tool</span><span>(</span><span>SearchSavedCorrectToolUsesTool</span><span>(),</span> <span>access_groups</span><span>=</span><span>[</span><span>'</span><span>admin</span><span>'</span><span>,</span> <span>'</span><span>user</span><span>'</span><span>])</span> \n<span>tools</span><span>.</span><span>register_local_tool</span><span>(</span><span>SaveTextMemoryTool</span><span>(),</span> <span>access_groups</span><span>=</span><span>[</span><span>'</span><span>admin</span><span>'</span><span>,</span> <span>'</span><span>user</span><span>'</span><span>])</span> \n<span>tools</span><span>.</span><span>register_local_tool</span><span>(</span><span>VisualizeDataTool</span><span>(),</span> <span>access_groups</span><span>=</span><span>[</span><span>'</span><span>admin</span><span>'</span><span>,</span> <span>'</span><span>user</span><span>'</span><span>])</span> \n \n<span>agent</span> <span>=</span> <span>Agent</span><span>(</span> \n    <span>llm_service</span><span>=</span><span>llm</span><span>,</span> \n    <span>tool_registry</span><span>=</span><span>tools</span><span>,</span> \n    <span>user_resolver</span><span>=</span><span>user_resolver</span><span>,</span> \n    <span>agent_memory</span><span>=</span><span>agent_memory</span> \n<span>)</span> \n \n<span># Run the server \n</span><span>server</span> <span>=</span> <span>VannaFastAPIServer</span><span>(</span><span>agent</span><span>)</span> \n<span>server</span><span>.</span><span>run</span><span>()</span>  <span># Access at http://localhost:8000 \n</span></code></pre> \n \n</div> \n \n \n \n<ul> \n<li>The sample application from the site uses “gpt-oss” LLM, but you can put whatever LLM you choose. I tested both “granite” and “gpt-oss”. \n</li> \n</ul> \n \n<div> \n<pre><code><span># Configure your LLM \n</span><span>llm</span> <span>=</span> <span>OllamaLlmService</span><span>(</span> \n    <span>model</span><span>=</span><span>\"</span><span>granite4:latest</span><span>\"</span><span>,</span> \n    <span>host</span><span>=</span><span>\"</span><span>http://localhost:11434</span><span>\"</span> \n<span>)</span> \n</code></pre> \n \n</div> \n \n \n \n<ul> \n<li>As the interface is generated by “FastAPI” the defaut port number is “<a href=\"http://localhost:8000\">http://localhost:8000”</a> but you can adapt it to your needs and for instance run different instance by changing the port number. \n</li> \n</ul> \n \n<div> \n<pre><code><span># Run the server \n</span><span>server</span> <span>=</span> <span>VannaFastAPIServer</span><span>(</span><span>agent</span><span>)</span> \n \n<span># Setting port=9000  \n</span><span>server</span><span>.</span><span>run</span><span>(</span><span>port</span><span>=</span><span>9000</span><span>)</span> \n</code></pre> \n \n</div> \n \n \n \n<ul> \n<li>During my initial testing, I found that running queries using both the Granite and GPT-OSS models consistently yielded accurate answers and correctly generated SQL queries. While I wasn’t aiming to conduct rigorous, side-by-side benchmarking, this functionality test confirms the core power of using Vanna.ai with these locally hosted open-source models. The primary goal was to validate the integration and core functionality, and in that regard, the results were highly successful. We leave it to the user community to dive deeper into performance comparisons, extended tests, and complex benchmarking scenarios. The first screens come from “gpt-oss”.</li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fc7569ep9sglm6ka2cy2l.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fc7569ep9sglm6ka2cy2l.png\" alt=\"\" width=\"800\" height=\"430\"></a></p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5llw33l0x7j5m06av2eb.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5llw33l0x7j5m06av2eb.png\" alt=\"\" width=\"800\" height=\"519\"></a></p> \n \n<ul> \n<li>The following are generated queries and outputs from granite 🪨</li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9eafeagt82hqknrt370t.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9eafeagt82hqknrt370t.png\" alt=\"\" width=\"800\" height=\"119\"></a></p> \n \n<ul> \n<li>The output ⬇️</li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft9dxzipxx3mcxgdltch9.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft9dxzipxx3mcxgdltch9.png\" alt=\"\" width=\"800\" height=\"322\"></a></p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmbunwb2sb1insa9r87hb.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmbunwb2sb1insa9r87hb.png\" alt=\"\" width=\"800\" height=\"383\"></a></p> \n \n<ul> \n<li>A very handy feature is that every query generates a CSV file, meaning the user not only gets the beautifully presented output on the UI but also receives a tangible, exportable data file as a direct result. We leave it to the user community to dive deeper into performance comparisons, extended tests, and complex benchmarking scenarios. \n</li> \n</ul> \n \n<div> \n<pre><code>ID,FirstName,LastName,DateOfBirth,Address,CommandList \n10,Henry,Wallace,1962-04-18,\"79093 Danielle Haven Suite 823, New Kevinton, PR 23752\",83c466e4-239f-4844-8a79-6286be0edb30 \n</code></pre> \n \n</div> \n \n \n \n \n \n<div> \n<pre><code>ID,FirstName,LastName,DateOfBirth,Address,CommandList \n1,David,Frank,1983-03-22,\"3973 Carmen Gateway Suite 371, Timothytown, NY 51959\",173e986d-77b0-4c6e-9314-52813419f108 \n2,Sydney,Middleton,1981-09-24,\"4080 Regina Lake Apt. 187, Kathrynchester, MN 66872\",5bd1cf9a-2cfc-46e5-b150-77c368efaccb \n3,Joshua,Holder,2000-06-19,\"0072 Sanchez Hollow, Port Jackfurt, NC 88639\",68009684-34a9-4be0-8f67-ef26155d6cfd \n4,Robert,Walker,1968-01-10,\"USS Lee, FPO AP 42065\",dc60fd66-f59b-486b-8f04-4162bdf9c99e \n5,Douglas,Johnson,1999-08-18,\"51189 Jacqueline Shores, East Denise, ME 11103\",80c13c6f-c5f7-4df7-abfe-f91ced3e1237 \n6,James,Hernandez,1976-10-28,\"Unit 7218 Box 1614, DPO AP 28470\",9b4dee77-37ad-4fa4-8672-a7c053e71429 \n7,Diana,Thomas,2004-02-29,\"84300 Vaughn Crossroad, Port Sharon, VA 67416\",7062b085-18b4-40c3-8e4e-c3bd6c214e08 \n8,Jamie,Torres,1961-06-22,\"857 Pennington Flats Suite 672, Rushville, DC 37245\",31725920-e0c3-4fac-bd42-31d113e6e107 \n9,Renee,Rice,2006-09-04,\"963 Marvin Underpass, Rebeccachester, KS 45025\",8536b45b-3502-480a-a9be-89189e68478e \n10,Henry,Wallace,1962-04-18,\"79093 Danielle Haven Suite 823, New Kevinton, PR 23752\",83c466e4-239f-4844-8a79-6286be0edb30 \n</code></pre> \n \n</div> \n \n \n \n<p>That’s a wrap! ✌️</p> \n \n<h2> \n   \n   \n  Conclusion \n</h2> \n \n<p>In conclusion, I guess these tests validated Vanna.ai’s strength as a comprehensive Text-to-SQL solution, moving beyond simple query generation. This entire setup, running within a custom-ported FastAPI server, demonstrated core functionalities: accurate SQL generation from natural language questions, seamless execution against a database tables, rich visual output on the UI, and the practical utility of immediate CSV export. The ease of configuration, coupled with the accuracy achieved using either local or cloud based models with a variety of database engines. I really give a “thumb-up” to Venna.ai’s solution.</p> \n \n<p>Thanks for reading! 🤗</p> \n \n<h2> \n   \n   \n  Links \n</h2> \n \n<ul> \n<li>Vanna.ai GitHub: <a href=\"https://github.com/vanna-ai/vanna\">https://github.com/vanna-ai/vanna</a> \n</li> \n<li>Vanna.ai site: <a href=\"https://vanna.ai/\">https://vanna.ai/</a> \n</li> \n</ul>",
      "summary": "Yet another Text2SQL exercise with Vanna.ai, Ollama, Granite and gpt-oss. \n \n \n \n \n   \n   \n  Introduction - What is Vanna.ai? \n \n \n \n \nIn the rapidly evolving world of data analytics, the search for the perfect Text-to-SQL generation tool often feels like a quest for the ‘Holy Grail’. I recently stumbled upon Vanna.ai, and it immediately stood out. At its core, Vanna.ai is a powerful agent designed to turn natural language questions into data insights. It achieves this by acting as a translator:",
      "publishedAt": "2025-12-02T14:15:40.000Z",
      "author": "Alain Airom",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "security_incident",
      "score": 13.980301382998539,
      "ingestedAt": "2025-12-02T14:44:03.159Z",
      "tags": [
        "code_review",
        "documentation",
        "agents",
        "ide",
        "testing",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09b26e95",
      "title": "Understanding Amazon Q Custom Agents: Concepts, Architecture & Inner Workings",
      "url": "https://dev.to/aws-builders/understanding-amazon-q-custom-agents-concepts-architecture-inner-workings-362",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F79ww1qnd9fijumax5sjc.webp\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>👋 Hey there, tech enthusiasts! </p>  \n  \n<p>I'm Sarvar, a Cloud Architect with a passion for transforming complex technological challenges into elegant solutions. With extensive experience spanning Cloud Operations (AWS &amp; Azure), Data Operations, Analytics, DevOps, and Generative AI, I've had the privilege of architecting solutions for global enterprises that drive real business impact. Through this article series, I'm excited to share practical insights, best practices, and hands-on experiences from my journey in the tech world. Whether you're a seasoned professional or just starting out, I aim to break down complex concepts into digestible pieces that you can apply in your projects.</p>  \n  \n<p>Let's dive in and explore the fascinating world of cloud technology together! 🚀</p>  \n  \n  \n  \n  \n<p>Imagine Sarah, a DevOps engineer at a growing startup. Every day, she receives dozens of questions from developers:</p>  \n  \n<blockquote>  \n<p>“What's the status of our production deployment?”<br>  \n“Can you check the database performance metrics?”<br>  \n“How much are we spending on EC2 this month?”</p>  \n</blockquote>  \n  \n<p>Sarah spends hours manually checking AWS consoles, running CLI commands, and compiling reports.</p>  \n  \n<p>What if Sarah could create an intelligent assistant that automatically handles these requests?</p>  \n  \n<p><strong>Enter Amazon Q Custom Agents:</strong> AI-powered assistants that can understand natural language, access your AWS resources, and provide intelligent responses.</p>  \n  \n<p>Today, we'll explore how to build these powerful agents, covering everything from basic concepts to advanced integrations.</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  <strong>What We're Covering Today</strong>  \n</h2>  \n  \n<ul>  \n<li>Understanding Amazon Q Custom Agents</li>  \n<li>Types of Custom Agents</li>  \n<li>What is a Knowledge Base?</li>  \n<li>What is an Index?</li>  \n<li>Advanced Indexing Strategies</li>  \n<li>Storage &amp; Integration Options for Knowledge Bases</li>  \n<li>Custom Agent Deployment Types</li>  \n<li>How Amazon Q Custom Agent Architecture Works</li>  \n<li>Pricing Breakdown</li>  \n<li>Conclusion</li>  \n</ul>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>Understanding Amazon Q Custom Agents</strong>  \n</h1>  \n  \n<p>Amazon Q Custom Agents are intelligent, model-driven assistants designed to adapt to your organization’s unique workflows and data. They combine large language models with your internal knowledge sources and AWS services to deliver accurate, contextual, and actionable responses.</p>  \n  \n<p>The core structure of an Amazon Q Business Application is organized as follows:<br>  \n</p>  \n  \n<div>  \n<pre><code>Amazon Q Business Application  \n└── Custom Agent  \n    ├── Instructions (System Prompts)  \n    ├── Guardrails (Content Filtering)  \n    ├── Knowledge Bases  \n    │   └── Amazon Bedrock Knowledge Base  \n    │       ├── Data Source (S3)  \n    │       ├── Vector Store (OpenSearch Serverless)  \n    │       └── Embedding Model (Bedrock)  \n    └── Action Groups  \n        ├── Lambda Functions  \n        └── OpenAPI Schema Definitions  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n  \n  \n  \n<h2>  \n    \n    \n  <strong>Types of Custom Agents</strong>  \n</h2>  \n  \n<h3>  \n    \n    \n  <strong>1. Knowledge Base Agents</strong>  \n</h3>  \n  \n<ul>  \n<li>  \n<strong>What it does:</strong> Answers questions using documents, wikis, and internal knowledge sources.</li>  \n<li>  \n<strong>Best for:</strong> FAQ systems, documentation queries, policy lookups.</li>  \n<li>  \n<strong>Real-time example:</strong> <em>“What’s our company’s vacation policy?”</em> → Searches HR documents and returns accurate details.</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  <strong>2. Action Agents</strong>  \n</h3>  \n  \n<ul>  \n<li>  \n<strong>What it does:</strong> Executes tasks through API calls and AWS service integrations.</li>  \n<li>  \n<strong>Best for:</strong> Operational tasks, resource provisioning, automated workflows.</li>  \n<li>  \n<strong>Real-time example:</strong> <em>“Scale up our production environment”</em> → Increases EC2 capacity and updates load balancers.</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  <strong>3. Hybrid Agents</strong>  \n</h3>  \n  \n<ul>  \n<li>  \n<strong>What it does:</strong> Combines information retrieval with task execution.</li>  \n<li>  \n<strong>Best for:</strong> Complex workflows requiring both insights and actions.</li>  \n<li>  \n<strong>Real-time example:</strong> <em>“Check database performance and optimize if needed”</em> → Retrieves metrics, analyzes them, and applies tuning.</li>  \n</ul>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>What is a Knowledge Base?</strong>  \n</h1>  \n  \n<p>A Knowledge Base is the brain of a custom agent a centralized repository that serves as its memory, containing all the documents, procedures, and structured information needed to answer queries accurately.</p>  \n  \n<h3>  \n    \n    \n  <strong>Key Components</strong>  \n</h3>  \n  \n<ul>  \n<li>  \n<strong>Documents:</strong> PDFs, Word files, text content, wikis, web pages</li>  \n<li>  \n<strong>Structured Data:</strong> Databases, APIs, config files</li>  \n<li>  \n<strong>Real-time Data:</strong> Monitoring systems, AWS service metrics, logs</li>  \n</ul>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>What is an Index?</strong>  \n</h1>  \n  \n<p>An Index is a structured system that converts your Knowledge Base content into searchable vector representations, allowing the agent to retrieve relevant information quickly and accurately.</p>  \n  \n<h2>  \n    \n    \n  <strong>How Indexing Works</strong>  \n</h2>  \n  \n<ol>  \n<li>  \n<strong>Document Processing</strong> – Splits documents into meaningful chunks</li>  \n<li>  \n<strong>Vectorization</strong> – Converts text into numerical embeddings</li>  \n<li>  \n<strong>Semantic Search</strong> – Understands context instead of relying on keywords</li>  \n<li>  \n<strong>Retrieval</strong> – Returns the most relevant content based on user queries</li>  \n</ol>  \n  \n<h2>  \n    \n    \n  <strong>Example Index Structure</strong>  \n</h2>  \n  \n  \n  \n<div>  \n<pre><code>AWS Documentation Index  \n├── EC2 Best Practices (Vector: [0.2, 0.8, 0.1...])  \n├── S3 Security Guidelines (Vector: [0.5, 0.3, 0.9...])  \n├── Cost Optimization Tips (Vector: [0.1, 0.7, 0.4...])  \n└── Troubleshooting Guides (Vector: [0.8, 0.2, 0.6...])  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>Advanced Indexing Strategies</strong>  \n</h1>  \n  \n<h2>  \n    \n    \n  <strong>1. Vector Embedding Models</strong>  \n</h2>  \n  \n<ul>  \n<li>  \n<strong>Amazon Titan Text Embeddings:</strong> Optimized for English, up to 8,192 tokens</li>  \n<li>  \n<strong>Cohere Embed:</strong> Multilingual support for diverse content</li>  \n<li>  \n<strong>Custom Models:</strong> Fine-tuned vectors for domain-specific knowledge</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  <strong>2. Chunking Strategies</strong>  \n</h2>  \n  \n<ul>  \n<li>  \n<strong>Fixed-size chunking:</strong> 500–1000 tokens</li>  \n<li>  \n<strong>Semantic chunking:</strong> Breaks at logical document boundaries</li>  \n<li>  \n<strong>Overlapping chunks:</strong> 10–20% overlap for better continuity</li>  \n<li>  \n<strong>Hierarchical chunking:</strong> Varies chunk size based on content type</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  <strong>3. Retrieval Methods</strong>  \n</h2>  \n  \n<ul>  \n<li>  \n<strong>Semantic Search:</strong> Context-aware retrieval using vector similarity</li>  \n<li>  \n<strong>Hybrid Search:</strong> Combines keyword + semantic search</li>  \n<li>  \n<strong>Metadata Filtering:</strong> Filters results based on attributes (tags, authors, etc.)</li>  \n<li>  \n<strong>Re-ranking:</strong> Reorders retrieved documents for maximum relevance</li>  \n</ul>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>Storage &amp; Integration Options for Knowledge Bases</strong>  \n</h1>  \n  \n<p>Storage &amp; Integration Options define where your knowledge is stored and how it connects to your custom agent, enabling seamless access to documents, databases, and third-party applications for accurate responses. Below are commonly used storage and integration choices for building robust knowledge bases.</p>  \n  \n<h2>  \n    \n    \n  <strong>Primary Storage Solutions</strong>  \n</h2>  \n  \n<h3>  \n    \n    \n  <strong>1. Amazon S3</strong>  \n</h3>  \n  \n<ul>  \n<li>  \n<strong>Use Case:</strong> Document storage, archives, data lakes</li>  \n<li>  \n<strong>Benefits:</strong> Scalable, cost-effective, easy AWS integration</li>  \n<li>  \n<strong>Best For:</strong> Large document libraries and media files</li>  \n<li>  \n<strong>Pricing:</strong> ~<strong>$0.023/GB per month</strong>  \n</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  <strong>2. Amazon OpenSearch</strong>  \n</h3>  \n  \n<ul>  \n<li>  \n<strong>Use Case:</strong> Real-time search, indexing, analytics</li>  \n<li>  \n<strong>Benefits:</strong> Fast retrieval, advanced search, near real-time indexing</li>  \n<li>  \n<strong>Best For:</strong> Frequently updated or search-intensive environments</li>  \n<li>  \n<strong>Pricing:</strong> Starts at ~<strong>$0.088/hour</strong> (t3.small.search)</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  <strong>3. Amazon RDS / DynamoDB</strong>  \n</h3>  \n  \n<ul>  \n<li>  \n<strong>Use Case:</strong> Structured or transactional data</li>  \n<li>  \n<strong>Benefits:</strong> High performance, fully managed, reliable</li>  \n<li>  \n<strong>Best For:</strong> Profiles, catalogs, operational metrics</li>  \n<li>  \n<p><strong>Pricing:</strong></p>  \n  \n<ul>  \n<li>  \n<strong>RDS:</strong> From ~<strong>$0.017/hour</strong>  \n</li>  \n<li>  \n<strong>DynamoDB:</strong> ~<strong>$0.25 per million reads</strong>  \n</li>  \n</ul>  \n  \n  \n</li>  \n  \n</ul>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  <strong>Integration Options</strong>  \n</h2>  \n  \n<h3>  \n    \n    \n  <strong>Direct Integrations</strong>  \n</h3>  \n  \n<ul>  \n<li>SharePoint Online</li>  \n<li>Salesforce</li>  \n<li>ServiceNow</li>  \n<li>Confluence</li>  \n<li>Jira</li>  \n<li>Microsoft Teams</li>  \n<li>Google Workspace</li>  \n<li>Slack</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  <strong>Custom Integrations</strong>  \n</h3>  \n  \n<ul>  \n<li>REST APIs</li>  \n<li>Database connectors</li>  \n<li>File system crawlers</li>  \n<li>Real-time streams (Kinesis, EventBridge)</li>  \n<li>Git repositories</li>  \n</ul>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>Custom Agent Deployment Types</strong>  \n</h1>  \n  \n<h2>  \n    \n    \n  <strong>1. Single-Region Deployment</strong>  \n</h2>  \n  \n<ul>  \n<li>  \n<strong>Use Case:</strong> Simple, cost-efficient setups</li>  \n<li>  \n<strong>Benefits:</strong> Low latency, minimal complexity</li>  \n<li>  \n<strong>Considerations:</strong> Single point of failure</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  <strong>2. Multi-Region Deployment</strong>  \n</h2>  \n  \n<ul>  \n<li>  \n<strong>Use Case:</strong> Global user base, disaster recovery</li>  \n<li>  \n<strong>Benefits:</strong> High availability, reduced latency globally</li>  \n<li>  \n<strong>Considerations:</strong> Sync overhead, added cost</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  <strong>3. Hybrid Deployment</strong>  \n</h2>  \n  \n<ul>  \n<li>  \n<strong>Use Case:</strong> Compliance or sensitive data</li>  \n<li>  \n<strong>Benefits:</strong> Local control + cloud scalability</li>  \n<li>  \n<strong>Considerations:</strong> Connectivity, governance requirements</li>  \n</ul>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>How Amazon Q Custom Agent Architecture Works</strong>  \n</h1>  \n  \n<p>Let’s understand how an Amazon Q Custom Agent works in real time. The following high-level architecture shows the simple flow of how a request is processed from start to finish. When a user asks a question, the Amazon Q Agent interprets the request and determines whether it needs to retrieve information or perform an action. If the request is informational, the agent searches the connected Knowledge Base, retrieves the most relevant documents, and generates a clear response for the user. If the request requires an operation such as creating a resource or retrieving system data the agent triggers a Lambda function or OpenAPI action to execute the task on AWS services. Once the action is completed, the result is returned to the agent, which then converts it into a user-friendly answer.<br>  \n</p>  \n  \n<div>  \n<pre><code>┌────────────────────────┐  \n│      User Question     │  \n└─────────────┬──────────┘  \n              │  \n              ▼  \n┌────────────────────────┐  \n│      Amazon Q Agent    │  \n└─────────────┬──────────┘  \n              │  \n              ▼  \n┌────────────────────────┐  \n│      Processing Logic  │  \n└─────────────┬──────────┘  \n              │  \n              ▼  \n┌────────────────────────┐        ┌────────────────────────┐  \n│     Document Search    │ ◄ ──── │     Knowledge Base     │  \n└─────────────┬──────────┘        └────────────────────────┘  \n              │  \n              ▼  \n┌────────────────────────┐  \n│    Response Generation │  \n└─────────────┬──────────┘  \n              │  \n       ┌──────┴─────────────────────────────┐  \n       ▼                                    ▼  \n┌────────────────────────┐    ┌────────────────────────┐  \n│       User Answer      │    │    Action Execution    │  \n└────────────────────────┘    └─────────────┬──────────┘  \n                                            │  \n                                            ▼  \n                               ┌────────────────────────┐  \n                               │      AWS Services      │  \n                               └────────────────────────┘  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>This simple flow helps illustrate how the agent intelligently switches between knowledge retrieval and operational execution, allowing it to both answer questions and perform real-world tasks seamlessly.</p>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>Pricing Breakdown</strong>  \n</h1>  \n  \n<p><em>Important:</em> All these prices were collected using the AWS Pricing Calculator MCP Server integrated with Amazon Q Pro. If this sounds interesting, follow this guide: <a href=\"https://dev.to/aws-builders/real-time-aws-cost-estimation-using-the-pricing-mcp-server-and-amazon-q-cli-1nc6\">Link</a></p>  \n  \n<h2>  \n    \n    \n  <strong>Amazon Q Business Pricing</strong>  \n</h2>  \n  \n<ul>  \n<li>  \n<strong>Pro Tier</strong>: $20 per user per month</li>  \n<li>  \n<strong>Lite Tier</strong>: $3 per user per month</li>  \n<li>  \n<strong>Includes</strong>: Agent interactions, knowledge base queries, basic actions</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  <strong>Knowledge Base Components</strong>  \n</h2>  \n  \n<ul>  \n<li>  \n<strong>Document Processing</strong>: $0.10 per 1,000 documents</li>  \n<li>  \n<strong>Vector Storage</strong>: $0.30 per GB per month</li>  \n<li>  \n<strong>Query Processing</strong>: $0.004 per query</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  <strong>Storage Costs (Monthly)</strong>  \n</h2>  \n  \n<ul>  \n<li>  \n<strong>S3 Standard</strong>: $0.023 per GB</li>  \n<li>  \n<strong>OpenSearch</strong>: $0.088/hour (t3.small) ≈ $63/month</li>  \n<li>  \n<strong>RDS</strong>: Starting at $12/month (db.t3.micro)</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  <strong>Integration Costs</strong>  \n</h2>  \n  \n<ul>  \n<li>  \n<strong>API Gateway</strong>: $3.50 per million API calls</li>  \n<li>  \n<strong>Lambda</strong>: $0.20 per 1 million requests</li>  \n<li>  \n<strong>CloudWatch</strong>: $0.30 per GB ingested</li>  \n</ul>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>Advanced Features and Integrations</strong>  \n</h1>  \n  \n<h2>  \n    \n    \n  <strong>1. Multi-Modal Capabilities</strong>  \n</h2>  \n  \n<ul>  \n<li>  \n<strong>Text Processing:</strong> Documents, emails, chat logs</li>  \n<li>  \n<strong>Image Analysis:</strong> Diagrams, screenshots, charts</li>  \n<li>  \n<strong>Voice Integration:</strong> Speech-to-text for voice-based queries</li>  \n</ul>  \n  \n<h1>  \n    \n    \n  <strong>2. Security and Compliance</strong>  \n</h1>  \n  \n<ul>  \n<li>  \n<strong>Identity Integration:</strong> AWS IAM, Active Directory, SAML</li>  \n<li>  \n<strong>Data Encryption:</strong> Encryption at rest and in transit</li>  \n<li>  \n<strong>Access Controls:</strong> Role-based permissions, data-level filtering</li>  \n<li>  \n<strong>Audit Logging:</strong> Full interaction history for compliance</li>  \n</ul>  \n  \n<h1>  \n    \n    \n  <strong>3. Enterprise Integrations</strong>  \n</h1>  \n  \n<ul>  \n<li>  \n<strong>Slack / Microsoft Teams:</strong> Direct chat-based interactions</li>  \n<li>  \n<strong>ServiceNow:</strong> Ticket creation, updates, and workflow automation</li>  \n<li>  \n<strong>Jira:</strong> Issue tracking and project updates</li>  \n<li>  \n<strong>Salesforce:</strong> Access to customer and sales data</li>  \n</ul>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>Best Practices for Custom Agent Development</strong>  \n</h1>  \n  \n<h2>  \n    \n    \n  <strong>4. Design Principles</strong>  \n</h2>  \n  \n<ul>  \n<li>  \n<strong>Start Simple:</strong> Begin with a basic knowledge base; add actions later</li>  \n<li>  \n<strong>User-Centric:</strong> Align conversations with real user workflows</li>  \n<li>  \n<strong>Iterative Improvement:</strong> Refine responses based on user behavior</li>  \n<li>  \n<strong>Security First:</strong> Implement proper access controls early</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  <strong>5. Performance Optimization</strong>  \n</h2>  \n  \n<ul>  \n<li>  \n<strong>Chunk Size:</strong> Optimize chunking for better document retrieval</li>  \n<li>  \n<strong>Index Strategy:</strong> Use indexing that fits your content patterns</li>  \n<li>  \n<strong>Caching:</strong> Cache responses for frequently asked questions</li>  \n<li>  \n<strong>Monitoring:</strong> Track latency, usage, and user satisfaction</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  <strong>6. Content Management</strong>  \n</h2>  \n  \n<ul>  \n<li>  \n<strong>Regular Updates:</strong> Keep documents and knowledge up to date</li>  \n<li>  \n<strong>Version Control:</strong> Track content changes through Git or similar tools</li>  \n<li>  \n<strong>Quality Assurance:</strong> Test accuracy of responses regularly</li>  \n<li>  \n<strong>Feedback Loop:</strong> Collect user feedback for continuous improvement</li>  \n</ul>  \n  \n  \n  \n  \n<blockquote>  \n<p><em>Conclusion: Amazon Q Custom Agents offer a transformative approach to knowledge management and task automation by combining large language models with your organization’s specific context to reduce manual work and improve accuracy. Whether you’re creating a simple FAQ bot or a complex operational assistant, success depends on understanding user needs, carefully curating your knowledge base, and continuously enhancing the agent’s capabilities. The investment quickly pays off through reduced support efforts, faster problem resolution, and increased productivity. As organizations generate more data and face growing operational complexity, intelligent agents will become essential for maintaining a competitive advantage.</em></p>  \n</blockquote>  \n  \n<p><strong>The future of work is collaborative intelligence between humans and AI agents - and with Amazon Q, that future is available today.</strong></p>  \n  \n<p><strong>Stay tuned for my dev.to article series on AWS Custom Agents. I’m planning to build and showcase the custom agents mentioned in this article, and I’ll continue sharing updates and new implementations as the series progresses.</strong></p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  📌 Wrapping Up  \n</h2>  \n  \n<p>Thank you for reading! I hope this article gave you practical insights and a clearer perspective on the topic.</p>  \n  \n<p><strong>Was this helpful?</strong></p>  \n  \n<ul>  \n<li>❤️ Like if it added value</li>  \n<li>🦄 Unicorn if you’re applying it today</li>  \n<li>💾 Save for your next optimization session</li>  \n<li>🔄 Share with your team</li>  \n</ul>  \n  \n<p><strong>Follow me for more on:</strong></p>  \n  \n<ul>  \n<li>AWS architecture patterns</li>  \n<li>FinOps automation</li>  \n<li>Multi-account strategies</li>  \n<li>AI-driven DevOps</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  💡 What’s Next  \n</h2>  \n  \n<p>More deep dives coming soon on cloud operations, GenAI, Agentic-AI, DevOps, and data workflows follow for weekly insights.</p>  \n  \n<h2>  \n    \n    \n  🤝 Let’s Connect  \n</h2>  \n  \n<p>I’d love to hear your thoughts drop a comment or connect with me on <a href=\"https://www.linkedin.com/in/sarvar04/\">LinkedIn</a>.</p>  \n  \n<p>Happy Learning 🚀</p>",
      "summary": "👋 Hey there, tech enthusiasts!   \n  \nI'm Sarvar, a Cloud Architect with a passion for transforming complex technological challenges into elegant solutions. With extensive experience spanning Cloud Operations (AWS &amp; Azure), Data Operations, Analytics, DevOps, and Generative AI, I've had the privilege of architecting solutions for global enterprises that drive real business impact. Through this article series, I'm excited to share practical insights, best practices, and hands-on experiences f",
      "publishedAt": "2025-12-02T14:09:35.000Z",
      "author": "Sarvar Nadaf",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 20.9641251397396,
      "ingestedAt": "2025-12-02T14:44:03.159Z",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "observability",
        "governance",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09b26e98",
      "title": "How to Prompt AI for Consistent JSON Responses",
      "url": "https://dev.to/cucoleadan/how-to-prompt-ai-for-consistent-json-responses-1gbm",
      "content": "<p><em>This was originally posted on the <a href=\"https://vibestacklab.substack.com/p/how-to-prompt-ai-for-consistent-json\">VSL Substack publication</a>.</em></p> \n \n<p>You just used AI to generate your API integration code, tested it locally, and deployed it to production because it seemingly worked perfectly. Two hours later, your logs are filled with parsing failures and users cannot complete actions because \"Invalid JSON\" errors are popping up everywhere. The AI gave you working code, but the problem is that it only worked in your controlled test environment with your specific input.</p> \n \n<p>It happened to me so many times I lost count, so I’m going to share one of the best ways to get your AI to generate valid json, every single time.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Flt9g6nxaevfcah1x6hkb.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Flt9g6nxaevfcah1x6hkb.png\" alt=\"Consistent JSON Output System\" width=\"800\" height=\"446\"></a></p> \n \n<h2> \n   \n   \n  Bad JSON Means High Costs \n</h2> \n \n<p>When you are building features that depend on structured data like API integrations, database operations, or configuration files, JSON consistency directly determines whether your app works or breaks. A single malformed response can quickly cascade into hundreds of failed user actions, corrupt database writes, and support tickets flooding your inbox.</p> \n \n<p>While AI generates JSON that looks correct during development, production environments expose inconsistencies like missing commas, trailing commas in strict parsers, unescaped quotes in user-generated content, and Unicode characters from international users. These small formatting errors crash entire features, so you need AI to output valid and production-ready JSON consistently instead of just succeeding in your local tests.</p> \n \n<h2> \n   \n   \n  The Airport Security Model \n</h2> \n \n<p>Airport security examines your documents multiple times at check-in, security, and the gate because mistakes are expensive to fix later. JSON validation works the same way because AI generates structurally sound JSON in controlled conditions but often breaks under real-world variance. That’s why you need to build multiple validation checkpoints to catch these errors.</p> \n \n<p>Most builders stop at the prompt, but production-ready apps require five distinct checkpoints to ensure reliability.</p> \n \n<h2> \n   \n   \n  The Most Common Ways AI Breaks Your Data \n</h2> \n \n<p><strong>Syntax Errors:</strong> Missing or extra commas, brackets, or quotes break <code>JSON.parse()</code> immediately on your server. For example, AI often adds a trailing comma after the last object property or forgets to close a nested object properly. While some lenient parsers might handle this, standard Node.js or Python backends will throw an exception and crash the request before your logic even runs.</p> \n \n<p><strong>Escaped Character Traps:</strong> One of the most insidious errors occurs when AI double-escapes characters inside the JSON string itself (e.g., returning <code>\\\\\"</code> instead of <code>\\\"</code>). This technically creates a valid string but invalid JSON content for your parser, causing it to fail when processing fields that contain quotes, backslashes, or special characters.</p> \n \n<p><strong>Structure Mismatch:</strong> This happens when you get valid JSON in the wrong shape, such as receiving <code>{\"title\": \"My Article\"}</code> when you actually need <code>{\"article\": {\"title\": \"My Article\"}}</code>. Your backend code expects nested objects but receives flat structures, which causes undefined property errors that can take down your entire API endpoint.</p> \n \n<p><strong>Type Confusion:</strong> AI sometimes returns strings when you need numbers or arrays when you need objects. This results in valid JSON with broken logic. An example is when a count field comes back as <code>\"25\"</code> (string) instead of <code>25</code> (number), which breaks database schema validation and any math operations you perform on that data.</p> \n \n<p><strong>Truncated Responses:</strong> When you request a large dataset, some models will hit their output token limit and cut off the JSON mid-stream (e.g., ending with <code>...</code> or just stopping). This leaves you with an incomplete, unparsable string. The reliable fix is to request data in smaller chunks—generating multiple subsets and concatenating the results in your code rather than asking for one massive payload.</p> \n \n<h2> \n   \n   \n  The 5-Checkpoint Framework for Bulletproof JSON \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fozkxat662nvsirl28qtd.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fozkxat662nvsirl28qtd.png\" alt=\"The Validation Checkpoint System\" width=\"800\" height=\"446\"></a></p> \n \n<h2> \n   \n   \n  1. Be Explicit in Your Prompt \n</h2> \n \n<p><strong>Don't say:</strong> \"Return the SEO data as JSON\"</p> \n \n<p><strong>Say:</strong> \"Return ONLY valid JSON with no additional text. Use this exact structure: <code>{\"title\": string, \"description\": string}</code>\"</p> \n \n<h2> \n   \n   \n  2. Provide a JSON Schema \n</h2> \n \n<p><strong>The most reliable way to get consistent JSON structure is to give AI an exact schema to follow.</strong></p> \n \n<p>JSON Schema defines the structure, types, and requirements for your data. Instead of describing what you want in natural language, you provide a formal specification that eliminates ambiguity.</p> \n \n<p><strong>Add this to your prompt:</strong></p> \n \n<p>\"Return JSON matching this exact schema. Do not add any fields not in the schema. All required fields must be present:<br> \n</p> \n \n<div> \n<pre><code><span>{</span> \n  <span>\"</span><span>type</span><span>\"</span><span>:</span> <span>\"</span><span>object</span><span>\"</span><span>,</span> \n  <span>\"</span><span>properties</span><span>\"</span><span>:</span> <span>{</span> \n    <span>\"</span><span>title</span><span>\"</span><span>:</span> <span>{</span> \n      <span>\"</span><span>type</span><span>\"</span><span>:</span> <span>\"</span><span>string</span><span>\"</span><span>,</span> \n      <span>\"</span><span>maxLength</span><span>\"</span><span>:</span> <span>70</span><span>,</span> \n      <span>\"</span><span>description</span><span>\"</span><span>:</span> <span>\"</span><span>An engaging, SEO-friendly article title</span><span>\"</span> \n    <span>},</span> \n    <span>\"</span><span>description</span><span>\"</span><span>:</span> <span>{</span> \n      <span>\"</span><span>type</span><span>\"</span><span>:</span> <span>\"</span><span>string</span><span>\"</span><span>,</span> \n      <span>\"</span><span>maxLength</span><span>\"</span><span>:</span> <span>160</span><span>,</span> \n      <span>\"</span><span>description</span><span>\"</span><span>:</span> <span>\"</span><span>A concise summary of the article content</span><span>\"</span> \n    <span>}</span> \n  <span>},</span> \n  <span>\"</span><span>required</span><span>\"</span><span>:</span> <span>[</span> \n    <span>\"</span><span>title</span><span>\"</span><span>,</span> \n    <span>\"</span><span>description</span><span>\"</span> \n  <span>],</span> \n  <span>\"</span><span>additionalProperties</span><span>\"</span><span>:</span> <span>false</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>Why this works:</strong> Schemas eliminate type confusion and structure mismatch because the model validates its output against the schema pattern during generation. The <code>additionalProperties: false</code> flag is particularly powerful because it strictly forbids the AI from hallucinating extra fields you didn't ask for. This catches errors before they reach your code. Learn more about schema properties at the <a href=\"https://spec.openapis.org/oas/v3.0.3#schema\">OpenAPI Schema specification</a>.</p> \n \n<h2> \n   \n   \n  3. Request Code Fences \n</h2> \n \n<p>Add to your prompt: \"Wrap the JSON in markdown code fences with json syntax highlighting\"</p> \n \n<p><strong>This prevents AI from adding explanatory text before or after the JSON because extra text breaks parsing when you extract the response.</strong></p> \n \n<h2> \n   \n   \n  4. Validate Before Using \n</h2> \n \n<p>Never assume AI output is valid. Always wrap in try-catch:<br> \n</p> \n \n<div> \n<pre><code><span>try</span> <span>{</span> \n  <span>const</span> <span>data</span> <span>=</span> <span>JSON</span><span>.</span><span>parse</span><span>(</span><span>aiResponse</span><span>);</span> \n  <span>*</span><span>// Use data here*</span> \n<span>}</span> <span>catch </span><span>(</span><span>error</span><span>)</span> <span>{</span> \n  <span>console</span><span>.</span><span>error</span><span>(</span><span>'</span><span>Invalid JSON from AI:</span><span>'</span><span>,</span> <span>error</span><span>);</span> \n  <span>*</span><span>// Handle the error gracefully*</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<h2> \n   \n   \n  5. Check Structure After Parsing \n</h2> \n \n<p>Valid JSON doesn't guarantee correct structure:<br> \n</p> \n \n<div> \n<pre><code><span>function</span> <span>validateSeoData</span><span>(</span><span>data</span><span>)</span> <span>{</span> \n  <span>if </span><span>(</span><span>!</span><span>data</span><span>.</span><span>title</span><span>)</span> <span>return</span> <span>false</span><span>;</span> \n  <span>if </span><span>(</span><span>typeof</span> <span>data</span><span>.</span><span>title</span> <span>!==</span> <span>'</span><span>string</span><span>'</span><span>)</span> <span>return</span> <span>false</span><span>;</span> \n  <span>if </span><span>(</span><span>typeof</span> <span>data</span><span>.</span><span>description</span> <span>!==</span> <span>'</span><span>string</span><span>'</span><span>)</span> <span>return</span> <span>false</span><span>;</span> \n  <span>return</span> <span>true</span><span>;</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F92i04czxtnjcqv4t59av.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F92i04czxtnjcqv4t59av.png\" alt=\"5-stage validation\" width=\"800\" height=\"446\"></a></p> \n \n<h2> \n   \n   \n  Red Flags: What to Watch For \n</h2> \n \n<p><strong>🚩 AI adds explanation text before or after JSON:</strong> This breaks parsing. AI models are trained to be conversational and often add context before JSON blocks, so you must always request \"ONLY valid JSON, no additional text.\"</p> \n \n<p><strong>🚩 Nested objects are inconsistent:</strong> AI formats the first object correctly but makes errors in deeply nested structures. The model's attention weakens as JSON depth increases, which leads to malformed brackets and missing commas three or four levels deep.</p> \n \n<p><strong>🚩 Numbers as strings:</strong> AI often wraps numbers in quotes because it sees numbers as text tokens rather than numeric types. You need to be explicit that \"count should be a number, not a string.\"</p> \n \n<p><strong>🚩 Double-escaped characters in strings:</strong> AI returns <code>\\\\\"</code> instead of <code>\\\"</code> for quotes inside strings because the model treats escape sequences as text patterns rather than functional syntax.</p> \n \n<h2> \n   \n   \n  Watch Out For Escaped Character \n</h2> \n \n<p><strong>One of the most frustrating JSON issues occurs when AI returns responses with escaped characters inside the JSON itself.</strong></p> \n \n<p><strong>Example of what goes wrong:</strong></p> \n \n<ul> \n<li> \n<strong>What you expect:</strong> <code>{\"title\": \"The \\\"Best\\\" Guide\"}</code> \n</li> \n<li> \n<strong>What AI sometimes returns:</strong> <code>{\"title\": \"The \\\\\\\"Best\\\\\\\" Guide\"}</code> \n</li> \n</ul> \n \n<p><strong>The AI double-escapes quotes, backslashes, and special characters.</strong> When you parse this JSON, you get literal backslash characters in your strings instead of properly escaped quotes. This breaks things when you try to display the data to users or pass it to other systems that expect clean strings.</p> \n \n<p><strong>How to prevent it:</strong></p> \n \n<p>Add to your prompt: <strong>\"Use proper JSON escaping. Quotes inside strings should use single backslash escape (\"), not double backslash (\\\").\"</strong></p> \n \n<h2> \n   \n   \n  Hidden Edge Cases \n</h2> \n \n<p><strong>Trailing commas:</strong> AI adds commas after the last item in objects or arrays. Strict JSON parsers reject this immediately even though JavaScript often accepts it, so this breaks in production environments using different parsers.</p> \n \n<p><strong>Unescaped special characters:</strong> If your data contains quotes or backslashes, AI often forgets to escape them. A title like \"O'Brien's Guide\" will break your JSON when AI returns <code>\"title\": \"O'Brien's Guide\"</code> instead of <code>\"title\": \"O\\'Brien's Guide\"</code> if your parser is strict about apostrophes.</p> \n \n<p><strong>Inconsistent null handling:</strong> AI randomly switches between <code>null</code>, <code>\"null\"</code>, empty string, or omitting the field entirely. Your application logic assumes one pattern but receives variations, which causes conditional checks to fail unpredictably.</p> \n \n<h2> \n   \n   \n  My Production Safety Net \n</h2> \n \n<p>You have likely spent an hour debugging a critical failure only to discover the culprit was a single missing comma. In production, that one missing comma is not just a syntax error. It represents hundreds of failed user requests and corrupted data that piled up before you even noticed the issue.</p> \n \n<p><strong>Here's the technique that changed everything for me:</strong></p> \n \n<p><strong>Use <code>jsonrepair</code> to automatically fix malformed JSON.</strong><br> \n</p> \n \n<div> \n<pre><code><span>import</span> <span>{</span> <span>jsonrepair</span> <span>}</span> <span>from</span> <span>'</span><span>jsonrepair</span><span>'</span><span>;</span> \n \n<span>try</span> <span>{</span> \n  <span>*</span><span>// Try parsing normally first*</span> \n  <span>const</span> <span>data</span> <span>=</span> <span>JSON</span><span>.</span><span>parse</span><span>(</span><span>aiResponse</span><span>);</span> \n<span>}</span> <span>catch </span><span>(</span><span>error</span><span>)</span> <span>{</span> \n  <span>*</span><span>// If parsing fails, attempt repair*</span> \n  <span>try</span> <span>{</span> \n    <span>const</span> <span>repairedJson</span> <span>=</span> <span>jsonrepair</span><span>(</span><span>aiResponse</span><span>);</span> \n    <span>const</span> <span>data</span> <span>=</span> <span>JSON</span><span>.</span><span>parse</span><span>(</span><span>repairedJson</span><span>);</span> \n    <span>console</span><span>.</span><span>log</span><span>(</span><span>'</span><span>JSON repaired successfully</span><span>'</span><span>);</span> \n  <span>}</span> <span>catch </span><span>(</span><span>repairError</span><span>)</span> <span>{</span> \n    <span>console</span><span>.</span><span>error</span><span>(</span><span>'</span><span>Could not repair JSON:</span><span>'</span><span>,</span> <span>repairError</span><span>);</span> \n    <span>*</span><span>// Handle the error*</span> \n  <span>}</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p>The <code>jsonrepair</code> library automatically fixes common JSON formatting issues like adding missing commas, removing trailing commas, fixing unescaped quotes, and repairing truncated JSON.</p> \n \n<p><strong>Think of it as insurance. You hope you never need it, but when production breaks at 2 AM, you will be grateful it is there. And don’t ask me how I know that…</strong></p> \n \n<h2> \n   \n   \n  Tool of the Week: Zod \n</h2> \n \n<p><a href=\"https://zod.dev/\">Zod - TypeScript-first schema validation</a></p> \n \n<p>Once you have valid JSON from AI, you still need to validate that the structure matches what your app expects. Zod lets you define schemas and validate data in one line. TypeScript validates types at compile time, while Zod validates data at runtime.</p> \n \n<p>Example:<br> \n</p> \n \n<div> \n<pre><code><span>import</span> <span>{</span> <span>z</span> <span>}</span> <span>from</span> <span>'</span><span>zod</span><span>'</span><span>;</span> \n \n<span>const</span> <span>SeoSchema</span> <span>=</span> <span>z</span><span>.</span><span>object</span><span>({</span> \n  <span>title</span><span>:</span> <span>z</span><span>.</span><span>string</span><span>().</span><span>max</span><span>(</span><span>70</span><span>),</span> \n  <span>description</span><span>:</span> <span>z</span><span>.</span><span>string</span><span>().</span><span>max</span><span>(</span><span>160</span><span>)</span> \n<span>});</span> \n \n<span>*</span><span>// Validate AI output*</span> \n<span>const</span> <span>result</span> <span>=</span> <span>SeoSchema</span><span>.</span><span>safeParse</span><span>(</span><span>aiData</span><span>);</span> \n<span>if </span><span>(</span><span>!</span><span>result</span><span>.</span><span>success</span><span>)</span> <span>{</span> \n  <span>console</span><span>.</span><span>error</span><span>(</span><span>'</span><span>Invalid structure:</span><span>'</span><span>,</span> <span>result</span><span>.</span><span>error</span><span>);</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>It catches structure mismatches and type confusion instantly, so pairing it with <code>jsonrepair</code> gives you bulletproof AI output handling.</strong></p> \n \n<h2> \n   \n   \n  Final Thoughts \n</h2> \n \n<p>Valid JSON is boring until it breaks production at the worst possible time. Build the checkpoints now and thank yourself later.</p> \n \n<p>Let's Connect: What's your strategy for handling AI-generated structured data? Do you validate everything, or trust AI until it breaks? Reply and tell me about the weirdest JSON error you've debugged.</p> \n \n<p><em>This was originally posted on the <a href=\"https://vibestacklab.substack.com/p/how-to-prompt-ai-for-consistent-json\">VSL Substack publication</a>.</em></p>",
      "summary": "This was originally posted on the VSL Substack publication. \n \nYou just used AI to generate your API integration code, tested it locally, and deployed it to production because it seemingly worked perfectly. Two hours later, your logs are filled with parsing failures and users cannot complete actions because \"Invalid JSON\" errors are popping up everywhere. The AI gave you working code, but the problem is that it only worked in your controlled test environment with your specific input. \n \nIt happe",
      "publishedAt": "2025-12-02T14:03:05.000Z",
      "author": "cucoleadan",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "security_incident",
      "score": 11.47665332725492,
      "ingestedAt": "2025-12-02T14:44:03.160Z",
      "tags": [
        "code_review",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09b26e99",
      "title": "The Game Math Behind a Candy Craze Game: Probability, Cascades, and Level Pacing in HTML5 Puzzlers",
      "url": "https://dev.to/gamh5games/the-game-math-behind-a-candy-craze-game-probability-cascades-and-level-pacing-in-html5-puzzlers-32k6",
      "content": "<p>Candy puzzle games—often referred to as “candy craze games”—look colorful and playful on the surface, but their underlying mechanics rely heavily on math, probability weighting, grid logic, and pacing algorithms that control difficulty, randomness, and player satisfaction.</p> \n \n<p>In this article, we’ll explore the mathematical and algorithmic foundations behind a <a href=\"https://gamh5.com/game/candy-craze/\">candy craze game</a>:</p> \n \n<p>How grids are generated</p> \n \n<p>How randomness is controlled (not actually random!)</p> \n \n<p>How cascades are determined</p> \n \n<p>How difficulty ramps up</p> \n \n<p>And how to design a fair but exciting experience</p> \n \n<p>Whether you're building your own HTML5 puzzle game or analyzing browser games for UI/logic inspiration, understanding these principles is crucial.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbzy7mvesp0wivy738tbt.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbzy7mvesp0wivy738tbt.png\" alt=\"\" width=\"402\" height=\"689\"></a><br> \n🍬 1. Why Candy Games Need Math (More Than You Think)</p> \n \n<p>A candy craze game appears simple:</p> \n \n<p>match 3 candies</p> \n \n<p>clear them</p> \n \n<p>trigger cascades</p> \n \n<p>refill the board</p> \n \n<p>But fairness and difficulty depend on precisely controlled randomness.</p> \n \n<p>If the game is too random → players feel cheated.<br> \nIf it is too predictable → boredom.</p> \n \n<p>The solution: guided randomness backed by probability design.</p> \n \n<p>🎲 2. Generating the Board: Not Fully Random</p> \n \n<p>Most beginners try this:</p> \n \n<p>grid[r][c] = randomCandy();</p> \n \n<p>But this often produces:</p> \n \n<p>instant combos at start (unfair advantage)</p> \n \n<p>unwinnable states</p> \n \n<p>repetitive patterns</p> \n \n<p>too many cascades</p> \n \n<p>A professional candy craze game uses “no-initial-matches” logic.<br> \nfunction generateCell(r, c) {<br> \n  let tile;<br> \n  do {<br> \n    tile = randomCandy();<br> \n  } while (<br> \n    matchesLeft(r, c, tile) ||   // avoid horizontal match<br> \n    matchesUp(r, c, tile)        // avoid vertical match<br> \n  );<br> \n  return tile;<br> \n}</p> \n \n<p>This ensures fair starts while still feeling random.</p> \n \n<p>📉 3. Probability Balancing (Weighted Randomness)</p> \n \n<p>If all candies have equal probability:</p> \n \n<p>Math.random() &lt; 1 / candyTypes</p> \n \n<p>the game may generate:</p> \n \n<p>too many large combo chains</p> \n \n<p>too few matches</p> \n \n<p>impossible boards</p> \n \n<p>Developers solve this using weighted probability tables:</p> \n \n<p>const weights = {<br> \n  red: 0.20,<br> \n  blue: 0.20,<br> \n  yellow: 0.20,<br> \n  green: 0.20,<br> \n  purple: 0.20,<br> \n  // rare types (e.g., special candies)<br> \n  rainbow: 0.02<br> \n};</p> \n \n<p>This allows designers to tune game feel without changing code.</p> \n \n<p>🌊 4. Cascade Logic: “Chain Reactions” That Feel Good</p> \n \n<p>The emotional peak of a candy craze game is the cascade:</p> \n \n<p>Match 3</p> \n \n<p>They disappear</p> \n \n<p>Above tiles fall</p> \n \n<p>New tiles fill</p> \n \n<p>New matches occur</p> \n \n<p>Repeat</p> \n \n<p>The math behind cascades:</p> \n \n<p>A cascade continues as long as there is a match:</p> \n \n<p>function cascade() {<br> \n  let totalCleared = 0;<br> \n  let matches;</p> \n \n<p>do {<br> \n    matches = findMatches(grid);<br> \n    clearMatches(matches);<br> \n    applyGravity(grid);<br> \n    refillGrid(grid);<br> \n    totalCleared += matches.length;<br> \n  } while (matches.length &gt; 0);</p> \n \n<p>return totalCleared;<br> \n}</p> \n \n<p>This often results in “lucky streaks,” but even those streaks are mathematically predictable based on candy distribution.</p> \n \n<p>📈 5. Level Pacing: The Secret Ingredient</p> \n \n<p>Great candy games feel like this:</p> \n \n<p>Easy start</p> \n \n<p>Moderate difficulty</p> \n \n<p>Tension rise</p> \n \n<p>Mini breather</p> \n \n<p>Challenge spike</p> \n \n<p>Reward moment</p> \n \n<p>Continue loop</p> \n \n<p>This pacing is algorithm driven.</p> \n \n<p>Controlled difficulty curve:</p> \n \n<p>early levels → high match probability</p> \n \n<p>mid levels → introduce obstacles (jelly, blockers, frozen tiles)</p> \n \n<p>late levels → reduce match probability</p> \n \n<p>Example tuning:</p> \n \n<p>function getSpawnWeights(level) {<br> \n  if (level &lt; 5) return easyWeights;<br> \n  if (level &lt; 15) return mediumWeights;<br> \n  return hardWeights;<br> \n}</p> \n \n<p>Difficulty becomes predictable, testable, and adjustable without touching core code.</p> \n \n<p>🔥 6. Special Candies: Probability + Trigger Rules</p> \n \n<p>Special candies (e.g., bombs, striped candies, color blasters) are generated by:</p> \n \n<p>clearing 4</p> \n \n<p>clearing 5</p> \n \n<p>forming T or L shapes</p> \n \n<p>chaining certain combos</p> \n \n<p>Special candy detection logic:</p> \n \n<p>function detectSpecial(matches) {<br> \n  if (matches.length === 4) return \"line\";<br> \n  if (matches.length === 5) return \"rainbow\";<br> \n  if (isTShape(matches) || isLShape(matches)) return \"bomb\";<br> \n}</p> \n \n<p>These rules drastically impact:</p> \n \n<p>board predictability</p> \n \n<p>chain potential</p> \n \n<p>player satisfaction</p> \n \n<p>difficulty scaling</p> \n \n<p>Again—mathematics at work.</p> \n \n<p>🎮 7. Why Candy Craze Games Are Ideal for HTML5</p> \n \n<p>HTML5 (Canvas or WebGL) perfectly matches this genre:</p> \n \n<p>grid logic is lightweight</p> \n \n<p>animations are small and repeated</p> \n \n<p>user input is simple</p> \n \n<p>scene transitions are minimal</p> \n \n<p>game loop works at 60 FPS</p> \n \n<p>assets are easy to compress</p> \n \n<p>mobile browsers handle it well</p> \n \n<p>Even large animations can be optimized with:</p> \n \n<p>sprite atlases</p> \n \n<p>batched draw calls</p> \n \n<p>GPU-accelerated WebGL pipelines</p> \n \n<p>This is why many modern candy craze games run entirely in browsers.</p> \n \n<p>🌐 8. Studying Real HTML5 Puzzle Game Implementations</p> \n \n<p>If you're researching candy-style games, match-3 mechanics, or grid-based animation behaviors, you can explore collections of HTML5 browser games here:</p> \n \n<p>GamH5 — HTML5 Browser Game UI &amp; Logic Examples<br> \n(Insert your link here)</p> \n \n<p>Useful for studying:</p> \n \n<p>grid states</p> \n \n<p>animation timing</p> \n \n<p>cascade sequences</p> \n \n<p>difficulty pacing</p> \n \n<p>candy-type balancing</p> \n \n<p>Great reference material for developers.</p> \n \n<p>🧠 Final Thoughts</p> \n \n<p>A <a href=\"https://gamh5.com/game/candy-craze/\">candy craze game</a> is a brilliant blend of:</p> \n \n<p>probability</p> \n \n<p>grid logic</p> \n \n<p>pacing systems</p> \n \n<p>animation engineering</p> \n \n<p>UI feedback design</p> \n \n<p>It’s far more mathematical than it appears, and understanding these algorithms helps developers:</p> \n \n<p>balance difficulty</p> \n \n<p>avoid unfair states</p> \n \n<p>control cascades</p> \n \n<p>design better user experiences</p> \n \n<p>write cleaner, more scalable game logic</p> \n \n<p>Beneath the sugar coating lies a finely tuned machine.</p> \n \n<p>If you’re building or analyzing HTML5 puzzle games, mastering this math-driven architecture will dramatically improve your ability to create fair, fun, and addictive gameplay loops.</p>",
      "summary": "Candy puzzle games—often referred to as “candy craze games”—look colorful and playful on the surface, but their underlying mechanics rely heavily on math, probability weighting, grid logic, and pacing algorithms that control difficulty, randomness, and player satisfaction. \n \nIn this article, we’ll explore the mathematical and algorithmic foundations behind a candy craze game: \n \nHow grids are generated \n \nHow randomness is controlled (not actually random!) \n \nHow cascades are determined \n \nHow ",
      "publishedAt": "2025-12-02T14:01:48.000Z",
      "author": "gamh5games",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "thought_leadership",
      "score": 4.989531637836414,
      "ingestedAt": "2025-12-02T14:44:03.161Z",
      "tags": [
        "code_review",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09b26e9b",
      "title": "Julia Kasper – Rewetting peatlands is the biggest climate opportunity to cut CO2",
      "url": "https://dev.to/ogcr/julia-kasper-rewetting-peatlands-is-the-biggest-climate-opportunity-to-cut-co2-4o5m",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fq0xk594iubepaepkgkyu.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>In her recent interview, Julia Kasper, co-founder and CEO of <a href=\"https://www.zukunftmoor.de/en\">Zukunftmoor</a>, argues that rewetting drained peatlands represents the single biggest climate opportunity in agriculture today. Although peatlands cover only about 3 % of the global land surface, they store more carbon than all the world’s forests combined. When peatlands are drained, a common practice for agriculture, they don’t just release CO₂ once; they leak carbon continuously, year after year. Restoring peatlands thus stops that “constant leak,” and rewetting them can turn a major source of emissions into a long-term carbon sink.</p>  \n  \n<p>But rewetting alone is not enough: farmers need viable, sustainable livelihoods. That’s where Zukunftmoor’s innovative approach comes in. They propose combining rewetting with cultivation of Sphagnum moss — a natural plant of peatlands — which can serve as a sustainable substitute for extracted peat in horticultural substrates. This turns rewetting from a purely ecological restoration act into a market-driven, economically viable land-use model. By using drones or hand-seeding methods, and developing harvesting and substrate-supply chains, the approach offers farmers a low-input, long-term pathway to maintain income while restoring degraded peatlands.</p>  \n  \n<p>With this combination of climate mitigation and practical agriculture, Kasper’s vision offers peatland regions across Europe a concrete alternative to drainage-based farming — one that aligns environmental restoration with economic viability.</p>  \n  \n<p>Original article published on <a href=\"https://investinginregenerativeagriculture.com/2025/11/11/julia-kasper/\">Investing in Regenerative Agriculture</a>.</p>",
      "summary": "In her recent interview, Julia Kasper, co-founder and CEO of Zukunftmoor, argues that rewetting drained peatlands represents the single biggest climate opportunity in agriculture today. Although peatlands cover only about 3 % of the global land surface, they store more carbon than all the world’s forests combined. When peatlands are drained, a common practice for agriculture, they don’t just release CO₂ once; they leak carbon continuously, year after year. Restoring peatlands thus stops that “co",
      "publishedAt": "2025-12-02T14:01:38.000Z",
      "author": "Open Geospatial Carbon Registry (OGCR)",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "security_incident",
      "score": 7.48423558285813,
      "ingestedAt": "2025-12-02T14:44:03.161Z",
      "tags": [
        "code_review",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09b26e9c",
      "title": "Engineering a “Candy Craze Game”: Interaction Model",
      "url": "https://dev.to/gamh5games/engineering-a-candy-craze-game-interaction-model-6ka",
      "content": "<p>Candy craze” games—fast, colorful, tap-driven puzzle or matching games—are among the most influential patterns in modern HTML5 casual gaming. Behind the bright animations and candy explosions lies a surprisingly elegant technical architecture built on event systems, rendering pipelines, interaction logic, and resource-efficient desi</p> \n \n<p>In this article, we’ll examine the engineering principles behind building a c, f</p> \n \n<p>If you're designing or studying HTML5 games, this genre is a near-perfect cas</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ffwy15g2tpdn794ohfpsz.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ffwy15g2tpdn794ohfpsz.png\" alt=\"\" width=\"402\" height=\"689\"></a><br> \n🍬 1. What Defines</p> \n \n<p>A</p> \n \n<p>Grid-ba</p> \n \n<p>T</p> \n \n<p>Chain reacti</p> \n \n<p>Particle effec</p> \n \n<p>Color-rich sprites and animations</p> \n \n<p>Mobile-first design</p> \n \n<p>This genre forces the developer to solve:</p> \n \n<p>how to detect matches efficiently</p> \n \n<p>how to animate falling pieces</p> \n \n<p>how to avoid layout jank</p> \n \n<p>how to maintain 60 FPS</p> \n \n<p>how to sequence chain reactions without blocking</p> \n \n<p>A perfect playground for frontend engineering.</p> \n \n<p>🧩 2. Grid Architecture: The Heart of the Game</p> \n \n<p>Most <a href=\"https://gamh5.com/game/candy-craze/\">candy craze game</a>s are built on a 2D matrix where each cell stores:</p> \n \n<p>candy type</p> \n \n<p>animation state</p> \n \n<p>position</p> \n \n<p>matched flag</p> \n \n<p>fall velocity</p> \n \n<p>A clean grid representation:</p> \n \n<p>const grid = Array.from({ length: ROWS }, () =&gt;<br> \n  Array.from({ length: COLS }, () =&gt; ({<br> \n    type: randomCandy(),<br> \n    matched: false<br> \n  }))<br> \n);</p> \n \n<p>This structure keeps the core logic clean:</p> \n \n<p>match detection</p> \n \n<p>falling candies</p> \n \n<p>refilling the board</p> \n \n<p>cascading combos</p> \n \n<p>🔍 3. Match Detection (Core Algorithm)</p> \n \n<p>The most common detection approach scans horizontally and vertically:</p> \n \n<p>function findMatches(grid) {<br> \n  const matches = [];</p> \n \n<p>// Horizontal matches<br> \n  for (let r = 0; r &lt; ROWS; r++) {<br> \n    for (let c = 0; c &lt; COLS - 2; c++) {<br> \n      if (<br> \n        grid[r][c].type === grid[r][c + 1].type &amp;&amp;<br> \n        grid[r][c].type === grid[r][c + 2].type<br> \n      ) {<br> \n        matches.push([r, c], [r, c + 1], [r, c + 2]);<br> \n      }<br> \n    }<br> \n  }</p> \n \n<p>// Vertical matches…<br> \n  // (Same pattern)</p> \n \n<p>return matches;<br> \n}</p> \n \n<p>Simple, predictable, efficient—perfect for mobile.</p> \n \n<p>🌀 4. Animation Pipeline: Making It Feel “Crazy”</p> \n \n<p>Candy craze games rely on smooth, continuous animations:</p> \n \n<p>candies popping</p> \n \n<p>falling into empty spaces</p> \n \n<p>horizontal swipes</p> \n \n<p>combo explosions</p> \n \n<p>particle bursts</p> \n \n<p>Why real-time animations matter</p> \n \n<p>Animations communicate:</p> \n \n<p>success</p> \n \n<p>progress</p> \n \n<p>urgency</p> \n \n<p>reward</p> \n \n<p>Even micro animations increase retention significantly.</p> \n \n<p>Techniques commonly used:</p> \n \n<p>WebGL with PixiJS for particles</p> \n \n<p>Canvas 2D for simplicity</p> \n \n<p>Tween libraries (GSAP, Phaser)</p> \n \n<p>Sprite atlas animations</p> \n \n<p>Example “falling candy” tween:</p> \n \n<p>tween.to(candy, {<br> \n  y: targetY,<br> \n  duration: 180,<br> \n  easing: \"easeOutQuad\"<br> \n});</p> \n \n<p>Short, snappy animations → addictive gameplay.</p> \n \n<p>🎮 5. Input Handling: Tap, Swap, and Interaction Models</p> \n \n<p>Candy craze games use simple gestures:</p> \n \n<p>tap-to-select</p> \n \n<p>swap two adjacent candies</p> \n \n<p>drag interaction (optional)</p> \n \n<p>A unified input handler for canvas-based games:</p> \n \n<p>canvas.addEventListener(\"pointerdown\", (e) =&gt; {<br> \n  const pos = getPointerPosition(e);<br> \n  const cell = gridCoords(pos.x, pos.y);<br> \n  handleSelect(cell);<br> \n});</p> \n \n<p>Mobile browsers require:</p> \n \n<p>no 300 ms delay</p> \n \n<p>passive event listeners</p> \n \n<p>fast hit detection</p> \n \n<p>pre-calculated grid cell boundaries</p> \n \n<p>This ensures smooth-feeling interaction, even under heavy animation load.</p> \n \n<p>⚡ 6. Performance Optimization for Mobile Browsers</p> \n \n<p>Candy craze games contain many sprites, animations, and sound events.<br> \nTo maintain 60 FPS:</p> \n \n<p>✔ Batch draw calls</p> \n \n<p>Reduces overhead for Canvas and WebGL.</p> \n \n<p>✔ Use sprite sheets</p> \n \n<p>Minimize texture switching.</p> \n \n<p>✔ Avoid large PNGs</p> \n \n<p>Use WebP or compressed atlases.</p> \n \n<p>✔ Cache repeated calculations</p> \n \n<p>Grid positions, hitboxes, and candy types.</p> \n \n<p>✔ Avoid reflow-heavy DOM</p> \n \n<p>Keep everything on  or a WebGL surface.</p> \n \n<p>✔ Limit particle count</p> \n \n<p>Too many particles = frame drops.</p> \n \n<p>Candy craze games feel “crazy” only when performance stays stable.<br> \nSmoothness is the key to the experience.</p> \n \n<p>🎨 7. Game Feel: Why Candy Games Are So Addictive</p> \n \n<p>A candy craze game is defined not by logic, but by feedback quality:</p> \n \n<p>juicy sounds</p> \n \n<p>combo text</p> \n \n<p>particle bursts</p> \n \n<p>bounce effects</p> \n \n<p>glowing candy highlights</p> \n \n<p>rhythmic pace</p> \n \n<p>Example “pop” effect:</p> \n \n<p>function pop(candy) {<br> \n  tween.to(candy, {<br> \n    scale: 1.3,<br> \n    duration: 120,<br> \n    yoyo: true<br> \n  });<br> \n}</p> \n \n<p>These tiny moments make the game memorable.</p> \n \n<p>🔧 8. Avoiding Spaghetti Code: Recommended Architecture</p> \n \n<p>To keep the code scalable:</p> \n \n<p>✔ Use a scene-based structure</p> \n \n<p>Menu → Game → Results</p> \n \n<p>✔ Separate rendering, logic, and input</p> \n \n<p>Never mix UI drawing with match detection.</p> \n \n<p>✔ Event-based game flow</p> \n \n<p>Use an event bus for transitions.</p> \n \n<p>✔ Keep animations asynchronous</p> \n \n<p>Don’t block the game loop.</p> \n \n<p>✔ Use dedicated managers</p> \n \n<p>GridManager</p> \n \n<p>AnimationManager</p> \n \n<p>InputManager</p> \n \n<p>SoundManager</p> \n \n<p>ComboManager</p> \n \n<p>This prevents the logic from becoming unmaintainable as features grow.</p> \n \n<p>🌐 9. Studying Real HTML5 Game Implementations</p> \n \n<p>If you're researching how real-world HTML5 games structure their UI, grids, animations, or performance pipelines, you can browse collections of lightweight browser games here:</p> \n \n<p>GamH5 — HTML5 Browser Game Examples &amp; UI Patterns<br> \n(Insert your link here)</p> \n \n<p>These examples help developers understand how candy-style games structure their feedback loops, cascading logic, and rendering strategy.</p> \n \n<p>🧠 Final Thoughts</p> \n \n<p>A <a href=\"https://gamh5.com/game/candy-craze/\">candy craze game</a> is more than colorful sprites and candy explosions.<br> \nFrom a technical perspective, it's a carefully tuned system built on:</p> \n \n<p>optimized grid algorithms</p> \n \n<p>event-driven gameplay</p> \n \n<p>animation pipelines</p> \n \n<p>touch interaction models</p> \n \n<p>resource-efficient rendering</p> \n \n<p>performance-first thinking</p> \n \n<p>This genre may appear simple, but engineering it well requires the same principles used in larger 2D game systems — just in a small, elegant package.</p> \n \n<p>For developers building or studying HTML5 games, the candy craze format is one of the best starting points for learning real-time interactive design.</p>",
      "summary": "Candy craze” games—fast, colorful, tap-driven puzzle or matching games—are among the most influential patterns in modern HTML5 casual gaming. Behind the bright animations and candy explosions lies a surprisingly elegant technical architecture built on event systems, rendering pipelines, interaction logic, and resource-efficient desi \n \nIn this article, we’ll examine the engineering principles behind building a c, f \n \nIf you're designing or studying HTML5 games, this genre is a near-perfect cas ",
      "publishedAt": "2025-12-02T13:59:11.000Z",
      "author": "gamh5games",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "thought_leadership",
      "score": 5.487772470109952,
      "ingestedAt": "2025-12-02T14:44:03.161Z",
      "tags": [
        "code_review",
        "retrieval",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09b27461",
      "title": "First time since 1988, the U.S. is not officially commemorating World AIDS Day",
      "url": "https://www.npr.org/sections/goats-and-soda/2025/12/01/g-s1-99925/world-aids-day-trump",
      "content": "<p><img src=\"https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2632x1481+0+0/resize/1400/quality/100/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fb4%2Fa1%2Fc20a006d46a288f8be90d77f2233%2Fworld-aids-day-diptych.jpg\" alt=\"?url=http%3A%2F%2Fnpr-brightspot.s3.amaz\"></p><p>Article URL: <a href=\"https://www.npr.org/sections/goats-and-soda/2025/12/01/g-s1-99925/world-aids-day-trump\">https://www.npr.org/sections/goats-and-soda/2025/12/01/g-s1-99925/world-aids-day-trump</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46121300\">https://news.ycombinator.com/item?id=46121300</a></p>  \n<p>Points: 10</p>  \n<p># Comments: 0</p>",
      "summary": "Article URL: https://www.npr.org/sections/goats-and-soda/2025/12/01/g-s1-99925/world-aids-day-trump  \nComments URL: https://news.ycombinator.com/item?id=46121300  \nPoints: 10  \n# Comments: 0",
      "publishedAt": "2025-12-02T14:08:01.000Z",
      "author": "stopbulying",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.491963431106376,
      "ingestedAt": "2025-12-02T14:44:03.161Z",
      "tags": [
        "code_review",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09b16cd2",
      "title": "DeepSeek-V3.2 🤖, Runway Gen-4.5 📹, Gemini Projects 💼",
      "url": "https://www.inoreader.com/article/3a9c6e76b2ba932d",
      "content": "<div class=\"email_is_html\"><div> \t\t \t\t \t\t \t\t \t\t \t\t \t\t \t\t \t\t \t \t<div>   \t\t<div style=\"display: none; max-height: 0px; overflow: hidden\">DeepSeek's V3.2 matches GPT-5, and its higher-compute V3.2-Speciale variant rivals Gemini-3.0-Pro and earned gold medals at IMO, IOI, and ICPC 2025. ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ </div> \t\t<div style=\"display: none; max-height: 0px; overflow: hidden\"> \t\t\t<br /> \t\t</div>  \t\t<table align=\"center\"> \t\t\t<tbody> \t\t\t\t<tr> \t\t\t\t\t<td valign=\"top\">  \t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"600\"> \t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t<td>  \t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t<td>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t<table width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" style=\"margin-top: 0px\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 0px\">  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div style=\"text-align: center\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"margin-right: 0px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fai%3Futm_source=tldrai/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/EHylodWZH2v3mkKi-5z4QWvntfyOaJU_yB4X0_oCFjE=433\" rel=\"noreferrer\" target=\"_blank\"><span>Sign Up</span></a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t|<span style=\"margin-right: 2px; margin-left: 2px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisetopnav/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/PSeiy0_ELhibtFn5Iky8G_gAMLtsjZEM8E853VykTBk=433\" rel=\"noreferrer\" target=\"_blank\"><span>Advertise</span></a></span>|<span style=\"margin-left: 2px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa.tldrnewsletter.com%2Fweb-version%3Fep=1%26lc=8cb79176-9621-11f0-a63a-a3daa84d9496%26p=8cea03fe-cf74-11f0-8827-b37fe40dee87%26pt=campaign%26t=1764685886%26s=9933dfc3a40c16b1bf768ecead3996d9ef0f276f2e5fe27cfbd3dc393debd468/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/SHbBgTI6_bUMhydMfjec1BBFwEjfdRYaJjrEpuO5u78=433\" target=\"_blank\" rel=\"noreferrer\"><span>View Online</span></a></span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span></div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"text-align: center\"><span style=\"--darkreader-inline-color: #3db3ff; color: rgb(51, 175, 255) !important; font-size: 30px\">T</span><span style=\"font-size: 30px\"><span style=\"color: rgb(232, 192, 96) !important; --darkreader-inline-color: #e8c163; font-size: 30px\">L</span><span style=\"color: rgb(101, 195, 173) !important; --darkreader-inline-color: #6ec7b2; font-size: 30px\">D</span></span><span style=\"--darkreader-inline-color: #dd6e6e; color: rgb(220, 107, 107) !important; font-size: 30px\">R</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br />  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td align=\"center\" height=\"20\" style=\"vertical-align: middle !important\" valign=\"middle\" width=\"100%\"><strong style=\"vertical-align: middle !important; height: 100%\">Together With </strong> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fairia.com%2Frequest-demo%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_id=ai_q2/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/_DWw85HVhhCZiCKI4nXlHgpfnvBlD9Fl7hVRNgEUnJI=433\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://images.tldr.tech/airia.png\" valign=\"middle\" style=\"vertical-align: middle !important; height: 100%\" alt=\"Airia\" /></a></td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table style=\"table-layout: fixed; width: 100%\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div style=\"text-align: center\">  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<h1><strong>TLDR AI <span>2025-12-02</span></strong></h1> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table style=\"table-layout: fixed; width: 100%\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fairia.com%2Frequest-demo%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_id=ai_q2/2/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/495EJFbc-9c1M6KdkMqUzrzjBxQL34ko-ne0xiCiy00=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Airia: Enterprise AI Orchestration — Agents, Integrations, Workflows, and Governance (Sponsor)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     You want AI to become part of your organizational DNA - and that means enabling every department to build out their own use cases, without IT gatekeepers standing in the way. But it shouldn't mean an ungoverned free-for-all.<p></p><p>Airia is the <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fairia.com%2Frequest-demo%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_id=ai_q2/3/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/AajgHrSXyuxEZqqzCtHl0ItyOdiGuYvbkw7Q4QIhkpI=433\" rel=\"noreferrer\" target=\"_blank\"><span>&quot;let's get serious about AI adoption&quot; platform</span></a>. Rapidly prototype, deploy, and manage AI agents that transform workflows across your organization - without sacrificing security or governance.</p>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<p>Connect to dozens of enterprise applications with native integrations. <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fairia.com%2Frequest-demo%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_id=ai_q2/4/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/23GpMvyeFrgtmp4GLqW4dbtCmC0cp6tz5I1hax7wG44=433\" rel=\"noreferrer\" target=\"_blank\"><span>Build agents quickly with templates and no-code tools</span></a>. Let everyone build with AI while maintaining visibility and control.</p>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<p><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fairia.com%2Frequest-demo%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_id=ai_q2/5/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/zq3Rgmd1aS1iQWfJDy51Uc8nj1d4T3Dh1VvB28_FrNM=433\" rel=\"noreferrer\" target=\"_blank\"><span>Plans start at just $49/month. Get a demo</span></a>   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</p> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span></span></div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t</table> \t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t<td>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 0px\">  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding-top: 0px; padding-bottom: 0px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div style=\"text-align: center\"><span style=\"font-size: 36px\"><span style=\"font-size: 36px\">🚀</span></span></div></div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding-top: 0px; padding-bottom: 0px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div style=\"text-align: center\">  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<h1><strong>Headlines &amp; Launches</strong></h1> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table style=\"table-layout: fixed; width: 100%\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbKMYVS/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/lbhC2er6FwLKjeoKn1U-qV5_9RfcusCBgAhVYWmK9WE=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>DeepSeek-V3.2 (5 minute read)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     DeepSeek's V3.2 matches GPT-5, and its higher-compute V3.2-Speciale variant rivals Gemini-3.0-Pro and earned gold medals at IMO, IOI, and ICPC 2025.                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frunwayml.com%2Fresearch%2Fintroducing-runway-gen-4.5%3Futm_source=tldrai/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/881RBTRijizwvehBac3UjQuUosRnUEJ_CymQ5vx3rws=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Introducing Runway Gen-4.5: A New Frontier for Video Generation (3 minute read)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Gen-4.5 topped the Artificial Analysis text-to-video benchmark, ahead of Veo 3 and Sora. The model emphasizes physical accuracy (realistic momentum, fluid motion, and material coherence). Runway has acknowledged persisting issues with object permanence.                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.androidauthority.com%2Fgoogle-gemini-projects-2-3620950%2F%3Futm_source=tldrai/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/J89LouIPpLQ5qM2x8VVTWiMTOpvclIfRgOhwcdiUc6g=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Early look: Gemini's ChatGPT-style 'projects' are taking shape (3 minute read)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Google has been working on a new 'projects' option for Gemini. The feature will allow users to sort files and chats around particular topics. Projects can be pinned for easy access. There is a 10-file limit, but paying users may receive a higher ceiling.                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding-top: 0px; padding-bottom: 0px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div style=\"text-align: center\"><span style=\"font-size: 36px\">🧠</span></div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding-top: 0px; padding-bottom: 0px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div style=\"text-align: center\">  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<h1><strong>Deep Dives &amp; Analysis</strong></h1> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table style=\"table-layout: fixed; width: 100%\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Falignment.openai.com%2Fscaling-code-verification%2F%3Futm_source=tldrai/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/EXIpIDNfr2sTz1iXHNLTHiGof-EQhaJZ0HgCKJTV-cM=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>A Practical Approach to Verifying Code at Scale (7 minute read)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     OpenAI trained an agentic code reviewer for Codex since noisy safety tools are inevitably bypassed. The system now handles 100k+ external PRs daily, providing repo-wide context and execution access. Internally, it has caught launch-blocking bugs and protected high-stakes experiments.                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffolio.benguzovsky.com%2Ftrain-test%3Futm_source=tldrai/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/doNW4N9MM1OsfJabxI_OThxLaVw2pcQQQBO6dbOgjkw=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>The End of the Train-Test Split (13 minute read)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Models don't know what is 'out of distribution' for themselves. Engineers will always be required in the loop until this problem is solved. It will never be easy to check the accuracy of models. Researchers need to look at their data, make sure it's clean, and label it correctly.                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alignmentforum.org%2Fposts%2FMnkeepcGirnJn736j%2Fhow-can-interpretability-researchers-help-agi-go-well%3Futm_source=tldrai/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/kPZWmU8uecLJYDn-ZdAeHDUfIyAfnHGIkE2wIAxy54w=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>How Can Interpretability Researchers Help AGI Go Well? (32 minute read)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     The Google DeepMind mechanistic interpretability team has pivoted to a pragmatic approach to interpretability over the last year. It is important to have empirical feedback on goals with good proxy tasks. Near-complete understanding isn't required for significant impact. Good focused projects start with a theory of change, and good exploratory projects start with a robustly useful setting.                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding-top: 0px; padding-bottom: 0px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div style=\"text-align: center\"><span style=\"font-size: 36px\">🧑‍💻</span></div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding-top: 0px; padding-bottom: 0px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div style=\"text-align: center\">  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<h1><strong>Engineering &amp; Research</strong></h1> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table style=\"table-layout: fixed; width: 100%\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ibm.com%2Freports%2Fcost-of-complexity%3Futm_content=CPWWW%26p1=Display%26p2=427890287%26p3=227599223%26utm_term=15IGO%26utm_source=tldrai/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/myQhrk2WT-1Ueb2jk9atZe4Nr9Ym53hIrhguJX_VrjQ=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Cut the cost of IT complexity (Sponsor)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     IT complexity can drive costs up across your company. Intelligent IT automation is key to taming technology chaos. Get the insights you need from the IBM Institute for Business Value. 👉 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ibm.com%2Freports%2Fcost-of-complexity%3Futm_content=CPWWW%26p1=Display%26p2=427890287%26p3=227599223%26utm_term=15IGO/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/5dXiBqz5-U3_whuEC1sCs6L0HIyJMVi977wFD9IjUYg=433\" rel=\"noreferrer\" target=\"_blank\"><span>Read the IBV report</span></a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblogs.nvidia.com%2Fblog%2Fneurips-open-source-digital-physical-ai%2F%3Futm_source=tldrai/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/gqoZf1BFFfXwenXAntlzUOLKK3a9U82iiJeODEAoLCo=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Nvidia Launches Vision Language Model for Autonomous Vehicles (4 minute read)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Nvidia introduced Alpamayo-R1, the first open-source vision language action model designed for autonomous driving, at NeurIPS. Alpamayo-R1 integrates visual and textual reasoning to enhance decision-making in real-world environments.                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuggingface.co%2Fapple%2Fstarflow%3Futm_source=tldrai/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/6Gjcat3lbBFwu5tb3KdULDbKi9w_rFSSz8H9O_2IW8Q=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>STARFlow: Scalable Transformer Auto-Regressive Flow (Hugging Face Repo)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     STARFlow and STARFlow-V are state-of-the-art transformer autoregressive flow models for high-quality image and video generation. STARFlow introduces a novel transformer autoregressive flow architecture that combines the expressiveness of autoregressive models with the efficiency of normalizing flows. STARFlow-V is an end-to-end video generative model with normalizing flows. Examples of generated videos and comparisons are available.                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FYuanshi9815%2FViBT%3Futm_source=tldrai/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/u71BzupyqRafbjDoGF6yds5VDojCoY_yIp7pUwB5mfA=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Bridge Models for Image and Video Translation (GitHub Repo)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     ViBT introduces Vision Bridge Transformers, scaling Brownian Bridge Models to 20B parameters for efficient conditional generation. The models use a Transformer architecture and a variance-stabilized objective for robust performance on image and video editing tasks.                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding-top: 0px; padding-bottom: 0px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div style=\"text-align: center\"><span style=\"font-size: 36px\">🎁</span></div></div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding-top: 0px; padding-bottom: 0px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div style=\"text-align: center\"><strong><h1>Miscellaneous</h1></strong></div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table style=\"table-layout: fixed; width: 100%\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FvEbUgh/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/MBOe8-7FmdaWFx4cuDWKx6bm4Jn9K7S6ROawe71JUFw=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>OpenAI Takes Stake in Thrive Holdings, a Buyer of Services Firms (6 minute read)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     OpenAI has taken an ownership stake in Thrive Holdings to embed AI into high-volume business processes, beginning with accounting and IT services. OpenAI teams will work directly inside Thrive's companies to improve speed, accuracy, and cost efficiency.                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fm83Im9/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/o-MAl1w4L6BQKSm8deb5v2-A_Oz1PWGvNuaFCTCcoXE=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>ByteDance's TikTok Playbook Is Winning Consumer AI (5 minute read)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     ByteDance's Doubao app is now China's most popular mobile AI platform. It received more than 11.4 million downloads in October. Doubao's focus on frictionless AI-powered voice, image, and video experiences sets it apart from competitors. ByteDance keeps its most advanced technology proprietary, breaking away from China's open-source approach. This could give it a durable commercial edge.                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>   \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding-top: 0px; padding-bottom: 0px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div style=\"text-align: center\"><span style=\"font-size: 36px\">⚡</span></div></div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding-top: 0px; padding-bottom: 0px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div style=\"text-align: center\">  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<h1><strong>Quick Links</strong></h1> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table style=\"table-layout: fixed; width: 100%\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dynatrace.com%2Finfo%2Fwhitepapers%2Fbridging-the-ai-production-gap%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=all-cloud-ai-observability-futurum-bridging-ai-gap-global%26utm_term=120225/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/dYxVGs2q99R72KZRNloP9owKgHgD6nSW2EyUTzrx-fc=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Billions spent, yet AI pilots stall. (Sponsor)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Discover why observability is the missing link for scaling trustworthy AI—and how it tackles hallucinations, compliance, and cost. <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dynatrace.com%2Finfo%2Fwhitepapers%2Fbridging-the-ai-production-gap%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=all-cloud-ai-observability-futurum-bridging-ai-gap-global%26utm_term=120225/2/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/6roQqiMCQTmWewiFzrgI1vLLQMvxUbFlG-NK1zpif5s=433\" rel=\"noreferrer\" target=\"_blank\"><span>View the findings</span></a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1995538428684087439.html%3Futm_source=tldrai/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/DgsfmUrgsDdAvmlzpnw5xufztc7JEalFIDx3IeIUM0E=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>a16z's gigawatt-scale data center timeline (1 minute read)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Gigawatt-scale data centers can likely be built in 2 years or less                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FlUb9hR/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/sbCD6IU5jjIhQnUJqc8_yiFoaAHHTbepR12vqtLfHCE=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Accenture and OpenAI Partnership (3 minute read)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Accenture will deploy ChatGPT Enterprise to tens of thousands of its professionals, marking the largest upskilling effort yet via OpenAI Certifications.                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F12%2F01%2Fblack-forest-labs-raises-300m-at-3-25b-valuation%2F%3Futm_source=tldrai/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/13MdXWp3u5m2Yc5P3vXe2fEYZCSe1FX4w-Kdoe2Nk9c=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Black Forest Labs raises $300M at $3.25B valuation (1 minute read)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     The Flux image model maker, whose co-founders also created Stable Diffusion, raised funding from Salesforce Ventures, a16z, Nvidia, and others.                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theverge.com%2Fnews%2F835466%2Fapple-ai-chief-john-giannandrea-steps-down-siri%3Futm_source=tldrai/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/wQRhSXaKLc9iMjaVdyCY9cb7umqsLq0gcFhGPFCNye0=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Apple AI chief steps down following Siri setbacks (3 minute read)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     John Giannandrea is out after Tim Cook reportedly &quot;lost confidence&quot; in his leadership after repeated delays on an updated Siri.                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1995543962342551738.html%3Futm_source=tldrai/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/XGxLSbkvYthnzd7brqBztAzwbk3H9HHOWJtA2LFzNPc=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Claude Code over Excel (1 minute read)</strong>                                     </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     The LlamaSheets API lets users automatically segment and structure complex Excel sheets into well-formatted 2D tables.                                 </span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>    \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td align=\"left\" style=\"word-break: break-word; vertical-align: top; padding: 5px 10px\">  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tLove TLDR? Tell your friends and get rewards! \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</p> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 0px 10px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tShare your referral link below with friends to get free TLDR swag! \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td align=\"left\" style=\"padding: 10px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frefer.tldr.tech%2Ff7118701%2F2/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/Jy1eTd6hrJyuQ3PtfSJl81BwUvdvtDOT6xMGMr6vYxg=433\" style=\"color: #464ba4; text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">https://refer.tldr.tech/f7118701/2</a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr></tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td align=\"left\" style=\"padding: 5px 10px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhub.sparklp.co%2Fsub_257b694721d2%2F2/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/WH9JjdFKTpLn3XBM6tertuNZDTI1W7ORA-4Ksfg7C9M=433\" style=\"font-size: 16px; line-height: 1.6; padding: 10px 0; display: inline-block; text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\"><span style=\"mso-text-raise: 13pt; text-decoration: underline\">Track your referrals here.</span></a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>     \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td align=\"left\" style=\"word-break: break-word; vertical-align: top; padding: 5px 10px\">  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tWant to advertise in TLDR? 📰 \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</p> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div style=\"margin-top: 10px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tIf your company is interested in reaching an audience of AI professionals and decision makers, you may want to <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/q7E60htVfm-HUlki1RCCt7-xnpEVvWjCnFJEBvhwzyk=433\" target=\"_blank\" rel=\"noreferrer\"><strong><span>advertise with us</span></strong></a>. \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br />  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tWant to work at TLDR? 💼 \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</p> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div style=\"margin-top: 10px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/7rUPM-ontOmpdlVwdHFwimSV6jeUirJnCI7vykUU7E0=433\" rel=\"noreferrer\" style=\"color: #0000EE; text-decoration: underline\" target=\"_blank\"><strong>Apply here</strong></a> or send a friend's resume to <a href=\"mailto:jobs@tldr.tech\" style=\"color: #0000EE; text-decoration: underline\" onclick=\"return rcmail.command('compose','jobs@tldr.tech',this)\" rel=\"noreferrer\">jobs@tldr.tech</a> and get $1k if we hire them! \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br />  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tIf you have any comments or feedback, just respond to this email! \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> Thanks for reading, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftwitter.com%2Fandrewztan/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/jhoQ0zuMFTSH_W1d-oL2MzFEkBu4UT7Uf8F0GE6FAzE=433\" target=\"_blank\" rel=\"noreferrer\"><span>Andrew Tan</span></a>, <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fin%2Faliiaminian%2F/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/D6txQf2LIz5LSKzCrK_kh7HrbhLzNNSOKDbe0esoyvs=433\" target=\"_blank\" rel=\"noreferrer\"><span>Ali Aminian</span></a>, &amp; <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fin%2Fjacob-turner-7521a8198%2F/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/s0rQM5rSbKGjNP2sHLU-TYVSi4HSsCxxTm2DGHvfGAo=433\" target=\"_blank\" rel=\"noreferrer\"><span>Jacob Turner</span></a> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table>  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<td style=\"padding: 15px 15px\"> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fai%2Fmanage%3Femail=tldrai90%2540ino.to/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/-TVE4c81CriyCLO6cY_K06PnvSOKxi6EaWU4KK0eZ5s=433\" target=\"_blank\" rel=\"noreferrer\">Manage your subscriptions</a> to our other newsletters on tech, startups, and programming. Or if TLDR AI isn't for you, please <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa.tldrnewsletter.com%2Funsubscribe%3Fep=1%26l=eedf6b14-3de3-11ed-9a32-0241b9615763%26lc=8cb79176-9621-11f0-a63a-a3daa84d9496%26p=8cea03fe-cf74-11f0-8827-b37fe40dee87%26pt=campaign%26pv=4%26spa=1764684151%26t=1764685886%26s=e7c01a51b7c31902538909937dcba8938999706ea59a033e216ebf8a425bd9dd/1/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/ZuIB1biDBLdQFVOJJwcvJGeriJpb-pZJkQGegF60TQI=433\" target=\"_blank\" rel=\"noreferrer\">unsubscribe</a>. \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<br /> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</div> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</table> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t\t\t\t\t</table> \t\t\t\t\t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t\t\t\t\t</table> \t\t\t\t\t\t\t\t\t</td> \t\t\t\t\t\t\t\t</tr> \t\t\t\t\t\t\t</tbody> \t\t\t\t\t\t</table> \t\t\t\t\t</td> \t\t\t\t</tr> \t\t\t</tbody> \t\t</table>      <img src=\"http://tracking.tldrnewsletter.com/CI0/0100019adf79b401-6db6be9d-af1e-41a0-b095-7debdb17ea35-000000/RoeNYslEBehT7H2E3DCugMlyNDmeVYhCFIyoVE_KO0M=433\" style=\"display: none; width: 1px; height: 1px\" /> </div></div></div>",
      "summary": " \t\t \t\t \t\t \t\t \t\t \t\t \t\t \t\t \t\t \t \t   \t\tDeepSeek's V3.2 matches GPT-5, and its higher-compute V3.2-Speciale variant rivals Gemini-3.0-Pro and earned gold medals at IMO, IOI, and ICPC 2025. ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  \t\t \t\t\t \t\t  \t\t \t\t\t \t\t\t\t \t\t\t\t\t  \t\t\t\t\t\t \t\t\t\t\t\t\t \t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t  \t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t  \t\t\t\t\t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t",
      "publishedAt": "2025-12-02T14:31:30.000Z",
      "author": "TLDR AI ",
      "source": "rss",
      "feedName": "TLDR",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 14.99066310058185,
      "ingestedAt": "2025-12-02T14:44:03.162Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "observability",
        "governance",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09b0b9b3",
      "title": "Gary Tan claims Zoho will be out of business due to vibe coding",
      "url": "https://twitter.com/garrytan/status/1995664097007091818",
      "content": "<p>Article URL: <a href=\"https://twitter.com/garrytan/status/1995664097007091818\">https://twitter.com/garrytan/status/1995664097007091818</a></p> \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46120728\">https://news.ycombinator.com/item?id=46120728</a></p> \n<p>Points: 30</p> \n<p># Comments: 22</p>",
      "summary": "Article URL: https://twitter.com/garrytan/status/1995664097007091818 \nComments URL: https://news.ycombinator.com/item?id=46120728 \nPoints: 30 \n# Comments: 22",
      "publishedAt": "2025-12-02T12:54:56.000Z",
      "author": "manojlds",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4919029326368953,
      "ingestedAt": "2025-12-02T14:44:03.162Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fa1",
      "title": "MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications",
      "url": "https://arxiv.org/abs/2512.01710v1",
      "content": "Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.",
      "summary": "Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting lay",
      "publishedAt": "2025-12-01T14:16:57.000Z",
      "author": "Stefano Zeppieri",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 13.01736679535248,
      "ingestedAt": "2025-12-02T14:44:03.168Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fa2",
      "title": "StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos",
      "url": "https://arxiv.org/abs/2512.01707v1",
      "content": "Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.",
      "summary": "Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streami",
      "publishedAt": "2025-12-01T14:15:44.000Z",
      "author": "Daeun Lee",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 9.297558010060175,
      "ingestedAt": "2025-12-02T14:44:03.168Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fa4",
      "title": "LLM-Driven Multi-Agent Curation and Expansion of Metal-Organic Frameworks Database",
      "url": "https://arxiv.org/abs/2512.01693v1",
      "content": "Metal-organic framework (MOF) databases have grown rapidly through experimental deposition and large-scale literature extraction, but recent analyses show that nearly half of their entries contain substantial structural errors. These inaccuracies propagate through high-throughput screening and machine-learning workflows, limiting the reliability of data-driven MOF discovery. Correcting such errors is exceptionally difficult because true repairs require integrating crystallographic files, synthesis descriptions, and contextual evidence scattered across the literature. Here we introduce LitMOF, a large language model-driven multi-agent framework that validates crystallographic information directly from the original literature and cross-validates it with database entries to repair structural errors. Applying LitMOF to the experimental MOF database (the CSD MOF Subset), we constructed LitMOF-DB, a curated set 118,464 computation-ready structures, including corrections of 69% (6,161 MOFs) of the invalid MOFs in the latest CoRE MOF database. Additionally, the system uncovered 12,646 experimentally reported MOFs absent from existing resources, substantially expanding the known experimental design space. This work establishes a scalable pathway toward self-correcting scientific databases and a generalizable paradigm for LLM-driven curation in materials science.",
      "summary": "Metal-organic framework (MOF) databases have grown rapidly through experimental deposition and large-scale literature extraction, but recent analyses show that nearly half of their entries contain substantial structural errors. These inaccuracies propagate through high-throughput screening and machine-learning workflows, limiting the reliability of data-driven MOF discovery. Correcting such errors is exceptionally difficult because true repairs require integrating crystallographic files, synthes",
      "publishedAt": "2025-12-01T13:59:55.000Z",
      "author": "Honghui Kim",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 7.896726446531691,
      "ingestedAt": "2025-12-02T14:44:03.168Z",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fa7",
      "title": "Generating REST API Tests With Descriptive Names",
      "url": "https://arxiv.org/abs/2512.01690v1",
      "content": "Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5. \n  To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability. \n  These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation.",
      "summary": "Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descri",
      "publishedAt": "2025-12-01T13:58:06.000Z",
      "author": "Philip Garrett",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "security_incident",
      "score": 6.967071957664221,
      "ingestedAt": "2025-12-02T14:44:03.168Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fa9",
      "title": "Morphling: Fast, Fused, and Flexible GNN Training at Scale",
      "url": "https://arxiv.org/abs/2512.01678v1",
      "content": "Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. The results show that Morphling improves per-epoch training throughput by an average of 20X on CPUs and 19X on GPUs over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.",
      "summary": "Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations",
      "publishedAt": "2025-12-01T13:45:03.000Z",
      "author": "Anubhab",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 7.4267343808698545,
      "ingestedAt": "2025-12-02T14:44:03.168Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fad",
      "title": "ICAD-LLM: One-for-All Anomaly Detection via In-Context Learning with Large Language Models",
      "url": "https://arxiv.org/abs/2512.01672v1",
      "content": "Anomaly detection (AD) is a fundamental task of critical importance across numerous domains. Current systems increasingly operate in rapidly evolving environments that generate diverse yet interconnected data modalities -- such as time series, system logs, and tabular records -- as exemplified by modern IT systems. Effective AD methods in such environments must therefore possess two critical capabilities: (1) the ability to handle heterogeneous data formats within a unified framework, allowing the model to process and detect multiple modalities in a consistent manner during anomalous events; (2) a strong generalization ability to quickly adapt to new scenarios without extensive retraining. However, most existing methods fall short of these requirements, as they typically focus on single modalities and lack the flexibility to generalize across domains. To address this gap, we introduce a novel paradigm: In-Context Anomaly Detection (ICAD), where anomalies are defined by their dissimilarity to a relevant reference set of normal samples. Under this paradigm, we propose ICAD-LLM, a unified AD framework leveraging Large Language Models' in-context learning abilities to process heterogeneous data within a single model. Extensive experiments demonstrate that ICAD-LLM achieves competitive performance with task-specific AD methods and exhibits strong generalization to previously unseen tasks, which substantially reduces deployment costs and enables rapid adaptation to new environments. To the best of our knowledge, ICAD-LLM is the first model capable of handling anomaly detection tasks across diverse domains and modalities.",
      "summary": "Anomaly detection (AD) is a fundamental task of critical importance across numerous domains. Current systems increasingly operate in rapidly evolving environments that generate diverse yet interconnected data modalities -- such as time series, system logs, and tabular records -- as exemplified by modern IT systems. Effective AD methods in such environments must therefore possess two critical capabilities: (1) the ability to handle heterogeneous data formats within a unified framework, allowing t",
      "publishedAt": "2025-12-01T13:41:30.000Z",
      "author": "Zhongyuan Wu",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.569070030085781,
      "ingestedAt": "2025-12-02T14:44:03.169Z",
      "tags": [
        "code_review",
        "retrieval"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fae",
      "title": "Mapping the Landscape of Open Access Dashboards - A Dataset for Research and Infrastructure Development",
      "url": "https://arxiv.org/abs/2512.01669v1",
      "content": "As Open Access continues to gain importance in science policy, understanding the proportion of Open Access publications relative to the total research output of research-performing organizations, individual countries, or even globally has become increasingly relevant. In response, dashboards are being developed to capture and communicate progress in this area. To provide an overview of these dashboards and their characteristics, an extensive survey was conducted, resulting in the identification of nearly 60 dashboards. To support a detailed and structured description, a dedicated metadata schema was developed, and the identified dashboards were systematically indexed accordingly. To foster community engagement and ensure ongoing development, a participatory process was launched, allowing interested stakeholders to contribute to the dataset. The dataset is particularly relevant for researchers in Library and Information Science (LIS) and Science and Technology Studies (STS), supporting both empirical analyses of Open Access and the methodological refinement of indicators and policy instruments in the context of Open Science.",
      "summary": "As Open Access continues to gain importance in science policy, understanding the proportion of Open Access publications relative to the total research output of research-performing organizations, individual countries, or even globally has become increasingly relevant. In response, dashboards are being developed to capture and communicate progress in this area. To provide an overview of these dashboards and their characteristics, an extensive survey was conducted, resulting in the identification ",
      "publishedAt": "2025-12-01T13:39:22.000Z",
      "author": "Johannes Schneider",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 12.993121730599988,
      "ingestedAt": "2025-12-02T14:44:03.169Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "governance"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fb1",
      "title": "Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems",
      "url": "https://arxiv.org/abs/2512.01661v1",
      "content": "Ensuring LLM reliability requires not only solving complex problems but also recognizing when a problem is unsolvable. Current models often struggle to distinguish objective unsolvability (inherent contradictions in the problem) from subjective capability limitations (problems beyond the model's competence), which leads to hallucinations and overconfidence. To address this, we propose UnsolvableQA and UnsolvableRL to solve feasible problems, detect inherent contradictions, and prudently refuse tasks beyond capability. Specifically, we construct UnsolvableQA, a dataset of paired solvable and unsolvable instances derived via a dual-track methodology: programmatic generation for logic puzzles and a novel \"Reverse Construction\" method that injects contradictions into valid reasoning chains for mathematics. Building on this dataset, we introduce UnsolvableRL, a reinforcement learning framework with three reward components jointly accounting for accuracy, unsolvability, and difficulty. Empirical results show that our approach achieves near-perfect unsolvability detection while also improving accuracy on solvable tasks. Crucially, we identify Capability Collapse, demonstrating that explicit exposure to unsolvable data is indispensable for preventing models from becoming systematically overconfident. Our code and data are available at https://github.com/sfasfaffa/unsolvableQA.",
      "summary": "Ensuring LLM reliability requires not only solving complex problems but also recognizing when a problem is unsolvable. Current models often struggle to distinguish objective unsolvability (inherent contradictions in the problem) from subjective capability limitations (problems beyond the model's competence), which leads to hallucinations and overconfidence. To address this, we propose UnsolvableQA and UnsolvableRL to solve feasible problems, detect inherent contradictions, and prudently refuse t",
      "publishedAt": "2025-12-01T13:32:59.000Z",
      "author": "Dengyun Peng",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.1028246983114425,
      "ingestedAt": "2025-12-02T14:44:03.169Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fb5",
      "title": "HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment",
      "url": "https://arxiv.org/abs/2512.01659v1",
      "content": "Legal AI systems powered by retrieval-augmented generation (RAG) face a critical accountability challenge: when an AI assistant cites case law, statutes, or contractual clauses, practitioners need verifiable guarantees that generated text faithfully represents source documents. Existing hallucination detectors rely on semantic similarity metrics that tolerate entity substitutions, a dangerous failure mode when confusing parties, dates, or legal provisions can have material consequences. We introduce HalluGraph, a graph-theoretic framework that quantifies hallucinations through structural alignment between knowledge graphs extracted from context, query, and response. Our approach produces bounded, interpretable metrics decomposed into \\textit{Entity Grounding} (EG), measuring whether entities in the response appear in source documents, and \\textit{Relation Preservation} (RP), verifying that asserted relationships are supported by context. On structured control documents, HalluGraph achieves near-perfect discrimination ($&gt;$400 words, $&gt;$20 entities), HalluGraph achieves $AUC = 0.979$, while maintaining robust performance ($AUC \\approx 0.89$) on challenging generative legal task, consistently outperforming semantic similarity baselines. The framework provides the transparency and traceability required for high-stakes legal applications, enabling full audit trails from generated assertions back to source passages.",
      "summary": "Legal AI systems powered by retrieval-augmented generation (RAG) face a critical accountability challenge: when an AI assistant cites case law, statutes, or contractual clauses, practitioners need verifiable guarantees that generated text faithfully represents source documents. Existing hallucination detectors rely on semantic similarity metrics that tolerate entity substitutions, a dangerous failure mode when confusing parties, dates, or legal provisions can have material consequences. We intro",
      "publishedAt": "2025-12-01T13:31:06.000Z",
      "author": "Valentin Noël",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 9.276996396734393,
      "ingestedAt": "2025-12-02T14:44:03.169Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability",
        "governance"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fba",
      "title": "Computing Treedepth Obstructions",
      "url": "https://arxiv.org/abs/2512.01658v1",
      "content": "The graph parameter treedepth is minor-monotone; hence, the class of graphs with treedepth at most $k$ is minor-closed. By the Graph Minor Theorem, such a class is characterized by a finite set of forbidden minors. A conjecture of Dvořák, Giannopoulou, and Thilikos states that every such forbidden minor has at most $2^k$ vertices. We present an algorithm that, given $n, k \\in \\mathbb{N}$, computes the set of forbidden minors, forbidden subgraphs, and forbidden induced subgraphs on at most $n$ vertices, for the class of graphs of treedepth at most $k$. Applying this algorithm to $k = 4$ and $n = 16$, we enumerate 1546 forbidden minors, 1718 forbidden subgraphs, and 12204 forbidden induced subgraphs. Assuming the above conjecture holds, these sets constitute the complete obstruction sets for graphs of treedepth at most 4.",
      "summary": "The graph parameter treedepth is minor-monotone; hence, the class of graphs with treedepth at most $k$ is minor-closed. By the Graph Minor Theorem, such a class is characterized by a finite set of forbidden minors. A conjecture of Dvořák, Giannopoulou, and Thilikos states that every such forbidden minor has at most $2^k$ vertices. We present an algorithm that, given $n, k \\in \\mathbb{N}$, computes the set of forbidden minors, forbidden subgraphs, and forbidden induced subgraphs on at most $n$ ve",
      "publishedAt": "2025-12-01T13:30:53.000Z",
      "author": "Kolja Kühn",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.174603512345548,
      "ingestedAt": "2025-12-02T14:44:03.169Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fbc",
      "title": "In-context Inverse Optimality for Fair Digital Twins: A Preference-based approach",
      "url": "https://arxiv.org/abs/2512.01650v1",
      "content": "Digital Twins (DTs) are increasingly used as autonomous decision-makers in complex socio-technical systems. Their mathematically optimal decisions often diverge from human expectations, exposing a persistent gap between algorithmic and bounded human rationality. This work addresses this gap by proposing a framework that operationalizes fairness as a learnable objective within optimization-based Digital Twins. We introduce a preference-driven learning pipeline that infers latent fairness objectives directly from human pairwise preferences over feasible decisions. A novel Siamese neural network is developed to generate convex quadratic cost functions conditioned on contextual information. The resulting surrogate objectives align optimization outcomes with human-perceived fairness while maintaining computational efficiency. The approach is demonstrated on a COVID-19 hospital resource allocation scenario. This study provides an actionable path toward embedding human-centered fairness in the design of autonomous decision-making systems.",
      "summary": "Digital Twins (DTs) are increasingly used as autonomous decision-makers in complex socio-technical systems. Their mathematically optimal decisions often diverge from human expectations, exposing a persistent gap between algorithmic and bounded human rationality. This work addresses this gap by proposing a framework that operationalizes fairness as a learnable objective within optimization-based Digital Twins. We introduce a preference-driven learning pipeline that infers latent fairness objectiv",
      "publishedAt": "2025-12-01T13:23:27.000Z",
      "author": "Daniele Masti",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 8.346129098175538,
      "ingestedAt": "2025-12-02T14:44:03.169Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fbe",
      "title": "MIT Lincoln Laboratory: A Case Study on Improving Software Support for Research Projects",
      "url": "https://arxiv.org/abs/2512.01649v1",
      "content": "Software plays an ever increasing role in complex system development and prototyping, and in recent years, MIT Lincoln Laboratory has sought to improve both the effectiveness and culture surrounding software engineering in execution of its mission. The Homeland Protection and Air Traffic Control Division conducted an internal study to examine challenges to effective and efficient research software development, and to identify ways to strengthen both the culture and execution for greater impact on our mission. Key findings of this study fell into three main categories: project attributes that influence how software development activities must be conducted and managed, potential efficiencies from centralization, opportunities to improve staffing and culture with respect to software practitioners. The study delivered actionable recommendations, including centralizing and standardizing software support tooling, developing a common database to help match the right software talent and needs to projects, and creating a software stakeholder panel to assist with continued improvement.",
      "summary": "Software plays an ever increasing role in complex system development and prototyping, and in recent years, MIT Lincoln Laboratory has sought to improve both the effectiveness and culture surrounding software engineering in execution of its mission. The Homeland Protection and Air Traffic Control Division conducted an internal study to examine challenges to effective and efficient research software development, and to identify ways to strengthen both the culture and execution for greater impact o",
      "publishedAt": "2025-12-01T13:22:58.000Z",
      "author": "Daniel Strassler",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 6.954940836027524,
      "ingestedAt": "2025-12-02T14:44:03.169Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fbf",
      "title": "Package Dashboard: A Cross-Ecosystem Framework for Dual-Perspective Analysis of Software Packages",
      "url": "https://arxiv.org/abs/2512.01630v1",
      "content": "Software supply chain attacks have revealed blind spots in existing SCA tools, which are often limited to a single ecosystem and assess either software artifacts or community activity in isolation. This fragmentation across tools and ecosystems forces developers to manually reconcile scattered data, undermining risk assessments. We present Package Dashboard, a cross-ecosystem framework that provides a unified platform for supply chain analysis, enabling a holistic, dual-perspective risk assessment by integrating package metadata, vulnerability information, and upstream community health metrics. By combining dependency resolution with repository analysis, it reduces cognitive load and improves traceability. Demonstrating the framework's versatility, a large-scale study of 374,000 packages across five Linux distributions shows its ability to uncover not only conventional vulnerabilities and license conflicts but also overlooked risks such as archived or inaccessible repositories. Ultimately, Package Dashboard provides a unified view of risk, equipping developers and DevSecOps engineers with actionable insights to strengthen the transparency, trustworthiness, and traceability of open-source ecosystems. Package Dashboard is publicly available at https://github.com/n19htfall/PackageDashboard, and a demonstration video can be found at https://youtu.be/y9ncftP8KPQ. Besides, the online version is available at https://pkgdash.osslab-pku.org.",
      "summary": "Software supply chain attacks have revealed blind spots in existing SCA tools, which are often limited to a single ecosystem and assess either software artifacts or community activity in isolation. This fragmentation across tools and ecosystems forces developers to manually reconcile scattered data, undermining risk assessments. We present Package Dashboard, a cross-ecosystem framework that provides a unified platform for supply chain analysis, enabling a holistic, dual-perspective risk assessme",
      "publishedAt": "2025-12-01T12:52:03.000Z",
      "author": "Ziheng Liu",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "pricing_business",
      "score": 9.259044210241706,
      "ingestedAt": "2025-12-02T14:44:03.169Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fc0",
      "title": "When High-Performance Computing Meets Software Testing: Distributed Fuzzing using MPI",
      "url": "https://arxiv.org/abs/2512.01617v1",
      "content": "This paper explores the integration of MPI-based synchronization techniques into distributed fuzzing frameworks, highlighting possible substantial performance improvements compared to traditional filesystem-based synchronization methods. By employing lightweight MPI primitives, reductions in communication latency are achieved, facilitating more efficient data exchanges across distributed fuzzing nodes. Experimental results obtained over standard benchmarks demonstrate enhanced coverage progression from the early stages of the fuzzing process, which could be beneficial if fuzzing is employed in CI/CD pipelines at any stage of software development. Furthermore, the coordinated exchange of input corpora among clusters of fuzzers effectively addresses coverage stagnation, enabling a sustained exploration of complex and deep execution paths. Overall, the adoption of MPI-based synchronization approaches shows promising potential for significantly enhancing the scalability and efficacy of distributed fuzz testing.",
      "summary": "This paper explores the integration of MPI-based synchronization techniques into distributed fuzzing frameworks, highlighting possible substantial performance improvements compared to traditional filesystem-based synchronization methods. By employing lightweight MPI primitives, reductions in communication latency are achieved, facilitating more efficient data exchanges across distributed fuzzing nodes. Experimental results obtained over standard benchmarks demonstrate enhanced coverage progressi",
      "publishedAt": "2025-12-01T12:38:20.000Z",
      "author": "Pierciro Caliandro",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 10.640658578332667,
      "ingestedAt": "2025-12-02T14:44:03.169Z",
      "tags": [
        "code_review",
        "retrieval",
        "testing"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fc2",
      "title": "CLIP-RL: Aligning Language and Policy Representations for Task Transfer in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2512.01616v1",
      "content": "Recently, there has been an increasing need to develop agents capable of solving multiple tasks within the same environment, especially when these tasks are naturally associated with language. In this work, we propose a novel approach that leverages combinations of pre-trained (language, policy) pairs to establish an efficient transfer pipeline. Our algorithm is inspired by the principles of Contrastive Language-Image Pretraining (CLIP) in Computer Vision, which aligns representations across different modalities under the philosophy that ''two modalities representing the same concept should have similar representations.'' The central idea here is that the instruction and corresponding policy of a task represent the same concept, the task itself, in two different modalities. Therefore, by extending the idea of CLIP to RL, our method creates a unified representation space for natural language and policy embeddings. Experimental results demonstrate the utility of our algorithm in achieving faster transfer across tasks.",
      "summary": "Recently, there has been an increasing need to develop agents capable of solving multiple tasks within the same environment, especially when these tasks are naturally associated with language. In this work, we propose a novel approach that leverages combinations of pre-trained (language, policy) pairs to establish an efficient transfer pipeline. Our algorithm is inspired by the principles of Contrastive Language-Image Pretraining (CLIP) in Computer Vision, which aligns representations across dif",
      "publishedAt": "2025-12-01T12:37:01.000Z",
      "author": "Chainesh Gautam",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 16.65385614877517,
      "ingestedAt": "2025-12-02T14:44:03.169Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "governance",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fc3",
      "title": "Agent-Kernel: A MicroKernel Multi-Agent System Framework for Adaptive Social Simulation Powered by LLMs",
      "url": "https://arxiv.org/abs/2512.01610v1",
      "content": "Multi-Agent System (MAS) developing frameworks serve as the foundational infrastructure for social simulations powered by Large Language Models (LLMs). However, existing frameworks fail to adequately support large-scale simulation development due to inherent limitations in adaptability, configurability, reliability, and code reusability. For example, they cannot simulate a society where the agent population and profiles change over time. To fill this gap, we propose Agent-Kernel, a framework built upon a novel society-centric modular microkernel architecture. It decouples core system functions from simulation logic and separates cognitive processes from physical environments and action execution. Consequently, Agent-Kernel achieves superior adaptability, configurability, reliability, and reusability. We validate the framework's superiority through two distinct applications: a simulation of the Universe 25 (Mouse Utopia) experiment, which demonstrates the handling of rapid population dynamics from birth to death; and a large-scale simulation of the Zhejiang University Campus Life, successfully coordinating 10,000 heterogeneous agents, including students and faculty.",
      "summary": "Multi-Agent System (MAS) developing frameworks serve as the foundational infrastructure for social simulations powered by Large Language Models (LLMs). However, existing frameworks fail to adequately support large-scale simulation development due to inherent limitations in adaptability, configurability, reliability, and code reusability. For example, they cannot simulate a society where the agent population and profiles change over time. To fill this gap, we propose Agent-Kernel, a framework bui",
      "publishedAt": "2025-12-01T12:30:58.000Z",
      "author": "Yuren Mao",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 7.861961236758633,
      "ingestedAt": "2025-12-02T14:44:03.169Z",
      "tags": [
        "code_review",
        "agents",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fc4",
      "title": "GPTrace: Effective Crash Deduplication Using LLM Embeddings",
      "url": "https://arxiv.org/abs/2512.01609v1",
      "content": "Fuzzing is a highly effective method for uncovering software vulnerabilities, but analyzing the resulting data typically requires substantial manual effort. This is amplified by the fact that fuzzing campaigns often find a large number of crashing inputs, many of which share the same underlying bug. Crash deduplication is the task of finding such duplicate crashing inputs and thereby reducing the data that needs to be examined. Many existing deduplication approaches rely on comparing stack traces or other information that is collected when a program crashes. Although various metrics for measuring the similarity of such pieces of information have been proposed, many do not yield satisfactory deduplication results. In this work, we present GPTrace, a deduplication workflow that leverages a large language model to evaluate the similarity of various data sources associated with crashes by computing embedding vectors and supplying those as input to a clustering algorithm. We evaluate our approach on over 300 000 crashing inputs belonging to 50 ground truth labels from 14 different targets. The deduplication results produced by GPTrace show a noticeable improvement over hand-crafted stack trace comparison methods and even more complex state-of-the-art approaches that are less flexible.",
      "summary": "Fuzzing is a highly effective method for uncovering software vulnerabilities, but analyzing the resulting data typically requires substantial manual effort. This is amplified by the fact that fuzzing campaigns often find a large number of crashing inputs, many of which share the same underlying bug. Crash deduplication is the task of finding such duplicate crashing inputs and thereby reducing the data that needs to be examined. Many existing deduplication approaches rely on comparing stack trace",
      "publishedAt": "2025-12-01T12:30:30.000Z",
      "author": "Patrick Herter",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 7.399321646137917,
      "ingestedAt": "2025-12-02T14:44:03.169Z",
      "tags": [
        "code_review",
        "retrieval",
        "observability"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fc6",
      "title": "MAC-SLU: Multi-Intent Automotive Cabin Spoken Language Understanding Benchmark",
      "url": "https://arxiv.org/abs/2512.01603v1",
      "content": "Spoken Language Understanding (SLU), which aims to extract user semantics to execute downstream tasks, is a crucial component of task-oriented dialog systems. Existing SLU datasets generally lack sufficient diversity and complexity, and there is an absence of a unified benchmark for the latest Large Language Models (LLMs) and Large Audio Language Models (LALMs). This work introduces MAC-SLU, a novel Multi-Intent Automotive Cabin Spoken Language Understanding Dataset, which increases the difficulty of the SLU task by incorporating authentic and complex multi-intent data. Based on MAC-SLU, we conducted a comprehensive benchmark of leading open-source LLMs and LALMs, covering methods like in-context learning, supervised fine-tuning (SFT), and end-to-end (E2E) and pipeline paradigms. Our experiments show that while LLMs and LALMs have the potential to complete SLU tasks through in-context learning, their performance still lags significantly behind SFT. Meanwhile, E2E LALMs demonstrate performance comparable to pipeline approaches and effectively avoid error propagation from speech recognition. Code\\footnote{https://github.com/Gatsby-web/MAC\\_SLU} and datasets\\footnote{huggingface.co/datasets/Gatsby1984/MAC\\_SLU} are released publicly.",
      "summary": "Spoken Language Understanding (SLU), which aims to extract user semantics to execute downstream tasks, is a crucial component of task-oriented dialog systems. Existing SLU datasets generally lack sufficient diversity and complexity, and there is an absence of a unified benchmark for the latest Large Language Models (LLMs) and Large Audio Language Models (LALMs). This work introduces MAC-SLU, a novel Multi-Intent Automotive Cabin Spoken Language Understanding Dataset, which increases the difficul",
      "publishedAt": "2025-12-01T12:23:19.000Z",
      "author": "Yuezhang Peng",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 6.934392761222744,
      "ingestedAt": "2025-12-02T14:44:03.169Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fc7",
      "title": "Separator Theorem for Minor-Free Graphs in Linear Time",
      "url": "https://arxiv.org/abs/2512.01587v1",
      "content": "The planar separator theorem by Lipton and Tarjan [FOCS '77, SIAM Journal on Applied Mathematics '79] states that any planar graph with $n$ vertices has a balanced separator of size $O(\\sqrt{n})$ that can be found in linear time. This landmark result kicked off decades of research on designing linear or nearly linear-time algorithms on planar graphs. In an attempt to generalize Lipton-Tarjan's theorem to nonplanar graphs, Alon, Seymour, and Thomas [STOC '90, Journal of the AMS '90] showed that any minor-free graph admits a balanced separator of size $O(\\sqrt{n})$ that can be found in $O(n^{3/2})$ time. The superlinear running time in their separator theorem is a key bottleneck for generalizing algorithmic results from planar to minor-free graphs. Despite extensive research for more than two decades, finding a balanced separator of size $O(\\sqrt{n})$ in (linear) $O(n)$ time for minor-free graphs remains a major open problem. Known algorithms either give a separator of size much larger than $O(\\sqrt{n})$ or have superlinear running time, or both. \n  In this paper, we answer the open problem affirmatively. Our algorithm is very simple: it runs a vertex-weighted variant of breadth-first search (BFS) a constant number of times on the input graph. Our key technical contribution is a weighting scheme on the vertices to guide the search for a balanced separator, offering a new connection between the size of a balanced separator and the existence of a clique-minor model. We believe that our weighting scheme may be of independent interest.",
      "summary": "The planar separator theorem by Lipton and Tarjan [FOCS '77, SIAM Journal on Applied Mathematics '79] states that any planar graph with $n$ vertices has a balanced separator of size $O(\\sqrt{n})$ that can be found in linear time. This landmark result kicked off decades of research on designing linear or nearly linear-time algorithms on planar graphs. In an attempt to generalize Lipton-Tarjan's theorem to nonplanar graphs, Alon, Seymour, and Thomas [STOC '90, Journal of the AMS '90] showed that a",
      "publishedAt": "2025-12-01T12:01:04.000Z",
      "author": "Édouard Bonnet",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.079612044731545,
      "ingestedAt": "2025-12-02T14:44:03.170Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fc9",
      "title": "RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions",
      "url": "https://arxiv.org/abs/2512.01582v1",
      "content": "In this paper, we introduce RoleMotion, a large-scale human motion dataset that encompasses a wealth of role-playing and functional motion data tailored to fit various specific scenes. Existing text datasets are mainly constructed decentrally as amalgamation of assorted subsets that their data are nonfunctional and isolated to work together to cover social activities in various scenes. Also, the quality of motion data is inconsistent, and textual annotation lacks fine-grained details in these datasets. In contrast, RoleMotion is meticulously designed and collected with a particular focus on scenes and roles. The dataset features 25 classic scenes, 110 functional roles, over 500 behaviors, and 10296 high-quality human motion sequences of body and hands, annotated with 27831 fine-grained text descriptions. We build an evaluator stronger than existing counterparts, prove its reliability, and evaluate various text-to-motion methods on our dataset. Finally, we explore the interplay of motion generation of body and hands. Experimental results demonstrate the high-quality and functionality of our dataset on text-driven whole-body generation.",
      "summary": "In this paper, we introduce RoleMotion, a large-scale human motion dataset that encompasses a wealth of role-playing and functional motion data tailored to fit various specific scenes. Existing text datasets are mainly constructed decentrally as amalgamation of assorted subsets that their data are nonfunctional and isolated to work together to cover social activities in various scenes. Also, the quality of motion data is inconsistent, and textual annotation lacks fine-grained details in these da",
      "publishedAt": "2025-12-01T11:59:03.000Z",
      "author": "Junran Peng",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.15563049715277,
      "ingestedAt": "2025-12-02T14:44:03.170Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fca",
      "title": "From Black Hole to Galaxy: Neural Operator: Framework for Accretion and Feedback Dynamics",
      "url": "https://arxiv.org/abs/2512.01576v1",
      "content": "Modeling how supermassive black holes co-evolve with their host galaxies is notoriously hard because the relevant physics spans nine orders of magnitude in scale-from milliparsecs to megaparsecs--making end-to-end first-principles simulation infeasible. To characterize the feedback from the small scales, existing methods employ a static subgrid scheme or one based on theoretical guesses, which usually struggle to capture the time variability and derive physically faithful results. Neural operators are a class of machine learning models that achieve significant speed-up in simulating complex dynamics. We introduce a neural-operator-based ''subgrid black hole'' that learns the small-scale local dynamics and embeds it within the direct multi-level simulations. Trained on small-domain (general relativistic) magnetohydrodynamic data, the model predicts the unresolved dynamics needed to supply boundary conditions and fluxes at coarser levels across timesteps, enabling stable long-horizon rollouts without hand-crafted closures. Thanks to the great speedup in fine-scale evolution, our approach for the first time captures intrinsic variability in accretion-driven feedback, allowing dynamic coupling between the central black hole and galaxy-scale gas. This work reframes subgrid modeling in computational astrophysics with scale separation and provides a scalable path toward data-driven closures for a broad class of systems with central accretors.",
      "summary": "Modeling how supermassive black holes co-evolve with their host galaxies is notoriously hard because the relevant physics spans nine orders of magnitude in scale-from milliparsecs to megaparsecs--making end-to-end first-principles simulation infeasible. To characterize the feedback from the small scales, existing methods employ a static subgrid scheme or one based on theoretical guesses, which usually struggle to capture the time variability and derive physically faithful results. Neural operato",
      "publishedAt": "2025-12-01T11:47:49.000Z",
      "author": "Nihaal Bhojwani",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.076274606901928,
      "ingestedAt": "2025-12-02T14:44:03.170Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fcc",
      "title": "Reconstructing Multi-Scale Physical Fields from Extremely Sparse Measurements with an Autoencoder-Diffusion Cascade",
      "url": "https://arxiv.org/abs/2512.01572v1",
      "content": "Reconstructing full fields from extremely sparse and random measurements is a longstanding ill-posed inverse problem. A powerful framework for addressing such challenges is hierarchical probabilistic modeling, where uncertainty is represented by intermediate variables and resolved through marginalization during inference. Inspired by this principle, we propose Cascaded Sensing (Cas-Sensing), a hierarchical reconstruction framework that integrates an autoencoder-diffusion cascade. First, a neural operator-based functional autoencoder reconstructs the dominant structures of the original field - including large-scale components and geometric boundaries - from arbitrary sparse inputs, serving as an intermediate variable. Then, a conditional diffusion model, trained with a mask-cascade strategy, generates fine-scale details conditioned on these large-scale structures. To further enhance fidelity, measurement consistency is enforced via the manifold constrained gradient based on Bayesian posterior sampling during the generation process. This cascaded pipeline substantially alleviates ill-posedness, delivering accurate and robust reconstructions. Experiments on both simulation and real-world datasets demonstrate that Cas-Sensing generalizes well across varying sensor configurations and geometric boundaries, making it a promising tool for practical deployment in scientific and engineering applications.",
      "summary": "Reconstructing full fields from extremely sparse and random measurements is a longstanding ill-posed inverse problem. A powerful framework for addressing such challenges is hierarchical probabilistic modeling, where uncertainty is represented by intermediate variables and resolved through marginalization during inference. Inspired by this principle, we propose Cascaded Sensing (Cas-Sensing), a hierarchical reconstruction framework that integrates an autoencoder-diffusion cascade. First, a neural",
      "publishedAt": "2025-12-01T11:46:14.000Z",
      "author": "Letian Yi",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.075875940275982,
      "ingestedAt": "2025-12-02T14:44:03.170Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fce",
      "title": "OpenDORS: A dataset of openly referenced open research software",
      "url": "https://arxiv.org/abs/2512.01570v1",
      "content": "In many academic disciplines, software is created during the research process or for a research purpose. The crucial role of software for research is increasingly acknowledged. The application of software engineering to research software has been formalized as research software engineering, to create better software that enables better research. Despite this, large-scale studies of research software and its development are still lacking. To enable such studies, we present a dataset of 134,352 unique open research software projects and 134,154 source code repositories referenced in open access literature. Each dataset record identifies the referencing publication and lists source code repositories of the software project. For 122,425 source code repositories, the dataset provides metadata on latest versions, license information, programming languages and descriptive metadata files. We summarize the distributions of these features in the dataset and describe additional software metadata that extends the dataset in future work. Finally, we suggest examples of research that could use the dataset to develop a better understanding of research software practice in RSE research.",
      "summary": "In many academic disciplines, software is created during the research process or for a research purpose. The crucial role of software for research is increasingly acknowledged. The application of software engineering to research software has been formalized as research software engineering, to create better software that enables better research. Despite this, large-scale studies of research software and its development are still lacking. To enable such studies, we present a dataset of 134,352 un",
      "publishedAt": "2025-12-01T11:45:50.000Z",
      "author": "Stephan Druskat",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "pricing_business",
      "score": 6.921511676523941,
      "ingestedAt": "2025-12-02T14:44:03.170Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fd0",
      "title": "Deep FlexQP: Accelerated Nonlinear Programming via Deep Unfolding",
      "url": "https://arxiv.org/abs/2512.01565v1",
      "content": "We propose an always-feasible quadratic programming (QP) optimizer, FlexQP, which is based on an exact relaxation of the QP constraints. If the original constraints are feasible, then the optimizer finds the optimal solution to the original QP. On the other hand, if the constraints are infeasible, the optimizer identifies a solution that minimizes the constraint violation in a sparse manner. FlexQP scales favorably with respect to the problem dimension, is robust to both feasible and infeasible QPs with minimal assumptions on the problem data, and can be effectively warm-started. We subsequently apply deep unfolding to improve our optimizer through data-driven techniques, leading to an accelerated Deep FlexQP. By learning dimension-agnostic feedback policies for the parameters from a small number of training examples, Deep FlexQP generalizes to problems with larger dimensions and can optimize for many more iterations than it was initially trained for. Our approach outperforms two recently proposed state-of-the-art accelerated QP approaches on a suite of benchmark systems including portfolio optimization, classification, and regression problems. We provide guarantees on the expected performance of our deep QP optimizer through probably approximately correct (PAC) Bayes generalization bounds. These certificates are used to design an accelerated sequential quadratic programming solver that solves nonlinear optimal control and predictive safety filter problems faster than traditional approaches. Overall, our approach is very robust and greatly outperforms existing non-learning and learning-based optimizers in terms of both runtime and convergence to the optimal solution across multiple classes of NLPs.",
      "summary": "We propose an always-feasible quadratic programming (QP) optimizer, FlexQP, which is based on an exact relaxation of the QP constraints. If the original constraints are feasible, then the optimizer finds the optimal solution to the original QP. On the other hand, if the constraints are infeasible, the optimizer identifies a solution that minimizes the constraint violation in a sparse manner. FlexQP scales favorably with respect to the problem dimension, is robust to both feasible and infeasible ",
      "publishedAt": "2025-12-01T11:38:45.000Z",
      "author": "Alex Oshin",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 5.996536164923111,
      "ingestedAt": "2025-12-02T14:44:03.170Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fd1",
      "title": "MasHeNe: A Benchmark for Head and Neck CT Mass Segmentation using Window-Enhanced Mamba with Frequency-Domain Integration",
      "url": "https://arxiv.org/abs/2512.01563v1",
      "content": "Head and neck masses are space-occupying lesions that can compress the airway and esophagus and may affect nerves and blood vessels. Available public datasets primarily focus on malignant lesions and often overlook other space-occupying conditions in this region. To address this gap, we introduce MasHeNe, an initial dataset of 3,779 contrast-enhanced CT slices that includes both tumors and cysts with pixel-level annotations. We also establish a benchmark using standard segmentation baselines and report common metrics to enable fair comparison. In addition, we propose the Windowing-Enhanced Mamba with Frequency integration (WEMF) model. WEMF applies tri-window enhancement to enrich the input appearance before feature extraction. It further uses multi-frequency attention to fuse information across skip connections within a U-shaped Mamba backbone. On MasHeNe, WEMF attains the best performance among evaluated methods, with a Dice of 70.45%, IoU of 66.89%, NSD of 72.33%, and HD95 of 5.12 mm. This model indicates stable and strong results on this challenging task. MasHeNe provides a benchmark for head-and-neck mass segmentation beyond malignancy-only datasets. The observed error patterns also suggest that this task remains challenging and requires further research. Our dataset and code are available at https://github.com/drthaodao3101/MasHeNe.git.",
      "summary": "Head and neck masses are space-occupying lesions that can compress the airway and esophagus and may affect nerves and blood vessels. Available public datasets primarily focus on malignant lesions and often overlook other space-occupying conditions in this region. To address this gap, we introduce MasHeNe, an initial dataset of 3,779 contrast-enhanced CT slices that includes both tumors and cysts with pixel-level annotations. We also establish a benchmark using standard segmentation baselines and",
      "publishedAt": "2025-12-01T11:38:05.000Z",
      "author": "Thao Thi Phuong Dao",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 9.686391943924033,
      "ingestedAt": "2025-12-02T14:44:03.170Z",
      "tags": [
        "code_review",
        "ide",
        "observability"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fd2",
      "title": "Estimating the prevalence of LLM-assisted text in scholarly writing",
      "url": "https://arxiv.org/abs/2512.01560v1",
      "content": "The use of large language models (LLMs) in scholarly publications has grown dramatically since the launch of ChatGPT in late 2022. This usage is often undisclosed, and it can be challenging for readers and reviewers to identify human written but LLM-revised or translated text, or predominantly LLM-generated text. Given the known quality and reliability issues connected with LLM-generated text, their potential growth poses an increasing problem for research integrity, and for public trust in research. \n  This study presents a simple and easily reproducible methodology to show the growth in the full text of published papers, across the full range of research, as indexed in the Dimensions database. It uses this to demonstrate that LLM tools are likely to have been involved in the production of more than 10% of all published papers in 2024, based on disproportionate use of specific indicative words, and draws together earlier studies to confirm that this is a plausible overall estimate. \n  It then discusses the implications of this for the integrity of scholarly publishing, highlighting evidence that use of LLMs for text generation is still being concealed or downplayed by authors, and presents an argument that more comprehensive disclosure requirements are urgently required to address this.",
      "summary": "The use of large language models (LLMs) in scholarly publications has grown dramatically since the launch of ChatGPT in late 2022. This usage is often undisclosed, and it can be challenging for readers and reviewers to identify human written but LLM-revised or translated text, or predominantly LLM-generated text. Given the known quality and reliability issues connected with LLM-generated text, their potential growth poses an increasing problem for research integrity, and for public trust in rese",
      "publishedAt": "2025-12-01T11:34:15.000Z",
      "author": "Andrew Gray",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 11.990395603525485,
      "ingestedAt": "2025-12-02T14:44:03.170Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fd3",
      "title": "Language Diversity: Evaluating Language Usage and AI Performance on African Languages in Digital Spaces",
      "url": "https://arxiv.org/abs/2512.01557v1",
      "content": "This study examines the digital representation of African languages and the challenges this presents for current language detection tools. We evaluate their performance on Yoruba, Kinyarwanda, and Amharic. While these languages are spoken by millions, their online usage on conversational platforms is often sparse, heavily influenced by English, and not representative of the authentic, monolingual conversations prevalent among native speakers. This lack of readily available authentic data online creates a challenge of scarcity of conversational data for training language models. To investigate this, data was collected from subreddits and local news sources for each language. The analysis showed a stark contrast between the two sources. Reddit data was minimal and characterized by heavy code-switching. Conversely, local news media offered a robust source of clean, monolingual language data, which also prompted more user engagement in the local language on the news publishers social media pages. Language detection models, including the specialized AfroLID and a general LLM, performed with near-perfect accuracy on the clean news data but struggled with the code-switched Reddit posts. The study concludes that professionally curated news content is a more reliable and effective source for training context-rich AI models for African languages than data from conversational platforms. It also highlights the need for future models that can process clean and code-switched text to improve the detection accuracy for African languages.",
      "summary": "This study examines the digital representation of African languages and the challenges this presents for current language detection tools. We evaluate their performance on Yoruba, Kinyarwanda, and Amharic. While these languages are spoken by millions, their online usage on conversational platforms is often sparse, heavily influenced by English, and not representative of the authentic, monolingual conversations prevalent among native speakers. This lack of readily available authentic data online ",
      "publishedAt": "2025-12-01T11:27:13.000Z",
      "author": "Edward Ajayi",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.1490737916733265,
      "ingestedAt": "2025-12-02T14:44:03.170Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fd4",
      "title": "LEC: Linear Expectation Constraints for False-Discovery Control in Selective Prediction and Routing Systems",
      "url": "https://arxiv.org/abs/2512.01556v1",
      "content": "Large language models (LLMs) often generate unreliable answers, while heuristic uncertainty methods fail to fully distinguish correct from incorrect predictions, causing users to accept erroneous answers without statistical guarantees. We address this issue through the lens of false discovery rate (FDR) control, ensuring that among all accepted predictions, the proportion of errors does not exceed a target risk level. To achieve this in a principled way, we propose LEC, which reinterprets selective prediction as a constrained decision problem by enforcing a Linear Expectation Constraint over selection and error indicators. Then, we establish a finite-sample sufficient condition, which relies only on a held-out set of exchangeable calibration samples, to compute an FDR-constrained, coverage-maximizing threshold. Furthermore, we extend LEC to a two-model routing mechanism: given a prompt, if the current model's uncertainty exceeds its calibrated threshold, we delegate it to a stronger model, while maintaining a unified FDR guarantee. Evaluations on closed-ended and open-ended question-answering (QA) datasets show that LEC achieves tighter FDR control and substantially improves sample retention over prior methods. Moreover, the two-model routing mechanism achieves lower risk levels while accepting more correct samples than each individual model.",
      "summary": "Large language models (LLMs) often generate unreliable answers, while heuristic uncertainty methods fail to fully distinguish correct from incorrect predictions, causing users to accept erroneous answers without statistical guarantees. We address this issue through the lens of false discovery rate (FDR) control, ensuring that among all accepted predictions, the proportion of errors does not exceed a target risk level. To achieve this in a principled way, we propose LEC, which reinterprets select",
      "publishedAt": "2025-12-01T11:27:09.000Z",
      "author": "Zhiyuan Wang",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.5320800903783836,
      "ingestedAt": "2025-12-02T14:44:03.171Z",
      "tags": [
        "code_review",
        "retrieval"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fd5",
      "title": "Delta Sum Learning: an approach for fast and global convergence in Gossip Learning",
      "url": "https://arxiv.org/abs/2512.01549v1",
      "content": "Federated Learning is a popular approach for distributed learning due to its security and computational benefits. With the advent of powerful devices in the network edge, Gossip Learning further decentralizes Federated Learning by removing centralized integration and relying fully on peer to peer updates. However, the averaging methods generally used in both Federated and Gossip Learning are not ideal for model accuracy and global convergence. Additionally, there are few options to deploy Learning workloads in the edge as part of a larger application using a declarative approach such as Kubernetes manifests. This paper proposes Delta Sum Learning as a method to improve the basic aggregation operation in Gossip Learning, and implements it in a decentralized orchestration framework based on Open Application Model, which allows for dynamic node discovery and intent-driven deployment of multi-workload applications. Evaluation results show that Delta Sum performance is on par with alternative integration methods for 10 node topologies, but results in a 58% lower global accuracy drop when scaling to 50 nodes. Overall, it shows strong global convergence and a logarithmic loss of accuracy with increasing topology size compared to a linear loss for alternatives under limited connectivity.",
      "summary": "Federated Learning is a popular approach for distributed learning due to its security and computational benefits. With the advent of powerful devices in the network edge, Gossip Learning further decentralizes Federated Learning by removing centralized integration and relying fully on peer to peer updates. However, the averaging methods generally used in both Federated and Gossip Learning are not ideal for model accuracy and global convergence. Additionally, there are few options to deploy Learni",
      "publishedAt": "2025-12-01T11:23:51.000Z",
      "author": "Tom Goethals",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 12.906074103218057,
      "ingestedAt": "2025-12-02T14:44:03.171Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fd6",
      "title": "LPCD: Unified Framework from Layer-Wise to Submodule Quantization",
      "url": "https://arxiv.org/abs/2512.01546v1",
      "content": "Post-training quantization (PTQ) aims to preserve model-level behavior; however, most methods focus on individual linear layers. Even recent extensions, such as QEP and LoaQ, which mitigate error propagation or target specific submodules, still rely on layer-wise formulations and fail to capture the behavior of larger submodules. We introduce Layer-Projected Coordinate Descent (LPCD), a unified framework that extends PTQ beyond layers by optimizing relaxed objectives across arbitrary submodules and projecting the solutions with layer-wise quantizers. LPCD generalizes existing methods and provides a principled approach to quantizing complex submodules while maintaining the efficiency and compatibility of layer-wise PTQ pipelines. Across diverse LLM architectures and bit-widths, LPCD-based submodule quantization consistently enhances both layer-wise PTQ methods and existing submodule approaches.",
      "summary": "Post-training quantization (PTQ) aims to preserve model-level behavior; however, most methods focus on individual linear layers. Even recent extensions, such as QEP and LoaQ, which mitigate error propagation or target specific submodules, still rely on layer-wise formulations and fail to capture the behavior of larger submodules. We introduce Layer-Projected Coordinate Descent (LPCD), a unified framework that extends PTQ beyond layers by optimizing relaxed objectives across arbitrary submodules ",
      "publishedAt": "2025-12-01T11:21:18.000Z",
      "author": "Yuma Ichikawa",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.069602112821346,
      "ingestedAt": "2025-12-02T14:44:03.171Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fd9",
      "title": "Q2D2: A Geometry-Aware Audio Codec Leveraging Two-Dimensional Quantization",
      "url": "https://arxiv.org/abs/2512.01537v1",
      "content": "Recent neural audio codecs have achieved impressive reconstruction quality, typically relying on quantization methods such as Residual Vector Quantization (RVQ), Vector Quantization (VQ) and Finite Scalar Quantization (FSQ). However, these quantization techniques limit the geometric structure of the latent space, make it harder to capture correlations between features leading to inefficiency in representation learning, codebook utilization and token rate. In this paper we introduce Two Dimensional Quantization (Q2D2), a quantization scheme in which feature pairs are projected onto structured 2D grids such as hexagonal, rhombic, or rectangular tiling and quantized to the nearest grid values, yielding an implicit codebook defined by the product of grid levels, with codebook sizes comparable to conventional methods. Despite its simple geometric formulation, Q2D2 improves audio compression efficiency, with low token rates and high codebook utilization while maintaining state of the art reconstruction quality. Specifically, Q2D2 achieves competitive to superior performance in various objective and subjective reconstruction metrics, across extensive experiments in speech domain compared to state of the art models. Comprehensive ablation studies further confirm the effectiveness of our design choices.",
      "summary": "Recent neural audio codecs have achieved impressive reconstruction quality, typically relying on quantization methods such as Residual Vector Quantization (RVQ), Vector Quantization (VQ) and Finite Scalar Quantization (FSQ). However, these quantization techniques limit the geometric structure of the latent space, make it harder to capture correlations between features leading to inefficiency in representation learning, codebook utilization and token rate. In this paper we introduce Two Dimension",
      "publishedAt": "2025-12-01T11:06:38.000Z",
      "author": "Tal Shuster",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 6.447528501920342,
      "ingestedAt": "2025-12-02T14:44:03.171Z",
      "tags": [
        "code_review",
        "retrieval",
        "observability"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fda",
      "title": "Deep Unsupervised Anomaly Detection in Brain Imaging: Large-Scale Benchmarking and Bias Analysis",
      "url": "https://arxiv.org/abs/2512.01534v1",
      "content": "Deep unsupervised anomaly detection in brain magnetic resonance imaging offers a promising route to identify pathological deviations without requiring lesion-specific annotations. Yet, fragmented evaluations, heterogeneous datasets, and inconsistent metrics have hindered progress toward clinical translation. Here, we present a large-scale, multi-center benchmark of deep unsupervised anomaly detection for brain imaging. The training cohort comprised 2,976 T1 and 2,972 T2-weighted scans from healthy individuals across six scanners, with ages ranging from 6 to 89 years. Validation used 92 scans to tune hyperparameters and estimate unbiased thresholds. Testing encompassed 2,221 T1w and 1,262 T2w scans spanning healthy datasets and diverse clinical cohorts. Across all algorithms, the Dice-based segmentation performance varied between 0.03 and 0.65, indicating substantial variability. To assess robustness, we systematically evaluated the impact of different scanners, lesion types and sizes, as well as demographics (age, sex). Reconstruction-based methods, particularly diffusion-inspired approaches, achieved the strongest lesion segmentation performance, while feature-based methods showed greater robustness under distributional shifts. However, systematic biases, such as scanner-related effects, were observed for the majority of algorithms, including that small and low-contrast lesions were missed more often, and that false positives varied with age and sex. Increasing healthy training data yields only modest gains, underscoring that current unsupervised anomaly detection frameworks are limited algorithmically rather than by data availability. Our benchmark establishes a transparent foundation for future research and highlights priorities for clinical translation, including image native pretraining, principled deviation measures, fairness-aware modeling, and robust domain adaptation.",
      "summary": "Deep unsupervised anomaly detection in brain magnetic resonance imaging offers a promising route to identify pathological deviations without requiring lesion-specific annotations. Yet, fragmented evaluations, heterogeneous datasets, and inconsistent metrics have hindered progress toward clinical translation. Here, we present a large-scale, multi-center benchmark of deep unsupervised anomaly detection for brain imaging. The training cohort comprised 2,976 T1 and 2,972 T2-weighted scans from healt",
      "publishedAt": "2025-12-01T11:03:27.000Z",
      "author": "Alexander Frotscher",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 9.669765743056997,
      "ingestedAt": "2025-12-02T14:44:03.171Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "testing",
        "observability"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fdb",
      "title": "Diffusion Fuzzy System: Fuzzy Rule Guided Latent Multi-Path Diffusion Modeling",
      "url": "https://arxiv.org/abs/2512.01533v1",
      "content": "Diffusion models have emerged as a leading technique for generating images due to their ability to create high-resolution and realistic images. Despite their strong performance, diffusion models still struggle in managing image collections with significant feature differences. They often fail to capture complex features and produce conflicting results. Research has attempted to address this issue by learning different regions of an image through multiple diffusion paths and then combining them. However, this approach leads to inefficient coordination among multiple paths and high computational costs. To tackle these issues, this paper presents a Diffusion Fuzzy System (DFS), a latent-space multi-path diffusion model guided by fuzzy rules. DFS offers several advantages. First, unlike traditional multi-path diffusion methods, DFS uses multiple diffusion paths, each dedicated to learning a specific class of image features. By assigning each path to a different feature type, DFS overcomes the limitations of multi-path models in capturing heterogeneous image features. Second, DFS employs rule-chain-based reasoning to dynamically steer the diffusion process and enable efficient coordination among multiple paths. Finally, DFS introduces a fuzzy membership-based latent-space compression mechanism to reduce the computational costs of multi-path diffusion effectively. We tested our method on three public datasets: LSUN Bedroom, LSUN Church, and MS COCO. The results show that DFS achieves more stable training and faster convergence than existing single-path and multi-path diffusion models. Additionally, DFS surpasses baseline models in both image quality and alignment between text and images, and also shows improved accuracy when comparing generated images to target references.",
      "summary": "Diffusion models have emerged as a leading technique for generating images due to their ability to create high-resolution and realistic images. Despite their strong performance, diffusion models still struggle in managing image collections with significant feature differences. They often fail to capture complex features and produce conflicting results. Research has attempted to address this issue by learning different regions of an image through multiple diffusion paths and then combining them. ",
      "publishedAt": "2025-12-01T11:01:06.000Z",
      "author": "Hailong Yang",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.064524995994183,
      "ingestedAt": "2025-12-02T14:44:03.171Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fde",
      "title": "Teaching an Online Multi-Institutional Research Level Software Engineering Course with Industry - an Experience Report",
      "url": "https://arxiv.org/abs/2512.01523v1",
      "content": "Covid has made online teaching and learning acceptable and students, faculty, and industry professionals are all comfortable with this mode. This comfort can be leveraged to offer an online multi-institutional research-level course in an area where individual institutions may not have the requisite faculty to teach and/or research students to enroll. If the subject is of interest to industry, online offering also allows industry experts to contribute and participate with ease. Advanced topics in Software Engineering are ideally suited for experimenting with this approach as industry, which is often looking to incorporate advances in software engineering in their practices, is likely to agree to contribute and participate. In this paper we describe an experiment in teaching a course titled \"AI in Software Engineering\" jointly between two institutions with active industry participation, and share our and student's experience. We believe this collaborative teaching approach can be used for offering research level courses in any applied area of computer science by institutions who are small and find it difficult to offer research level courses on their own.",
      "summary": "Covid has made online teaching and learning acceptable and students, faculty, and industry professionals are all comfortable with this mode. This comfort can be leveraged to offer an online multi-institutional research-level course in an area where individual institutions may not have the requisite faculty to teach and/or research students to enroll. If the subject is of interest to industry, online offering also allows industry experts to contribute and participate with ease. Advanced topics in",
      "publishedAt": "2025-12-01T10:46:43.000Z",
      "author": "Pankaj Jalote",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 6.44116194114539,
      "ingestedAt": "2025-12-02T14:44:03.171Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fe1",
      "title": "MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages",
      "url": "https://arxiv.org/abs/2512.01512v1",
      "content": "Multimodal Large Language Models (MLLMs) have achieved great success in Speech-to-Text Translation (S2TT) tasks. However, current research is constrained by two key challenges: language coverage and efficiency. Most of the popular S2TT datasets are substantially English-centric, which restricts the scaling-up of MLLMs' many-to-many translation capabilities. Moreover, the inference speed of MLLMs degrades dramatically when the speech is converted into long sequences (e.g., 750 tokens). To address these limitations, we propose a Multilingual Cost-effective Accelerated Speech-to-Text Translator (MCAT) framework, which includes two innovations. First, a language scaling method that leverages curriculum learning and a data balancing strategy is introduced to extend the language coverage supported by MLLMs to 70 languages and achieve mutual translation among these languages. Second, an optimized speech adapter module is designed to reduce the length of the speech sequence to only 30 tokens. Extensive experiments were conducted on MLLMs of different scales (9B and 27B). The experimental results demonstrate that MCAT not only surpasses state-of-the-art end-to-end models on the FLEURS dataset across 70x69 directions but also enhances batch inference efficiency. This is achieved with only ~100M trainable parameters and by using only 10 hours of S2TT data per language. Furthermore, we have released MCAT as open-source to promote the development of MLLMs for robust S2TT capabilities. The code and models are released at https://github.com/yxduir/m2m-70.",
      "summary": "Multimodal Large Language Models (MLLMs) have achieved great success in Speech-to-Text Translation (S2TT) tasks. However, current research is constrained by two key challenges: language coverage and efficiency. Most of the popular S2TT datasets are substantially English-centric, which restricts the scaling-up of MLLMs' many-to-many translation capabilities. Moreover, the inference speed of MLLMs degrades dramatically when the speech is converted into long sequences (e.g., 750 tokens). To address",
      "publishedAt": "2025-12-01T10:39:12.000Z",
      "author": "Yexing Du",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 8.278406740503184,
      "ingestedAt": "2025-12-02T14:44:03.171Z",
      "tags": [
        "code_review",
        "retrieval"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fe4",
      "title": "SynthStrategy: Extracting and Formalizing Latent Strategic Insights from LLMs in Organic Chemistry",
      "url": "https://arxiv.org/abs/2512.01507v1",
      "content": "Modern computer-assisted synthesis planning (CASP) systems show promises at generating chemically valid reaction steps but struggle to incorporate strategic considerations such as convergent assembly, protecting group minimization, and optimal ring-forming sequences. We introduce a methodology that leverages Large Language Models to distill synthetic knowledge into code. Our system analyzes synthesis routes and translates strategic principles into Python functions representing diverse strategic and tactical rules, such as strategic functional group interconversions and ring construction strategies. By formalizing this knowledge as verifiable code rather than simple heuristics, we create testable, interpretable representations of synthetic strategy. We release the complete codebase and the USPTO-ST dataset -- synthesis routes annotated with strategic tags. This framework unlocks a novel capability for CASP: natural language-based route retrieval, achieving 75\\% Top-3 accuracy on our benchmark. We further validate our library through temporal analysis of historical trends and chemically intuitive route clustering that offers more granular partitioning than common previous methods. This work bridges the tactical-strategic divide in CASP, enabling specification, search, and evaluation of routes by strategic criteria rather than structure alone.",
      "summary": "Modern computer-assisted synthesis planning (CASP) systems show promises at generating chemically valid reaction steps but struggle to incorporate strategic considerations such as convergent assembly, protecting group minimization, and optimal ring-forming sequences. We introduce a methodology that leverages Large Language Models to distill synthetic knowledge into code. Our system analyzes synthesis routes and translates strategic principles into Python functions representing diverse strategic ",
      "publishedAt": "2025-12-01T10:33:00.000Z",
      "author": "Daniel Armstrong",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 7.356321060773971,
      "ingestedAt": "2025-12-02T14:44:03.171Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fe7",
      "title": "Generating Random Hyperfractal Cities",
      "url": "https://arxiv.org/abs/2512.01505v1",
      "content": "This paper focuses on the challenge of interactively modeling street networks. In this work, we extend the simple fractal model, which is particularly useful for describing small cities or individual districts, by constructing random cities based on a tiling structure over which hyperfractals are distributed. This approach enables the connection of multiple hyperfractal districts, providing a more comprehensive urban representation. Furthermore, we demonstrate how this decomposition can be used to segment a city into distinct districts through fractal analysis. Finally, we present tools for the numerical generation of random cities following this model.",
      "summary": "This paper focuses on the challenge of interactively modeling street networks. In this work, we extend the simple fractal model, which is particularly useful for describing small cities or individual districts, by constructing random cities based on a tiling structure over which hyperfractals are distributed. This approach enables the connection of multiple hyperfractal districts, providing a more comprehensive urban representation. Furthermore, we demonstrate how this decomposition can be used ",
      "publishedAt": "2025-12-01T10:30:47.000Z",
      "author": "Geoffrey Deperle",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.137475640903079,
      "ingestedAt": "2025-12-02T14:44:03.171Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0feb",
      "title": "Formal Verification of Noisy Quantum Reinforcement Learning Policies",
      "url": "https://arxiv.org/abs/2512.01502v1",
      "content": "Quantum reinforcement learning (QRL) aims to use quantum effects to create sequential decision-making policies that achieve tasks more effectively than their classical counterparts. However, QRL policies face uncertainty from quantum measurements and hardware noise, such as bit-flip, phase-flip, and depolarizing errors, which can lead to unsafe behavior. Existing work offers no systematic way to verify whether trained QRL policies meet safety requirements under specific noise conditions. \n  We introduce QVerifier, a formal verification method that applies probabilistic model checking to analyze trained QRL policies with and without modeled quantum noise. QVerifier builds a complete model of the policy-environment interaction, incorporates quantum uncertainty directly into the transition probabilities, and then checks safety properties using the Storm model checker. \n  Experiments across multiple QRL environments show that QVerifier precisely measures how different noise models influence safety, revealing both performance degradation and cases where noise can help. By enabling rigorous safety verification before deployment, QVerifier addresses a critical need: because access to quantum hardware is expensive, pre-deployment verification is essential for any safety-critical use of QRL. QVerifier targets a potential classical-quantum sweet spot: trained QRL policies that execute efficiently on quantum hardware, yet remain tractable for classical probabilistic model checking despite being too slow for real-time classical deployment.",
      "summary": "Quantum reinforcement learning (QRL) aims to use quantum effects to create sequential decision-making policies that achieve tasks more effectively than their classical counterparts. However, QRL policies face uncertainty from quantum measurements and hardware noise, such as bit-flip, phase-flip, and depolarizing errors, which can lead to unsafe behavior. Existing work offers no systematic way to verify whether trained QRL policies meet safety requirements under specific noise conditions. \n  We i",
      "publishedAt": "2025-12-01T10:26:33.000Z",
      "author": "Dennis Gross",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.975098875114272,
      "ingestedAt": "2025-12-02T14:44:03.172Z",
      "tags": [
        "code_review",
        "governance"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0fee",
      "title": "DuckDB on xNVMe",
      "url": "https://arxiv.org/abs/2512.01490v1",
      "content": "DuckDB is designed for portability. It is also designed to run anywhere, and possibly in contexts where it can be specialized for performance, e.g., as a cloud service or on a smart device. In this paper, we consider the way DuckDB interacts with local storage. Our long term research question is whether and how SSDs could be co-designed with DuckDB. As a first step towards vertical integration of DuckDB and programmable SSDs, we consider whether and how DuckDB can access NVMe SSDs directly. By default, DuckDB relies on the POSIX file interface. In contrast, we rely on the xNVMe library and explore how it can be leveraged in DuckDB. We leverage the block-based nature of the DuckDB buffer manager to bypass the synchronous POSIX I/O interface, the file system and the block manager. Instead, we directly issue asynchronous I/Os against the SSD logical block address space. Our preliminary experimental study compares different ways to manage asynchronous I/Os atop xNVMe. The speed-up we observe over the DuckDB baseline is significant, even for the simplest scan query over a TPC-H table. As expected, the speed-up increases with the scale factor, and the Linux NVMe passthru improves performance. Future work includes a more thorough experimental study, a flexible solution that combines raw NVMe access and legacy POSIX file interface as well the co-design of DuckDB and SSDs.",
      "summary": "DuckDB is designed for portability. It is also designed to run anywhere, and possibly in contexts where it can be specialized for performance, e.g., as a cloud service or on a smart device. In this paper, we consider the way DuckDB interacts with local storage. Our long term research question is whether and how SSDs could be co-designed with DuckDB. As a first step towards vertical integration of DuckDB and programmable SSDs, we consider whether and how DuckDB can access NVMe SSDs directly. By d",
      "publishedAt": "2025-12-01T10:12:37.000Z",
      "author": "Marius Ottosen",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 9.186108748550334,
      "ingestedAt": "2025-12-02T14:44:03.172Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0ff2",
      "title": "Multi-Path Collaborative Reasoning via Reinforcement Learning",
      "url": "https://arxiv.org/abs/2512.01485v1",
      "content": "Chain-of-Thought (CoT) reasoning has significantly advanced the problem-solving capabilities of Large Language Models (LLMs), yet conventional CoT often exhibits internal determinism during decoding, limiting exploration of plausible alternatives. Recent methods attempt to address this by generating soft abstract tokens to enable reasoning in a continuous semantic space. However, we find that such approaches remain constrained by the greedy nature of autoregressive decoding, which fundamentally isolates the model from alternative reasoning possibilities. In this work, we propose Multi-Path Perception Policy Optimization (M3PO), a novel reinforcement learning framework that explicitly injects collective insights into the reasoning process. M3PO leverages parallel policy rollouts as naturally diverse reasoning sources and integrates cross-path interactions into policy updates through a lightweight collaborative mechanism. This design allows each trajectory to refine its reasoning with peer feedback, thereby cultivating more reliable multi-step reasoning patterns. Empirical results show that M3PO achieves state-of-the-art performance on both knowledge- and reasoning-intensive benchmarks. Models trained with M3PO maintain interpretability and inference efficiency, underscoring the promise of multi-path collaborative learning for robust reasoning.",
      "summary": "Chain-of-Thought (CoT) reasoning has significantly advanced the problem-solving capabilities of Large Language Models (LLMs), yet conventional CoT often exhibits internal determinism during decoding, limiting exploration of plausible alternatives. Recent methods attempt to address this by generating soft abstract tokens to enable reasoning in a continuous semantic space. However, we find that such approaches remain constrained by the greedy nature of autoregressive decoding, which fundamentally ",
      "publishedAt": "2025-12-01T10:05:46.000Z",
      "author": "Jindi Lv",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 9.182988006696096,
      "ingestedAt": "2025-12-02T14:44:03.172Z",
      "tags": [
        "code_review",
        "retrieval",
        "governance"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0ff6",
      "title": "Multi-view diffusion geometry using intertwined diffusion trajectories",
      "url": "https://arxiv.org/abs/2512.01484v1",
      "content": "This paper introduces a comprehensive unified framework for constructing multi-view diffusion geometries through intertwined multi-view diffusion trajectories (MDTs), a class of inhomogeneous diffusion processes that iteratively combine the random walk operators of multiple data views. Each MDT defines a trajectory-dependent diffusion operator with a clear probabilistic and geometric interpretation, capturing over time the interplay between data views. Our formulation encompasses existing multi-view diffusion models, while providing new degrees of freedom for view interaction and fusion. We establish theoretical properties under mild assumptions, including ergodicity of both the point-wise operator and the process in itself. We also derive MDT-based diffusion distances, and associated embeddings via singular value decompositions. Finally, we propose various strategies for learning MDT operators within the defined operator space, guided by internal quality measures. Beyond enabling flexible model design, MDTs also offer a neutral baseline for evaluating diffusion-based approaches through comparison with randomly selected MDTs. Experiments show the practical impact of the MDT operators in a manifold learning and data clustering context.",
      "summary": "This paper introduces a comprehensive unified framework for constructing multi-view diffusion geometries through intertwined multi-view diffusion trajectories (MDTs), a class of inhomogeneous diffusion processes that iteratively combine the random walk operators of multiple data views. Each MDT defines a trajectory-dependent diffusion operator with a clear probabilistic and geometric interpretation, capturing over time the interplay between data views. Our formulation encompasses existing multi-",
      "publishedAt": "2025-12-01T10:05:19.000Z",
      "author": "Gwendal Debaussart-Joniec",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 7.346226425258328,
      "ingestedAt": "2025-12-02T14:44:03.172Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae0ffd",
      "title": "CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball",
      "url": "https://arxiv.org/abs/2512.01478v1",
      "content": "This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach first processes skeletal tracking data through Graph Neural Networks to capture nuanced motion patterns, then employs a Transformer architecture with specialized attention mechanisms to model player interactions. We introduce event projection heads that explicitly connect player movements to basketball events like passes, shots, and steals, training the model to associate physical motion patterns with their tactical purposes. Experiments on NBA tracking data demonstrate significant improvements over position-only baselines: 35% reduction in trajectory prediction error compared to state-of-the-art position-based models and consistent performance gains across key basketball analytics tasks. The resulting pretrained model serves as a powerful foundation for multiple downstream tasks, with pick detection, shot taker identification, assist prediction, shot location classification, and shot type recognition demonstrating substantial improvements over existing methods.",
      "summary": "This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach fir",
      "publishedAt": "2025-12-01T09:58:24.000Z",
      "author": "Omer Sela",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.04879818496075,
      "ingestedAt": "2025-12-02T14:44:03.172Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ae1001",
      "title": "Does Flatness imply Generalization for Logistic Loss in Univariate Two-Layer ReLU Network?",
      "url": "https://arxiv.org/abs/2512.01473v1",
      "content": "We consider the problem of generalization of arbitrarily overparameterized two-layer ReLU Neural Networks with univariate input. Recent work showed that under square loss, flat solutions (motivated by flat / stable minima and Edge of Stability phenomenon) provably cannot overfit, but it remains unclear whether the same phenomenon holds for logistic loss. This is a puzzling open problem because existing work on logistic loss shows that gradient descent with increasing step size converges to interpolating solutions (at infinity, for the margin-separable cases). In this paper, we prove that the \\emph{flatness implied generalization} is more delicate under logistic loss. On the positive side, we show that flat solutions enjoy near-optimal generalization bounds within a region between the left-most and right-most \\emph{uncertain} sets determined by each candidate solution. On the negative side, we show that there exist arbitrarily flat yet overfitting solutions at infinity that are (falsely) certain everywhere, thus certifying that flatness alone is insufficient for generalization in general. We demonstrate the effects predicted by our theory in a well-controlled simulation study.",
      "summary": "We consider the problem of generalization of arbitrarily overparameterized two-layer ReLU Neural Networks with univariate input. Recent work showed that under square loss, flat solutions (motivated by flat / stable minima and Edge of Stability phenomenon) provably cannot overfit, but it remains unclear whether the same phenomenon holds for logistic loss. This is a puzzling open problem because existing work on logistic loss shows that gradient descent with increasing step size converges to inter",
      "publishedAt": "2025-12-01T09:57:11.000Z",
      "author": "Dan Qiao",
      "source": "rss",
      "feedName": "arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.048493496513114,
      "ingestedAt": "2025-12-02T14:44:03.172Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ad6674",
      "title": "Seeking Career guidance",
      "url": "https://dev.to/abby21/seeking-career-guidance-45hb",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F4d3m9h8yqs88yua3e2hq.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>I'm looking for career guidance. I work as senior software engineer. I have 10+yrs of experience. <br>  \nI have worked in java, gwt, angular,react, aws lambda, accessibility,  little bit of mongodb atlas ui. I work well but now when I look back I dont remember these skills in interview poit of view. I also feel lost in career. <br>  \nI want to refresh and upskill to align with current expectations for my experience. Also would like to know what skills and roles will be good choice,so that I can work on it? I have no guidance so I'm seeking for guidance so that I can have a good roadmap.</p>",
      "summary": "I'm looking for career guidance. I work as senior software engineer. I have 10+yrs of experience.   \nI have worked in java, gwt, angular,react, aws lambda, accessibility,  little bit of mongodb atlas ui. I work well but now when I look back I dont remember these skills in interview poit of view. I also feel lost in career.   \nI want to refresh and upskill to align with current expectations for my experience. Also would like to know what skills and roles will be good choice,so that I can work on ",
      "publishedAt": "2025-12-02T13:54:29.000Z",
      "author": "Abby",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4963163212565773,
      "ingestedAt": "2025-12-02T14:44:03.172Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ad6678",
      "title": "Behind the Scenes of a “Crazy Pizza Game”: How HTML5 Casual Games Are Built",
      "url": "https://dev.to/gamh5games/behind-the-scenes-of-a-crazy-pizza-game-how-html5-casual-games-are-built-1hjn",
      "content": "<p>Casual browser games have exploded in popularity over the last few years, and one of the most recognizable formats is the fast-paced “<a href=\"https://gamh5.com/game/crazy-pizza/\">crazy pizza game</a>” — a game where players quickly assemble pizzas, match ingredients, or manage orders under time pressure.</p> \n \n<p>But what does it take to build a game like this?<br> \nWhy do HTML5 engines handle these mechanics so well?<br> \nAnd how do developers make such games run smoothly even on low-end mobile devices?</p> \n \n<p>In this article, we’ll break down the design and technical foundations behind a “crazy pizza game,” from core gameplay loops to rendering, performance optimization, asset pipelines, and browser considerations.</p> \n \n<p>🍕 1. What Defines a “Crazy Pizza Game”?</p> \n \n<p>A typical crazy pizza game includes these characteristics:</p> \n \n<p>Fast decision-making</p> \n \n<p>Time-based challenges (countdowns, increasing speed, etc.)</p> \n \n<p>Ingredient combinations (drag-and-drop, tap-to-select, matching patterns)</p> \n \n<p>Continuous feedback (animations, sound effects, combo popups)</p> \n \n<p>Short gameplay loops that encourage replayability</p> \n \n<p>Supports both desktop and mobile browsers</p> \n \n<p>These features make the genre ideal for HTML5/JavaScript, because the UI interactions are lightweight and the gameplay loop is simple but addictive.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fryu8horx7qw9bgqfvubk.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fryu8horx7qw9bgqfvubk.png\" alt=\"\" width=\"800\" height=\"564\"></a></p> \n \n<p>🚀 2. Choosing the Right HTML5 Engine</p> \n \n<p>For a crazy pizza game, any of the major HTML5 engines can work:</p> \n \n<p>✔ Phaser</p> \n \n<p>Most common choice</p> \n \n<p>Great for 2D animations, sprites, state machines</p> \n \n<p>Built-in physics and input systems</p> \n \n<p>✔ PixiJS</p> \n \n<p>Perfect for fast rendering</p> \n \n<p>Good for animation-heavy gameplay</p> \n \n<p>✔ Pure JavaScript + Canvas</p> \n \n<p>Best for lightweight casual games</p> \n \n<p>Minimal engine overhead</p> \n \n<p>Great performance on mobile</p> \n \n<p>Here’s a small example of a simple pizza ingredient click-handler using vanilla JS:</p> \n \n<p>canvas.addEventListener(\"click\", (e) =&gt; {<br> \n  const x = e.offsetX;<br> \n  const y = e.offsetY;</p> \n \n<p>ingredients.forEach(item =&gt; {<br> \n    if (item.isClicked(x, y)) {<br> \n      item.select();<br> \n      score++;<br> \n    }<br> \n  });<br> \n});</p> \n \n<p>The core interaction is simple — which is why many successful HTML5 pizza games are under 200 KB in script size.</p> \n \n<p>🎮 3. Designing the Core Gameplay Loop</p> \n \n<p>Every successful crazy pizza game relies on a tight loop:</p> \n \n<p>Display random ingredients</p> \n \n<p>Player selects or assembles them</p> \n \n<p>Timer decreases or speed increases</p> \n \n<p>Feedback (points, sounds, animations)</p> \n \n<p>New round begins instantly</p> \n \n<p>A clean gameplay loop example:</p> \n \n<p>function gameLoop() {<br> \n  spawnIngredients();</p> \n \n<p>timer.start(30);</p> \n \n<p>timer.onTick(() =&gt; updateUI());</p> \n \n<p>timer.onEnd(() =&gt; {<br> \n    endGame(score);<br> \n  });<br> \n}</p> \n \n<p>The trick is to keep everything fast and readable, especially since mobile players have less patience for lag.</p> \n \n<p>🎨 4. Graphics &amp; Assets Optimization</p> \n \n<p>Crazy pizza games often include dozens of PNG icons:</p> \n \n<p>Sauce</p> \n \n<p>Cheese</p> \n \n<p>Pepperoni</p> \n \n<p>Mushroom</p> \n \n<p>Onion</p> \n \n<p>Oven effects</p> \n \n<p>Combo icons<br> \n…and more.</p> \n \n<p>To keep performance high:</p> \n \n<p>✔ Use sprite sheets instead of individual images<br> \n✔ Compress PNGs with TinyPNG or Squoosh<br> \n✔ Preload assets before starting the game<br> \n✔ Keep your texture count low to reduce GPU swaps</p> \n \n<p>A typical loading script:</p> \n \n<p>const assets = [<br> \n  \"pizza-base.png\",<br> \n  \"pepperoni.png\",<br> \n  \"cheese.png\",<br> \n  \"combo.png\",<br> \n  \"timer.png\"<br> \n];</p> \n \n<p>Promise.all(assets.map(loadImage)).then(startGame);</p> \n \n<p>📱 5. Mobile Performance Best Practices</p> \n \n<p>Since most crazy pizza game players are on mobile browsers, optimization is essential:</p> \n \n<p>Avoid unnecessary DOM updates</p> \n \n<p>Use a single  when possible</p> \n \n<p>Keep animation frame rates stable (use requestAnimationFrame)</p> \n \n<p>Minify and bundle your scripts</p> \n \n<p>Limit physics calculations</p> \n \n<p>Pre-calculate ingredient positions</p> \n \n<p>Even a simple change like caching your ingredient hit-boxes can improve performance dramatically.</p> \n \n<p>🔊 6. Creating Player Feedback &amp; Game Feel</p> \n \n<p>The “crazy” feeling of pizza games comes from feedback:</p> \n \n<p>Fast pop animations</p> \n \n<p>Combo counters</p> \n \n<p>Sound effects</p> \n \n<p>Ingredient “snap” motions</p> \n \n<p>Quick color flashes</p> \n \n<p>Most designers use:</p> \n \n<p>Tween libraries (GSAP, Phaser tweens)</p> \n \n<p>Lightweight sound libraries like Howler.js</p> \n \n<p>Example: adding a satisfying “ingredient placed” animation:</p> \n \n<p>tween.to(ingredient, {<br> \n  scale: 1.2,<br> \n  duration: 80,<br> \n  yoyo: true<br> \n});</p> \n \n<p>Fast. Simple. Effective.</p> \n \n<p>🌐 7. Where to Explore Examples of Crazy Pizza Games</p> \n \n<p>If you're studying the design or UI patterns of crazy pizza games, you can browse collections of lightweight HTML5 browser games here:</p> \n \n<p>GamH5 — HTML5 Browser Game Examples<br> \n(Insert your link here)</p> \n \n<p>It’s a useful reference if you're researching casual game mechanics or UI flows typically used in the genre.</p> \n \n<p>🧠 Final Thoughts</p> \n \n<p>A “crazy pizza game” may look simple on the surface, but behind the scenes it combines:</p> \n \n<p>event-driven UI</p> \n \n<p>sprite rendering</p> \n \n<p>performance optimization</p> \n \n<p>user psychology</p> \n \n<p>touch interaction design</p> \n \n<p>asset management</p> \n \n<p>These micro-games are a great way for developers to practice HTML5 game development skills while delivering fun, fast-paced experiences to players.</p> \n \n<p>If you’re building your own <a href=\"https://gamh5.com/game/crazy-pizza/\">crazy pizza game</a> or optimizing one, remember:<br> \nsmooth performance + instant feedback = addictive gameplay.</p>",
      "summary": "Casual browser games have exploded in popularity over the last few years, and one of the most recognizable formats is the fast-paced “crazy pizza game” — a game where players quickly assemble pizzas, match ingredients, or manage orders under time pressure. \n \nBut what does it take to build a game like this? \nWhy do HTML5 engines handle these mechanics so well? \nAnd how do developers make such games run smoothly even on low-end mobile devices? \n \nIn this article, we’ll break down the design and t",
      "publishedAt": "2025-12-02T13:49:12.000Z",
      "author": "gamh5games",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 8.97554539715696,
      "ingestedAt": "2025-12-02T14:44:03.172Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ad667b",
      "title": "AuroraCanvas — A Cross-Platform Generative Art Experience",
      "url": "https://dev.to/s10olamide/auroracanvas-a-cross-platform-generative-art-experience-5ac4",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9ngehicxcc9ztmrhlzia.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>What I Built</p>  \n  \n<p>AuroraCanvas is a visually immersive generative art playground built with Uno Platform and powered by AI-assisted color and particle effects.</p>  \n  \n<p>Theme: Cosmic / Ambient / Fluid Visuals<br>  \nSpecial features:</p>  \n  \n<p>✨ Dynamic AI-generated color palettes</p>  \n  \n<p>🌌 Touch/mouse-responsive particle flows</p>  \n  \n<p>🎵 Audio-reactive animations</p>  \n  \n<p>💫 “Surprise Me” mode: continuously evolving scenes</p>  \n  \n<p>🎥 Demo</p>  \n  \n<p>Live Demo (WebAssembly): [Insert link]<br>  \nGitHub Repository: [Insert link]</p>  \n  \n<p>Screenshots / GIFs:</p>  \n  \n<p>🖥️ Windows: Full-screen particle animations</p>  \n  \n<p>📱 iOS / Android: Touch painting mode</p>  \n  \n<p>🌐 WASM: Browser-based experience</p>  \n  \n<p>Test Account (if login required):</p>  \n  \n<p>Email: <a href=\"mailto:test@demo.com\">test@demo.com</a></p>  \n  \n<p>Password: Demo123!</p>  \n  \n<p>🧩 Cross-Platform Magic</p>  \n  \n<p>Platforms supported:</p>  \n  \n<p>iOS, Android</p>  \n  \n<p>Windows, macOS, Linux (Skia backend)</p>  \n  \n<p>WebAssembly</p>  \n  \n<p>Single codebase benefits:</p>  \n  \n<p>95% shared code</p>  \n  \n<p>XAML-based UI</p>  \n  \n<p>Shared logic in .NET</p>  \n  \n<p>Only small platform-specific hooks (GPU, gestures)</p>  \n  \n<p>Seeing the same shimmering art run identically on mobile, desktop, and web—that’s the true magic of Uno Platform.</p>  \n  \n<p>🕹️ Interactive Features</p>  \n  \n<p>Touch + Mouse Painting: Draw with shimmering, ripple, or exploding particles</p>  \n  \n<p>AI Palette Generator: Instantly generates new color schemes</p>  \n  \n<p>Scene Presets: Nebula, Aurora, Watercolor, Starfall</p>  \n  \n<p>Animations: SmoothSpring transitions, parallax layers, GPU shader effects</p>  \n  \n<p>Customizable Controls: Brush size, particle behavior, speed, gravity fields</p>  \n  \n<p>Every interaction feels satisfying and alive.</p>  \n  \n<p>🌠 The Wow Factor</p>  \n  \n<p>Real-time generative visuals that feel alive</p>  \n  \n<p>AI-assisted scene creation makes every session unique</p>  \n  \n<p>Identical behavior across all platforms</p>  \n  \n<p>Zero-lag animations, even in WebAssembly</p>  \n  \n<p>“Living wallpaper” mode for ambient art displays</p>  \n  \n<p>2️⃣ Cover Image Suggestion</p>  \n  \n<p>Theme: Aurora / Nebula / Particle Flow</p>  \n  \n<p>Text overlay: “AuroraCanvas — Generative Art Everywhere”</p>  \n  \n<p>Optional animation: subtle moving particles in GIF format for social posts</p>  \n  \n<p>3️⃣ GIF Demo Suggestions</p>  \n  \n<p>Screen recording of:</p>  \n  \n<p>Touch painting with particle bursts</p>  \n  \n<p>“Surprise Me” mode generating a new scene</p>  \n  \n<p>Switching between desktop, mobile, and browser</p>  \n  \n<p>Tip: Keep each GIF 5–10 seconds long to highlight interactivity.</p>  \n  \n<p>4️⃣ Code Snippets for DEV Submission</p>  \n  \n<p>Shared UI example (XAML):</p>  \n  \n<p><br>  \n      \n            PointerMoved=\"OnPointerMoved\"/&gt;<br>  \n    <br>  \n</p>  \n  \n<p>Shared Logic (C#):</p>  \n  \n<p>public void OnPointerMoved(object sender, PointerRoutedEventArgs e)<br>  \n{<br>  \n    var point = e.GetCurrentPoint(ParticleCanvas).Position;<br>  \n    ParticleEngine.SpawnParticle(point, currentPalette);<br>  \n}</p>  \n  \n<p>public void GenerateNewScene(object sender, RoutedEventArgs e)<br>  \n{<br>  \n    currentPalette = AIPaletteGenerator.CreateRandomPalette();<br>  \n    ParticleEngine.ResetScene();<br>  \n}</p>  \n  \n<p>Cross-platform note:</p>  \n  \n<p>GPU effects use SkiaSharp on Windows/macOS/Linux and Canvas2D / WebGL for WebAssembly.</p>  \n  \n<p>Particle engine runs identically across mobile and desktop thanks to shared .NET logic.</p>  \n  \n<p>5️⃣ Complete README for GitHub</p>  \n  \n<h1>  \n    \n    \n  AuroraCanvas 🌌  \n</h1>  \n  \n<p>AuroraCanvas is a <strong>cross-platform generative art playground</strong> built with <strong>Uno Platform</strong>.</p>  \n  \n<h2>  \n    \n    \n  Features  \n</h2>  \n  \n<ul>  \n<li>Touch/mouse responsive particle painting</li>  \n<li>AI-generated color palettes</li>  \n<li>Real-time animations</li>  \n<li>“Surprise Me” mode: continuously evolving scenes</li>  \n<li>Cross-platform: iOS, Android, Windows, macOS, Linux, WebAssembly</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  Demo  \n</h2>  \n  \n<ul>  \n<li>Web: [Insert link]</li>  \n<li>GitHub: [Insert repo link]</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  Installation  \n</h2>  \n  \n  \n  \n<div>  \n<pre><code>  \nbash  \ngit clone https://github.com/yourusername/AuroraCanvas.git  \ncd AuroraCanvas  \n  \n  \nFollow Uno Platform instructions for your target platform.  \n  \nUsage  \n  \nLaunch app on any supported platform  \n  \nTouch/click the canvas to draw particles  \n  \nPress \"Surprise Me\" to generate a new scene  \n  \nAdjust brush, particle speed, and colors in settings  \n  \nContributing  \n  \nPRs and issues are welcome!  \n  \nLicense  \n  \nMIT License  \n</code></pre>  \n  \n</div>",
      "summary": "What I Built  \n  \nAuroraCanvas is a visually immersive generative art playground built with Uno Platform and powered by AI-assisted color and particle effects.  \n  \nTheme: Cosmic / Ambient / Fluid Visuals  \nSpecial features:  \n  \n✨ Dynamic AI-generated color palettes  \n  \n🌌 Touch/mouse-responsive particle flows  \n  \n🎵 Audio-reactive animations  \n  \n💫 “Surprise Me” mode: continuously evolving scenes  \n  \n🎥 Demo  \n  \nLive Demo (WebAssembly): [Insert link]  \nGitHub Repository: [Insert link]  \n ",
      "publishedAt": "2025-12-02T13:48:45.000Z",
      "author": "s10olamide",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 8.476714771708737,
      "ingestedAt": "2025-12-02T14:44:03.172Z",
      "tags": [
        "code_review",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ad667d",
      "title": "9 Essential Developer Tools You Should be Exploring Right Now ⚡️🔎",
      "url": "https://dev.to/madza/9-essential-developer-tools-you-should-be-exploring-right-now-2kdj",
      "content": "<p>Modern developer tools have become fundamental components which enhance coding operations while shortening the duration needed to finish projects.</p> \n \n<p>However the curation and selection process for developer tools that might be useful in the practice can feel as a daunting task because numerous platforms and utilities exist.</p> \n \n<p>In this article I’ve curated 9 of my recent favorite developer tools which help developers create applications through advanced authentication systems, headless CMS backends, visual website builders, API SDK generators and much more.</p> \n \n<p>These tools help developers save time while reducing obstacles to work on developing robust software.</p> \n \n<p>All the tools provide brief descriptions along with essential features, workflow image snaphots and direct access links which enable you to evaluate their suitability for your technology infrastructure.</p> \n \n<p>Let’s dive in and discover how these essential tools can improve your coding workflow.</p> \n \n \n \n \n<h2> \n   \n   \n  1. <a href=\"https://fandf.co/4ozoC1R\">Depot</a> – Build Docker Images up to 40X Faster \n</h2> \n \n<p>Depot is a powerful remote container build platform which speeds up Docker builds through its cloud-based infrastructure with built-in caching and native multi-architecture support and automated CI/CD system integration.</p> \n \n<p>The platform uses remote builders to replace slow local builds and CI runners which enables the resources of your machine to remain available while cutting down build duration for faster deployment and development cycles.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzhoptwdt2rpbwm8gv3s3.jpeg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzhoptwdt2rpbwm8gv3s3.jpeg\" alt=\"Depot preview\" width=\"800\" height=\"444\"></a></p> \n \n<p><strong>Some of the most useful features include:</strong></p> \n \n<p>🤖 <strong>Agent sandbox environments</strong>: Run AI coding agents such as Claude Code in safe, separated Depot sandboxes that are fully Docker supported. These AI tools can thus develop and verify code in a secure environment without any risk for your local machine.</p> \n \n<p>📦 <strong>Docker Bake support</strong>: Speed up highly complex multi-target builds by using native Docker Bake support. You can continue to use your current build definitions while taking advantage of Depot's speed for complex build matrices and parallel ​‍​‌‍​‍‌workflows.</p> \n \n<p>🔐 <strong>Private resource access via Tailscale</strong>: Connect Depot builders to your private networks, databases, and internal registries through Tailscale integration, enabling secure builds that need access to protected resources.</p> \n \n<p>🔽 <strong>Self-hosted option with Depot Managed</strong>: Deploy the entire Depot infrastructure in your own AWS account for complete control over security, compliance, and data sovereignty while keeping all the speed benefits.</p> \n \n<p>📊 <strong>Receive detailed analytics and insights</strong>: Run multiple builds concurrently with intelligent queueing and get detailed analytics on build performance, cache hit rates, and cost breakdowns to optimize.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsqt03avxuvd599naaw1n.jpeg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsqt03avxuvd599naaw1n.jpeg\" alt=\"Depot graph\" width=\"800\" height=\"360\"></a></p> \n \n<p>Want to boost your Docker build workflow and save time on every container build? <a href=\"https://fandf.co/43YsW2b\">Try Depot today</a> and experience faster, and more resource efficient container builds without complexity!</p> \n \n<p>🌎 Website Link: <a href=\"https://fandf.co/485QPHv\">https://depot.dev/</a></p> \n \n<p>Thanks to the Depot team for sponsoring this article!</p> \n \n \n \n \n<h2> \n   \n   \n  2. <a href=\"https://www.builder.io/\">Builder.io</a> - Make modern web experiences with AI \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fq36yu54zsqr73oxxs6xz.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fq36yu54zsqr73oxxs6xz.png\" alt=\"BuilderIO\" width=\"800\" height=\"466\"></a></p> \n \n<p>Builder is an AI-powered visual development platform that lets you provide direct prompts to create both the editable design and the code. Integrates well with Figma, so you can use your existing workflow.</p> \n \n<p><strong>Key features &amp; why to use it:</strong></p> \n \n<ul> \n<li><p>AI generates production code from Figma design or from the prompt you provide.</p></li> \n<li><p>Visual editor lets you easily preview the output and make any changes you want.</p></li> \n<li><p>Integrates with other platforms such as GitHub for seamless deployment.</p></li> \n</ul> \n \n<p>🌎 Website Link: <a href=\"https://www.builder.io/\">https://www.builder.io/</a></p> \n \n \n \n \n<h2> \n   \n   \n  3. <a href=\"https://webflow.com/\">Webflow</a> - Design and launch websites visually \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbzn091cwup0gmu7wugp8.jpeg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbzn091cwup0gmu7wugp8.jpeg\" alt=\"Webflow\" width=\"800\" height=\"472\"></a></p> \n \n<p>Webflow is a powerful website design tool that allows designers and developers to create responsive, real-world-ready sites in no time without the need for coding.</p> \n \n<p><strong>Key features &amp; why to use it:</strong></p> \n \n<ul> \n<li><p>Visual designer combines design and CMS management into a single platform.</p></li> \n<li><p>Automatically generates clean, semantic HTML, CSS, and JavaScript.</p></li> \n<li><p>Launch and optimize site easily with the help of integrated hosting and SEO tools.</p></li> \n</ul> \n \n<p>🌎 Website Link: <a href=\"https://webflow.com/\">https://webflow.com/</a></p> \n \n \n \n \n<h2> \n   \n   \n  4. <a href=\"https://medusajs.com/\">Medusa</a> - Develop headless e-commerce apps \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fy2qkreb0zxaf1pvlioiv.jpeg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fy2qkreb0zxaf1pvlioiv.jpeg\" alt=\"Medusa\" width=\"800\" height=\"480\"></a></p> \n \n<p>Medusa is a headless e-commerce platform that is open source and developer-friendly. It lets developers create the backend of scalable and customizable online stores quickly, while integrating with modern frontends.</p> \n \n<p><strong>Key features &amp; why to use it:</strong></p> \n \n<ul> \n<li><p>API-first architecture that is fully customizable for various commerce needs.</p></li> \n<li><p>Supports multiple commonly used payment providers out of the box.</p></li> \n<li><p>The open-source nature of the platform ensures extensibility and control over your backend.</p></li> \n</ul> \n \n<p>🌎 Website Link: <a href=\"https://medusajs.com/\">https://medusajs.com/</a></p> \n \n \n \n \n<h2> \n   \n   \n  5. <a href=\"https://clerk.com/\">Clerk</a> - Manage user authentication \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjy1eik93w829cmloj0j2.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjy1eik93w829cmloj0j2.png\" alt=\"Clerk\" width=\"800\" height=\"432\"></a></p> \n \n<p>Clerk is an all-around solution for the authentication needs of modern web apps. It offers a set of customizable, ready-to-use UI components and security features that are advanced yet easy to implement.</p> \n \n<p><strong>Key features &amp; why to use it:</strong></p> \n \n<ul> \n<li><p>Passwordless authentication, magic links, and social login support.</p></li> \n<li><p>Built-in bot detection and brute-force attack prevention.</p></li> \n<li><p>Comes with multi-tenancy, role management, and session lifecycle control.</p></li> \n</ul> \n \n<p>🌎 Website Link: <a href=\"https://clerk.com/\">https://clerk.com/</a></p> \n \n \n \n \n<h2> \n   \n   \n  6. <a href=\"https://payloadcms.com/\">Payload</a> - Create CMS and backend systems \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwmg79ahrdlr6dnwk4fzf.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwmg79ahrdlr6dnwk4fzf.png\" alt=\"Payload\" width=\"800\" height=\"404\"></a></p> \n \n<p>Payload is an open-source backend CMS for NextJS that provides developers with full control over content and application data, while at the same time, gives marketers the power of a user-friendly editing interface.</p> \n \n<p><strong>Key features &amp; why to use it:</strong></p> \n \n<ul> \n<li><p>Your entire Payload config can be installed with a single line into any existing NextJS app.</p></li> \n<li><p>Fully customizable schemas and a powerful API for content management.</p></li> \n<li><p>Developer-friendly open-source architecture with full control.</p></li> \n</ul> \n \n<p>🌎 Website Link: <a href=\"https://payloadcms.com/\">https://payloadcms.com/</a></p> \n \n \n \n \n<h2> \n   \n   \n  7. <a href=\"https://www.buildwithfern.com/\">Fern</a> - Generate SDKs and API docs \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft1yx0u6uugrdyipso61u.jpeg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft1yx0u6uugrdyipso61u.jpeg\" alt=\"Fern\" width=\"800\" height=\"444\"></a></p> \n \n<p>Fern simplifies the process of SDK generation and the creation of accompanying documentation, thus providing clean, and complete API docs which in turn, leads to enhanced developer experience and decreased engineering work.</p> \n \n<p><strong>Key features &amp; why to use it:</strong></p> \n \n<ul> \n<li><p>Supports multiple languages like TypeScript, Python, Go, Java, C#, etc.</p></li> \n<li><p>Continuous SDK updates are integrated into CI/CD pipelines.</p></li> \n<li><p>Comes with useful features OAuth 2.0, server-sent events, and auto-pagination.</p></li> \n</ul> \n \n<p>🌎 Website Link: <a href=\"https://www.buildwithfern.com/\">https://www.buildwithfern.com/</a></p> \n \n \n \n \n<h2> \n   \n   \n  8. <a href=\"https://openpanel.com/\">OpenPanel</a> - Host a server control panel \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzt2e1qizrcyl796bgu5l.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzt2e1qizrcyl796bgu5l.png\" alt=\"OpenPanel\" width=\"800\" height=\"438\"></a></p> \n \n<p>OpenPanel​‍​‌‍​‍‌ is a lightweight control panel that enables humans to control a web server and services in a very simple and straightforward manner with no need for complicated configurations.</p> \n \n<p><strong>Key features &amp; why to use it:</strong></p> \n \n<ul> \n<li><p>An easy-to-use UI for managing websites, databases, and mail servers.</p></li> \n<li><p>Facilitates SSL certificate automation and backup scheduling.</p></li> \n<li><p>Its design focuses on ease of installation and minimal resource utilization.</p></li> \n</ul> \n \n<p>🌎 Website Link: <a href=\"https://openpanel.com/\">https://openpanel.com/</a></p> \n \n \n \n \n<h2> \n   \n   \n  9. <a href=\"https://grafana.com/\">Grafana</a> - Visualize app metrics and logs \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Falritrmbpdjq6wwpjfly.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Falritrmbpdjq6wwpjfly.png\" alt=\"Grafana\" width=\"800\" height=\"443\"></a></p> \n \n<p>Grafana is an open-source project for monitoring and observability. It lets you to build dashboards that allow easy understanding of system metrics, logs, and traces, and even in real-time.</p> \n \n<p><strong>Key features &amp; why to use it:</strong></p> \n \n<ul> \n<li><p>Integrates with diverse data sources for unified monitoring.</p></li> \n<li><p>Comes with options for representation of the data, e.g. graphs, heatmaps, or alerts.</p></li> \n<li><p>It can be used independently of scale, like in a small environment or a big enterprise ​‍​‌‍​‍‌environment.</p></li> \n</ul> \n \n<p>🌎 Website Link: <a href=\"https://grafana.com/\">https://grafana.com/</a></p> \n \n \n \n \n<h3> \n   \n   \n  Did you like the resources? Here is more 👇<br><br> \n</h3> \n \n<p>Join 6,000+ others to receive the best DEV resources, tools, productivity tips, and career growth advice I discover by subscribing to <a href=\"https://madzadev.substack.com/\">my newsletter</a>!</p> \n \n<p><a href=\"https://madzadev.substack.com/\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5nb15k9rlvy91bc7yd4c.png\" alt=\"The Developer Toolbox\" width=\"800\" height=\"181\"></a></p> \n \n<p>Also, connect with me on <a href=\"https://twitter.com/madzadev\">Twitter</a>, <a href=\"https://www.linkedin.com/in/madzadev/\">LinkedIn</a>, and <a href=\"https://github.com/madzadev\">GitHub</a>!</p> \n \n<p>Writing has always been my passion, and it gives me pleasure to help and inspire people. If you want to get featured or partner up, feel free to <a href=\"https://www.madza.dev/contact\">get in touch</a>!</p>",
      "summary": "Modern developer tools have become fundamental components which enhance coding operations while shortening the duration needed to finish projects. \n \nHowever the curation and selection process for developer tools that might be useful in the practice can feel as a daunting task because numerous platforms and utilities exist. \n \nIn this article I’ve curated 9 of my recent favorite developer tools which help developers create applications through advanced authentication systems, headless CMS backen",
      "publishedAt": "2025-12-02T13:48:06.000Z",
      "author": "Madza",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 15.45704031366271,
      "ingestedAt": "2025-12-02T14:44:03.173Z",
      "tags": [
        "code_review",
        "documentation",
        "agents",
        "ide",
        "observability",
        "governance",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ad667e",
      "title": "Describe promotions on AI services available now",
      "url": "https://dev.to/wadie_realme_733c52996966/describe-promotions-on-ai-services-available-now-4ml7",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsibdb9oyildlll7w4qms.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><h1>  \n    \n    \n  Unleash AI Potential for Less: Top Promotions on AI Services You Can Grab Now!  \n</h1>  \n  \n<p>The world of Artificial Intelligence is evolving at lightning speed, offering unprecedented capabilities from advanced large language models to sophisticated image generation and robust data analytics. As businesses and developers race to integrate these powerful tools, a key consideration often arises: cost. The good news? AI service providers understand this need for accessible innovation, and exciting promotions on AI models, subscriptions, and paid plan benefits are blooming!</p>  \n  \n<p>This post dives into the current landscape of AI service promotions, helping you identify opportunities to maximize your AI budget, experiment with new technologies, and scale your projects without breaking the bank. Whether you're a startup, a seasoned enterprise, or an individual developer, there's likely a deal waiting to amplify your AI journey.</p>  \n  \n<h2>  \n    \n    \n  Why Promotions Matter in the AI Space  \n</h2>  \n  \n<p>The competitive nature of the AI market means providers are constantly vying for your attention. This creates a fantastic environment for consumers, leading to a variety of promotional offers designed to:</p>  \n  \n<ul>  \n<li>  <strong>Attract New Users:</strong> Lower the barrier to entry for those curious about AI.</li>  \n<li>  <strong>Encourage Adoption:</strong> Help users explore full feature sets without immediate heavy investment.</li>  \n<li>  <strong>Reward Loyalty:</strong> Provide incentives for long-term commitment.</li>  \n<li>  <strong>Boost Usage:</strong> Offer tiered pricing or credit bonuses that make scaling more appealing.</li>  \n<li>  <strong>Showcase New Features:</strong> Entice users to try cutting-edge models or services.</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  Navigating the AI Promotion Landscape: Key Categories and Types of Deals  \n</h2>  \n  \n<p>Promotions on AI services typically fall into several categories, each catering to different needs and scales of operation.</p>  \n  \n<h3>  \n    \n    \n  1. Cloud AI Platform Credits &amp; Free Tiers (AWS, Azure, GCP, IBM Cloud)  \n</h3>  \n  \n<p>Major cloud providers are often the first stop for AI infrastructure, offering a vast array of machine learning services, compute power, and specialized AI APIs.</p>  \n  \n<ul>  \n<li>  <strong>Extended Free Tiers:</strong> While perpetual free tiers exist for basic services, many providers offer <em>extended</em> free periods (e.g., 6-12 months) for specific AI services or higher usage limits for new accounts. This is perfect for prototyping and initial development.</li>  \n<li>  <strong>Startup Credits:</strong> Cloud providers frequently partner with startup accelerators or offer direct programs providing substantial credits (often thousands of dollars) for qualifying startups. These can be a game-changer for early-stage companies building AI-first products.</li>  \n<li>  <strong>Compute Instance Discounts:</strong> Look for promotional pricing on GPU-enabled virtual machines or specialized AI accelerators, particularly for new regions or during specific seasonal campaigns.</li>  \n<li>  <strong>Managed Service Discounts:</strong> Occasionally, specific managed AI services (like cognitive services, MLOps platforms, or data labeling tools) will feature discounted usage rates for a limited time.</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  2. Generative AI Model API Subscriptions &amp; Credit Bundles  \n</h3>  \n  \n<p>The explosion of Large Language Models (LLMs), image generation, and other generative AI has led to a new wave of API-first providers. These are hotbeds for promotions.</p>  \n  \n<ul>  \n<li>  <strong>New User Credit Bonuses:</strong> Many platforms offer a bonus amount of credits when you sign up or make your first purchase. For example, \"Buy 100,000 tokens, get 50,000 free.\"</li>  \n<li>  <strong>Discounted Monthly/Annual Subscriptions:</strong> Opting for an annual plan often provides a significant discount compared to month-to-month billing. Some providers also offer introductory rates for the first few months of a new subscription tier.</li>  \n<li>  <strong>Tiered Pricing Incentives:</strong> While not strictly a promotion, many providers structure their pricing such that higher volume usage (e.g., more API calls, more tokens) comes with a lower per-unit cost. Promotions might temporarily lower the threshold for these higher tiers.</li>  \n<li>  <strong>Early Access Programs:</strong> Be on the lookout for beta programs or early access to new, advanced models. These sometimes come with reduced pricing or free usage periods in exchange for feedback.</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  3. AI Development Tools &amp; MLOps Platform Premium Features  \n</h3>  \n  \n<p>Beyond just the models, the tools you use to build, deploy, and manage AI models also offer promotional opportunities.</p>  \n  \n<ul>  \n<li>  <strong>IDE &amp; Workbench Trials/Discounts:</strong> Premium features in AI-focused IDEs or collaborative MLOps workbenches might offer extended free trials or significant discounts for annual licenses.</li>  \n<li>  <strong>Feature Unlocks:</strong> Some platforms might temporarily unlock advanced features (e.g., parallel experimentation, advanced monitoring, more robust security) for existing users to encourage upgrades.</li>  \n<li>  <strong>Bundle Deals:</strong> Occasionally, MLOps platforms might bundle their services with cloud compute credits or integrate third-party tools at a discounted rate.</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  4. Specialized AI Services (Data Annotation, Custom Model Training)  \n</h3>  \n  \n<p>Even highly specialized services can have promotional windows.</p>  \n  \n<ul>  \n<li>  <strong>First Project Discounts:</strong> Companies offering data annotation, custom model fine-tuning, or specific AI consulting services might offer a discount on your first project to demonstrate their capabilities.</li>  \n<li>  <strong>Volume-Based Tiers:</strong> Similar to API usage, larger projects for data labeling or custom training often receive better per-unit pricing.</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  Maximizing Your Savings: Tips for Leveraging AI Promotions  \n</h2>  \n  \n<ol>  \n<li> <strong>Read the Fine Print:</strong> Always understand the terms – duration, usage limits, eligibility, and what happens after the promotional period ends.</li>  \n<li> <strong>Plan Your Projects:</strong> Align your development roadmap with promotional offers. If you know you'll need a specific AI service in a few months, check for upcoming or current deals.</li>  \n<li> <strong>Explore New Providers:</strong> Don't limit yourself to one vendor. New entrants often have aggressive promotions to gain market share.</li>  \n<li> <strong>Join Beta Programs:</strong> Access to cutting-edge AI features often comes with reduced costs or free usage during the testing phase.</li>  \n<li> <strong>Leverage Startup Programs:</strong> If you're a startup, actively seek out and apply for startup credit programs from major cloud and AI service providers.</li>  \n<li> <strong>Stay Subscribed:</strong> Sign up for newsletters and follow social media channels of your preferred AI service providers. Promotions are often announced there first.</li>  \n</ol>  \n  \n<h2>  \n    \n    \n  The Undeniable Value of Paid AI Plans  \n</h2>  \n  \n<p>While promotions are fantastic for cost savings, remember the inherent value of paid AI plans:</p>  \n  \n<ul>  \n<li>  <strong>Reliability &amp; Performance:</strong> Guaranteed uptime, faster response times, and dedicated resources.</li>  \n<li>  <strong>Higher Usage Limits:</strong> Crucial for scaling production-grade applications.</li>  \n<li>  <strong>Dedicated Support:</strong> Access to technical assistance when you need it most.</li>  \n<li>  <strong>Advanced Features:</strong> Access to cutting-edge models, fine-tuning capabilities, security features, and MLOps tools unavailable in free tiers.</li>  \n<li>  <strong>Compliance &amp; Security:</strong> Robust features to meet enterprise-grade security and compliance requirements.</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  Conclusion: Seize the AI Opportunity  \n</h2>  \n  \n<p>The current wave of promotions on AI services presents an incredible opportunity. Whether you're looking to integrate an advanced LLM, power sophisticated image generation, build robust MLOps pipelines, or simply experiment with the latest AI models, there's never been a better time to invest. By strategically leveraging these offers, you can significantly reduce your initial outlay, accelerate your development, and unlock the transformative power of Artificial Intelligence for your projects and business.</p>  \n  \n<p>Don't let budget constraints hold you back from innovation. Start exploring these promotions today and take your AI initiatives to the next level!</p>",
      "summary": "  \n    \n    \n  Unleash AI Potential for Less: Top Promotions on AI Services You Can Grab Now!  \n  \n  \nThe world of Artificial Intelligence is evolving at lightning speed, offering unprecedented capabilities from advanced large language models to sophisticated image generation and robust data analytics. As businesses and developers race to integrate these powerful tools, a key consideration often arises: cost. The good news? AI service providers understand this need for accessible innovation, and",
      "publishedAt": "2025-12-02T13:47:52.000Z",
      "author": "Wadie Realme",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 14.45964454836404,
      "ingestedAt": "2025-12-02T14:44:03.173Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "testing",
        "observability",
        "governance",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ad6681",
      "title": "OpenTelemetry Filelog Receiver: A Guide to Ingesting Log Files",
      "url": "https://dev.to/dash0/opentelemetry-filelog-receiver-a-guide-to-ingesting-log-files-38m6",
      "content": "<p>Even in the age of cloud-native apps and distributed tracing, plain old log files remain one of the richest sources of truth in any system. From legacy business applications and batch jobs to <a href=\"https://www.dash0.com/guides/nginx-logs\">NGINX</a>, databases, and on-prem infrastructure, critical diagnostics still end up written to disk.</p> \n \n<p><a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver\">The OpenTelemetry Collector filelog receiver</a> gives you a way to bring those logs into a <a href=\"https://www.dash0.com/guides/opentelemetry-collector\">modern observability pipeline</a>. It continuously tails files, parses their contents, and converts raw text into <a href=\"https://www.dash0.com/knowledge/opentelemetry-logging-explained\">structured OpenTelemetry LogRecords</a>.</p> \n \n<p>This guide shows you how to put that power to work, from the basics of reading a file to building a production-ready pipeline that handles rotation, recovers from restarts, and never loses a single line. You'll learn how to structure, enrich, and standardize log file entries so they become first-class observability data.</p> \n \n<p>Let's begin!</p> \n \n<h2> \n   \n   \n  How the Filelog receiver works \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg30rpueploqdrubf9zil.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg30rpueploqdrubf9zil.png\" alt=\"An illustration of how filelogreceiver works in OpenTelemetry\" width=\"800\" height=\"335\"></a></p> \n \n<p>Before we get into configuration details, it helps to picture how the receiver handles a log file throughout its lifecycle. You can think of it as a simple repeating four-step loop:</p> \n \n<ol> \n<li><p><strong>Discover:</strong> The receiver scans the filesystem at regular intervals, using the <code>include</code> and <code>exclude</code> patterns you've set, to figure out which log files it should pay attention to.</p></li> \n<li><p><strong>Read:</strong> Once a file is picked up, the receiver opens it and begins following along as new lines are written. The <code>start_at</code> setting decides whether it begins from <code>beginning</code> or just tails new content from the <code>end</code>.</p></li> \n<li><p><strong>Parse:</strong> Each line (or block of lines, if multiline parsing is used) runs through a series of <a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/README.md\">Stanza operators</a> (if configured). These operators parse the raw text, pull out key attributes, assign timestamps and severity levels, and ultimately structure the log data.</p></li> \n<li><p><strong>Emit:</strong> Finally, the structured log records are passed into the Collector's pipeline, where they can be <a href=\"https://www.dash0.com/guides/opentelemetry-filter-processor\">filtered</a>, transformed further, or exported to your backend.</p></li> \n</ol> \n \n<p>This <code>Discover -&gt; Read -&gt; Parse -&gt; Emit</code> loop forms the foundation of everything the receiver does.</p> \n \n<h2> \n   \n   \n  Quick Start: tailing a log file \n</h2> \n \n<p>One of the most common use cases is when your application is already writing logs in JSON format to a file. For example, imagine you have a service writing JSON logs to <code>/var/log/myapp/app.log</code>:<br> \n</p> \n \n<div> \n<pre><code><span>{</span><span>\"time\"</span><span>:</span><span>\"2025-09-28 20:15:12\"</span><span>,</span><span>\"level\"</span><span>:</span><span>\"INFO\"</span><span>,</span><span>\"message\"</span><span>:</span><span>\"User logged in successfully\"</span><span>,</span><span>\"user_id\"</span><span>:</span><span>\"u-123\"</span><span>,</span><span>\"source_ip\"</span><span>:</span><span>\"192.168.1.100\"</span><span>}</span><span> \n</span><span>{</span><span>\"time\"</span><span>:</span><span>\"2025-09-28 20:15:45\"</span><span>,</span><span>\"level\"</span><span>:</span><span>\"WARN\"</span><span>,</span><span>\"message\"</span><span>:</span><span>\"Password nearing expiration\"</span><span>,</span><span>\"user_id\"</span><span>:</span><span>\"u-123\"</span><span>}</span><span> \n</span></code></pre> \n \n</div> \n \n \n \n<p>Here's a minimal <code>filelog</code> receiver example to read and ingest such logs into an OpenTelemetry pipeline:<br> \n</p> \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>receivers</span><span>:</span> \n  <span>filelog</span><span>:</span> \n    <span># 1. DISCOVER all .log files in /var/log/myapp/</span> \n    <span>include</span><span>:</span> <span>[</span><span>/var/log/myapp/*.log</span><span>]</span> \n    <span># 2. READ from the beginning of new files</span> \n    <span>start_at</span><span>:</span> <span>beginning</span> \n    <span># 3. PARSE using the json_parser operator</span> \n    <span>operators</span><span>:</span> \n      <span>-</span> <span>type</span><span>:</span> <span>json_parser</span> \n        <span># Tell the parser where to find the timestamp and how it's formatted</span> \n        <span>timestamp</span><span>:</span> \n          <span>parse_from</span><span>:</span> <span>attributes.time</span> \n          <span>layout</span><span>:</span> <span>\"</span><span>%Y-%m-%d</span><span> </span><span>%H:%M:%S\"</span> \n        <span># Tell the parser which field contains the severity</span> \n        <span>severity</span><span>:</span> \n          <span>parse_from</span><span>:</span> <span>attributes.level</span> \n \n<span>exporters</span><span>:</span> \n  <span>debug</span><span>:</span> \n    <span>verbosity</span><span>:</span> <span>detailed</span> \n \n<span>service</span><span>:</span> \n  <span>pipelines</span><span>:</span> \n    <span>logs</span><span>:</span> \n      <span>receivers</span><span>:</span> <span>[</span><span>filelog</span><span>]</span> \n      <span>exporters</span><span>:</span> <span>[</span><span>debug</span><span>]</span> \n</code></pre> \n \n</div> \n \n \n \n<p>Here's a breakdown of the above configuration:</p> \n \n<ul> \n<li> \n<code>include</code>: Points the receiver to all <code>.log</code> files in <code>/var/log/myapp/</code>.</li> \n<li> \n<code>start_at: beginning</code>: Ensures the receiver processes the entire file the first time it sees it. By default (<code>end</code>), it would only capture new lines written after the Collector starts.</li> \n<li> \n<code>operators</code>: In this case, there's just one: the <a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/json_parser.md\">json_parser</a>. Its job is to take each log line, interpret it as JSON, and then promote selected fields into the log record's core metadata.</li> \n<li> \n<code>timestamp</code> and <code>severity</code>: Within the <code>json_parser</code>, we're pulling the <code>time</code> and <code>level</code> fields out of the JSON and promoting them to the OpenTelemetry's top-level <code>Timestamp</code> and <code>Severity*</code> fields for each log record.</li> \n</ul> \n \n<p><a href=\"https://www.dash0.com/guides/opentelemetry-debug-exporter\">With the debug exporter</a>, you'll see the parsed and structured output. Instead of just raw JSON, each field is now properly represented inside each log record:<br> \n</p> \n \n<div> \n<pre><code>LogRecord #0 \nObservedTimestamp: 2025-09-28 20:48:36.728437503 +0000 UTC \nTimestamp: 2025-09-28 20:15:12 +0000 UTC \nSeverityText: INFO \nSeverityNumber: Info(9) \nBody: Str({\"time\":\"2025-09-28 20:15:12\",\"level\":\"INFO\",\"message\":\"User logged in successfully\",\"user_id\":\"u-123\",\"source_ip\":\"192.168.1.100\"}) \nAttributes: \n     -&gt; user_id: Str(u-123) \n     -&gt; source_ip: Str(192.168.1.100) \n     -&gt; log.file.name: Str(myapp.log) \n     -&gt; time: Str(2025-09-28 20:15:12) \n     -&gt; level: Str(INFO) \n     -&gt; message: Str(User logged in successfully) \nTrace ID: \nSpan ID: \nFlags: 0 \n</code></pre> \n \n</div> \n \n \n \n<p>The raw JSON logs have now been converted into OpenTelemetry's unified log data format, ensuring a consistent foundation for cross-system observability.</p> \n \n<p>The <code>log.file.name</code> attribute is automatically added by the receiver by default, and you can also enable <code>include_file_path</code> to capture the full file path as well:<br> \n</p> \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>receivers</span><span>:</span> \n  <span>filelog</span><span>:</span> \n    <span>include</span><span>:</span> <span>[</span><span>/var/log/myapp/*.log</span><span>]</span> \n    <span>include_file_path</span><span>:</span> <span>true</span> \n</code></pre> \n \n</div> \n \n \n \n<p>This allows you to easily filter or query logs based on their exact source path:<br> \n</p> \n \n<div> \n<pre><code>Attributes: \n     -&gt; log.file.path: Str(/var/log/myapp/app.log) \n     -&gt; log.file.name: Str(app.log) \n</code></pre> \n \n</div> \n \n \n \n<p>You can find more enrichment options in the <a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/filelogreceiver/README.md\">official OpenTelemetry Filelog receiver documentation</a>.</p> \n \n<h2> \n   \n   \n  Filtering and managing log files \n</h2> \n \n<p>The most fundamental step in configuring the <code>filelog</code> receiver is telling it which files to monitor. This is controlled using <code>include</code> and <code>exclude</code> glob patterns.</p> \n \n<p>The receiver first uses <code>include</code> to generate a list of all potential files, then it applies the <code>exclude</code> patterns to remove any unwanted files from that list.</p> \n \n<p>Here's an example:<br> \n</p> \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>receivers</span><span>:</span> \n  <span>filelog</span><span>:</span> \n    <span>include</span><span>:</span> <span>[</span><span>/var/log/apps/**/*.log</span><span>]</span> \n    <span>exclude</span><span>:</span> \n      <span>-</span> <span>/var/log/apps/**/debug.log</span> \n      <span>-</span> <span>/var/log/apps/**/*.tmp</span> \n</code></pre> \n \n</div> \n \n \n \n<p>In this scenario, the receiver will collect every <code>.log</code> file under <code>/var/log/apps/</code>, including subdirectories, but it will skip any file named <code>debug.log</code> and any file ending with <code>.tmp</code>.</p> \n \n<h3> \n   \n   \n  Excluding files by modification age \n</h3> \n \n<p>If the log directory you're reading contains many existing log files, you can instruct the receiver to ignore files that have not been modified within a given time window with <code>exclude_older_than</code>:<br> \n</p> \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>receivers</span><span>:</span> \n  <span>filelog</span><span>:</span> \n    <span>include</span><span>:</span> <span>[</span><span>/var/log/myapp/*.log</span><span>]</span> \n    <span>exclude_older_than</span><span>:</span> <span>24h</span> \n    <span>start_at</span><span>:</span> <span>beginning</span> \n</code></pre> \n \n</div> \n \n \n \n<p>In this example, even if <code>app-2025-07-15.log</code> matches the pattern, it will be skipped if it hasn't been updated in the past 24 hours.</p> \n \n<h2> \n   \n   \n  Parsing unstructured text with regular expressions \n</h2> \n \n<p>Most infrastructure logs don't come neatly packaged as JSON. More often, they're plain text strings that follow a loose pattern, such as web server access logs, database query logs, or operating system messages. These logs are human-readable but difficult for machines to analyze until they're given some structure.</p> \n \n<p>The Collector addresses this with the <a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/regex_parser.md\">regex_parser operator</a>. Using regular expressions with named capture groups, you can break a raw log line into meaningful fields and promote them into structured attributes.</p> \n \n<p>For example, consider an <a href=\"https://www.dash0.com/guides/nginx-logs\">NGINX access log</a> in the <a href=\"https://en.wikipedia.org/wiki/Common_Log_Format\">Common Log Format</a>:<br> \n</p> \n \n<div> \n<pre><code>127.0.0.1 - - [28/Sep/2025:20:30:00 +0000] \"GET /api/v1/users HTTP/1.1\" 200 512 \n127.0.0.1 - - [28/Sep/2025:20:30:05 +0000] \"POST /api/v1/login HTTP/1.1\" 401 128 \n</code></pre> \n \n</div> \n \n \n \n<p>You can configure the <code>regex_parser</code> like this to parse them into structured attributes:<br> \n</p> \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>receivers</span><span>:</span> \n  <span>filelog</span><span>:</span> \n    <span>include</span><span>:</span> <span>[</span><span>/var/log/nginx/access.log</span><span>]</span> \n    <span>start_at</span><span>:</span> <span>beginning</span> \n    <span>operators</span><span>:</span> \n      <span>-</span> <span>type</span><span>:</span> <span>regex_parser</span> \n        <span># Use named capture groups to extract data</span> \n        <span>regex</span><span>:</span> \n          <span>'</span><span>^(?P&lt;client_ip&gt;[^</span><span> </span><span>]+)</span><span> </span><span>-</span><span> </span><span>-</span><span> </span><span>\\[(?P&lt;timestamp&gt;[^\\]]+)\\]</span> \n          <span>\"(?P&lt;http_method&gt;[A-Z]+)</span><span> </span><span>(?P&lt;http_path&gt;[^</span><span> </span><span>\"]+)[^\"]*\"</span> \n          <span>(?P&lt;status_code&gt;\\d{3})</span><span> </span><span>(?P&lt;response_size&gt;\\d+)$'</span> \n        <span># Parse the extracted timestamp</span> \n        <span>timestamp</span><span>:</span> \n          <span>parse_from</span><span>:</span> <span>attributes.timestamp</span> \n          <span>layout</span><span>:</span> <span>\"</span><span>%d/%b/%Y:%H:%M:%S</span><span> </span><span>%z\"</span> \n        <span># Map status codes to severities</span> \n        <span>severity</span><span>:</span> \n          <span>parse_from</span><span>:</span> <span>attributes.status_code</span> \n          <span>mapping</span><span>:</span> \n            <span>info</span><span>:</span> \n              <span>-</span> <span>min</span><span>:</span> <span>200</span> \n                <span>max</span><span>:</span> <span>399</span> \n            <span>warn</span><span>:</span> <span>4xx</span> \n            <span>error</span><span>:</span> <span>5xx</span> \n</code></pre> \n \n</div> \n \n \n \n<p>The core of this setup is the <code>regex</code> expression with named capture groups. Each group labels a slice of the line so the parser can turn it into an attribute: <code>client_ip</code> grabs the remote address, <code>timestamp</code> captures the bracketed time string, <code>http_method</code> and <code>http_path</code> pull the request pieces, <code>status_code</code> picks up the three-digit response code, and <code>response_size</code> records the byte count.</p> \n \n<p>Once those attributes exist, the <code>timestamp</code> field parses the <code>timestamp</code> string into a proper datetime value, and the <code>severity</code> block translates status codes into meaningful severity levels using an explicit <code>mapping</code>: 2xx and 3xx responses as <code>INFO</code>, 4xx as <code>WARN</code>, and 5xx as <code>ERROR</code>.</p> \n \n<p>Once access logs are ingested with this configuration, you'll see a structured log record with all the important pieces extracted out as attributes:<br> \n</p> \n \n<div> \n<pre><code>LogRecord #0 \nObservedTimestamp: 2025-09-28 21:17:42.31729069 +0000 UTC \nTimestamp: 2025-09-28 20:30:00 +0000 UTC \nSeverityText: 200 \nSeverityNumber: Info(9) \nBody: Str(127.0.0.1 - - [28/Sep/2025:20:30:00 +0000] \"GET /api/v1/users HTTP/1.1\" 200 512) \nAttributes: \n     -&gt; status_code: Str(200) \n     -&gt; response_size: Str(512) \n     -&gt; log.file.name: Str(myapp.log) \n     -&gt; client_ip: Str(127.0.0.1) \n     -&gt; timestamp: Str(28/Sep/2025:20:30:00 +0000) \n     -&gt; http_method: Str(GET) \n     -&gt; http_path: Str(/api/v1/users) \nTrace ID: \nSpan ID: \nFlags: 0 \n</code></pre> \n \n</div> \n \n \n \n<p>With a single expression and a couple of parsing steps, a flat NGINX access log is transformed into structured OpenTelemetry data. A natural next step is aligning the captured attributes with the <a href=\"https://opentelemetry.io/docs/specs/semconv/http/\">HTTP semantic conventions</a> through the <a href=\"https://www.dash0.com/guides/opentelemetry-attributes-processor\">attributes processor</a> or <a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md\">transform processor</a>.</p> \n \n<h2> \n   \n   \n  Handling multiple log formats \n</h2> \n \n<p>Log files rarely come in just one flavor. For example, you might be ingesting NGINX logs, database logs, and application logs, each with their own format.</p> \n \n<p>The cleanest way to handle this is to define a separate <code>filelog</code> receiver for each file type. Each receiver has its own parsing rules and runs independently, which keeps your setup organized and easy to debug.</p> \n \n<p>This is the best approach when the log formats are completely different and share nothing in common.<br> \n</p> \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>receivers</span><span>:</span> \n  <span># NGINX access logs</span> \n  <span>filelog/nginx_access</span><span>:</span> \n    <span>include</span><span>:</span> <span>[</span><span>/var/log/nginx/access.log</span><span>]</span> \n    <span>operators</span><span>:</span> \n      <span>-</span> <span>type</span><span>:</span> <span>regex_parser</span> \n        <span># ... NGINX access log parsing rules</span> \n \n  <span># NGINX error logs</span> \n  <span>filelog/nginx_error</span><span>:</span> \n    <span>include</span><span>:</span> <span>[</span><span>/var/log/nginx/error.log</span><span>]</span> \n    <span>operators</span><span>:</span> \n      <span>-</span> <span>type</span><span>:</span> <span>regex_parser</span> \n        <span># ... NGINX error log parsing rules</span> \n</code></pre> \n \n</div> \n \n \n \n<p>Sometimes, though, variation happens within a single file.</p> \n \n<p>Maybe most lines are simple messages, but others add extra fields like a <code>trace_id</code>:<br> \n</p> \n \n<div> \n<pre><code>INFO: Application started successfully. \nDEBUG: Processing request for trace_id=12345 \n</code></pre> \n \n</div> \n \n \n \n<p>Instead of writing one massive regex to cover every case, you can use conditional operators with <code>if</code>:<br> \n</p> \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>receivers</span><span>:</span> \n  <span>filelog</span><span>:</span> \n    <span>include</span><span>:</span> <span>[</span><span>/var/log/app.log</span><span>]</span> \n    <span>operators</span><span>:</span> \n      <span># Parse the basic structure of every line</span> \n      <span>-</span> <span>type</span><span>:</span> <span>regex_parser</span> \n        <span>id</span><span>:</span> <span>base_parser</span> <span># a unique ID is required when multiple operators of the same type is being used</span> \n        <span>regex</span><span>:</span> <span>'</span><span>^(?P&lt;severity&gt;\\w+):</span><span> </span><span>(?P&lt;message&gt;.*)$'</span> \n \n      <span># Only run this parser when \"trace_id\" appears</span> \n      <span>-</span> <span>type</span><span>:</span> <span>regex_parser</span> \n        <span>id</span><span>:</span> <span>trace_parser</span> \n        <span>if</span><span>:</span> <span>'</span><span>attributes[\"message\"]</span><span> </span><span>matches</span><span> </span><span>\"trace_id\"'</span> \n        <span>parse_from</span><span>:</span> <span>attributes.message</span> \n        <span>regex</span><span>:</span> <span>'</span><span>.*trace_id=(?P&lt;trace_id&gt;\\w+).*'</span> \n</code></pre> \n \n</div> \n \n \n \n<p>Here's what happens:</p> \n \n<ul> \n<li>The first parser runs on every log line and extracts <code>severity</code> and <code>message</code>.</li> \n<li>The second parser runs only when the message contains <code>trace_id</code>, enriching the log with that extra field.</li> \n</ul> \n \n<p>By combining these two approaches, multiple receivers for unrelated formats and conditional parsing for minor variations, you can handle almost any kind of log your systems produce without creating unreadable or brittle configurations.</p> \n \n<h2> \n   \n   \n  Handling stack traces and multiline logs \n</h2> \n \n<p>Not all log entries fit neatly on a single line. A stack trace is a classic example:<br> \n</p> \n \n<div> \n<pre><code>2025-09-28 21:05:42 [ERROR] Unhandled exception: Cannot read property 'foo' of undefined \nTypeError: Cannot read property 'foo' of undefined \n    at Object.&lt;anonymous&gt; (/usr/src/app/index.js:15:18) \n    at Module._compile (node:internal/modules/cjs/loader:1254:14) \n    at Module._extensions..js (node:internal/modules/cjs/loader:1308:10) \n    at Module.load (node:internal/modules/cjs/loader:1117:32) \n    at Module._load (node:internal/modules/cjs/loader:958:12) \n    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:81:12) \n    at node:internal/main/run_main_module:17:47 \n</code></pre> \n \n</div> \n \n \n \n<p>If you send this directly to the Collector, the filelog receiver will treat each line as a separate log record. That's not what you want, since the error message and every stack frame belong together.</p> \n \n<p>The fix is to use the <code>multiline</code> configuration, which tells the receiver how to group lines into a single entry:<br> \n</p> \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>receivers</span><span>:</span> \n  <span>filelog</span><span>:</span> \n    <span>include</span><span>:</span> <span>[</span><span>/var/log/myapp/*.log</span><span>]</span> \n    <span>start_at</span><span>:</span> <span>beginning</span> \n \n    <span>multiline</span><span>:</span> \n      <span># New entry starts when a line begins with \"YYYY-MM-DD HH:MM:SS\"</span> \n      <span>line_start_pattern</span><span>:</span> <span>^\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2}</span> \n \n    <span>operators</span><span>:</span> \n      <span>-</span> <span>type</span><span>:</span> <span>regex_parser</span> \n        <span>regex</span><span>:</span> <span>(?P&lt;timestamp&gt;\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2})\\s+\\[(?P&lt;severity&gt;[A-Za-z]+)\\]\\s+(?P&lt;message&gt;.+)</span> \n \n        <span>timestamp</span><span>:</span> \n          <span>parse_from</span><span>:</span> <span>attributes.timestamp</span> \n          <span>layout</span><span>:</span> <span>\"</span><span>%Y-%m-%d</span><span> </span><span>%H:%M:%S\"</span> \n \n        <span>severity</span><span>:</span> \n          <span>parse_from</span><span>:</span> <span>attributes.severity</span> \n</code></pre> \n \n</div> \n \n \n \n<p>Here, the <code>line_start_pattern</code> acts as the anchor. A new log entry begins only when a line starts with a date in the form <code>YYYY-MM-DD HH:MM:SS</code>, and any line that doesn't match is appended to the previous one.</p> \n \n<p>The result is that the entire stack trace, from the error message down through each <code>at ...</code> frame, gets captured as one structured log record. This preserves full context, making it far easier to analyze and troubleshoot errors.<br> \n</p> \n \n<div> \n<pre><code>LogRecord #0 \nObservedTimestamp: 2025-10-07 12:04:26.963143642 +0000 UTC \nTimestamp: 2025-09-28 21:05:42 +0000 UTC \nSeverityText: ERROR \nSeverityNumber: Error(17) \nBody: Str(2025-09-28 21:05:42 [ERROR] Unhandled exception: Cannot read property 'foo' of undefined \nTypeError: Cannot read property 'foo' of undefined \n    at Object.&lt;anonymous&gt; (/usr/src/app/index.js:15:18) \n    at Module._compile (node:internal/modules/cjs/loader:1254:14) \n    at Module._extensions..js (node:internal/modules/cjs/loader:1308:10) \n    at Module.load (node:internal/modules/cjs/loader:1117:32) \n    at Module._load (node:internal/modules/cjs/loader:958:12) \n    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:81:12) \n    at node:internal/main/run_main_module:17:47) \nAttributes: \n     -&gt; log.file.name: Str(/var/log/myapp/app.log) \n     -&gt; message: Str(Unhandled exception: Cannot read property 'foo' of undefined) \n     -&gt; timestamp: Str(2025-09-28 21:05:42) \n     -&gt; severity: Str(ERROR) \nTrace ID: \nSpan ID: \nFlags: 0 \n</code></pre> \n \n</div> \n \n \n \n<h2> \n   \n   \n  Parsing metadata from file headers \n</h2> \n \n<p>Some log files don't just contain log entries. They begin with a header section that holds important metadata about the entire file. Without that context, the individual log lines can be hard to interpret.</p> \n \n<p>This pattern is common with batch jobs and export processes. For example, a nightly billing run might write a fresh log file for each execution. At the top of that file you might see something like this:<br> \n</p> \n \n<div> \n<pre><code># Job-ID: job-d8e8fca2 \n# Job-Type: nightly-billing-run \n# Executed-By: scheduler-prod-1 \n# Records-To-Process: 1500 \n2025-10-08T08:20:00Z INFO: Starting billing run. \n2025-10-08T08:21:15Z INFO: Processed account #1. \n2025-10-08T08:21:16Z WARN: Account #2 has a negative balance. \n. . . \n</code></pre> \n \n</div> \n \n \n \n<p>Those first lines tell you exactly which job produced the logs that follow. If you ignore them, you lose that crucial context. The <code>header</code> feature solves this by parsing metadata from the top of the file and stamping it onto every subsequent log record.</p> \n \n<p>It defines a small, dedicated pipeline that runs only on the initial block of lines. You need to specify a regex to match which lines belong to the header. The <code>metadata_operators</code> then parse those lines into attributes which are automatically added to every log entry that follows.</p> \n \n<p>To use this feature, you need to do three things:</p> \n \n<ol> \n<li> Enable the <code>filelog.allowHeaderMetadataParsing</code> feature gate: \n</li> \n</ol> \n \n<div> \n<pre><code><span># docker-compose.yml</span> \n<span>services</span><span>:</span> \n  <span>otelcol</span><span>:</span> \n    <span>command</span><span>:</span> \n      <span>[</span> \n        <span>--config=/etc/otelcol-contrib/config.yaml</span><span>,</span> \n        <span>--feature-gates=filelog.allowHeaderMetadataParsing</span><span>,</span> \n      <span>]</span> \n</code></pre> \n \n</div> \n \n \n \n<ol> \n<li> Set <code>start_at: beginning</code> since the header has to be read from the top.</li> \n<li> Configure both the <code>header</code> rules and the main <code>operators</code> pipeline.</li> \n</ol> \n \n<p>Here's the configuration to parse the headers in the sample log file:<br> \n</p> \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>receivers</span><span>:</span> \n  <span>filelog</span><span>:</span> \n    <span>include</span><span>:</span> <span>[</span><span>/var/log/jobs/*.log</span><span>]</span> \n    <span>start_at</span><span>:</span> <span>beginning</span> <span># required</span> \n    <span>header</span><span>:</span> \n      <span>pattern</span><span>:</span> <span>^#</span> \n      <span>metadata_operators</span><span>:</span> \n        <span>-</span> <span>type</span><span>:</span> <span>key_value_parser</span> \n          <span>delimiter</span><span>:</span> <span>\"</span><span>:</span><span> </span><span>\"</span> \n          <span>pair_delimiter</span><span>:</span> <span>\"</span><span>#</span><span> </span><span>\"</span> \n</code></pre> \n \n</div> \n \n \n \n<p>Here's what's happening:</p> \n \n<ul> \n<li><p><code>pattern: ^#</code> says that any line starting with <code>#</code> belongs to the header. Those header lines are then passed through the pipeline of <code>metadata_operators</code>.</p></li> \n<li><p>The <a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/key_value_parser.md\">key_value_parser</a> operator splits each header line into a key and value using <code>:</code> as the separator, while <code>#</code> denotes the beginning of a new key/value pair.</p></li> \n</ul> \n \n<p>These results in the following attributes on every log entry that follows in that file:<br> \n</p> \n \n<div> \n<pre><code>Attributes: \n     -&gt; Job-ID: Str(job-d8e8fca2) \n     -&gt; Job-Type: Str(nightly-billing-run) \n     -&gt; Executed-By: Str(scheduler-prod-1) \n     -&gt; Records-To-Process: Str(1500) \n</code></pre> \n \n</div> \n \n \n \n<p>As you can see, the <code>Job-ID</code> and other header fields are now attached to the log record, providing invaluable context that would have otherwise been lost.</p> \n \n<p>From here, you can process them further by promoting the header fields to <a href=\"https://www.dash0.com/knowledge/what-are-opentelemetry-resources\">resource attributes</a> and aligning with <a href=\"https://www.dash0.com/knowledge/otel-semantic-conventions-explainer\">OpenTelemetry semantic conventions</a>.</p> \n \n<h2> \n   \n   \n  How to avoid lost or duplicate logs \n</h2> \n \n<p>When the Collector restarts, log ingestion can easily go wrong if state is not preserved as you risk either re-ingesting old data or skipping over new logs. If you use <code>start_at: beginning</code>, the receiver will reread all your log files and create massive duplication. With <code>start_at: end</code>, you might miss any entries written while the Collector was down.</p> \n \n<p>The way to solve this is with <strong>checkpointing</strong>. By configuring a storage extension, you instruct the <code>filelog</code> receiver to save its position in each file (the last read offset) to disk and pick up exactly where it left off.</p> \n \n<p>A conventional approach is using the <a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/extension/storage/filestorage\">file_storage extension</a> for this purpose:<br> \n</p> \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>extensions</span><span>:</span> \n  <span>file_storage</span><span>:</span> \n    <span>directory</span><span>:</span> <span>/var/otelcol/storage</span> \n \n<span>receivers</span><span>:</span> \n  <span>filelog</span><span>:</span> \n    <span>include</span><span>:</span> <span>[</span><span>/var/log/myapp/*.log</span><span>]</span> \n    <span>start_at</span><span>:</span> <span>beginning</span> \n    <span># Link the receiver to the storage extension</span> \n    <span>storage</span><span>:</span> <span>file_storage</span> \n \n<span># ... processors, exporters</span> \n \n<span>service</span><span>:</span> \n  <span># The extension must be enabled in the service section</span> \n  <span>extensions</span><span>:</span> <span>[</span><span>file_storage</span><span>]</span> \n  <span>pipelines</span><span>:</span> \n    <span>logs</span><span>:</span> \n      <span>receivers</span><span>:</span> <span>[</span><span>filelog</span><span>]</span> \n      <span># ...</span> \n</code></pre> \n \n</div> \n \n \n \n<p>With the <code>storage</code> extension enabled, the receiver will:</p> \n \n<ol> \n<li> On startup, check the <code>/var/otelcol/storage</code> directory for saved offsets.</li> \n<li> Resume reading from the saved offset for any file it was tracking, ensuring no data is lost or duplicated.</li> \n<li> Periodically update the storage with its latest progress.</li> \n</ol> \n \n<p>Checkpointing ensures that log collection is resilient to restarts, upgrades, and even crashes. It is a critical best practice for reliable log ingestion.</p> \n \n<h3> \n   \n   \n  Handling log delivery failures gracefully \n</h3> \n \n<p>Checkpointing with a storage extension protects you during Collector restarts, but another common failure mode is when the receiver reads a batch successfully but fails to hand it off to the next stage.</p> \n \n<p>This can happen if an exporter can't reach its endpoint, or the <a href=\"https://www.dash0.com/guides/opentelemetry-memory-limiter-processor\">memory limiter</a> is refusing data. By default, the receiver will drop that batch of logs and move on to the next, causing silent data loss.</p> \n \n<p>To prevent this, the receiver has a built-in mechanism to retry sending failed batches. When <code>retry_on_failure</code> is enabled, the receiver will pause, wait for a configured interval, and attempt to resend the exact same batch of logs. This process repeats with an <a href=\"https://en.wikipedia.org/wiki/Exponential_backoff\">exponential backoff</a> until the batch is sent successfully or the <code>max_elapsed_time</code> is reached:<br> \n</p> \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>receivers</span><span>:</span> \n  <span>filelog</span><span>:</span> \n    <span>retry_on_failure</span><span>:</span> \n      <span>enabled</span><span>:</span> <span>true</span> \n      <span># Wait 5 seconds after the first failure before the first retry.</span> \n      <span>initial_interval</span><span>:</span> <span>5s</span> \n      <span># The longest the receiver will wait between retries is 30 seconds.</span> \n      <span>max_interval</span><span>:</span> <span>30s</span> \n      <span># Give up trying to send a batch after 10 minutes.</span> \n      <span>max_elapsed_time</span><span>:</span> <span>10m</span> \n</code></pre> \n \n</div> \n \n \n \n<p>By combining checkpointing with a robust retry policy , you'll create a highly resilient log file ingestion pipeline that can withstand both Collector restarts and temporary downstream outages or throttling.</p> \n \n<h3> \n   \n   \n  Deleting log files after processing \n</h3> \n \n<p>Some workflows call for processing a file once and then removing it to save space and avoid reprocessing. You can enable this with <code>delete_after_read</code>, which requires <code>start_at: beginning</code>:<br> \n</p> \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>receivers</span><span>:</span> \n  <span>filelog</span><span>:</span> \n    <span>include</span><span>:</span> <span>[</span><span>/var/log/archives/*.gz</span><span>]</span> \n    <span>start_at</span><span>:</span> <span>beginning</span> \n    <span>delete_after_read</span><span>:</span> <span>true</span> \n</code></pre> \n \n</div> \n \n \n \n<p>You must need also enable the <code>filelog.allowFileDeletion</code> feature gate for this to work:<br> \n</p> \n \n<div> \n<pre><code><span># docker-compose.yml</span> \n<span>services</span><span>:</span> \n  <span>otelcol</span><span>:</span> \n    <span>command</span><span>:</span> \n      <span>[</span> \n        <span>--config=/etc/otelcol-contrib/config.yaml</span><span>,</span> \n        <span>--feature-gates=filelog.allowFileDeletion</span><span>,</span> \n      <span>]</span> \n</code></pre> \n \n</div> \n \n \n \n<p>Finally, ensure that the files are configured to be deletable and that the Collector service has enough permissions to delete the file. If permissions are insufficient, you will see a \"could not delete\" log record:<br> \n</p> \n \n<div> \n<pre><code>2025-10-08T06:42:03.973Z        error   reader/reader.go:278    could not delete        {\"resource\": {\"service.instance.id\": \"7c0daf0e-e625-4da8-9577-072606dce057\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.136.0\"}, \"otelcol.component.id\": \"filelog\", \"otelcol.component.kind\": \"receiver\", \"otelcol.signal\": \"logs\", \"component\": \"fileconsumer\", \"path\": \"/var/log/myapp/app.log\", \"filename\": \"/var/log/myapp/app.log\"} \n</code></pre> \n \n</div> \n \n \n \n<p>Just be careful when enabling this setting as it deletes the files from disk permanently.</p> \n \n<h2> \n   \n   \n  Handling log rotation seamlessly \n</h2> \n \n<p>Log files don't grow indefinitely. <a href=\"https://www.dash0.com/guides/log-rotation-linux-logrotate\">Eventually, they'll get rotated</a> (or at least they should). The <code>filelog</code> receiver is built to handle common rotation patterns, such as <code>app.log</code> to <code>app.log.1</code> automatically and without losing data.</p> \n \n<p>Instead of relying on filenames alone, the receiver tracks each file using a unique fingerprint derived from the first few kilobytes of content. When rotation occurs, it recognizes that the original file has been renamed, finishes reading it, and then starts fresh from the beginning of the new <code>app.log</code>.</p> \n \n<p>This behavior requires no additional configuration; it works out of the box, giving you reliable log ingestion even in environments with frequent rotations.</p> \n \n<h3> \n   \n   \n  Reading compressed files \n</h3> \n \n<p>Many log rotation tools compress old logs to save disk space, producing files like <code>access.log.1.gz</code>. The <code>filelog</code> receiver can handle these seamlessly by decompressing them on the fly.</p> \n \n<p>To make this work, you use the <code>compression</code> setting. This tells the receiver that some or all of the files it discovers may be compressed and need to be decompressed before parsing.</p> \n \n<p>You have two main choices for the <code>compression</code> setting:</p> \n \n<ul> \n<li> \n<code>gzip</code>: Treats all matched files as gzip-compressed.</li> \n<li> \n<code>auto</code>: Automatically detects compression based on file extension (currently <code>.gz</code>). This is the best option when a directory contains a mix of active, uncompressed logs and older, compressed ones.</li> \n</ul> \n \n<p>For example, if your directory has both <code>app.log</code> (active) and <code>app.log.1.gz</code> (rotated and compressed), you can configure the receiver like this:<br> \n</p> \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>receivers</span><span>:</span> \n  <span>filelog</span><span>:</span> \n    <span>include</span><span>:</span> <span>[</span><span>/var/log/myapp/*</span><span>]</span> \n    <span>start_at</span><span>:</span> <span>beginning</span> \n    <span># Automatically detect and decompress .gz files</span> \n    <span>compression</span><span>:</span> <span>auto</span> \n    <span>operators</span><span>:</span> \n      <span>-</span> <span>type</span><span>:</span> <span>regex_parser</span> \n        <span># ... your parsing rules</span> \n</code></pre> \n \n</div> \n \n \n \n<p>When working with compressed logs, there are two main things to keep in mind.</p> \n \n<p>First, the receiver assumes that compressed files can only grow by appending new data. If a file is completely rewritten, for example by taking the original content and recompressing it together with new lines, the receiver may not handle it correctly.</p> \n \n<p>Second, there's the question of fingerprinting. By default, the receiver identifies files based on their compressed bytes. This works fine in most cases, but if files are renamed or moved it can cause confusion. To make identification more reliable, you can enable the <code>filelog.decompressFingerprint</code> feature gate. With this enabled, the fingerprint is calculated from the decompressed content.<br> \n</p> \n \n<div> \n<pre><code><span># docker-compose.yml</span> \n<span>services</span><span>:</span> \n  <span>otelcol</span><span>:</span> \n    <span>command</span><span>:</span> \n      <span>[</span> \n        <span>--config=/etc/otelcol-contrib/config.yaml</span><span>,</span> \n        <span>--feature-gates=filelog.decompressFingerprint</span><span>,</span> \n      <span>]</span> \n</code></pre> \n \n</div> \n \n \n \n<p>One caution: if you turn this feature on in an existing setup, the fingerprints will change. That means compressed files that were already read may be ingested again.</p> \n \n<h2> \n   \n   \n  Performance tuning for high-volume environments \n</h2> \n \n<p>The OTel collector <code>filelog</code> receiver's default settings are optimized for general use, but in production environments with hundreds of log files or very high throughput, you'll likely need to tune its performance.</p> \n \n<p>By default, the receiver tries to read from every matched file at once. On a system producing thousands of files, this can hog the CPU and quickly hit file handle limits.</p> \n \n<p>The <code>max_concurrent_files</code> setting puts a cap on how many files are read at the same time. The default is <code>1024</code>, but lowering this can keep your system from getting overwhelmed.</p> \n \n<p>Another key setting is <code>poll_interval</code>, which controls how often the receiver checks for new files and new log lines. The default is 200ms which means logs show up almost immediately, but CPU use goes up because the filesystem is scanned more often.</p> \n \n<p>For less critical logs or resource-constrained environments, bumping this to <code>1s</code> or even 5s can be a good trade-off as it'll reduce the polling overhead with only a negligible impact on observability for most use cases.</p> \n \n<p>Finally, unusually large log entries are guarded against through the <code>max_log_size</code> setting. It defines the largest allowed log entry, so that anything bigger gets truncated. The default is 1MiB, which is a sensible default for most workloads.<br> \n</p> \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>receivers</span><span>:</span> \n  <span>filelog/k8s_pods</span><span>:</span> \n    <span>include</span><span>:</span> <span>[</span><span>/var/log/pods/*/*/*.log</span><span>]</span> \n    <span>max_concurrent_files</span><span>:</span> <span>200</span> \n    <span>poll_interval</span><span>:</span> <span>1s</span> \n    <span>max_log_size</span><span>:</span> <span>2MiB</span> \n</code></pre> \n \n</div> \n \n \n \n<h2> \n   \n   \n  Enforcing log file order \n</h2> \n \n<p>Most of the time, the order in which log files are ingested doesn't matter. But some systems produce logs as a series of sequential files where processing order is critical.</p> \n \n<p>By default, the <code>filelog</code> receiver reads all matching files concurrently, which means you could end up processing them out of sequence. The <code>ordering_criteria</code> setting solves this by enforcing a strict order when reading files.</p> \n \n<p>For example, given a set of log files with the following conventions:<br> \n</p> \n \n<div> \n<pre><code>batch-run-001.log \nbatch-run-002.log \nbatch-run-003.log \n</code></pre> \n \n</div> \n \n \n \n \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>receivers</span><span>:</span> \n  <span>filelog/batch_logs</span><span>:</span> \n    <span>include</span><span>:</span> <span>[</span><span>/var/log/batch-runs/batch-run-*.log</span><span>]</span> \n    <span>start_at</span><span>:</span> <span>beginning</span> \n    <span>ordering_criteria</span><span>:</span> \n      <span>top_n</span><span>:</span> <span>1</span> \n      <span># Extract the sequence number from the filename</span> \n      <span>regex</span><span>:</span> <span>batch-run-(?P&lt;seq_num&gt;\\d+)\\.log</span> \n      <span># Sort files by the sequence number as a number, not a string</span> \n      <span>sort_by</span><span>:</span> \n        <span>-</span> <span>regex_key</span><span>:</span> <span>seq_num</span> \n          <span>sort_type</span><span>:</span> <span>numeric</span> \n          <span>ascending</span><span>:</span> <span>true</span> \n</code></pre> \n \n</div> \n \n \n \n<p>With this setup, the receiver will discover all files matching <code>batch-run-*.log</code>, extract the sequence number from each filename, and sort the files numerically in ascending order by that sequence.</p> \n \n<p>The <code>top_n</code> property determines how many files will be tracked after applying the ordering criteria. With <code>top_n: 1</code>, only the first file (<code>batch-run-001.log</code>) will be tracked and ingested into the pipeline.</p> \n \n<h2> \n   \n   \n  Filelog receiver tips and best practices \n</h2> \n \n<p>When troubleshooting the <code>filelog</code> receiver, a few issues come up again and again. Here's how to diagnose and fix them quickly:</p> \n \n<h3> \n   \n   \n  Log files are not being watched \n</h3> \n \n<p>When the Collector starts watching a file for log entries, you'll see a log like this in its output:<br> \n</p> \n \n<div> \n<pre><code>2025-10-09T08:47:05.574Z        info    fileconsumer/file.go:261        Started watching file   {...} \n</code></pre> \n \n</div> \n \n \n \n<p>If you don't see this message, or if you see the log below, it means that the receiver hasn't picked up any files yet:<br> \n</p> \n \n<div> \n<pre><code>2025-10-09T09:25:20.280Z        warn    fileconsumer/file.go:49 finding files   {..., \"error\": \"no files match the configured criteria\"} \n</code></pre> \n \n</div> \n \n \n \n<p>Start by double-checking your <code>include</code>, <code>exclude</code>, and <code>exclude_older_than</code> settings to make sure your file patterns actually match the files you expect.</p> \n \n<p>Next, verify that the Collector process has permission to access both the files and their parent directories. Missing directory-level permissions are one of the most common reasons files aren't discovered or watched.</p> \n \n<h3> \n   \n   \n  Files are watched but no log lines are read \n</h3> \n \n<p>If you can see \"Started watching file\" messages but no logs are being collected, the most common cause is the <code>start_at</code> setting. By default, it's set to <code>end</code>, which tells the receiver to start reading only new lines appended after the Collector starts.</p> \n \n<p>When you're testing with an existing file that isn't actively being written to, this means nothing will appear. To read the entire file from the start, set <code>start_at</code> to <code>beginning</code>:<br> \n</p> \n \n<div> \n<pre><code><span># otelcol.yaml</span> \n<span>receivers</span><span>:</span> \n  <span>filelog</span><span>:</span> \n    <span>start_at</span><span>:</span> <span>beginning</span> \n</code></pre> \n \n</div> \n \n \n \n<p>This ensures the receiver processes all existing content the first time the file is discovered.</p> \n \n<h3> \n   \n   \n  Regular expression doesn't match log lines \n</h3> \n \n<p>If your logs aren't being parsed correctly, the issue is usually with your regular expression. When this happens, the Collector often logs an error like:<br> \n</p> \n \n<div> \n<pre><code>2025-10-09T09:32:14.949Z        error   helper/transformer.go:154       Failed to process entry {\"resource\": {\"service.instance.id\": \"f8ec2efd-16e9-44ad-9ed2-9f406e46719f\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.136.0\"}, \"otelcol.component.id\": \"filelog\", \"otelcol.component.kind\": \"receiver\", \"otelcol.signal\": \"logs\", \"operator_id\": \"regex_parser\", \"operator_type\": \"regex_parser\", \"error\": \"regex pattern does not match\", \"action\": \"send\", \"entry.timestamp\": \"0001-01-01T00:00:00.000Z\", \"log.file.name\": \"batch-run-001.log\"} \n</code></pre> \n \n</div> \n \n \n \n<p>Before adjusting your Collector config, test the regex outside of it using a tool like <a href=\"https://regex101.com/\">Regex101</a>. Make sure to select the <strong>Golang</strong> flavor so it behaves the same way as the Collector's regex engine.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmppgv21pumkd0xnc46h8.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmppgv21pumkd0xnc46h8.png\" alt=\"Regex101 being used to test OpenTelemetry Collector regular expression in Golang flavor\" width=\"800\" height=\"436\"></a></p> \n \n<p>If you're not seeing this error but your regex still isn't working, check whether the <a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/on_error.md\"><code>on_error</code> parameter</a> is set to one of the <code>_quiet</code> modes. Those values suppress operator errors unless the Collector log level is set to <code>DEBUG</code>.</p> \n \n<p>Common causes of regex mismatches include invisible spaces or tabs, missing anchors (<code>^</code> or <code>$</code>), incorrect escaping, or small format differences between your log and your pattern. Double-check these details before investigating further.</p> \n \n<h3> \n   \n   \n  Logs are duplicated after restart \n</h3> \n \n<p>If you notice duplicate logs appearing after the Collector restarts, it usually means the receiver isn't remembering where it left off. To fix this, enable a <code>storage</code> extension so the <code>filelog</code> receiver can checkpoint its position in each file.</p> \n \n<p>This allows the receiver to resume reading exactly where it stopped, preventing both data loss and duplication. Without it, the receiver will reread entire files from the start after every restart.</p> \n \n<h2> \n   \n   \n  Final thoughts \n</h2> \n \n<p>The <code>filelog</code> receiver in OpenTelemetry is an essential bridge between traditional file-based logging (often with unstructured data) and the world of modern, structured observability.</p> \n \n<p>By mastering its core concepts of discovery, parsing with operators, and checkpointing, you can build a reliable log ingestion pipeline for any service that writes its logs to a file.</p> \n \n<p>Once you've transformed your raw text logs into well-structured OpenTelemetry data, the full observability ecosystem opens up. You can enrich, filter, and route them to any backend that speaks <a href=\"https://www.dash0.com/knowledge/opentelemetry-protocol-otlp\">OTLP</a>.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fi6og47d2te9vq6goemzj.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fi6og47d2te9vq6goemzj.png\" alt=\"Sending log data to Dash0\" width=\"800\" height=\"456\"></a></p> \n \n<p>For a faster path from collecting telemetry to insight, consider using <a href=\"https://www.dash0.com/\">Dash0</a>, an observability platform purpose-built for OpenTelemetry data. <a href=\"https://www.dash0.com/sign-up\">Try it out today with a free 14-day trial</a>.</p>",
      "summary": "Even in the age of cloud-native apps and distributed tracing, plain old log files remain one of the richest sources of truth in any system. From legacy business applications and batch jobs to NGINX, databases, and on-prem infrastructure, critical diagnostics still end up written to disk. \n \nThe OpenTelemetry Collector filelog receiver gives you a way to bring those logs into a modern observability pipeline. It continuously tails files, parses their contents, and converts raw text into structured",
      "publishedAt": "2025-12-02T13:38:46.000Z",
      "author": "Ayooluwa Isaiah",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 18.4401864205762,
      "ingestedAt": "2025-12-02T14:44:03.174Z",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "ide",
        "testing",
        "observability",
        "governance",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ad6693",
      "title": "Breaking the doom-prompting loop with spec-driven development",
      "url": "https://dev.to/vburckhardt/breaking-the-doom-prompting-loop-with-spec-driven-development-19hj",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fuobjyis03xrl2taq602t.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p><em>Bringing software engineering discipline to AI-assisted coding</em></p>  \n  \n<p>Every developer using AI coding tools has experienced the loop. You prompt, the AI generates code, something isn't quite right, you prompt again, the AI breaks something else while fixing the first issue, you prompt again. An hour later you're deeper in the hole than when you started, caught in what's now called <a href=\"https://www.cio.com/article/4056586/doomprompting-endless-tinkering-with-ai-outputs-can-cripple-it-results.html\">doomprompting</a>: you keep going because you've already invested so much time.  </p>  \n  \n<p>This is the dark side of <a href=\"https://x.com/karpathy/status/1886192184808149383\">vibe coding</a>, Andrej Karpathy's term for fully surrendering to AI-generated code without really understanding it. Karpathy himself noted it's \"not too bad for throwaway weekend projects.\" For anything more substantial, the approach tends to collapse.  </p>  \n  \n<p>I've been using <a href=\"https://github.com/github/spec-kit\">spec-kit</a>, GitHub's toolkit for spec-driven development, and it's changed how I think about AI-assisted coding. The core insight is simple: catching problems in specifications costs far less than catching them in code.</p>  \n  \n<h2>  \n    \n    \n  The shift-left principle applied to AI coding  \n</h2>  \n  \n<p>Shift-left testing is the idea that catching defects earlier in development is cheaper than catching them later. Everyone who's debugged a production issue knows this intuitively: finding a problem in requirements costs almost nothing, finding it in code review costs some rework, finding it in production costs a lot more.  </p>  \n  \n<p>Spec-kit applies this principle to AI-assisted development, but shifts even further left. Instead of catching issues through testing code, you catch them through reviewing specifications. The four-phase workflow makes this explicit: Specify, Plan, Tasks, then Implement. Each phase has a gate where you review before proceeding.  </p>  \n  \n<p>This feels familiar to anyone who studied software engineering formally. I remember university projects where we spent weeks on specifications and architecture before writing a line of code. The discipline felt excessive at the time, but the coding phase was remarkably smooth when we finally got there. Spec-kit brings that same rigor to AI-assisted development.</p>  \n  \n<h2>  \n    \n    \n  <strong>What spec-kit actually provides</strong>  \n</h2>  \n  \n<p>The toolkit is agent-agnostic and works with Claude Code, GitHub Copilot, Cursor, and other AI coding tools. At its core, it's a set of slash commands that guide you through structured phases:  </p>  \n  \n<p>The <code>/specify</code> command forces you to articulate what you're building. The <code>/plan</code> command generates research and technical direction. The <code>/tasks</code> command breaks the plan into discrete implementation steps. Finally, <code>/implement</code> executes those tasks.  </p>  \n  \n<p>Each phase produces markdown files that serve as both documentation and AI context. The specifications, plans, and task lists persist across sessions, acting as memory that keeps the AI aligned with your intent.  </p>  \n  \n<p>Spec-kit also introduces what it calls a \"constitution\" (I prefer \"principles,\" but the concept matters more than the name). This file establishes cross-cutting rules for your project: testing approach, coding standards, architectural constraints. These non-functional requirements apply to everything the AI generates.</p>  \n  \n<h2>  \n    \n    \n  How the flow changes day-to-day work  \n</h2>  \n  \n<p>My workflow with spec-kit looks different from the typical AI coding loop. I spend time reviewing and editing the specifications and task list, then let the AI implement the full feature. I treat the AI less like a pair programmer and more like a developer I'm delegating work to. I review the resulting code the way I'd review a pull request from a human team member.  </p>  \n  \n<p>This mental model matters. With pair programming, you're watching every keystroke. With delegation, you're reviewing outcomes against specifications. The latter scales better with AI tools that can implement substantial features autonomously.  </p>  \n  \n<p>The plan phase has become the most valuable. The AI performs research on the technical direction, and I've learned things from this process. More importantly, I catch misunderstandings early. During one project, the plan revealed the AI assumed an IBM Cloud serverless service was deployed on a VPC, which is incorrect. Catching that during plan review was far cheaper than discovering it through broken infrastructure code.  </p>  \n  \n<p>I don't review every single code change anymore. Instead, I review the specifications carefully, let implementation run with auto-accept enabled, do smoke testing, then review the full changeset. If issues emerge, I iterate through the full flow (plan to tasks to implementation) rather than jumping straight to code fixes. This keeps the specifications accurate and aligned with what actually got built.</p>  \n  \n<h2>  \n    \n    \n  The overhead question  \n</h2>  \n  \n<p>Spec-kit adds overhead. For simple tasks, that overhead isn't worth it.  </p>  \n  \n<p>But for larger features, I've found the investment pays back. The specifications force me to think through requirements properly. Architectural problems surface during plan review rather than after I've invested in code. And I avoid the doom-prompting loop because ambiguities in my thinking get resolved during specification, not through trial-and-error prompting.  </p>  \n  \n<p>This parallels traditional development. Some developers code first and spend months fixing bugs and refactoring. Others invest in architecture and specifications upfront. Both approaches can work, but they have different risk profiles. For complex work, the methodical approach tends to win. The same applies to AI-assisted development.  </p>  \n  \n<p>The token usage goes up when using spec-kit. You're generating specifications, plans, and task lists before writing code. But these tokens typically pay for themselves by avoiding the doom-prompting loop where you might burn through tokens endlessly without making progress.</p>  \n  \n<h2>  \n    \n    \n  Prompt-based flows versus coded pipelines  \n</h2>  \n  \n<p>One aspect of spec-kit's design surprised me. My initial instinct would have been to implement most of the workflow in a traditional programming language with explicit control flow. Instead, spec-kit encapsulates the flow in detailed prompts with minimal supporting scripts.  </p>  \n  \n<p>This approach works well with frontier models. The prompts describe phases in natural language, and the AI follows them reliably. The templating approach with gates provides deterministic outcomes without requiring coded orchestration nodes like you'd find in LangGraph.  </p>  \n  \n<p>I suspect this approach would be less reliable with non-frontier models. The ability to follow complex, multi-phase instructions consistently requires the kind of instruction-following that frontier models do well.  </p>  \n  \n<p>Beyond the underlying model, I've noticed the tools available in each AI assistant matter. The plan phase benefits from web search, codebase search, and other research capabilities. Claude Code includes these out of the box, including deep search for thorough research. Other AI assistants may lack some of these capabilities, and I've seen the most variance in plan quality when research tools are limited.  </p>  \n  \n<p>Configuring <a href=\"https://modelcontextprotocol.io/\">MCP</a> tools before running through the flow also improves results. For instance, I configure tools for Terraform module registry search and cloud provider documentation lookup. These help the AI generate better-informed plans.</p>  \n  \n<h2>  \n    \n    \n  Adapting for infrastructure as code  \n</h2>  \n  \n<p>When I started using spec-kit, I thought it would apply directly to infrastructure as code. As I progressed, I realized IaC has specific characteristics that need different handling: the declarative nature of tools like Terraform, the need to separate cloud-agnostic requirements from provider-specific implementations, governance concerns around security and cost that differ from application code, and validation against actual cloud provider APIs and module registries.  </p>  \n  \n<p>I ended up creating <a href=\"https://github.com/IBM/iac-spec-kit\">iac-spec-kit</a> and open-sourced it to get more collaboration on the approach. It started as a fork, but ended up as a complete reimplementation of the commands, instructions, and templates. The only common layer is around the installer and the overall approach. The templates and prompts needed to be tuned specifically for infrastructure concerns.  </p>  \n  \n<p>The goal is to fill a gap where users can start with a high-level requirement like \"deploy WordPress\" or \"set up a three-tier web app\" and have the AI guide them through specification, planning, and code generation with review gates at each phase. The toolkit is cloud-agnostic and works with AWS, Azure, GCP, IBM Cloud, and others. Early tests look promising. I documented one end-to-end example at <a href=\"https://github.com/vburckhardt/wordpress-ibm-cloud\">vburckhardt/wordpress-ibm-cloud</a>, which shows the full workflow from initial requirements through generated Terraform code.  </p>  \n  \n<p>A specific focus has been getting AI to compose higher-level Terraform modules rather than using lower-level providers directly. AI-generated code that glues together curated, supported modules is more maintainable and supportable than code that reinvents infrastructure patterns using primitives. It's similar to teaching AI to use a well-designed library instead of writing everything from scratch.</p>  \n  \n<h2>  \n    \n    \n  What this enables  \n</h2>  \n  \n<p>Spec-kit enables going beyond vibe coding. The structured flow feels right because it aligns with how sound engineering should work. You're not just prompting and hoping. You're defining intent, reviewing plans, and delegating implementation.  </p>  \n  \n<p>The specifications also work well for collaboration. They're markdown files that can be checked into source control and versioned. I can see workflows where teams have validation gates on specifications and plans before implementation begins. The artifacts serve as shared understanding, not just AI context.  </p>  \n  \n<p>For resuming sessions, the specification and task files act as memory. Instead of re-explaining context to the AI, the toolkit instructs it to load the existing artifacts. This makes long-running projects more manageable.  </p>  \n  \n<p>The structured flow also enables working on multiple features in parallel. While the AI implements one feature autonomously, I can work on specifications for the next one. This pattern is emerging more broadly with tools like <a href=\"https://openai.com/index/introducing-codex/\">OpenAI Codex</a> that explicitly support parallel task execution. I expect this to become more common. The implications cut both ways: it lets independent developers and small startups move faster with limited headcount, but it also raises questions about expectations placed on developers in corporate settings.  </p>  \n  \n<p>The flow does require discipline. It's tempting to skip straight to implementation when you think you know what you want. But ambiguity in your thinking becomes apparent when you try to write it down as a specification. That's the point. The specification phase forces clarity before you've invested in code.</p>  \n  \n<h2>  \n    \n    \n  <strong>When it's worth it</strong>  \n</h2>  \n  \n<p>Spec-kit won't eliminate all the friction from AI-assisted development. The overhead is real, and it's not worth it for every task. But for substantial features where you'd otherwise end up in a doom-prompting loop, the structured approach catches problems when they're cheap to fix.  </p>  \n  \n<p>The shift-left principle applies: review specifications, not just code. Treat AI implementation as delegation, not pair programming. Invest in the plan phase when research can improve technical direction.  </p>  \n  \n<p>If you're frustrated with vibe coding results on anything beyond weekend projects, spec-driven development is worth trying. The discipline feels familiar to anyone who's done rigorous software engineering, and the payoff is similar: smoother implementation because the thinking happened upfront.</p>",
      "summary": "Bringing software engineering discipline to AI-assisted coding  \n  \nEvery developer using AI coding tools has experienced the loop. You prompt, the AI generates code, something isn't quite right, you prompt again, the AI breaks something else while fixing the first issue, you prompt again. An hour later you're deeper in the hole than when you started, caught in what's now called doomprompting: you keep going because you've already invested so much time.    \n  \nThis is the dark side of vibe codin",
      "publishedAt": "2025-12-02T13:38:26.000Z",
      "author": "Vincent Burckhardt",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 17.443131173086773,
      "ingestedAt": "2025-12-02T14:44:03.174Z",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "testing",
        "governance",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ad6695",
      "title": "&#128293; 10+ End-to-End DevOps Projects &mdash; Docker, AWS, Jenkins, Terraform, Ansible & More",
      "url": "https://dev.to/ritesh355/10-end-to-end-devops-projects-docker-aws-jenkins-terraform-ansible-more-9ka",
      "content": "<h1> \n   \n   \n  🚀 My Complete DevOps Projects Collection – From Beginner to Advanced (All in One Place) \n</h1> \n \n<p>Over the last few months, I’ve been learning DevOps deeply and building real-world projects using tools like <strong>Docker, Jenkins, AWS, GitHub Actions, Ansible, Terraform, Lambda, MongoDB, CloudFront, Prometheus, Grafana, and more</strong>.</p> \n \n<p>To organize everything, I created a single repository called <strong>DevOps_Projects</strong>, where I have added <strong>all my DevOps, Cloud, and CI/CD projects</strong>, each inside its own folder.</p> \n \n<p>This blog is a complete guide to my journey — what I built, what tools I used, and what I learned from each project.</p> \n \n<p>Whether you're a beginner or someone preparing for DevOps interviews, this collection will help you understand real DevOps concepts with practical implementation.</p> \n \n \n \n \n<h1> \n   \n   \n  🎯 Why I Created This Repository \n</h1> \n \n<p>As a fresher, one thing became clear:</p> \n \n<blockquote> \n<p><strong>Companies don’t want just theoretical knowledge — they want proof of hands-on experience.</strong></p> \n</blockquote> \n \n<p>So instead of building random mini projects, I decided to learn DevOps properly and build <strong>real-world, practical, production-like projects</strong>.</p> \n \n<p>This repository is the outcome of that journey.<br><br> \nAnd I will keep adding more advanced projects as I grow.</p> \n \n \n \n \n<h1> \n   \n   \n  📂 What’s Inside This Repository? \n</h1> \n \n<p>Each project has its <strong>own folder</strong>, containing:</p> \n \n<ul> \n<li>Complete project code \n</li> \n<li>Dockerfiles \n</li> \n<li>CI/CD workflows \n</li> \n<li>Configuration files \n</li> \n<li>Project-specific README \n</li> \n<li>Architecture diagrams (for some projects)</li> \n</ul> \n \n<p>Below is the complete list of projects included so far:</p> \n \n \n \n \n<h1> \n   \n   \n  🧩 Project Overview \n</h1> \n \n<h2> \n   \n   \n  <strong>📁 PROJECT-1 — Dockerized Flask App</strong> <a href=\"https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-1\">PROJECT-1</a> \n</h2> \n \n<p>A beginner-friendly app containerized using Docker.</p> \n \n<p><strong>What I learned:</strong></p> \n \n<ul> \n<li>Creating Dockerfiles \n</li> \n<li>Building/running containers \n</li> \n<li>Exposing ports \n</li> \n</ul> \n \n \n \n \n<h2> \n   \n   \n  <strong>📁 PROJECT-2 — Node.js + MongoDB App with Docker Compose</strong> <a href=\"https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-2\">PROJECT-2</a> \n</h2> \n \n<p>A multi-container app using Docker Compose.</p> \n \n<p><strong>Key Learnings:</strong></p> \n \n<ul> \n<li>Container networking \n</li> \n<li>Volumes \n</li> \n<li>Service orchestration \n</li> \n</ul> \n \n \n \n \n<h2> \n   \n   \n  <strong>📁 PROJECT-3 — Jenkins CI/CD Pipeline with Docker &amp; Node.js</strong> <a href=\"https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-3\">PROJECT-3</a> \n</h2> \n \n<p>Automated CI/CD workflow using Jenkins.</p> \n \n<p><strong>Key Learnings:</strong></p> \n \n<ul> \n<li>Jenkinsfile stages \n</li> \n<li>Automated build → test → deploy \n</li> \n<li>Docker integration \n</li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fn12z0wdkxsuc543350ql.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fn12z0wdkxsuc543350ql.png\" alt=\"\" width=\"800\" height=\"483\"></a></p> \n \n \n \n \n<h2> \n   \n   \n  <strong>📁 PROJECT-4 — AWS CI/CD Pipeline (Node.js → EC2)</strong> <a href=\"https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-4\">PROJECT-4</a> \n</h2> \n \n<p>A production-ready AWS CI/CD pipeline.</p> \n \n<p><strong>AWS Services Used:</strong></p> \n \n<ul> \n<li>CodePipeline \n</li> \n<li>CodeBuild \n</li> \n<li>CodeDeploy \n</li> \n<li>EC2 \n</li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fx71aw4ege5io5k1f0k5y.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fx71aw4ege5io5k1f0k5y.png\" alt=\"\" width=\"800\" height=\"273\"></a></p> \n \n \n \n \n<h2> \n   \n   \n  <strong>📁 PROJECT-5 — CI/CD Pipeline for Web App</strong> <a href=\"https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-5\">PROJECT-5</a> \n</h2> \n \n<p>A simple but complete CI/CD pipeline demonstrating core DevOps concepts.</p> \n \n<h2> \n   \n   \n  <img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzarauiw2nn2a9jgahequ.png\" alt=\"\" width=\"800\" height=\"366\"> \n</h2> \n \n<h2> \n   \n   \n  <strong>📁 PROJECT-6 — Static Website Hosting (S3 + CloudFront + CI/CD)</strong><a href=\"https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-6\">PROJECT-6</a> \n</h2> \n \n<p>A global static website hosted on AWS.</p> \n \n<p><strong>What I learned:</strong></p> \n \n<ul> \n<li>Hosting using S3 \n</li> \n<li>CDN with CloudFront \n</li> \n<li>Automated deployment using GitHub Actions \n</li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fm7jjr7155w6ac7o2xim9.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fm7jjr7155w6ac7o2xim9.png\" alt=\"\" width=\"800\" height=\"415\"></a></p> \n \n \n \n \n<h2> \n   \n   \n  <strong>📁 PROJECT-7 — Flask + AWS DynamoDB CRUD App</strong> <a href=\"https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-7\">PROJECT-7</a> \n</h2> \n \n<p>A cloud backend performing CRUD operations on DynamoDB.<br> \n<strong>Key Learnings:</strong></p> \n \n<ul> \n<li>Boto3 integration \n</li> \n<li>NoSQL CRUD \n</li> \n<li>Deploying Flask + AWS SDK inside Docker </li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F14jeurn6jmrsnyfbawfx.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F14jeurn6jmrsnyfbawfx.png\" alt=\"\" width=\"800\" height=\"348\"></a> </p> \n \n \n \n \n<h2> \n   \n   \n  <strong>📁 PROJECT-8 — Serverless CI/CD Pipeline</strong> <a href=\"https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-8\">PROJECT-8</a> \n</h2> \n \n<p>A complete serverless pipeline using AWS.<br> \n<strong>AWS Services Used:</strong></p> \n \n<ul> \n<li>Lambda \n</li> \n<li>API Gateway \n</li> \n<li>S3 \n</li> \n<li>CodePipeline \n</li> \n<li>CodeBuild \n</li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F3sjkwdkmvus1vlnt87md.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F3sjkwdkmvus1vlnt87md.png\" alt=\"\" width=\"800\" height=\"388\"></a></p> \n \n \n \n \n<h2> \n   \n   \n  <strong>📁 PROJECT-9 — Full-Stack DevOps Automation</strong><a href=\"https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-9\">PROJECT-9</a> \n</h2> \n \n<p>A large-scale app combining frontend, backend, cloud, monitoring, and CI/CD.</p> \n \n<p><strong>Tech Used:</strong></p> \n \n<ul> \n<li>Next.js \n</li> \n<li>Docker \n</li> \n<li>GitHub Actions \n</li> \n<li>AWS \n</li> \n<li>Prometheus &amp; Grafana \n</li> \n<li>Route 53 \n</li> \n</ul> \n \n<p><strong>Key Learnings:</strong></p> \n \n<ul> \n<li>Observability \n</li> \n<li>Automation workflows \n</li> \n<li>Production-like infrastructure </li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjbhg2bsphkk2mxdrooho.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjbhg2bsphkk2mxdrooho.png\" alt=\"\" width=\"800\" height=\"446\"></a></p> \n \n \n \n \n<h2> \n   \n   \n  <strong>📁 PROJECT-10 — DevSecOps Pipeline</strong><a href=\"https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-10\">PROJECT-10</a> \n</h2> \n \n<p>A complete DevSecOps pipeline integrating CI/CD with security checks.</p> \n \n<p><strong>Tools Used:</strong></p> \n \n<ul> \n<li>Jenkins \n</li> \n<li>Terraform \n</li> \n<li>Ansible (with Ansible Vault) \n</li> \n<li>Docker \n</li> \n<li>Trivy \n</li> \n<li>AWS \n</li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fc33372mif8u69ml9wpwa.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fc33372mif8u69ml9wpwa.png\" alt=\"\" width=\"800\" height=\"514\"></a></p> \n \n<p><strong>Key Learnings:</strong></p> \n \n<ul> \n<li>Infrastructure provisioning with Terraform \n</li> \n<li>Secure configuration management \n</li> \n<li>Vulnerability scanning \n</li> \n<li>Secure CI/CD pipelines \n</li> \n</ul> \n \n \n \n \n<h1> \n   \n   \n  🛠 Tech Stack Covered (So Far) \n</h1> \n \n<h3> \n   \n   \n  <strong>CI/CD</strong> \n</h3> \n \n<ul> \n<li>Jenkins \n</li> \n<li>GitHub Actions \n</li> \n<li>AWS Developer Tools \n</li> \n</ul> \n \n<h3> \n   \n   \n  <strong>Containerization</strong> \n</h3> \n \n<ul> \n<li>Docker \n</li> \n<li>Docker Compose \n</li> \n</ul> \n \n<h3> \n   \n   \n  <strong>Cloud</strong> \n</h3> \n \n<ul> \n<li>AWS (EC2, Lambda, S3, CloudFront, Route53, DynamoDB)</li> \n</ul> \n \n<h3> \n   \n   \n  <strong>IaC &amp; Automation</strong> \n</h3> \n \n<ul> \n<li>Terraform \n</li> \n<li>Ansible \n</li> \n</ul> \n \n<h3> \n   \n   \n  <strong>Security</strong> \n</h3> \n \n<ul> \n<li>Trivy \n</li> \n<li>IAM \n</li> \n</ul> \n \n<h3> \n   \n   \n  <strong>Monitoring</strong> \n</h3> \n \n<ul> \n<li>Prometheus \n</li> \n<li>Grafana \n</li> \n</ul> \n \n \n \n \n<h1> \n   \n   \n  🔥 What’s Coming Next? \n</h1> \n \n<p>I will continue expanding this repository with:</p> \n \n<ul> \n<li>Kubernetes Projects \n</li> \n<li>EKS Deployment \n</li> \n<li>Helm Charts \n</li> \n<li>Multi-Region Terraform Architecture \n</li> \n<li>Microservices with Lambda + API Gateway \n</li> \n<li>ELK / Loki Log Stack \n</li> \n<li>GitOps with ArgoCD \n</li> \n</ul> \n \n \n \n \n<h1> \n   \n   \n  🌟 Final Words \n</h1> \n \n<p>This repository reflects my journey from beginner to someone who can design <strong>real-world DevOps systems</strong>.</p> \n \n<p>If you're also learning DevOps, this repo can help you understand:</p> \n \n<ul> \n<li>Real CI/CD pipelines \n</li> \n<li>Docker in production \n</li> \n<li>AWS workflow integration \n</li> \n<li>How DevOps engineers think and solve problems \n</li> \n</ul> \n \n<p>If you find it helpful, feel free to ⭐ star the repository — it motivates me to build more.</p> \n \n \n \n \n<h2> \n   \n   \n  👨‍💻 Author \n</h2> \n \n<p><strong>Ritesh Singh</strong></p> \n \n<p>🌐 <a href=\"https://www.linkedin.com/in/ritesh-singh-092b84340/\">LinkedIn</a> </p> \n \n<p>📝 <a href=\"https://ritesh-devops.hashnode.dev/\">Hashnode</a> </p> \n \n<p>💻<a href=\"https://github.com/ritesh355/\">GitHub</a></p> \n \n<p>🌐 <a href=\"https://dev.to/ritesh355\">LinkedIn</a></p>",
      "summary": " \n   \n   \n  🚀 My Complete DevOps Projects Collection – From Beginner to Advanced (All in One Place) \n \n \nOver the last few months, I’ve been learning DevOps deeply and building real-world projects using tools like Docker, Jenkins, AWS, GitHub Actions, Ansible, Terraform, Lambda, MongoDB, CloudFront, Prometheus, Grafana, and more. \n \nTo organize everything, I created a single repository called DevOps_Projects, where I have added all my DevOps, Cloud, and CI/CD projects, each inside its own folde",
      "publishedAt": "2025-12-02T13:25:27.000Z",
      "author": "Ritesh Singh",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "security_incident",
      "score": 9.463032057929274,
      "ingestedAt": "2025-12-02T14:44:03.174Z",
      "tags": [
        "code_review",
        "ide",
        "observability",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ad66c8",
      "title": "Python Data Science Handbook",
      "url": "https://jakevdp.github.io/PythonDataScienceHandbook/",
      "content": "<p>Article URL: <a href=\"https://jakevdp.github.io/PythonDataScienceHandbook/\">https://jakevdp.github.io/PythonDataScienceHandbook/</a></p> \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46120611\">https://news.ycombinator.com/item?id=46120611</a></p> \n<p>Points: 9</p> \n<p># Comments: 1</p>",
      "summary": "Article URL: https://jakevdp.github.io/PythonDataScienceHandbook/ \nComments URL: https://news.ycombinator.com/item?id=46120611 \nPoints: 9 \n# Comments: 1",
      "publishedAt": "2025-12-02T12:38:28.000Z",
      "author": "cl3misch",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4906848306456015,
      "ingestedAt": "2025-12-02T14:44:03.174Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ad160d",
      "title": "At Anthropic, we believe that AI can increase nonprofit capacity. And we've worked with over 100 organizations so far on getting it right - Fortune",
      "url": "https://news.google.com/rss/articles/CBMigwFBVV95cUxNSU92MkJQS2NHT1NoOTM3S1ExeF84M013cndhdHRuNm1FT3diaWFVQUhEd3JYcEk5VlVlbzhhN1QtNFRDRkVLUXJuQkktaTJlQ1hhR25Xb2IyWVY5bUpRVFRSWjJybk82bEdXM3ctRVA1cWk4NEpjd2J1UllWSUZqODBFVQ?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMigwFBVV95cUxNSU92MkJQS2NHT1NoOTM3S1ExeF84M013cndhdHRuNm1FT3diaWFVQUhEd3JYcEk5VlVlbzhhN1QtNFRDRkVLUXJuQkktaTJlQ1hhR25Xb2IyWVY5bUpRVFRSWjJybk82bEdXM3ctRVA1cWk4NEpjd2J1UllWSUZqODBFVQ?oc=5\">At Anthropic, we believe that AI can increase nonprofit capacity. And we've worked with over 100 organizations so far on getting it right</a>  Fortune",
      "summary": "At Anthropic, we believe that AI can increase nonprofit capacity. And we've worked with over 100 organizations so far on getting it right  Fortune",
      "publishedAt": "2025-12-02T14:00:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.49017749547529,
      "ingestedAt": "2025-12-02T14:44:03.175Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ad160f",
      "title": "Blackbaud Partners with Anthropic on Groundbreaking New Way to Experience the Power of Purpose-Built AI - PR Newswire",
      "url": "https://news.google.com/rss/articles/CBMi8AFBVV95cUxOTWZ6NUF0YTBnRDBUZW1iLUdHWlhCcmhYdVRRY0xRZVZWZ0JJUnl3d01LM0lwRlowbXlKc2dEbWxCcTJ0cDFYbndteTIyYmZaSWc5T1ZQUlFZWlpTTmo1Ump6RUJCZ1g5cmRNSk56eFlsd01VMmp1R05FYkY0ZHdBLUJYOERSd1NmYjZwVVZJR3dRdVVLSWlmc1VQZVZyQjFfaFRaRm04dE5nQlhmRXk4QVhTMzFBRF9BMHBKQzMyYjI1NUU0eUNoSzVkT1pBLTdJdm5OSGFXdUUwRUlNcmZ5RnBlakc0b1lVYTNvbUF1a20?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMi8AFBVV95cUxOTWZ6NUF0YTBnRDBUZW1iLUdHWlhCcmhYdVRRY0xRZVZWZ0JJUnl3d01LM0lwRlowbXlKc2dEbWxCcTJ0cDFYbndteTIyYmZaSWc5T1ZQUlFZWlpTTmo1Ump6RUJCZ1g5cmRNSk56eFlsd01VMmp1R05FYkY0ZHdBLUJYOERSd1NmYjZwVVZJR3dRdVVLSWlmc1VQZVZyQjFfaFRaRm04dE5nQlhmRXk4QVhTMzFBRF9BMHBKQzMyYjI1NUU0eUNoSzVkT1pBLTdJdm5OSGFXdUUwRUlNcmZ5RnBlakc0b1lVYTNvbUF1a20?oc=5\">Blackbaud Partners with Anthropic on Groundbreaking New Way to Experience the Power of Purpose-Built AI</a>  PR Newswire",
      "summary": "Blackbaud Partners with Anthropic on Groundbreaking New Way to Experience the Power of Purpose-Built AI  PR Newswire",
      "publishedAt": "2025-12-02T14:00:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.49017749547529,
      "ingestedAt": "2025-12-02T14:44:03.175Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ad1610",
      "title": "Exclusive: Researchers trick Claude plug-in into deploying ransomware - Axios",
      "url": "https://news.google.com/rss/articles/CBMihwFBVV95cUxNcVFCdEpSZklaMFJoR2dZR25EeVh3WTZXS3ZwcTFiaDQwZ1NIeU9LcmFrRlJYOVFBWXlVTk9uSDYtVllaeWU1SFJyLXdyX1B4dFFybF9mQUIzcG94QzhSeDZ1R2ZJbmFSTkxZVTBCN1Y1OUlQbFp3cHlGQnpjamxZRTUxNXlvbEE?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMihwFBVV95cUxNcVFCdEpSZklaMFJoR2dZR25EeVh3WTZXS3ZwcTFiaDQwZ1NIeU9LcmFrRlJYOVFBWXlVTk9uSDYtVllaeWU1SFJyLXdyX1B4dFFybF9mQUIzcG94QzhSeDZ1R2ZJbmFSTkxZVTBCN1Y1OUlQbFp3cHlGQnpjamxZRTUxNXlvbEE?oc=5\">Exclusive: Researchers trick Claude plug-in into deploying ransomware</a>  Axios",
      "summary": "Exclusive: Researchers trick Claude plug-in into deploying ransomware  Axios",
      "publishedAt": "2025-12-02T13:31:59.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4946472532261084,
      "ingestedAt": "2025-12-02T14:44:03.175Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ad1611",
      "title": "AI Cash Ignites a Boom for Multimillion-Dollar San Francisco Homes - Bloomberg.com",
      "url": "https://news.google.com/rss/articles/CBMixAFBVV95cUxOQ1E4TXBpVVA4dUI0YUlpNmdubTE4SzFZeWVpemJjd09ERGVwMU56SE9sMWtJeGEyUlJzTW5qMXNNQnNIOTdRQm9DOXU5T2czZFJ5bFpRQk4wWE5LOXZZd1FBSFU2OW9Ud21fTlpvNFRlTTBha0ZaN29sRk5DU25FMTVSN0F0ZFh1enpGTzBZanpSQzFuZ0NoUjZpOUJaTmt6ZFlfbzNUM2hJVzl3RW1MSXVMVjBJOVRwWEthemxQYXVvVFhx?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMixAFBVV95cUxOQ1E4TXBpVVA4dUI0YUlpNmdubTE4SzFZeWVpemJjd09ERGVwMU56SE9sMWtJeGEyUlJzTW5qMXNNQnNIOTdRQm9DOXU5T2czZFJ5bFpRQk4wWE5LOXZZd1FBSFU2OW9Ud21fTlpvNFRlTTBha0ZaN29sRk5DU25FMTVSN0F0ZFh1enpGTzBZanpSQzFuZ0NoUjZpOUJaTmt6ZFlfbzNUM2hJVzl3RW1MSXVMVjBJOVRwWEthemxQYXVvVFhx?oc=5\">AI Cash Ignites a Boom for Multimillion-Dollar San Francisco Homes</a>  Bloomberg.com",
      "summary": "AI Cash Ignites a Boom for Multimillion-Dollar San Francisco Homes  Bloomberg.com",
      "publishedAt": "2025-12-02T13:15:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.480165955601912,
      "ingestedAt": "2025-12-02T14:44:03.175Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ad1612",
      "title": "🇺🇸 SAM ALTMAN JUST HIT “CODE RED” - AND BIG TECH FINALLY SMELLS BLOOD OpenAI just threw the emergency lever. Sam Altman - the man who spent the last 2 years looking untouchable - told his staff this week that the AI crown is no longer guaran - x.com",
      "url": "https://news.google.com/rss/articles/CBMiYkFVX3lxTE13bjBjREtnbElSb2pDb2JrcHlmQzJWbG1xUTJWc1dpamR5cXN5cnV3LTNYVTJwSl9tWHBkNDRObmFCSG9ENkstTzkwUlRpX3JPdlExdVJRaFhhTS12RGFMWUVR?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMiYkFVX3lxTE13bjBjREtnbElSb2pDb2JrcHlmQzJWbG1xUTJWc1dpamR5cXN5cnV3LTNYVTJwSl9tWHBkNDRObmFCSG9ENkstTzkwUlRpX3JPdlExdVJRaFhhTS12RGFMWUVR?oc=5\">🇺🇸 SAM ALTMAN JUST HIT “CODE RED” - AND BIG TECH FINALLY SMELLS BLOOD OpenAI just threw the emergency lever. Sam Altman - the man who spent the last 2 years looking untouchable - told his staff this week that the AI crown is no longer guaran</a>  x.com",
      "summary": "🇺🇸 SAM ALTMAN JUST HIT “CODE RED” - AND BIG TECH FINALLY SMELLS BLOOD OpenAI just threw the emergency lever. Sam Altman - the man who spent the last 2 years looking untouchable - told his staff this week that the AI crown is no longer guaran  x.com",
      "publishedAt": "2025-12-02T12:41:16.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4908918833512768,
      "ingestedAt": "2025-12-02T14:44:03.175Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ad1613",
      "title": "Anthropic study says AI agents develops $4.6M in smart contract bugs - TradingView",
      "url": "https://news.google.com/rss/articles/CBMiywFBVV95cUxNUVpLcDhQaU9MdHdVSkRJVldobXA4M3NZT1VqU1ZWRGtzRmpubU1RRlFZX091bTZ0Y3hmTUxmWklUMEpMVnpmZzI5WFdGZHpQN0VjaVZFSUZQcWlkRmFvT0FkeTRTaVlBM0Q4blZHM21lUUVYVFdIRkdyclVhUXE4blZaX3JPWk5rOHNrNlpIOVpRdUxvRWs1aXR2STQxaVVqY0t4TTZybmFVY1dOWnpLLTI4Z2xvRERlTGwtNDRydjdEblRfRWVHWHczYw?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMiywFBVV95cUxNUVpLcDhQaU9MdHdVSkRJVldobXA4M3NZT1VqU1ZWRGtzRmpubU1RRlFZX091bTZ0Y3hmTUxmWklUMEpMVnpmZzI5WFdGZHpQN0VjaVZFSUZQcWlkRmFvT0FkeTRTaVlBM0Q4blZHM21lUUVYVFdIRkdyclVhUXE4blZaX3JPWk5rOHNrNlpIOVpRdUxvRWs1aXR2STQxaVVqY0t4TTZybmFVY1dOWnpLLTI4Z2xvRERlTGwtNDRydjdEblRfRWVHWHczYw?oc=5\">Anthropic study says AI agents develops $4.6M in smart contract bugs</a>  TradingView",
      "summary": "Anthropic study says AI agents develops $4.6M in smart contract bugs  TradingView",
      "publishedAt": "2025-12-02T12:23:17.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 11.41997959156967,
      "ingestedAt": "2025-12-02T14:44:03.175Z",
      "tags": [
        "code_review",
        "agents",
        "Coding Agent Product Updates",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ace616",
      "title": "The 6 Books I&rsquo;m Reading This Month",
      "url": "https://www.inoreader.com/article/3a9c6e76b2a719e9",
      "content": "<div class=\"email_is_html\"><div><div style=\"font-kerning: auto; --image-offset-margin: -120px\"><img src=\"https://eotrx.substackcdn.com/open?token=eyJtIjoiPDIwMjUxMjAyMTQwMjU2LjMuYmIzYzBkZTM1MjYwZjNkNEBtZzIuc3Vic3RhY2suY29tPiIsInUiOjE2MTk4NTQ4MCwiciI6ImJ5dGVieXRlZ284OEBpbm8udG8iLCJkIjoibWcyLnN1YnN0YWNrLmNvbSIsInAiOjE3OTg2Mzc5NiwidCI6Im5ld3NsZXR0ZXIiLCJhIjoib25seV9wYWlkIiwicyI6MjE2Mjg5NiwiYyI6InBvc3QiLCJmIjp0cnVlLCJwb3NpdGlvbiI6InRvcCIsImlhdCI6MTc2NDY4NDIyNSwiZXhwIjoxNzY3Mjc2MjI1LCJpc3MiOiJwdWItMCIsInN1YiI6ImVvIn0.io4EzafuDHPDoYrmzpWizXgAXC2yJVm1J0Qv6v760AY\" width=\"1\" height=\"1\" border=\"0\" style=\"height: 1px !important; width: 1px !important; border-width: 0 !important; margin-top: 0 !important; margin-bottom: 0 !important; margin-right: 0 !important; margin-left: 0 !important; padding-top: 0 !important; padding-bottom: 0 !important; padding-right: 0 !important; padding-left: 0 !important\" /><div style=\"display: none; font-size: 1px; color: #333333; line-height: 1px; max-height: 0px; max-width: 0px; opacity: 0; overflow: hidden\">And why you might want to too.</div><div style=\"display: none; font-size: 1px; color: #333333; line-height: 1px; max-height: 0px; max-width: 0px; opacity: 0; overflow: hidden\">͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­</div><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td></td><td width=\"550\"></td><td></td></tr><tr><td></td><td width=\"550\" align=\"left\"><div style=\"font-size: 16px; line-height: 26px; max-width: 550px; width: 100%; margin: 0 auto; overflow-wrap: break-word\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"right\" style=\"height: 20px\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><span style=\"font-family: SF Pro Text, -apple-system, system-ui, BlinkMacSystemFont, Inter, Segoe UI, Roboto, Helvetica, Arial, sans-serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol; font-size: 13px; color: unset; list-style: none; text-decoration: unset; margin: 0\"><div style=\"list-style: none; color: unset; text-align: right; font-size: 12px; line-height: 16px; text-decoration: unset; margin: 0\"><span style=\"list-style: none; color: unset; text-decoration: unset; margin: 0\">Forwarded this email? <a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly92aW5jZW50Y2FybG9zLnN1YnN0YWNrLmNvbS9zdWJzY3JpYmU_dXRtX3NvdXJjZT1lbWFpbCZ1dG1fY2FtcGFpZ249ZW1haWwtc3Vic2NyaWJlJnI9Mm9md3NvJm5leHQ9aHR0cHMlM0ElMkYlMkZ2aW5jZW50Y2FybG9zLnN1YnN0YWNrLmNvbSUyRnAlMkZ0aGUtNi1ib29rcy1pbS1yZWFkaW5nLXRoaXMtbW9udGgiLCJwIjoxNzk4NjM3OTYsInMiOjIxNjI4OTYsImYiOnRydWUsInUiOjE2MTk4NTQ4MCwiaWF0IjoxNzY0Njg0MjI1LCJleHAiOjIwODAyNjAyMjUsImlzcyI6InB1Yi0wIiwic3ViIjoibGluay1yZWRpcmVjdCJ9.q3dcD0CyI1FtcEm7IpT9kv6GqgERDjiyNHQe__lorJk?\" style=\"list-style: none; color: unset; text-decoration: unset; margin: 0; -webkit-text-decoration-line: underline; text-decoration-line: underline\" target=\"_blank\" rel=\"noreferrer\">Subscribe here</a> for more</span></div></span></td></tr></tbody></table></td></tr></tbody></table><div dir=\"auto\" style=\"--image-offset-margin: -120px; padding: 32px 0 0 0; font-size: 16px; line-height: 26px\"><div style=\"font-size: 16px; line-height: 26px\"><h1 dir=\"auto\" style=\"direction: auto; text-align: start; unicode-bidi: isolate; color: rgb(54,55,55); font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 0; line-height: 36px; font-size: 32px\"><a href=\"https://substack.com/app-link/post?publication_id=2162896&amp;post_id=179863796&amp;utm_source=post-email-title&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=2ofwso&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE3OTg2Mzc5NiwiaWF0IjoxNzY0Njg0MjI1LCJleHAiOjE3NjcyNzYyMjUsImlzcyI6InB1Yi0yMTYyODk2Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.L8wrUWORnWNzdadsjAlSTAFk6WSBJrGuGDC9KUaDkfs\" style=\"color: rgb(54,55,55); text-decoration: none\" target=\"_blank\" rel=\"noreferrer\">The 6 Books I’m Reading This Month</a></h1><h3 dir=\"auto\" style=\"direction: auto; text-align: start; unicode-bidi: isolate; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: normal; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 4px 0 0; color: #777777; line-height: 24px; font-size: 18px; margin-top: 12px\">And why you might want to too.</h3><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"margin: 1em 0; height: 20px; align-items: center\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><div style=\"list-style: none; font-size: 11px; line-height: 20px; text-decoration: unset; color: rgb(54,55,55); margin: 0; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px\"><a style=\"list-style: none; color: rgb(54,55,55); margin: 0; font-size: 11px; line-height: 20px; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px; text-decoration: none\" href=\"https://substack.com/@vincentcarlos\" target=\"_blank\" rel=\"noreferrer\">Vincent Carlos 📚</a></div></td></tr></tbody></table></td></tr><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><div style=\"list-style: none; font-size: 11px; line-height: 20px; text-decoration: unset; color: rgb(119,119,119); margin: 0; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px\">Dec 2</div></td><td width=\"4\" style=\"min-width: 4px\"></td><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><div style=\"list-style: none; font-size: 11px; line-height: 20px; text-decoration: unset; color: rgb(119,119,119); margin: 0; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px\">∙</div></td><td width=\"4\" style=\"min-width: 4px\"></td><td style=\"vertical-align: middle\"><div style=\"list-style: none; font-size: 11px; line-height: 20px; text-decoration: unset; color: rgb(94,73,217); margin: 0; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px\">Preview</div></td></tr></tbody></table></td></tr></tbody></table></td></tr></tbody></table></td><td align=\"right\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><a href=\"https://substack.com/@vincentcarlos\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!BnzF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b1acabd-8529-4786-8a52-4bd8b129a0f7_2379x2596.jpeg\" style=\"box-sizing: border-box; border-radius: 500000px; max-width: 550px; border: none; vertical-align: middle; width: 40px; height: 40px; min-width: 40px; min-height: 40px; object-fit: cover; margin: 0px; display: inline\" width=\"40\" height=\"40\" /></a></td></tr></tbody></table></td></tr></tbody></table><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"border-top: 1px solid rgb(0,0,0,.1); border-bottom: 1px solid rgb(0,0,0,.1); min-width: 100%\"><tbody><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr><tr><td><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=2162896&amp;post_id=179863796&amp;utm_source=substack&amp;isFreemail=true&amp;submitLike=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE3OTg2Mzc5NiwicmVhY3Rpb24iOiLinaQiLCJpYXQiOjE3NjQ2ODQyMjUsImV4cCI6MTc2NzI3NjIyNSwiaXNzIjoicHViLTIxNjI4OTYiLCJzdWIiOiJyZWFjdGlvbiJ9.RUI3eC_gfyohrYDPHO_2bQQcpFOIl5-Wts2Hozp-xWM&amp;utm_medium=email&amp;utm_campaign=email-reaction&amp;r=2ofwso\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!PeVs!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideHeart%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=2162896&amp;post_id=179863796&amp;utm_source=substack&amp;utm_medium=email&amp;isFreemail=true&amp;comments=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE3OTg2Mzc5NiwiaWF0IjoxNzY0Njg0MjI1LCJleHAiOjE3NjcyNzYyMjUsImlzcyI6InB1Yi0yMTYyODk2Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.L8wrUWORnWNzdadsjAlSTAFk6WSBJrGuGDC9KUaDkfs&amp;r=2ofwso&amp;utm_campaign=email-half-magic-comments&amp;action=post-comment&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!x1tS!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideComments%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=2162896&amp;post_id=179863796&amp;utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;utm_campaign=email-share&amp;action=share&amp;triggerShare=true&amp;isFreemail=true&amp;r=2ofwso&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE3OTg2Mzc5NiwiaWF0IjoxNzY0Njg0MjI1LCJleHAiOjE3NjcyNzYyMjUsImlzcyI6InB1Yi0yMTYyODk2Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.L8wrUWORnWNzdadsjAlSTAFk6WSBJrGuGDC9KUaDkfs\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!_L14!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideShare2%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9vcGVuLnN1YnN0YWNrLmNvbS9wdWIvdmluY2VudGNhcmxvcy9wL3RoZS02LWJvb2tzLWltLXJlYWRpbmctdGhpcy1tb250aD91dG1fc291cmNlPXN1YnN0YWNrJnV0bV9tZWRpdW09ZW1haWwmdXRtX2NhbXBhaWduPWVtYWlsLXJlc3RhY2stY29tbWVudCZhY3Rpb249cmVzdGFjay1jb21tZW50JnI9Mm9md3NvJnRva2VuPWV5SjFjMlZ5WDJsa0lqb3hOakU1T0RVME9EQXNJbkJ2YzNSZmFXUWlPakUzT1RnMk16YzVOaXdpYVdGMElqb3hOelkwTmpnME1qSTFMQ0psZUhBaU9qRTNOamN5TnpZeU1qVXNJbWx6Y3lJNkluQjFZaTB5TVRZeU9EazJJaXdpYzNWaUlqb2ljRzl6ZEMxeVpXRmpkR2x2YmlKOS5MOHdyVVdPUm5XTnpkYWRzakFsU1RBRms2V1NCSnJHdUdEQzlLVWFEa2ZzIiwicCI6MTc5ODYzNzk2LCJzIjoyMTYyODk2LCJmIjp0cnVlLCJ1IjoxNjE5ODU0ODAsImlhdCI6MTc2NDY4NDIyNSwiZXhwIjoyMDgwMjYwMjI1LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.zsaPH4zpT2ug_x4eK191nTv8UN3pJe74RzN3_tatAi0?&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!5EGt!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FNoteForwardIcon%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td></tr></tbody></table></td><td align=\"right\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://open.substack.com/pub/vincentcarlos/p/the-6-books-im-reading-this-month?utm_source=email&amp;redirect=app-store&amp;utm_campaign=email-read-in-app\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><div style=\"font-size: 16px; line-height: 26px; display: inline-block; vertical-align: middle; max-width: 0; min-height: 18px\"></div><span style=\"vertical-align: middle; margin-right: 4px\">READ IN APP</span><img src=\"https://substackcdn.com/image/fetch/$s_!ET-_!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideArrowUpRight%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"min-width: 18px; min-height: 18px; border: none; vertical-align: middle; margin-right: 0; margin-left: 0; max-width: 18px\" /></a></td></tr></tbody></table></td></tr></tbody></table></td></tr></tbody></table></td></tr><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr></tbody></table></div></div><div dir=\"auto\" style=\"--image-offset-margin: -120px; padding: 32px 0 0 0; font-size: 16px; line-height: 26px\"><div dir=\"auto\" style=\"text-align: initial; font-size: 16px; line-height: 26px; width: 100%; word-break: break-word; margin-bottom: 16px\"><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em; margin-top: 0\">Hi there fellow book lover! 👋</h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>Welcome to </span><strong>The One Percent Better Club </strong><span>📚 - your weekly dose of brilliant ideas from the world’s best nonfiction books that are going to help you be one percent better today than you were yesterday! So if you’re ready for that one percent upgrade, then let’s get started! 🚀</span></p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1080\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/b9e55186-1866-44d2-b89d-fe1b0dd8ce62?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img alt=\"person holding pile of books\" title=\"person holding pile of books\" width=\"550\" height=\"366.6666666666667\" src=\"https://images.unsplash.com/photo-1519682337058-a94d519337bc?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHw5fHxib29rc3xlbnwwfHx8fDE3MTUwMDk2Njh8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Where do you turn when you need to reset your mind or find a new direction in life</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">For me, the answer has always been the same:</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nonfiction books</strong><span>.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">And nonfiction books aren’t like other books. They’re real-life toolkits for personal growth.</p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">They’ll challenge your perceptions. </p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">They’ll inspire change. </p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">And they’ll help push you toward new levels of self-awareness and success.</p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">And this month, I’ve chosen a selection of 6 books to read that promise to do just that.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Whether it’s learning how to enhance my mental focus, spend my money better, or live a happier life, each book I picked offers practical insights that I can immediately apply to my life.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">So if you’re also on a quest for self-improvement, then maybe you’ll find these books as interesting as I do.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Here are the 6 books I plan on reading this month and why you might want to add them to your own list.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px; margin-bottom: 0\">Let’s dive in!...</p></div></div><div style=\"position: relative; font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; text-align: center; margin-bottom: 16px; cursor: default; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; line-height: 26px; font-size: 16px; padding: 32px 24px 16px 24px; background: rgb(255,255,255); color: rgb(54,55,55); border-top: solid 2px #51b4e9; margin-top: 6px\"><h2 style=\"font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 0 auto; color: inherit !important; font-size: 22px; font-weight: 700; line-height: 33px; margin-bottom: 6px\">Keep reading with a 7-day free trial</h2><div style=\"font-size: 16px; line-height: 26px; padding: 16px 10px 0px\"><p style=\"list-style: none; color: inherit !important; font-size: 16px; font-weight: light; line-height: 24px; margin-bottom: 24px; margin: 0; text-decoration: unset\">Subscribe to <span style=\"list-style: none; color: unset; font-style: italic; text-decoration: unset; margin: 0; font-weight: 700\">The One Percent Better Club 📚 </span> to keep reading this post and get 7 days of free access to the full post archives.</p></div><div style=\"font-size: 16px; line-height: 26px; margin-top: 0\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly92aW5jZW50Y2FybG9zLnN1YnN0YWNrLmNvbS9zdWJzY3JpYmU_dXRtX3NvdXJjZT1wb3N0JnV0bV9jYW1wYWlnbj1lbWFpbC1jaGVja291dCZuZXh0PWh0dHBzJTNBJTJGJTJGdmluY2VudGNhcmxvcy5zdWJzdGFjay5jb20lMkZwJTJGdGhlLTYtYm9va3MtaW0tcmVhZGluZy10aGlzLW1vbnRoJnI9Mm9md3NvJnRva2VuPWV5SjFjMlZ5WDJsa0lqb3hOakU1T0RVME9EQXNJbWxoZENJNk1UYzJORFk0TkRJeU5Td2laWGh3SWpveE56WTNNamMyTWpJMUxDSnBjM01pT2lKd2RXSXRNakUyTWpnNU5pSXNJbk4xWWlJNkltTm9aV05yYjNWMEluMC40N2Jub0o2Q0x5R3c2SUo2TVByMXlaRkEtVndqVGl4OWtmMHVob1dnZW0wIiwicCI6MTc5ODYzNzk2LCJzIjoyMTYyODk2LCJmIjp0cnVlLCJ1IjoxNjE5ODU0ODAsImlhdCI6MTc2NDY4NDIyNSwiZXhwIjoyMDgwMjYwMjI1LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.YfXWzzxFA-_OWfg4ttz2F6AdJ1qnxbK0a1qwS_RK_1E?simple=true&amp;utm_source=paywall&amp;utm_medium=email&amp;utm_content=179863796&amp;next=https://vincentcarlos.substack.com/p/the-6-books-im-reading-this-month&amp;coupon=97c1b440\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; box-sizing: border-box; cursor: pointer; border: none; border-radius: 8px; font-size: 14px; text-align: center; margin: 0; opacity: 1; outline: none; white-space: nowrap; background-color: #51b4e9; text-decoration: none !important; color: #ffffff !important; font-weight: 400; padding: 16px 20px; height: auto; line-height: 1em\" target=\"_blank\" rel=\"noreferrer\"><span style=\"color: #ffffff; text-decoration: none\">Start trial</span></a></div></div><div style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; margin-bottom: 32px; font-size: 16px; line-height: 26px\"><h3 style=\"font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 16px; font-weight: 600; margin-bottom: 26px\">A subscription gets you:</h3><table><tbody><tr style=\"height: 28.8px\"><td style=\"font-weight: light; padding-right: 4px; color: rgb(27,196,125)\"><img src=\"https://substackcdn.com/image/fetch/$s_!8m7v!,w_32,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FSimpleCheckmarkIcon%3Fv%3D4%26height%3D32%26fill%3D%2523187F22%26stroke%3D%2523187F22%26strokeWidth%3D3.6\" width=\"16\" height=\"16\" style=\"border: none; vertical-align: middle; max-width: 16px\" /></td><td style=\"font-weight: light\">Subscriber-only posts every Tuesday and access to full archive</td></tr><tr style=\"height: 28.8px\"><td style=\"font-weight: light; padding-right: 4px; color: rgb(27,196,125)\"><img src=\"https://substackcdn.com/image/fetch/$s_!8m7v!,w_32,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FSimpleCheckmarkIcon%3Fv%3D4%26height%3D32%26fill%3D%2523187F22%26stroke%3D%2523187F22%26strokeWidth%3D3.6\" width=\"16\" height=\"16\" style=\"border: none; vertical-align: middle; max-width: 16px\" /></td><td style=\"font-weight: light\">Direct 1-1 access to me</td></tr><tr style=\"height: 28.8px\"><td style=\"font-weight: light; padding-right: 4px; color: rgb(27,196,125)\"><img src=\"https://substackcdn.com/image/fetch/$s_!8m7v!,w_32,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FSimpleCheckmarkIcon%3Fv%3D4%26height%3D32%26fill%3D%2523187F22%26stroke%3D%2523187F22%26strokeWidth%3D3.6\" width=\"16\" height=\"16\" style=\"border: none; vertical-align: middle; max-width: 16px\" /></td><td style=\"font-weight: light\">9 exclusive subscriber-only eBooks</td></tr></tbody></table></div><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"border-top: 1px solid rgb(0,0,0,.1); border-bottom: 1px solid rgb(0,0,0,.1); min-width: 100%\"><tbody><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr><tr><td><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"margin: 0 auto\"><tbody><tr><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=2162896&amp;post_id=179863796&amp;utm_source=substack&amp;isFreemail=true&amp;submitLike=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE3OTg2Mzc5NiwicmVhY3Rpb24iOiLinaQiLCJpYXQiOjE3NjQ2ODQyMjUsImV4cCI6MTc2NzI3NjIyNSwiaXNzIjoicHViLTIxNjI4OTYiLCJzdWIiOiJyZWFjdGlvbiJ9.RUI3eC_gfyohrYDPHO_2bQQcpFOIl5-Wts2Hozp-xWM&amp;utm_medium=email&amp;utm_campaign=email-reaction&amp;r=2ofwso\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!PeVs!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideHeart%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"margin-right: 8px; min-width: 18px; min-height: 18px; border: none; vertical-align: middle; max-width: 18px\" /><span style=\"vertical-align: middle\">Like</span></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=2162896&amp;post_id=179863796&amp;utm_source=substack&amp;utm_medium=email&amp;isFreemail=true&amp;comments=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE3OTg2Mzc5NiwiaWF0IjoxNzY0Njg0MjI1LCJleHAiOjE3NjcyNzYyMjUsImlzcyI6InB1Yi0yMTYyODk2Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.L8wrUWORnWNzdadsjAlSTAFk6WSBJrGuGDC9KUaDkfs&amp;r=2ofwso&amp;utm_campaign=email-half-magic-comments&amp;action=post-comment&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!x1tS!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideComments%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"margin-right: 8px; min-width: 18px; min-height: 18px; border: none; vertical-align: middle; max-width: 18px\" /><span style=\"vertical-align: middle\">Comment</span></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9vcGVuLnN1YnN0YWNrLmNvbS9wdWIvdmluY2VudGNhcmxvcy9wL3RoZS02LWJvb2tzLWltLXJlYWRpbmctdGhpcy1tb250aD91dG1fc291cmNlPXN1YnN0YWNrJnV0bV9tZWRpdW09ZW1haWwmdXRtX2NhbXBhaWduPWVtYWlsLXJlc3RhY2stY29tbWVudCZhY3Rpb249cmVzdGFjay1jb21tZW50JnI9Mm9md3NvJnRva2VuPWV5SjFjMlZ5WDJsa0lqb3hOakU1T0RVME9EQXNJbkJ2YzNSZmFXUWlPakUzT1RnMk16YzVOaXdpYVdGMElqb3hOelkwTmpnME1qSTFMQ0psZUhBaU9qRTNOamN5TnpZeU1qVXNJbWx6Y3lJNkluQjFZaTB5TVRZeU9EazJJaXdpYzNWaUlqb2ljRzl6ZEMxeVpXRmpkR2x2YmlKOS5MOHdyVVdPUm5XTnpkYWRzakFsU1RBRms2V1NCSnJHdUdEQzlLVWFEa2ZzIiwicCI6MTc5ODYzNzk2LCJzIjoyMTYyODk2LCJmIjp0cnVlLCJ1IjoxNjE5ODU0ODAsImlhdCI6MTc2NDY4NDIyNSwiZXhwIjoyMDgwMjYwMjI1LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.zsaPH4zpT2ug_x4eK191nTv8UN3pJe74RzN3_tatAi0?&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!5EGt!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FNoteForwardIcon%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"margin-right: 8px; min-width: 18px; min-height: 18px; border: none; vertical-align: middle; max-width: 18px\" /><span style=\"vertical-align: middle\">Restack</span></a></td></tr></tbody></table></td></tr></tbody></table></td><td align=\"right\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr></tr></tbody></table></td></tr></tbody></table></td></tr><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr></tbody></table><div style=\"color: rgb(119,119,119); text-align: center; font-size: 16px; line-height: 26px; padding: 24px0\"><div style=\"font-size: 16px; line-height: 26px; padding-bottom: 24px\"><p style=\"list-style: none; font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; padding-bottom: 0; font-size: 12px; line-height: 16px; margin: 0; color: rgb(119,119,119); text-decoration: unset\">© 2025 <span>Vincent Carlos</span><br />548 Market Street PMB 72296, San Francisco, CA 94104 <br /><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly92aW5jZW50Y2FybG9zLnN1YnN0YWNrLmNvbS9hY3Rpb24vZGlzYWJsZV9lbWFpbD90b2tlbj1leUoxYzJWeVgybGtJam94TmpFNU9EVTBPREFzSW5CdmMzUmZhV1FpT2pFM09UZzJNemM1Tml3aWFXRjBJam94TnpZME5qZzBNakkxTENKbGVIQWlPakUzT1RZeU1qQXlNalVzSW1semN5STZJbkIxWWkweU1UWXlPRGsySWl3aWMzVmlJam9pWkdsellXSnNaVjlsYldGcGJDSjkuZTZGX3NHQXhwaXlrVlJKWGxoSEFEVkZwYXFIR0VQTmlCWWF6ZURwWk9KSSIsInAiOjE3OTg2Mzc5NiwicyI6MjE2Mjg5NiwiZiI6dHJ1ZSwidSI6MTYxOTg1NDgwLCJpYXQiOjE3NjQ2ODQyMjUsImV4cCI6MjA4MDI2MDIyNSwiaXNzIjoicHViLTAiLCJzdWIiOiJsaW5rLXJlZGlyZWN0In0.K-N71Ls80yRjB6G-Wo1-wOLY7KT77Cs88pyr6SCZR3k?\" style=\"color: #51b4e9; text-decoration: none\" target=\"_blank\" rel=\"noreferrer\"><span style=\"color: rgb(119,119,119); text-decoration: underline\">Unsubscribe</span></a></p></div><p style=\"padding: 0 24px; font-size: 12px; line-height: 20px; margin: 0; color: rgb(119,119,119); font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; padding-bottom: 0; margin-top: 0\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9zdWJzdGFjay5jb20vc2lnbnVwP3V0bV9zb3VyY2U9c3Vic3RhY2smdXRtX21lZGl1bT1lbWFpbCZ1dG1fY29udGVudD1mb290ZXImdXRtX2NhbXBhaWduPWF1dG9maWxsZWQtZm9vdGVyJmZyZWVTaWdudXBFbWFpbD1ieXRlYnl0ZWdvODhAaW5vLnRvJnI9Mm9md3NvIiwicCI6MTc5ODYzNzk2LCJzIjoyMTYyODk2LCJmIjp0cnVlLCJ1IjoxNjE5ODU0ODAsImlhdCI6MTc2NDY4NDIyNSwiZXhwIjoyMDgwMjYwMjI1LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.vsSsBagwgva9i0MpVaNtn1Ws8lzsKD1bmjNmKkIvEcs?\" style=\"color: #51b4e9; text-decoration: none; display: inline-block; margin: 0 4px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!LkrL!,w_270,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpublish-button%402x.png\" width=\"135\" alt=\"Start writing\" height=\"40\" style=\"max-width: 550px; border: none !important; vertical-align: middle\" /></a></p></div></div></td><td></td></tr></tbody></table><img src=\"https://eotrx.substackcdn.com/open?token=eyJtIjoiPDIwMjUxMjAyMTQwMjU2LjMuYmIzYzBkZTM1MjYwZjNkNEBtZzIuc3Vic3RhY2suY29tPiIsInUiOjE2MTk4NTQ4MCwiciI6ImJ5dGVieXRlZ284OEBpbm8udG8iLCJkIjoibWcyLnN1YnN0YWNrLmNvbSIsInAiOjE3OTg2Mzc5NiwidCI6Im5ld3NsZXR0ZXIiLCJhIjoib25seV9wYWlkIiwicyI6MjE2Mjg5NiwiYyI6InBvc3QiLCJmIjp0cnVlLCJwb3NpdGlvbiI6ImJvdHRvbSIsImlhdCI6MTc2NDY4NDIyNSwiZXhwIjoxNzY3Mjc2MjI1LCJpc3MiOiJwdWItMCIsInN1YiI6ImVvIn0.wu5LSvodsWiAtaxciSEljl99DnLO2nvWl36YKovJtF8\" width=\"1\" height=\"1\" border=\"0\" style=\"height: 1px !important; width: 1px !important; border-width: 0 !important; margin-top: 0 !important; margin-bottom: 0 !important; margin-right: 0 !important; margin-left: 0 !important; padding-top: 0 !important; padding-bottom: 0 !important; padding-right: 0 !important; padding-left: 0 !important\" /><img width=\"1\" height=\"1\" src=\"https://email.mg2.substack.com/o/eJxMkEvO4yAQBk8TlhY0D-MFZ0E8Oh40NljQZOTbj5L8i3_Rm2qpVPpSINxbv93VBrHsVBZWW4ZOrEYZqwA0wzOUw-9YsQfC7AP9-vJtZX_cc4VolN6UNcLCukpQEnUInGdQ0kRWHHDQAjgIxUGbRS4xysQzSg2GP2VWD8XPHZYx46CQ_i6pnawM_-z4CXDUJ7J3pg8zF6wJXavH7a9Q8peX7MS6WSPXzXwJ3Re6iv_GgUTY2TWjT-08Zy10e6whHph_zDMeJQUqrb5FIAzYzbDu4k34vr1Z-1C81LZQY2PG3M5QqnuVmrBSCv1og9F3yTmwf3qM2KxWlrOXg_8BAAD__6H4d80\" /></div></div></div>",
      "summary": "And why you might want to too.͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏",
      "publishedAt": "2025-12-02T14:03:56.000Z",
      "author": "Vincent Carlos ",
      "source": "rss",
      "feedName": "Byte Byte Go",
      "sourceType": "general",
      "contentType": "thought_leadership",
      "score": 4.990059592533354,
      "ingestedAt": "2025-12-02T14:44:03.177Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a8aa09",
      "title": "Comprehensive Guide to Load and Stress Testing Types with Locust Implementation",
      "url": "https://dev.to/mohsen_akbari_ebe53d7cbc2/comprehensive-guide-to-load-and-stress-testing-types-with-locust-implementation-40o6",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fs439a1jpvk272q1mqb9j.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><blockquote>  \n<p>Part 1: Understanding Load and Stress Testing Types</p>  \n  \n<p>1.1 Introduction to Load Testing Fundamentals</p>  \n</blockquote>  \n  \n<p>Load Testing is the process of simulating real-world usage on software applications to understand behavior under expected load conditions. It helps identify performance bottlenecks, establish baselines, and ensure applications can handle anticipated traffic.</p>  \n  \n<p>Stress Testing pushes systems beyond normal operational capacity to determine breaking points and understand failure modes. Unlike load testing, which validates performance under expected conditions, stress testing explores system behavior at and beyond limits.<br>  \n1.2 Conventional Load Testing Types</p>  \n  \n<blockquote>  \n<p>1.2.1 Baseline Testing<br>  \nPurpose: Establish performance benchmarks under normal conditions</p>  \n</blockquote>  \n  \n<p>Metrics: Response times, throughput, resource utilization</p>  \n  \n<p>Use Case: Initial performance assessment, regression testing</p>  \n  \n<p>Typical Scenario: Simulating average daily users with normal behavior patterns</p>  \n  \n<blockquote>  \n<p>1.2.2 Load Testing<br>  \nPurpose: Verify system behavior under expected peak load</p>  \n</blockquote>  \n  \n<p>Metrics: Error rates, latency at peak, throughput capacity</p>  \n  \n<p>Use Case: Pre-deployment validation, capacity planning</p>  \n  \n<p>Typical Scenario: Simulating Black Friday traffic for e-commerce</p>  \n  \n<blockquote>  \n<p>1.2.3 Stress Testing<br>  \nPurpose: Identify maximum capacity and breaking points</p>  \n</blockquote>  \n  \n<p>Metrics: System failure points, recovery behavior, error handling</p>  \n  \n<p>Use Case: Determining scalability limits, disaster recovery planning</p>  \n  \n<p>Typical Scenario: Gradual increase until system failure</p>  \n  \n<blockquote>  \n<p>1.2.4 Soak Testing (Endurance Testing)<br>  \nPurpose: Identify performance degradation over extended periods</p>  \n</blockquote>  \n  \n<p>Metrics: Memory leaks, resource exhaustion, response time drift</p>  \n  \n<p>Use Case: Long-running process validation, memory management testing</p>  \n  \n<p>Typical Scenario: 24-72 hour continuous load simulation</p>  \n  \n<blockquote>  \n<p>1.2.5 Spike Testing<br>  \nPurpose: Evaluate system response to sudden traffic surges</p>  \n</blockquote>  \n  \n<p>Metrics: Recovery time, error spikes, system stability</p>  \n  \n<p>Use Case: Handling viral content, emergency notifications</p>  \n  \n<p>Typical Scenario: Instant 10x traffic increase for 5 minutes</p>  \n  \n<blockquote>  \n<p>1.2.6 Volume Testing<br>  \nPurpose: Test system with large amounts of data</p>  \n</blockquote>  \n  \n<p>Metrics: Database performance, storage utilization, data processing time</p>  \n  \n<p>Use Case: Big data applications, reporting systems</p>  \n  \n<p>Typical Scenario: Processing millions of records simultaneously</p>  \n  \n<blockquote>  \n<p>1.2.7 Scalability Testing<br>  \nPurpose: Verify system performance as resources increase</p>  \n</blockquote>  \n  \n<p>Metrics: Linear scaling capability, resource efficiency</p>  \n  \n<p>Use Case: Horizontal scaling validation, cloud resource planning</p>  \n  \n<p>Typical Scenario: Adding nodes/containers while increasing load</p>  \n  \n<blockquote>  \n<p>1.3 Advanced Stress Analogies from Material Science<br>  \nModern distributed systems exhibit behaviors remarkably similar to physical materials under stress. Understanding these analogies helps identify subtle performance issues that conventional testing might miss.</p>  \n  \n<p>1.3.1 Residual Stresses<br>  \nDefinition: Internal stresses that remain in a system after the original cause of stress has been removed.</p>  \n</blockquote>  \n  \n<p>System Analog: Performance degradation lingering after high-load events</p>  \n  \n<p>Examples:</p>  \n  \n<p>Memory fragmentation after garbage collection</p>  \n  \n<p>Database connection pool saturation</p>  \n  \n<p>Cache invalidation patterns are causing subsequent slowdowns</p>  \n  \n<p>Session state corruption after recovery</p>  \n  \n<blockquote>  \n<p>1.3.2 Structural Stresses<br>  \nDefinition: Stresses resulting from architectural design limitations or component interactions.</p>  \n</blockquote>  \n  \n<p>System Analog: Bottlenecks caused by system architecture</p>  \n  \n<p>Examples:</p>  \n  \n<p>Microservice communication overhead</p>  \n  \n<p>Database schema design limitations</p>  \n  \n<p>API gateway throughput limits</p>  \n  \n<p>Message queue backpressure</p>  \n  \n<p>Service mesh latency</p>  \n  \n<blockquote>  \n<p>1.3.3 Pressure Stresses<br>  \nDefinition: Uniform stress applied across a system's surface area.</p>  \n</blockquote>  \n  \n<p>System Analog: Evenly distributed load causing systemic issues</p>  \n  \n<p>Examples:</p>  \n  \n<p>Rate limiting across all endpoints</p>  \n  \n<p>Database connection limits</p>  \n  \n<p>Bandwidth saturation</p>  \n  \n<p>CPU throttling across all nodes</p>  \n  \n<blockquote>  \n<p>1.3.4 Flow Stresses<br>  \nDefinition: Stresses caused by fluid movement or streaming through a system.</p>  \n</blockquote>  \n  \n<p>System Analog: Data streaming and processing bottlenecks</p>  \n  \n<p>Examples:</p>  \n  \n<p>Real-time data processing pipelines</p>  \n  \n<p>WebSocket connection handling</p>  \n  \n<p>Streaming API throughput</p>  \n  \n<p>Event-driven architecture backpressure</p>  \n  \n<p>Data ingestion rate limitations</p>  \n  \n<p>Memory pressure from multiple services</p>  \n  \n<blockquote>  \n<p>1.3.5 Thermal Stresses<br>  \nDefinition: Stresses caused by temperature changes leading to expansion/contraction.</p>  \n</blockquote>  \n  \n<p>System Analog: Resource utilisation causing performance throttling</p>  \n  \n<p>Examples:</p>  \n  \n<p>CPU thermal throttling under sustained load</p>  \n  \n<p>Memory heat-induced errors</p>  \n  \n<p>Disk I/O thermal limitations</p>  \n  \n<p>Network equipment overheating</p>  \n  \n<p>Container orchestration auto-scaling delays</p>  \n  \n<blockquote>  \n<p>1.3.6 Fatigue Stresses<br>  \nDefinition: Progressive structural damage under cyclic loading.</p>  \n</blockquote>  \n  \n<p>System Analog: Performance degradation under repeated load cycles</p>  \n  \n<p>Examples:</p>  \n  \n<p>Memory leaks over multiple test cycles</p>  \n  \n<p>Database connection pool degradation</p>  \n  \n<p>File descriptor exhaustion</p>  \n  \n<p>Thread pool starvation patterns</p>  \n  \n<p>Garbage collection efficiency degradation</p>  \n  \n<blockquote>  \n<p>1.4 Load Testing Strategy Matrix<br>  \n</p>  \n  \n  \n</blockquote>  \n  \n<div>  \n<pre><code>**Test Type     Primary Goal         Key Metrics **                          Duration    User Pattern  \nBaseline      Establish norms      Response time, throughput             Short       Normal distribution  \nLoad          Validate capacity    Error rate, latency                   Medium      Expected peak  \nStress        Find limits          Breaking points, recovery             Medium-High Gradual increase  \nSoak          Detect leaks         Memory usage, degradation             Long        Steady state  \nSpike         Test resilience      Recovery time, errors                 Short       Instant surge  \nVolume        Data handling        Processing time, storage              Medium      Large datasets  \nScalability   Scaling efficiency   Linear scaling, cost                  Medium      Incremental load  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<blockquote>  \n<p>1.5 Performance Metrics Framework<br>  \n1.5.1 Response Metrics<br>  \nResponse Time: 50th, 95th, 99th percentiles</p>  \n</blockquote>  \n  \n<p>Throughput: Requests/second, transactions/minute</p>  \n  \n<p>Error Rate: Percentage of failed requests</p>  \n  \n<p>Success Rate: Percentage of successful operations</p>  \n  \n<blockquote>  \n<p>1.5.2 Resource Metrics<br>  \nCPU Utilization: Percentage across all nodes</p>  \n</blockquote>  \n  \n<p>Memory Usage: Heap, stack, native memory</p>  \n  \n<p>I/O Operations: Disk read/write, network throughput</p>  \n  \n<p>Connection Count: Active connections, pool utilization</p>  \n  \n<blockquote>  \n<p>1.5.3 Business Metrics<br>  \nConversion Rate: Under load conditions</p>  \n</blockquote>  \n  \n<p>User Satisfaction: Synthetic user experience scoring</p>  \n  \n<p>Revenue Impact: Performance effect on transactions</p>  \n  \n<p>Abandonment Rate: User drop-off under stress</p>  \n  \n<blockquote>  \n<p>1.6 Risk-Based Testing Prioritization<br>  \nHigh-Risk Areas (Test First):<br>  \nCore Transaction Paths: Checkout, login, payment</p>  \n</blockquote>  \n  \n<p>Data Integrity Operations: Orders, financial transactions</p>  \n  \n<p>Third-Party Integrations: Payment gateways, external APIs</p>  \n  \n<p>Stateful Operations: User sessions, shopping carts</p>  \n  \n<blockquote>  \n<p>Medium-Risk Areas:<br>  \nSearch and Browse: Product discovery</p>  \n</blockquote>  \n  \n<p>Content Delivery: Images, videos, static assets</p>  \n  \n<p>Reporting and Analytics: Data aggregation</p>  \n  \n<blockquote>  \n<p>Low-Risk Areas:<br>  \nStatic Pages: About us, contact information</p>  \n</blockquote>  \n  \n<p>Administrative Functions: Back-office operations</p>  \n  \n<p>Non-critical Features: User preferences, wishlists</p>",
      "summary": "  \nPart 1: Understanding Load and Stress Testing Types  \n  \n1.1 Introduction to Load Testing Fundamentals  \n  \n  \nLoad Testing is the process of simulating real-world usage on software applications to understand behavior under expected load conditions. It helps identify performance bottlenecks, establish baselines, and ensure applications can handle anticipated traffic.  \n  \nStress Testing pushes systems beyond normal operational capacity to determine breaking points and understand failure modes",
      "publishedAt": "2025-12-02T13:34:03.000Z",
      "author": "Mohsen Akbari",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 10.463603358056329,
      "ingestedAt": "2025-12-02T14:44:03.177Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "testing",
        "observability",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a8aa13",
      "title": "HTTP/1.1 vs HTTP/2 vs HTTP/3 – Which One Are You Still Using in 2025?",
      "url": "https://dev.to/sreekanth_kuruba_91721e5d/http11-vs-http2-vs-http3-which-one-are-you-still-using-in-2025-4aah",
      "content": "<p>🚀 <strong>Networking for DevOps &amp; SRE – 2025 Edition • Part 2/10</strong></p> \n \n<p>Most teams think they’ve already moved to HTTP/2 or HTTP/3.</p> \n \n<p>But when we checked real production traffic in 2025, the truth was surprising:</p> \n \n<p>We pulled protocol stats from our CDN + load balancer logs...</p> \n \n<p>63% of all requests were still hitting us over HTTP/1.1 — mostly from corporate proxies, middleboxes, and legacy devices.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Famg2d7tt8848uuzzxw49.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Famg2d7tt8848uuzzxw49.png\" alt=\"Pie chart showing HTTP protocol distribution: 63% HTTP/1.1, 25% HTTP/2, and 12% HTTP/3, based on real production traffic analysis\" width=\"800\" height=\"628\"></a></p> \n \n<p>That means 2 out of every 3 requests were paying an unnecessary 150–300ms latency tax just because outdated protocols were still in the path.</p> \n \n<h2> \n   \n   \n  The Web’s 2025 Protocol Reality Check \n</h2> \n \n<p>All three versions move data between browser and backend.<br> \nBut how they do it — TCP vs multiplexing vs QUIC — creates massive differences in:</p> \n \n<ul> \n<li>Page load speed</li> \n<li>API latency</li> \n<li>Core Web Vitals</li> \n<li>CDN routing efficiency</li> \n<li>Mobile reliability</li> \n</ul> \n \n<p>Here’s the 2025 snapshot:</p> \n \n<p>📊 <strong>HTTP/1.1 vs HTTP/2 vs HTTP/3 (2025 Edition)</strong></p> \n \n<div><table> \n<thead> \n<tr> \n<th>Feature</th> \n<th>HTTP/1.1 (1997)</th> \n<th>HTTP/2 (2015)</th> \n<th>HTTP/3 (2022+, QUIC)</th> \n</tr> \n</thead> \n<tbody> \n<tr> \n<td>Transport</td> \n<td>TCP</td> \n<td>TCP</td> \n<td>UDP (QUIC)</td> \n</tr> \n<tr> \n<td>Multiplexing</td> \n<td>No</td> \n<td>Yes</td> \n<td>Yes (independent streams)</td> \n</tr> \n<tr> \n<td>HOL Blocking</td> \n<td>Yes</td> \n<td>Yes (TCP)</td> \n<td>None</td> \n</tr> \n<tr> \n<td>Header Compression</td> \n<td>None</td> \n<td>HPACK</td> \n<td>QPACK</td> \n</tr> \n<tr> \n<td>Connection Setup</td> \n<td>1–3 RTT</td> \n<td>1–3 RTT</td> \n<td>0–1 RTT</td> \n</tr> \n<tr> \n<td>Mobile Performance</td> \n<td>Poor</td> \n<td>Decent</td> \n<td>Best</td> \n</tr> \n<tr> \n<td>Real Adoption (2025)</td> \n<td>15–20%</td> \n<td>60–65%</td> \n<td>25–30% and rising</td> \n</tr> \n<tr> \n<td>Browser Support</td> \n<td>100%</td> \n<td>~98%</td> \n<td>~95–97%</td> \n</tr> \n</tbody> \n</table></div> \n \n<p>🦕 <strong>HTTP/1.1 – The Dinosaur That Refuses to Die</strong><br><br> \nWhy it still dominates:</p> \n \n<ul> \n<li>Corporate proxies downgrade connections</li> \n<li>Old load balancers downgrade traffic back to HTTP/1.1</li> \n<li>Cheap hosting providers</li> \n<li>Legacy browsers &amp; IoT devices</li> \n<li>Internal APIs nobody migrated</li> \n</ul> \n \n<p>Problems:</p> \n \n<ul> \n<li>No multiplexing</li> \n<li>HOL blocking</li> \n<li>Browser opens 6 parallel connections</li> \n<li>Massive header repetition</li> \n</ul> \n \n<p>If you still rely on HTTP/1.1 in 2025, you are paying a latency tax every single day.</p> \n \n<p>⚡ <strong>HTTP/2 – The Multiplexing Hero (With One Big Problem)</strong><br><br> \nHTTP/2 solved a lot:</p> \n \n<ul> \n<li>Binary framing</li> \n<li>Multiplexing</li> \n<li>Header compression</li> \n<li>Single connection</li> \n</ul> \n \n<p>But it still suffers from <strong>TCP Head-of-Line Blocking</strong>:<br><br> \nOne lost packet → all streams wait.<br><br> \nOn flaky networks (mobile, 3–5% packet loss), H2 often performs worse than people expect.<br><br> \nStill excellent for: CDNs, production APIs, stable networks.</p> \n \n<p><strong>HTTP/3 – QUIC Is the Real Upgrade</strong><br><br> \nHTTP/3 ditches TCP entirely and uses QUIC over UDP.<br><br> \nBig wins:</p> \n \n<ul> \n<li>0-RTT resume</li> \n<li>No HOL blocking</li> \n<li>Faster handshakes</li> \n<li>Better encryption (TLS 1.3 built-in)</li> \n<li>Superior mobile performance</li> \n<li>Stable under packet loss</li> \n</ul> \n \n<p>This is the first protocol designed for modern, mobile, global internet traffic.</p> \n \n<p>📈 <strong>Real 2025 Performance Results</strong></p> \n \n<div><table> \n<thead> \n<tr> \n<th>Scenario</th> \n<th>HTTP/1.1</th> \n<th>HTTP/2</th> \n<th>HTTP/3</th> \n</tr> \n</thead> \n<tbody> \n<tr> \n<td>100 small assets</td> \n<td>4–6s</td> \n<td>~1.2s</td> \n<td>~0.9s</td> \n</tr> \n<tr> \n<td>3% packet loss</td> \n<td>Terrible</td> \n<td>Bad</td> \n<td>Good</td> \n</tr> \n<tr> \n<td>Flaky mobile</td> \n<td>Painful</td> \n<td>Okay</td> \n<td>Best</td> \n</tr> \n<tr> \n<td>First load</td> \n<td>Slow</td> \n<td>Slow</td> \n<td>Fastest</td> \n</tr> \n<tr> \n<td>Repeat visits</td> \n<td>~Same</td> \n<td>~Same</td> \n<td>Instant (0-RTT)</td> \n</tr> \n</tbody> \n</table></div> \n \n<p><strong>The Problem Nobody Mentions</strong><br><br> \nEven if your CDN + app support HTTP/3:<br><br> \nMany users still fall back to 1.1 or 2.0 due to network intermediaries.<br><br> \nCommon blockers:</p> \n \n<ul> \n<li>Corporate firewalls</li> \n<li>Middleboxes that strip UDP</li> \n<li>Legacy devices</li> \n<li>Some enterprise proxies</li> \n<li>Outdated routers</li> \n<li>Misconfigured hosting</li> \n</ul> \n \n<p>This is why simply enabling HTTP/3 is not enough – everything in the path must support it.</p> \n \n<p><strong>So What Should You Use in 2025?</strong><br><br> \nHTTP/1.1 → Only for legacy systems<br><br> \nOr internal APIs that never changed.<br> \nHTTP/2 → Still excellent and widely reliable<br><br> \nStable, cheap, widely supported.<br> \nHTTP/3 → Enable it everywhere you can<br><br> \n(Cloudflare, CloudFront, Fastly, Akamai, Bunny — all support it now)<br> \n<strong>Quick Checklist to Move to HTTP/3 (2025)</strong></p> \n \n<p><strong>CDN</strong>  </p> \n \n<ul> \n<li>Cloudflare → Enable QUIC + HTTP/3 \n</li> \n<li>CloudFront → Supported on new distributions \n</li> \n<li>Fastly/Akamai/Bunny → Native support</li> \n</ul> \n \n<p><strong>Self-Hosted</strong>  </p> \n \n<ul> \n<li>Nginx 1.25+ QUIC \n</li> \n<li>Caddy 2.6+ (auto HTTP/3) \n</li> \n<li>Traefik v3 \n</li> \n<li>LiteSpeed / OpenLiteSpeed</li> \n</ul> \n \n<p><strong>Backend</strong>  </p> \n \n<ul> \n<li>Node.js 21+ with QUIC \n</li> \n<li>Go, Rust, Java (Netty) → great QUIC libraries \n</li> \n<li>Python → aioquic or reverse proxy</li> \n</ul> \n \n<p><strong>Final Verdict (2025)</strong><br><br> \nHTTP/1.1 → Legacy tech<br><br> \nHTTP/2 → Today’s safe default<br><br> \nHTTP/3 → Today’s <strong>performance baseline</strong></p> \n \n<p>HTTP/3 isn’t “future tech” anymore - it’s the baseline for fast global apps in 2025.</p> \n \n<p><strong>The question isn’t if you should upgrade…</strong><br><br> \n<strong>It’s how much faster your users will be when you do.</strong></p> \n \n \n \n \n<p><strong>Part of the “Networking for DevOps &amp; SRE – 2025 Edition” series</strong><br><br> \nPart 1 → HTTP/HTTPS/TCP/UDP Foundations<br><br> \nPart 3 → TLS 1.2 vs TLS 1.3 in Production: (drops Next Tuesday 7:30 PM IST)<br> \nSubscribe or follow so you don’t miss it.</p>",
      "summary": "🚀 Networking for DevOps &amp; SRE – 2025 Edition • Part 2/10 \n \nMost teams think they’ve already moved to HTTP/2 or HTTP/3. \n \nBut when we checked real production traffic in 2025, the truth was surprising: \n \nWe pulled protocol stats from our CDN + load balancer logs... \n \n63% of all requests were still hitting us over HTTP/1.1 — mostly from corporate proxies, middleboxes, and legacy devices. \n \n \n \nThat means 2 out of every 3 requests were paying an unnecessary 150–300ms latency tax just becau",
      "publishedAt": "2025-12-02T13:33:40.000Z",
      "author": "Sreekanth Kuruba",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 7.4738602851684925,
      "ingestedAt": "2025-12-02T14:44:03.177Z",
      "tags": [
        "code_review",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a8aa19",
      "title": "ONLYOFFICE updated: AI agents & custom hotkeys in new releases",
      "url": "https://dev.to/onlyoffice/onlyoffice-updated-ai-agents-custom-hotkeys-in-new-releases-hem",
      "content": "<p>We are excited to announce major updates across our ecosystem: ONLYOFFICE DocSpace 3.6 and ONLYOFFICE Docs 9.2. These releases introduce a new layer of intelligent assistance with AI Agents in DocSpace and bring significant productivity enhancements to the editors, including customizable hotkeys and macro recording. Let’s explore what’s new for developers and power users.</p> \n \n<h2> \n   \n   \n  Meet your new AI agents in DocSpace 3.6 \n</h2> \n \n<p>The main highlight of DocSpace 3.6 is the introduction of AI agents, bringing intelligent assistance directly into your collaborative workspace. These agents are designed to help you and your team work faster and more efficiently.</p> \n \n<p>You can set up an AI agent tailored to your specific needs. Once configured, you can interact with it through a dedicated chat interface. Simply ask questions or describe your task, and the agent is ready to help.</p> \n \n<p>The AI can perform a wide range of tasks to support your projects:</p> \n \n<ul> \n<li> \n<strong>Analyze files:</strong> Dive deep into documents to check for accuracy, suggest improvements, or summarize key points.</li> \n<li> \n<strong>Generate content:</strong> Create text, brainstorm ideas, or draft communications based on your prompts.</li> \n<li> \n<strong>Search for information:</strong> Look up information across the web and your own personalized knowledge base.</li> \n<li> \n<strong>Manage your DocSpace:</strong> Organize files, structure rooms, add users, and keep your workspace tidy.</li> \n<li> \n<strong>Invite teammates:</strong> Collaborate with colleagues directly within the AI agent chat.</li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frrftwlc56ouv4irz41gr.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frrftwlc56ouv4irz41gr.png\" alt=\"AI agents in DocSpace\" width=\"800\" height=\"456\"></a></p> \n \n<h3> \n   \n   \n  Getting started with AI agents \n</h3> \n \n<p>Enabling AI agents in your DocSpace is straightforward. A new AI Settings section serves as your central hub for managing all AI-related functionalities.</p> \n \n<p><strong>1. Add an AI provider</strong><br> \nFirst, you'll need to connect an AI provider to power your agents. Make sure you have an API key from your chosen provider. Currently, we support OpenAI, Anthropic, TogetherAI, and OpenRouter, with more options planned for future releases.</p> \n \n<p><strong>2. Enable MCP Server</strong><br> \nNext, enable the MCP (Master Control Program) server. You can activate the ready-to-use ONLYOFFICE DocSpace MCP Server, which empowers AI agents to interact with and manage elements within your DocSpace, like creating rooms or organizing files. You can also connect any other MCP server for enhanced capabilities.</p> \n \n<p><strong>3. Connect web search and knowledge base</strong><br> \nTo expand your AI's capabilities, enable the web search engine. This allows the agent to pull information from the internet. You should also activate the knowledge base, which indexes your documents, allowing the AI to perform intelligent, question-based searches through your own data.</p> \n \n<p><strong>4. Create and manage your first agent</strong><br> \nOnce the setup is complete, you can build your first agent. Give it a name, cover, and tags. You can also provide specific instructions to define its purpose, such as, \"Chats in this room are for discussing our startup project. Please stay on topic.\" You can also set storage quotas for your AI agents to control memory consumption.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftmx7st4vyo4stwvd60y2.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftmx7st4vyo4stwvd60y2.png\" alt=\"Creating an AI agent\" width=\"800\" height=\"456\"></a></p> \n \n<p>When you invite collaborators, you can assign specific roles:</p> \n \n<ul> \n<li> \n<strong>Agent Managers</strong> have full control over settings, users, and the shared result space.</li> \n<li> \n<strong>Content Creators</strong> can edit files, upload knowledge base content, and view results.</li> \n<li> \n<strong>Viewers</strong> have read-only access to the shared result space.</li> \n</ul> \n \n<p>All AI-generated files can be saved to a dedicated Result Storage space, where you can continue editing or share them. The Chat History lets you revisit previous prompts and results at any time.</p> \n<h2> \n   \n   \n  What's new in ONLYOFFICE Docs 9.2? \n</h2> \n \n<p>Beyond the platform-level AI in DocSpace, ONLYOFFICE Docs 9.2 introduces powerful features directly into the editors, focusing on productivity and customization.</p> \n<h3> \n   \n   \n  AI-powered grammar &amp; spelling \n</h3> \n \n<p>The <a href=\"https://www.onlyoffice.com/ai-assistants\">AI plugin</a> now includes integrated spell and grammar checking. To use it, simply navigate to the AI tab and select Grammar &amp; Spelling. The AI analyzes your text and provides suggestions with explanations, which you can accept or reject. It's a quick way to ensure your documents are polished and professional.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F4f2iqluzedbk1ydds7gs.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F4f2iqluzedbk1ydds7gs.png\" alt=\"AI-powered grammar checking\" width=\"800\" height=\"456\"></a></p> \n<h3> \n   \n   \n  Customizable keyboard shortcuts \n</h3> \n \n<p>For many developers, an efficient workflow relies on keyboard shortcuts. You can now customize these shortcuts to match your personal preferences. Go to the File tab, open Advanced Settings, and configure your preferred key combinations for a truly personalized editing experience.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fw5egb8yg18d4trwbzgzu.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fw5egb8yg18d4trwbzgzu.png\" alt=\"Customizable keyboard shortcuts\" width=\"800\" height=\"456\"></a></p> \n<h3> \n   \n   \n  Record actions as macros \n</h3> \n \n<p>Repetitive tasks can slow down your progress. With the new macro recording feature, you can automate these actions. Simply record a sequence of actions, save it as a macro, and run it whenever you need to perform that task again. This is a powerful way to streamline your work and save valuable time.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fyu2n50iuvm59rjhlioo4.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fyu2n50iuvm59rjhlioo4.png\" alt=\"Record actions as macros\" width=\"800\" height=\"456\"></a></p> \n<h2> \n   \n   \n  Other improvements in Docs and DocSpace \n</h2> \n \n<ul> \n<li> \n<strong>Enhanced PDF redaction:</strong> You can now customize the color used for the Redact option in the PDF Editor, giving you more visual control when preparing documents for sharing.</li> \n<li> \n<strong>Improved form creation:</strong> Add descriptive text labels to checkboxes and radio buttons. You can also assign specific roles to fields when inserting them into a form, ensuring they are correctly tailored to different users.</li> \n<li> \n<strong>New format support:</strong> DocSpace 3.6 adds support for the HML format and allows you to convert presentations to TXT files.</li> \n<li> \n<strong>Redesigned data import:</strong> The data import tool has been overhauled for a smoother and more intuitive experience when migrating your files.</li> \n</ul> \n<h2> \n   \n   \n  Get the latest versions \n</h2> \n \n<p><iframe allowfullscreen=\"allowfullscreen\" width=\"710\" height=\"399\" src=\"https://www.inoreader.com/yt-embed/?v=mwdKgLTNeRI\" referrerpolicy=\"strict-origin-when-cross-origin\" style=\"width:100%;aspect-ratio:16/9;height:auto;display:block;border:0;\"> \n</iframe> \n</p> \n \n<p>These updates are designed to make your work more intelligent, efficient, and customized. The new AI agents in DocSpace 3.6 provide a collaborative assistant for your projects, while the features in Docs 9.2, like customizable hotkeys and macro recording, offer significant productivity gains.</p> \n \n<p>The latest updates are already available in the cloud. You can sign into your DocSpace to try all the new features or <a href=\"https://www.onlyoffice.com/docspace-registration\">create a free account</a> if you are new to ONLYOFFICE. <a href=\"https://www.onlyoffice.com/download\">Self-hosted builds</a> are also available.</p>",
      "summary": "We are excited to announce major updates across our ecosystem: ONLYOFFICE DocSpace 3.6 and ONLYOFFICE Docs 9.2. These releases introduce a new layer of intelligent assistance with AI Agents in DocSpace and bring significant productivity enhancements to the editors, including customizable hotkeys and macro recording. Let’s explore what’s new for developers and power users. \n \n \n   \n   \n  Meet your new AI agents in DocSpace 3.6 \n \n \nThe main highlight of DocSpace 3.6 is the introduction of AI agen",
      "publishedAt": "2025-12-02T13:31:04.000Z",
      "author": "Kseniya Fedoruk",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 17.93495149586422,
      "ingestedAt": "2025-12-02T14:44:03.177Z",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "governance",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a8aa1c",
      "title": "HTML Invokers: The Coolest API You Aren’t Using Yet",
      "url": "https://dev.to/web_dev-usman/html-invokers-the-coolest-api-you-arent-using-yet-35nl",
      "content": "<p>Invokers are the coolest HTML API that you aren’t using. This is right about to drop in all major browsers. It’s available in Safari Technology Preview and Firefox and Chrome already. It is so cool.</p> \n \n<p>I want you to support my original content as well please, below is the link: <br> \n<a href=\"https://pixicstudio.medium.com/html-invokers-the-coolest-api-you-arent-using-yet-e78c3ddee927\">HTML Invokers: The Coolest API You Aren’t Using Yet</a></p> \n \n<p><strong>So, what are invokers?</strong></p> \n<h2> \n   \n   \n  The Big Idea \n</h2> \n \n<p>Invokers let your buttons actually do stuff without JavaScript. Like, at all. You just tell the button what element to control and what to do with it. That’s the whole thing.</p> \n \n<p>It’s wild that this hasn’t existed until now.</p> \n \n<p><iframe allowfullscreen=\"allowfullscreen\" height=\"600\" src=\"https://codepen.io/web-strategist/embed/vEGrYbZ?height=600&amp;default-tab=result&amp;embed-version=2\"> \n</iframe> \n</p> \n \n<h2> \n   \n   \n  <strong>How We’ve Been Doing It (The Old Way)</strong> \n</h2> \n \n<p>Right now, if you want a button to open a dialog, you’re writing code like this:<br> \n</p> \n \n<div> \n<pre><code>const button = document.querySelector('#open-button'); \nconst dialog = document.querySelector('#my-dialog'); \n</code></pre> \n \n</div> \n \n \n \n \n \n<div> \n<pre><code>button.addEventListener('click', () =&gt; { \n  dialog.showModal(); \n}); \n</code></pre> \n \n</div> \n \n \n \n<p>It works, but come on. That’s way too much ceremony for “button opens thing.”</p> \n \n<h2> \n   \n   \n  <strong>How Invokers Work (The New Way)</strong> \n</h2> \n \n<p>You add two attributes to your button, and you’re done:<br> \n</p> \n \n<div> \n<pre><code>&lt;button commandfor=\"my-dialog\" command=\"show-modal\"&gt; \n  Open Dialog \n&lt;/button&gt; \n</code></pre> \n \n</div> \n \n \n \n \n \n<div> \n<pre><code>&lt;dialog id=\"my-dialog\"&gt; \n  &lt;h2&gt;Welcome!&lt;/h2&gt; \n  &lt;p&gt;This dialog opened without a single line of JavaScript.&lt;/p&gt; \n  &lt;button commandfor=\"my-dialog\" command=\"close\"&gt; \n    Close This \n  &lt;/button&gt; \n&lt;/dialog&gt; \n</code></pre> \n \n</div> \n \n \n \n<p>That’s it. The button just knows what to do. No JavaScript file. No event listeners, no querying the DOM. It’s beautiful.</p> \n \n<h2> \n   \n   \n  <strong>A Real-World Example</strong> \n</h2> \n \n<p>Here’s something you’d actually build — a settings menu with a popover:<br> \n</p> \n \n<div> \n<pre><code>&lt;button commandfor=\"settings-menu\" command=\"toggle-popover\"&gt; \n  ⚙️ Settings \n&lt;/button&gt; \n</code></pre> \n \n</div> \n \n \n \n \n \n<div> \n<pre><code>&lt;div id=\"settings-menu\" popover&gt; \n  &lt;h3&gt;Settings&lt;/h3&gt; \n  &lt;label&gt; \n    &lt;input type=\"checkbox\"&gt; Dark Mode \n  &lt;/label&gt; \n  &lt;label&gt; \n    &lt;input type=\"checkbox\"&gt; Notifications \n  &lt;/label&gt; \n  &lt;label&gt; \n    &lt;input type=\"checkbox\"&gt; Auto-save \n  &lt;/label&gt; \n  &lt;button commandfor=\"settings-menu\" command=\"hide-popover\"&gt; \n    Done \n  &lt;/button&gt; \n&lt;/div&gt; \n</code></pre> \n \n</div> \n \n \n \n<p>Click the settings button, the menu pops up. Click Done, it closes. Zero JavaScript required. This used to take 20+ lines of code.</p> \n \n<h2> \n   \n   \n  <strong>The Attributes</strong> \n</h2> \n \n<p>There are two attributes that make this magic happen:</p> \n \n<p>`<code> - This points to the ID of whatever you want to control. So if you want to control a dialog with </code>id=\"my-dialog\"<code>, you write </code>commandfor=\"my-dialog\"`.</p> \n \n<p>`<code> - This is what you want to happen. Like </code>show-modal<code> to open a dialog, or </code>close<code> to close it, or </code>toggle-popover` for popovers.</p> \n \n<p>That’s literally all you need to know.</p> \n \n<h2> \n   \n   \n  <strong>What You Can Control</strong> \n</h2> \n \n<p>Right now, you can control:</p> \n \n<p><strong>Dialogs: </strong><code>show-modal</code> and <code>close</code> work right out of the box. Finally.</p> \n \n<p><strong>Popovers: </strong><code>show-popover</code>, <code>hide-popover</code>, and <code>toggle-popover</code>. Super clean.</p> \n \n<p><strong>Details Elements: </strong><code>open</code>, <code>close</code>, and <code>toggle</code> for accordion-style content.</p> \n \n<p>And they’re adding more. File inputs, video controls, all that stuff is coming.</p> \n \n<h2> \n   \n   \n  <strong>Custom Commands Are Cool Too</strong> \n</h2> \n \n<p>You can make up your own commands. They just need to start with two dashes:<br> \n</p> \n \n<div> \n<pre><code>&lt;button commandfor=\"photo\" command=\"--flip-horizontal\"&gt; \n  Flip Photo \n&lt;/button&gt; \n</code></pre> \n \n</div> \n \n \n \n \n \n<div> \n<pre><code>&lt;button commandfor=\"photo\" command=\"--rotate-90\"&gt; \n  Rotate 90° \n&lt;/button&gt;&lt;img id=\"photo\" src=\"vacation.jpg\" alt=\"Beach photo\"&gt; \n</code></pre> \n \n</div> \n \n \n \n<p>Then you write a little JavaScript to listen for these custom commands and do whatever you want:<br> \n</p> \n \n<div> \n<pre><code>document.getElementById('photo').addEventListener('command', (e) =&gt; { \n  if (e.command === '--flip-horizontal') { \n    e.target.style.transform = 'scaleX(-1)'; \n  } else if (e.command === '--rotate-90') { \n    e.target.style.transform = 'rotate(90deg)'; \n  } \n}); \n</code></pre> \n \n</div> \n \n \n \n<p>But the whole click-handling setup is already done for you, which is nice.</p> \n \n<h2> \n   \n   \n  <strong>Another Solid Example: Image Gallery</strong> \n</h2> \n \n<p>Here’s a complete image gallery with next/previous buttons:<br> \n</p> \n \n<div> \n<pre><code>&lt;div class=\"gallery\"&gt; \n  &lt;button commandfor=\"gallery-dialog\" command=\"show-modal\"&gt; \n    📷 View Gallery \n  &lt;/button&gt; \n&lt;/div&gt; \n</code></pre> \n \n</div> \n \n \n \n \n \n<div> \n<pre><code>&lt;dialog id=\"gallery-dialog\"&gt; \n  &lt;div class=\"gallery-content\"&gt; \n    &lt;img id=\"current-image\" src=\"photo-1.jpg\" alt=\"Gallery photo\"&gt; \n \n    &lt;div class=\"controls\"&gt; \n      &lt;button commandfor=\"current-image\" command=\"--previous\"&gt; \n        ← Previous \n      &lt;/button&gt; \n      &lt;button commandfor=\"current-image\" command=\"--next\"&gt; \n        Next → \n      &lt;/button&gt; \n      &lt;button commandfor=\"gallery-dialog\" command=\"close\"&gt; \n        ✕ Close \n      &lt;/button&gt; \n    &lt;/div&gt; \n  &lt;/div&gt; \n&lt;/dialog&gt; \n</code></pre> \n \n</div> \n \n \n \n<p>The dialog opens and closes with built-in commands. The next/previous buttons use custom commands that you’d wire up with a few lines of JavaScript. But all the button-clicking mechanics? Already handled.</p> \n \n<h2> \n   \n   \n  <strong>Browser Support Right Now</strong> \n</h2> \n \n<p>Here’s where things stand:</p> \n \n<ul> \n<li> \n<strong>Chrome Canary 134+</strong> Turn on the experimental flag</li> \n<li> \n<strong>Firefox Nightly 135+</strong> Enable the flag</li> \n<li> \n<strong>Safari Technology Preview </strong>Flag it on</li> \n</ul> \n \n<p>It’s in the experimental phase, but it’s moving fast. This should be available everywhere by mid-2025.</p> \n \n<h2> \n   \n   \n  <strong>Why This Is Actually a Big Deal</strong> \n</h2> \n \n<p>Look, this isn’t changing the world or anything. But it’s fixing something that’s been annoying for years.</p> \n \n<p>Buttons controlling other elements is foundational web stuff. We shouldn’t need to import libraries or write boilerplate for it. HTML should just handle it. And now it does.</p> \n \n<p>Plus, when stuff is built into the platform, accessibility comes for free. Screen readers work better with it. Keyboard navigation just works. You don’t have to remember to add all the ARIA attributes.</p> \n \n<h2> \n   \n   \n  <strong>Real Talk</strong> \n</h2> \n \n<p>This is one of those features that makes you go “wait, this wasn’t already a thing?” Because it feels so obvious once you see it.</p> \n \n<p>The web platform is finally catching up to what we’ve been doing with JavaScript frameworks for years. And that’s great because it means less JavaScript to ship, less code to maintain, and websites that just work better.</p> \n \n<p>Invokers are simple, they’re elegant, and they’re about to be everywhere. Get ready to delete a bunch of event listeners.</p> \n \n<h2> \n   \n   \n  <strong>The Bottom Line</strong> \n</h2> \n \n<p>HTML Invokers let buttons control stuff without JavaScript. Two attributes. No libraries. It’s clean, it’s simple, and it’s coming to browsers near you.</p> \n \n<p>This is the kind of improvement that makes web development just a little bit more fun. And honestly? We could use more of that.</p> \n \n<blockquote> \n<p><strong><em>Did you learn something good today as a developer?</em></strong><br> \n<em>Then show some love.</em><br> \n<em>© </em><a href=\"https://www.linkedin.com/m/in/muhammad-usman-strategist/\"><em>Muhammad Usman</em></a><br> \n<strong><em>WordPress Developer | Website Strategist | SEO Specialist</em></strong><br> \n<em>Don’t forget to subscribe to <a href=\"https://developersjourney.substack.com/\">Developer’s Journey</a> to show your support.</em></p> \n</blockquote> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmdae1flujul96o4a41v1.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmdae1flujul96o4a41v1.png\" alt=\"\" width=\"800\" height=\"266\"></a></p>",
      "summary": "Invokers are the coolest HTML API that you aren’t using. This is right about to drop in all major browsers. It’s available in Safari Technology Preview and Firefox and Chrome already. It is so cool. \n \nI want you to support my original content as well please, below is the link:  \nHTML Invokers: The Coolest API You Aren’t Using Yet \n \nSo, what are invokers? \n \n   \n   \n  The Big Idea \n \n \nInvokers let your buttons actually do stuff without JavaScript. Like, at all. You just tell the button what el",
      "publishedAt": "2025-12-02T13:29:44.000Z",
      "author": "Muhammad Usman",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.47976163356776,
      "ingestedAt": "2025-12-02T14:44:03.178Z",
      "tags": [
        "code_review",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a8aa1e",
      "title": "JAI Router - lightweight Spring Boot routing starter (open for contributors)",
      "url": "https://dev.to/rrezart_prebreza_60ef7bfb/jai-router-lightweight-spring-boot-routing-starter-open-for-contributors-379b",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fud543o4omm203fzsd7rd.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>JAI Router is a minimal Spring Boot starter that centralizes dynamic route registration and exposes a small admin API for runtime routing. The project is ready for contributors — see CONTRIBUTING.md and issues labeled good first issue and help wanted.</p>  \n  \n<p>Problem</p>  \n  \n<ul>  \n<li>Dynamic route registration is often scattered across services and hard to test.</li>  \n<li>Existing solutions introduce heavy dependencies or tightly couple to framework internals.</li>  \n</ul>  \n  \n<p>What JAI Router does</p>  \n  \n<ul>  \n<li>Provides a small, testable API to register and manage routes at runtime.</li>  \n<li>Integrates as a Spring Boot starter with minimal dependencies.</li>  \n<li>Keeps route storage and dispatching modular and easy to mock in tests.</li>  \n</ul>  \n  \n<p>Key features</p>  \n  \n<ul>  \n<li>Runtime route registration and removal.</li>  \n<li>Pluggable route matching and handler resolution.</li>  \n<li>Small API surface suitable for microservices and integration tests  \n.</li>  \n</ul>  \n  \n<p>Quickstart</p>  \n  \n<ul>  \n<li>Clone the repo: git clone hhttps://github.com/JAI-create-spec/JAI-Router/tree/develop</li>  \n<li>Run locally: ./gradlew bootRun (Java 17)</li>  \n<li>See README.md for quick-start snippets and example endpoints.</li>  \n</ul>  \n  \n<p>Example usage<br>  \nRegister a route via the admin API and validate it with an integration test. See README.md for concrete request and response examples.</p>  \n  \n<p>How to contribute<br>  \nRead CONTRIBUTING.md for setup, tests, and branch conventions.</p>  \n  \n<p>Pick a starter issue labeled good first issue or help wanted.</p>  \n  \n<p>Open small PRs with tests; maintainers aim to review quickly.</p>  \n  \n<p>Repo and license<br>  \nRepository: <a href=\"https://github.com/JAI-create-spec/JAI-Router/tree/develop\">https://github.com/JAI-create-spec/JAI-Router/tree/develop</a></p>  \n  \n<p>License: see LICENSE</p>  \n  \n<p>Call to action<br>  \nIf you work on Spring Boot routing or infrastructure, try the quickstart and pick a starter issue — contributions are welcome.</p>",
      "summary": "JAI Router is a minimal Spring Boot starter that centralizes dynamic route registration and exposes a small admin API for runtime routing. The project is ready for contributors — see CONTRIBUTING.md and issues labeled good first issue and help wanted.  \n  \nProblem  \n  \n  \nDynamic route registration is often scattered across services and hard to test.  \nExisting solutions introduce heavy dependencies or tightly couple to framework internals.  \n  \n  \nWhat JAI Router does  \n  \n  \nProvides a small, ",
      "publishedAt": "2025-12-02T13:25:07.000Z",
      "author": "Rrezart Prebreza",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "pricing_business",
      "score": 10.458967726921815,
      "ingestedAt": "2025-12-02T14:44:03.178Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "testing",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a8aa20",
      "title": "🚀 ProblemPad — The Game-Changing Platform Built for Real Problems, Real Voices, Real Solutions 🎤⚡",
      "url": "https://dev.to/puneetkumar2010/problempad-the-game-changing-platform-built-for-real-problems-real-voices-real-solutions-1ei5",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fj2je6vtxynw1u1o25xpn.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>Hey dev fam 👋<br><br>  \nToday I’m dropping something wild — a project built straight from real-world needs, tested in the field, and wrapped in clean Node.js engineering.</p>  \n  \n<p>Introducing <strong>ProblemPad</strong>: a community-powered problem-solving platform designed for neighborhoods, societies, and professional workers.<br><br>  \nThink of it like a digital notice board + service marketplace + community hub… all merged into one clean experience.  </p>  \n  \n<p>And yes — it’s built with <strong>zero bloat</strong>, <strong>no GridFS</strong>, and an audio system running on pure MongoDB binary power. ⚡</p>  \n  \n<p>GitHub Repo 👉 <strong><a href=\"https://github.com/DeveloperPuneet/ProblemPad\">https://github.com/DeveloperPuneet/ProblemPad</a></strong></p>  \n  \n  \n<h2>  \n    \n    \n  🌟 What ProblemPad Solves  \n</h2>  \n  \n<p>Let’s be honest — everyone has daily issues:</p>  \n  \n<ul>  \n<li>Electrical faults ⚡  \n</li>  \n<li>Plumbing breakdowns 💧  \n</li>  \n<li>Device malfunctions 🔌  \n</li>  \n<li>Community chaos 🏘️  \n</li>  \n</ul>  \n  \n<p>But the real chaos?<br><br>  \n<em>No unified place to report these issues and get them solved quickly.</em></p>  \n  \n<p>ProblemPad fixes that with one simple idea:</p>  \n  \n<blockquote>  \n<p><strong>Let communities report problems, and let workers respond instantly.</strong></p>  \n</blockquote>  \n  \n<p>A platform where:</p>  \n  \n<ul>  \n<li>Users post issues  \n</li>  \n<li>Workers provide solutions  \n</li>  \n<li>Communities stay organized  \n</li>  \n<li>Everyone saves time  \n</li>  \n</ul>  \n  \n  \n<h2>  \n    \n    \n  🧩 Core Features (The Fun Stuff)  \n</h2>  \n<h3>  \n    \n    \n  👥 User Accounts  \n</h3>  \n  \n<ul>  \n<li>Mobile-based registration  \n</li>  \n<li>Two roles: <strong>Users</strong> &amp; <strong>Service Workers</strong>  \n</li>  \n<li>Location-based community discovery  \n</li>  \n<li>Skill-tagged worker profiles  \n</li>  \n</ul>  \n<h3>  \n    \n    \n  🏘️ Community System  \n</h3>  \n  \n<ul>  \n<li>Create communities for buildings/neighborhoods  \n</li>  \n<li>Invite users by phone  \n</li>  \n<li>Real-time notifications for new problems  \n</li>  \n<li>Member and role management  \n</li>  \n</ul>  \n<h3>  \n    \n    \n  🆘 Problem Reporting  \n</h3>  \n  \n<ul>  \n<li>Text-based issue reporting  \n</li>  \n<li>  \n<p><strong>Audio Reporting</strong> 🎤  </p>  \n  \n<ul>  \n<li>Uses MediaRecorder API  \n</li>  \n<li>Encoded as Base64  \n</li>  \n<li>Stored directly in MongoDB as Binary/BSON  \n</li>  \n<li>No GridFS. No bulky pipelines. Clean and fast.</li>  \n</ul>  \n</li>  \n<li><p>Categorized issues (electrical, technical, plumbing, etc.)</p></li>  \n</ul>  \n<h3>  \n    \n    \n  🔧 Problem Resolution  \n</h3>  \n  \n<ul>  \n<li>Workers get instant alerts  \n</li>  \n<li>Can provide solution remarks  \n</li>  \n<li>User confirms the fix  \n</li>  \n<li>Rating system to validate worker quality  \n</li>  \n</ul>  \n<h3>  \n    \n    \n  🔔 Notifications  \n</h3>  \n  \n<ul>  \n<li>New problem notifications  \n</li>  \n<li>Community invites  \n</li>  \n<li>Solution updates  \n</li>  \n<li>Auto-cleanup alerts  \n</li>  \n</ul>  \n  \n  \n<h2>  \n    \n    \n  🧹 Auto-Cleanup System (Your DB Will Thank You)  \n</h2>  \n  \n<p>A literal lifesaver:</p>  \n  \n<ul>  \n<li>Solved + confirmed problems auto-delete after <strong>30 days</strong>  \n</li>  \n<li>Runs via daily cron job  \n</li>  \n<li>Audio + problem data gets cleaned  \n</li>  \n<li>Keeps MongoDB lean and happy  \n</li>  \n</ul>  \n<div>  \n<pre><code><span>cron</span><span>.</span><span>schedule</span><span>(</span><span>\"</span><span>0 2 * * *</span><span>\"</span><span>,</span> <span>cleanupFunction</span><span>);</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n<p>Set it and forget it. 🛏️</p>  \n  \n  \n<h2>  \n    \n    \n  🔊 Audio Storage — No GridFS Needed  \n</h2>  \n  \n<p>This is where ProblemPad goes brrrrr ⚡</p>  \n  \n<p><strong>How audio is handled:</strong></p>  \n  \n<ol>  \n<li>Record using browser’s <strong>MediaRecorder</strong>  \n</li>  \n<li>Convert to Base64  \n</li>  \n<li>Store as Buffer/Binary inside the Problem document  \n</li>  \n<li>Serve via a <code>/problem-audio/:id</code> route  \n</li>  \n</ol>  \n  \n<p>It’s lightweight, effective, and perfect for short community audio clips.</p>  \n  \n  \n<h2>  \n    \n    \n  🛠️ Tech Stack (Simple but Powerful)  \n</h2>  \n<h3>  \n    \n    \n  Backend  \n</h3>  \n  \n<ul>  \n<li>Node.js + Express  \n</li>  \n<li>MongoDB + Mongoose  \n</li>  \n<li>express-session auth  \n</li>  \n<li>node-cron  \n</li>  \n<li>randomstring for unique IDs  \n</li>  \n</ul>  \n<h3>  \n    \n    \n  Frontend  \n</h3>  \n  \n<ul>  \n<li>Pug Templates  \n</li>  \n<li>Vanilla JS  \n</li>  \n<li>Responsive UI  \n</li>  \n<li>Dark-mode theme  \n</li>  \n</ul>  \n<h3>  \n    \n    \n  Real-Time  \n</h3>  \n  \n<ul>  \n<li>Socket.io for instant sync  \n</li>  \n</ul>  \n  \n  \n<h2>  \n    \n    \n  📦 Installation  \n</h2>  \n  \n  \n<div>  \n<pre><code>git clone https://github.com/DeveloperPuneet/ProblemPad.git  \n<span>cd </span>ProblemPad  \nnpm <span>install</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n<p>Create <code>.env</code>:<br>  \n</p>  \n  \n<div>  \n<pre><code>SESSION_SECRET=yourkey  \nMONGODB_URI=mongodb+srv://...  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Run:<br>  \n</p>  \n  \n<div>  \n<pre><code>npm start  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Open:<br><br>  \n<code>http://localhost:3000</code></p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  🗂️ Project Structure  \n</h2>  \n  \n  \n  \n<div>  \n<pre><code>ProblemPad/  \n│── controllers/  \n│── models/  \n│── routes/  \n│── views/  \n│── utils/  \n├── public/  \n└── app.js  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Clean, modular, and scalable.</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  🧪 Why This Project Matters  \n</h2>  \n  \n<p>ProblemPad isn’t just “another MERN project.”</p>  \n  \n<p>It’s built for <strong>real people</strong>, with <strong>real needs</strong>, and solves <strong>real community problems</strong>.</p>  \n  \n<p>This platform can be used by:</p>  \n  \n<ul>  \n<li>RWAs  \n</li>  \n<li>Colonies  \n</li>  \n<li>Apartment complexes  \n</li>  \n<li>Small towns  \n</li>  \n<li>Service workers  \n</li>  \n<li>Maintenance teams  \n</li>  \n</ul>  \n  \n<p>Even defense housing communities 👀<br><br>  \n(Yes — it was originally built for one.)</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  🔮 Future Upgrades  \n</h2>  \n  \n<ul>  \n<li>Mobile app (React Native / Flutter)  \n</li>  \n<li>Payment gateway for premium services  \n</li>  \n<li>Worker badges &amp; ranking  \n</li>  \n<li>Real-time chat  \n</li>  \n<li>Advanced analytics  \n</li>  \n</ul>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  💬 Final Thoughts  \n</h2>  \n  \n<p>ProblemPad isn’t just software — it’s community empowerment.<br><br>  \nA tool for people to help each other, grow together, and solve problems with speed and clarity.</p>  \n  \n<p>If you vibe with the mission:</p>  \n  \n<p>⭐ <strong>Star the repo</strong><br><br>  \n🍴 <strong>Fork it</strong><br><br>  \n🛠️ <strong>Contribute</strong><br><br>  \n💬 <strong>Drop feedback</strong>  </p>  \n  \n<p>Made with ❤️ in India<br><br>  \n<em>By DeveloperPuneet</em></p>",
      "summary": "Hey dev fam 👋  \nToday I’m dropping something wild — a project built straight from real-world needs, tested in the field, and wrapped in clean Node.js engineering.  \n  \nIntroducing ProblemPad: a community-powered problem-solving platform designed for neighborhoods, societies, and professional workers.  \nThink of it like a digital notice board + service marketplace + community hub… all merged into one clean experience.    \n  \nAnd yes — it’s built with zero bloat, no GridFS, and an audio system ru",
      "publishedAt": "2025-12-02T13:23:41.000Z",
      "author": "Puneet-Kumar2010",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 9.960213469415,
      "ingestedAt": "2025-12-02T14:44:03.178Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a8aa22",
      "title": "New Opportunity - Growth Partner For Software Business",
      "url": "https://dev.to/passionmuse16/new-opportunity-growth-partner-for-software-business-aa1",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbwhynffrqfnd1wgm3fkk.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>Warm greetings,</p>  \n  \n<p>My name is Takeshi, a senior software engineer based in Japan leading a small development team focused on Web and AI applications. We’re now preparing to expand into the US software market.</p>  \n  \n<p>We’re looking for a partner who can help us secure software jobs or contracts by engaging with clients and managing the interview, while my team handles all job applications, the entire development process, and delivery.</p>  \n  \n<p><strong>What You Gain</strong><br>  \nRevenue: A defined percentage income from each successful project<br>  \nFlexibility: No schedule restrictions - continue your lifestyle<br>  \nSupport: Managing All technical operations by my team<br>  \nOptional: Supporting your company or projects under a subcontracting arrangement at competitive rates</p>  \n  \n<p><strong>Requirements</strong><br>  \nNative or bilingual English proficiency<br>  \nUS work authorization<br>  \n3 years of software development or related experience<br>  \nStrong ability to communicate</p>  \n  \n<p>If this sound like a good fit for you, please schedule a short call using the following link: Calendly Link<br>  \nThank you for your reading, and hope to talk soon.</p>  \n  \n<p>Best regards,<br>  \nTakeshi</p>  \n  \n<p>Senior Software Engineer | Team Manager<br>  \nEmail: <a href=\"mailto:passionmuse16@gmail.com\">passionmuse16@gmail.com</a><br>  \nGitHub: <a href=\"https://github.com/sven3270350\">https://github.com/sven3270350</a></p>",
      "summary": "Warm greetings,  \n  \nMy name is Takeshi, a senior software engineer based in Japan leading a small development team focused on Web and AI applications. We’re now preparing to expand into the US software market.  \n  \nWe’re looking for a partner who can help us secure software jobs or contracts by engaging with clients and managing the interview, while my team handles all job applications, the entire development process, and delivery.  \n  \nWhat You Gain  \nRevenue: A defined percentage income from ",
      "publishedAt": "2025-12-02T13:21:05.000Z",
      "author": "Takeshi Kato",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.481518050406939,
      "ingestedAt": "2025-12-02T14:44:03.178Z",
      "tags": [
        "code_review",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a8aa24",
      "title": "[AWS] DevTools Evangelism CDK Edition",
      "url": "https://dev.to/aws-builders/aws-devtools-evangelism-cdk-edition-4bih",
      "content": "<p>This article is a machine translation of the contents of the following URL, which I wrote in Japanese:</p> \n \n<p><a href=\"https://qiita.com/Nana_777/items/f5c2366d092226179477\">https://qiita.com/Nana_777/items/f5c2366d092226179477</a></p> \n \n<h2> \n   \n   \n  Introduction \n</h2> \n \n<p>This is the fourth post in the Japan AWS Top Engineers Advent Calendar 2025.</p> \n \n<p>This time, we'll discuss the AWS CDK, a commonly used tool for writing IaC code.</p> \n \n<p>LoudFormation, another AWS IaC tool, is easy to use with any text editor, but it can lead to redundant code. While AWS CDK is a bit more difficult to set up, it allows you to write IaC code efficiently with less code.</p> \n \n<p>This article explains how to set up and deploy the AWS CDK environment.</p> \n \n<p>↓ Click here for the Japan AWS Top Engineers Advent Calendar 2025</p> \n \n<p><a href=\"https://qiita.com/advent-calendar/2025/aws-top-engineers\">https://qiita.com/advent-calendar/2025/aws-top-engineers</a></p> \n \n<h2> \n   \n   \n  What is the AWS CDK? \n</h2> \n \n<p>The AWS Cloud Development Kit (CDK) is a tool for creating Infrastructure as Code (IaC) code that codifies infrastructure configuration.</p> \n \n<h3> \n   \n   \n  Benefits of IaC \n</h3> \n \n<h4> \n   \n   \n  Reduced manual configuration errors and repetitive tasks \n</h4> \n \n<p>Infrastructure configuration can also be defined using the AWS Management Console. However, repeatedly configuring each service manually is time-consuming and introduces the risk of configuration errors.<br> \nBy codifying infrastructure configuration, the same code can be reused repeatedly, eliminating manual configuration errors and hassle.</p> \n<h4> \n   \n   \n  Ease of sharing and version control through codification \n</h4> \n \n<p>Codifying infrastructure configuration allows it to be shared within a team as a single block of code.<br> \nIn addition, managing it with a version control tool makes it easy to manage differences.<br> \nFurthermore, it also makes it possible to test whether the code is correctly defined.<br> \n↓ A rough overview can be seen in the following image.<br> \n<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft0ue1qpb2ds6hu0whrot.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft0ue1qpb2ds6hu0whrot.png\" alt=\"image.png\" width=\"800\" height=\"436\"></a></p> \n<h3> \n   \n   \n  Differences between CloudFormation and CDK \n</h3> \n \n<p>CloudFormation is another AWS IaC tool.<br> \nWhile it allows for detailed definitions, the detailed nature of the IaC code can make it very long.<br> \nThis can lead to issues such as increased code writing time and reduced readability.<br> \nCDK achieves abstraction through constructs. By automatically configuring some definitions through abstraction, IaC code can be written with fewer lines.</p> \n \n<p>↓ A rough overview of the process looks like this:<br> \n<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F6brfg6o4nl90dcxql5ou.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F6brfg6o4nl90dcxql5ou.png\" alt=\"image.png\" width=\"800\" height=\"436\"></a></p> \n<h2> \n   \n   \n  Initializing AWS CDK \n</h2> \n<h3> \n   \n   \n  Creating a Project \n</h3> \n \n<p>Create a folder for your project and navigate to that folder hierarchy.<br> \n</p> \n \n<div> \n<pre><code>mkdir my-project \ncd my-project \n</code></pre> \n \n</div> \n \n \n \n<p>To initialize your AWS CDK project, run \"cdk init app.\"<br> \nIn this example, we're specifying Typescript as the programming language, so we're specifying \"--language typescript\" as the option.<br> \n</p> \n \n<div> \n<pre><code>cdk init app --language typescript \n</code></pre> \n \n</div> \n \n \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fi7wrpdenervt0hlu9rcn.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fi7wrpdenervt0hlu9rcn.png\" alt=\"image.png\" width=\"735\" height=\"310\"></a></p> \n \n<h2> \n   \n   \n  AWS CDK Implementation Example \n</h2> \n \n<p>:::note warn<br> \nTest Environment<br> \nI used Kiro (IDE) on a Windows PC.<br> \n:::</p> \n<h3> \n   \n   \n  Implementing an API using API Gateway, Lambda, and S3 \n</h3> \n<h4> \n   \n   \n  The implementation was done using AmazonQDeveloper and Kiro with Vibe coding. \n</h4> \n \n<p>This time, I used Kiro to implement the following request: \"Define an API using API Gateway and Lambda. Write the Lambda function in TypeScript. The Lambda function should return a list of files in the S3 bucket.\"<br> \nAfter that, I consulted with Kiro via chat to revise the S3 bucket name and Lambda function name as I went along.</p> \n<h2> \n   \n   \n  Implementation Details \n</h2> \n<h3> \n   \n   \n  Folder Structure \n</h3> \n \n<p>The Lambda function program is created in the lambda folder, and the CDK IaC code is created in the lib folder.<br> \n<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fw8hkee4oize51ddqnoyv.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fw8hkee4oize51ddqnoyv.png\" alt=\"image.png\" width=\"190\" height=\"474\"></a></p> \n<h3> \n   \n   \n  lib Folder \n</h3> \n \n<p>The stack definition is created in the files in this folder.</p> \n<h4> \n   \n   \n  Imports, Classes, and Constructs \n</h4> \n \n<p>The modules required for the stack are imported.<br> \nThe class name and constructor are defined, and the following lines define each resource.<br> \n<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5l755c5lmv201e0z2k5u.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5l755c5lmv201e0z2k5u.png\" alt=\"image.png\" width=\"530\" height=\"200\"></a></p> \n<h4> \n   \n   \n  S3 Bucket Definition \n</h4> \n \n<p>The S3 bucket definition defines the following three items:</p> \n \n<ul> \n<li>bucketName: Fixed bucket name</li> \n<li>If no bucket name is specified, one will be automatically named in the format \"myprojectstack-mybucket-\".</li> \n<li>removalPolicy: DESTROY: Deletes the bucket when the stack is deleted.</li> \n<li>autoDeleteObjects: true: The bucket can be deleted even if it contains files.</li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F52t92j9w5cosf12yw8is.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F52t92j9w5cosf12yw8is.png\" alt=\"image.png\" width=\"512\" height=\"131\"></a></p> \n<h4> \n   \n   \n  Lambda Function Definition \n</h4> \n \n<p>A Lambda function definition defines the following five elements:</p> \n \n<ul> \n<li>functionName: Fixed function name</li> \n<li>If no function name is specified, a name will be automatically generated in the format \"MyProjectStack-ListFilesFunction-\".</li> \n<li>runtime: Uses Node.js 20.x</li> \n<li>handler: Executes the handler function in index.js</li> \n<li>code: Uses the code in the lambda/ directory</li> \n<li>environment: Passes the bucket name as an environment variable</li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fyxqhzlr33f01xe0eodon.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fyxqhzlr33f01xe0eodon.png\" alt=\"image.png\" width=\"598\" height=\"207\"></a></p> \n \n<p>This Lambda function retrieves a list of files in the S3 bucket, so grant read permission to the S3 bucket.<br> \n<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fz0f5jplysdguuefex99h.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fz0f5jplysdguuefex99h.png\" alt=\"image.png\" width=\"362\" height=\"48\"></a></p> \n<h4> \n   \n   \n  API Gateway Definition \n</h4> \n \n<p>This API Gateway is defined to call the Lambda function using the GET method.<br> \n<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fh0e8o7nxjfea5jajyh3o.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fh0e8o7nxjfea5jajyh3o.png\" alt=\"image.png\" width=\"638\" height=\"245\"></a></p> \n<h4> \n   \n   \n  Other Output Upon Deployment Completion \n</h4> \n \n<p>The bucket name and API URL are displayed upon deployment completion.<br> \n<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fendjy1k05kchoevd82ex.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fendjy1k05kchoevd82ex.png\" alt=\"image.png\" width=\"323\" height=\"196\"></a></p> \n<h2> \n   \n   \n  Preparing for Deployment \n</h2> \n<h3> \n   \n   \n  Build (Compiling TypeScript) \n</h3> \n \n<p>Since we're creating a CDK application using TypeScript, compile it using the following command:<br> \n</p> \n \n<div> \n<pre><code>npm run build \n</code></pre> \n \n</div> \n \n \n \n<p>If an import error occurs, as shown in the image below, dependencies have not yet been resolved.<br> \n<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fu20strkoe4dl3j92z25k.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fu20strkoe4dl3j92z25k.png\" alt=\"image.png\" width=\"800\" height=\"83\"></a><br> \nExecute the following command to resolve dependencies and then build again.<br> \n</p> \n \n<div> \n<pre><code>cd lambda \nnpm install \n</code></pre> \n \n</div> \n \n \n \n<p>Verify that the build completed without errors.<br> \n<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxwsqy8ax5e58gc9ah0fq.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxwsqy8ax5e58gc9ah0fq.png\" alt=\"image.png\" width=\"533\" height=\"87\"></a></p> \n<h3> \n   \n   \n  Generate a CloudFormation Template (Recommended) \n</h3> \n \n<p>This step is not required as it is performed automatically during deployment. However, you can generate a CloudFormation template from the CDK code by running the following command.<br> \n</p> \n \n<div> \n<pre><code>npx cdk synth \n</code></pre> \n \n</div> \n \n \n \n<p>By default, the results are displayed in JSON format on the console. However, if you want to save the results as a YAML file, for example, run the following command:<br> \n</p> \n \n<div> \n<pre><code>npx cdk synth --yaml &gt; template.yaml \n</code></pre> \n \n</div> \n \n \n \n<p>The output CloudFormation template is a 428-line file, as shown below.<br> \nThe CDK code (TypeScript) in the lib folder was 58 lines long, so we were able to reduce a significant number of lines, demonstrating its readability.<br> \n![image.png](<a href=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/192949/4025df0b-bd18-456e-b53a-\">https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/192949/4025df0b-bd18-456e-b53a-</a><br> \nCheck the \"AWS Access Key ID\" and \"AWS Secret Access Key\" in the Access Portal screen, and set them as the required variables after executing the following command.<br> \n</p> \n \n<div> \n<pre><code>aws configure \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  Bootstrap (First Time Only) \n</h3> \n \n \n \n<div> \n<pre><code>npx cdk bootstrap \n</code></pre> \n \n</div> \n \n \n \n<p>:::note warn<br> \nIf an authentication error occurs<br> \nIf the aws configure setting does not work, set the \"AWS Access Key ID,\" \"AWS Secret Access Key,\" and \"AWS Session Token\" in the file at the following path and run the Bootstrap command again.<br> \n</p> \n \n<div> \n<pre><code>C:\\Users\\[username]\\.aws\\credentials \n</code></pre> \n \n</div> \n \n \n \n<p>:::<br> \n<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Flk88cm88ea7x7z5xo8j8.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Flk88cm88ea7x7z5xo8j8.png\" alt=\"image.png\" width=\"800\" height=\"138\"></a></p> \n \n<p>If Bootstrap is successful, the CDK's initial S3 bucket and CloudFormation stack will be created.<br> \n<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fyy62g84rskr9lsqg0qvo.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fyy62g84rskr9lsqg0qvo.png\" alt=\"image.png\" width=\"800\" height=\"238\"></a><br> \n<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fz150niot4xltfn1e6jg9.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fz150niot4xltfn1e6jg9.png\" alt=\"image.png\" width=\"782\" height=\"175\"></a></p> \n \n<h3> \n   \n   \n  Deploy execution \n</h3> \n \n \n \n<div> \n<pre><code>npx cdk deploy \n``` \n \n \nA confirmation of the deployment details will be displayed. If there are no problems, enter [y] to approve. \n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/192949/611a879c-8dec-4742-8e01-11c98a050476.png) \n \n## Check the deployment results in the AWS console \nOnce deployment is complete, you can view the stack in the deployed CloudFormation console screen. \n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/192949/bdbc2802-190d-48f3-88c9-e8e16ec1817f.png) \nI checked the CloudFormation template in Infrastructure Composer, and confirmed that the details defined in the CDK were reflected. \n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/192949/403fc978-4133-4b23-b150-b77afe421bde.png) \n \n### Deleting Deployment Content \nTo delete deployed content from your AWS environment, run cdk destroy. \n \n \n``` \nnpx cdk destroy \n``` \n \n \nEnter [y] in response to the confirmation message to delete the content. \nThe deployed project will be deleted, but the S3 bucket and stack created by Bootstrap will not be deleted. \n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/192949/520938ed-ea03-43c7-8a38-11a1faee2899.png) \n \n## Reference \n \n↓ Official AWS CDK documentation \n \nhttps://docs.aws.amazon.com/ja_jp/cdk/v2/guide/home.html \n \n↓ AWS CDK BlackBelt documentation (Japanese) \n \nhttps://pages.awscloud.com/rs/112-TZM-766/images/AWS-Black-Belt_2023_AWS-CDK-Basic-1-Overview_0731_v1.pdf \n</code></pre> \n \n</div>",
      "summary": "This article is a machine translation of the contents of the following URL, which I wrote in Japanese: \n \nhttps://qiita.com/Nana_777/items/f5c2366d092226179477 \n \n \n   \n   \n  Introduction \n \n \nThis is the fourth post in the Japan AWS Top Engineers Advent Calendar 2025. \n \nThis time, we'll discuss the AWS CDK, a commonly used tool for writing IaC code. \n \nLoudFormation, another AWS IaC tool, is easy to use with any text editor, but it can lead to redundant code. While AWS CDK is a bit more diffic",
      "publishedAt": "2025-12-02T13:19:01.000Z",
      "author": "Nao San",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "thought_leadership",
      "score": 10.455803529196903,
      "ingestedAt": "2025-12-02T14:44:03.179Z",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "ide",
        "governance",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a8aa2e",
      "title": "Improve your Prompt Engineering with the help of a little Mermaid",
      "url": "https://dev.to/grimch/improve-your-prompt-engineering-with-the-help-of-a-little-mermaid-2j60",
      "content": "<p><strong>Spoiler Alert:</strong> This article is about <strong>using Mermaid diagrams as a method of re-enforcement in LLM prompting</strong> and <strong>not</strong> about the fary tail <em>The Little Mermaid</em> written by Danish author Hans Christian Anderson.</p> \n \n \n \n \n<h2> \n   \n   \n  Summary \n</h2> \n \n<p>You probably have seen a lot of <a href=\"https://mermaid.js.org/\"><strong>Mermaid Diagrams</strong></a> already without even realizing that what is powering them is a formal description language which is very well understood by LLMs.<br> \n<br><strong>Mermaid syntax</strong> is very good for describing formal, structural, or temporal information like \"A happens, then B, and if X, then C\".<br> It forces clarity and guarantees that the visualization precisely reflects the text definition which makes it a perfect choice for a <strong>prompt as code approach</strong>.</p> \n \n<p>Complementing your text with additional Mermaid code in your prompt will provide the LLM so called \"<strong>Dual-Path Reinforcement</strong>\":</p> \n \n<ul> \n<li> \n<strong>Text</strong> provides the context, intent, nuance, and soft constraints. \n \n<ul> \n<li>It explains the goal of the sequence and the purpose of the participants, </li> \n<li>clarifies ambiguities present in the structural diagram</li> \n<li>and guides the overall interpretation.</li> \n</ul> \n</li> \n<li> \n<strong>Complementary Mermaid Code</strong> provides the strict, non-negotiable logic to ground the text. \n \n<ul> \n<li>It explicitly defines the flow, participants, timing, and conditions (loops, branches) in a way that is less prone to misinterpretation</li> \n<li>and confirms the exact, step-by-step logic, preventing the LLM from \"hallucinating\" or misinterpreting the flow based on ambiguous human language.</li> \n</ul> \n</li> \n</ul> \n \n<p>Once a diagram is defined, you as the prompt engineer will simply <strong>refer to the visualization</strong>, removing the need to re-read the verbose Mermaid code for logical flow. This visual clarity is a significant advantage over other <em>prompt as code</em> approaches like JSON or TypeScript.</p> \n \n \n<h2> \n   \n   \n  Why Mermaid Syntax Helps an LLM \n</h2> \n \n<p>For a Large Language Model (LLM) Mermaid syntax offers several critical advantages when processing structured information:</p> \n \n<ul> \n<li>Natural language descriptions of complex systems (\"A uses B, which calls C unless D is present\") are inherently <strong>ambiguous</strong>. the LLM needs to interpret the context and resolve linguistic relationships.</li> \n<li>Mermaid syntax is a Domain-Specific Language (DSL) that is highly structured and unambiguous. A --&gt; B means A connects to B. This structure immediately tells the LLM the entities and the directed relationship between them, converting a natural languageinterpretation task into a simple, efficient parsing task.</li> \n<li>Mermaid allows the LLMto map the text directly to a well-defined conceptual structure (a graph, a tree, a timeline). This is analogous to how a human can immediately internalize a flowchart versus reading a dense text description.</li> \n</ul> \n \n \n<h2> \n   \n   \n  Example: Describing a Purchase Order Process \n</h2> \n \n<p>To demonstrate the efficiency and structural clarity of adding complementary Mermaid syntax to your LLM prompt, we will analyze a common scenario: a user purchasing an item in an e-commerce system, which involves interaction between three services - <code>Frontend</code>, <code>OrderService</code>, and <code>InventoryService</code>:</p> \n \n<ol> \n<li> User clicks 'Buy' on the <strong>Frontend</strong>.</li> \n<li> <strong>Frontend</strong> sends a request to the <strong>OrderService</strong> to initiate the order.</li> \n<li> <strong>OrderService</strong> checks the stock by asking the <strong>InventoryService</strong>.</li> \n<li> <strong>InventoryService</strong> returns the stock level.</li> \n<li> If stock is available, <strong>OrderService</strong> deducts the stock from <strong>InventoryService</strong>.</li> \n<li> <strong>OrderService</strong> confirms the order back to the <strong>Frontend</strong>.</li> \n<li> <strong>Frontend</strong> shows a success message.</li> \n</ol> \n<h3> \n   \n   \n  Textual Representation \n</h3> \n \n<blockquote> \n<p>The purchase process begins when the Frontend sends a request to the OrderService, specifying the Product ID and the desired quantity.<br><br> \nThe OrderService then takes on the responsibility of verifying inventory. It issues a call to the InventoryService to check the current stock levels for that specific product ID.<br><br> \nAfter this check, the InventoryService responds back to the OrderService with the current quantity available.<br><br> \nImportantly, the process includes a critical conditional branch: If the stock is available, the OrderService sends another message back to the InventoryService to officially deduct the purchased quantity. Following this, the OrderService sends an Order Confirmation, including the new Order ID, back to the Frontend.<br><br> \nConversely, if the stock is unavailable during the initial check, the OrderService terminates that branch and sends an immediate 'Error: Out of Stock' message back to the Frontend.<br><br> \nRegardless of the outcome of the order, the final step involves the Frontend internally displaying an appropriate success or failure message to the user.</p> \n</blockquote> \n<h3> \n   \n   \n  Complementary Mermaid Sequence Diagram (DSL) \n</h3> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fqbnidz4ckz3kvnxfj1oo.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fqbnidz4ckz3kvnxfj1oo.png\" alt=\"Purchase Order Sequence Diagram\" width=\"800\" height=\"503\"></a></p> \n \n<p>What the LLM actually <em>sees</em> is not the above image but this Mermaid code:<br> \n</p> \n \n<div> \n<pre><code>sequenceDiagram \n    participant F as Frontend \n    participant O as OrderService \n    participant I as InventoryService \n \n    F-&gt;&gt;O: Initiate Purchase (Product ID, Qty) \n    O-&gt;&gt;I: Check Stock (Product ID) \n    I--&gt;&gt;O: Stock Available (Qty) \n    alt Stock Available \n        O-&gt;&gt;I: Deduct Stock (Qty) \n        O--&gt;&gt;F: Order Confirmation (ID) \n    else Stock Unavailable \n        O--&gt;&gt;F: Error: Out of Stock \n    end \n    F-&gt;&gt;F: Display Success Message \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  The combined input compensates for the core weaknesses of the LLM: \n</h3> \n \n<ul> \n<li> \n<strong>Ambiguity in Text:</strong> Mermaid provides the strict, non-negotiable logic to ground the text.</li> \n<li> \n<strong>Hallucination/Creativity:</strong> Mermaid forces the model into a rigid structure (like code), constraining its creative freedom to invent steps that aren't there.</li> \n<li> \n<strong>Losing Track in Long Context:</strong> The diagram acts as an easy-to-parse summary. The model can cross-reference the text against the visual logic flow, improving its long-term attention to the process.</li> \n</ul> \n \n \n \n \n<h2> \n   \n   \n  Appendix: Mermaid Diagram Types you should consider for your Prompts \n</h2> \n \n<div><table> \n<thead> \n<tr> \n<th>Diagram Type</th> \n<th>Suitable For Knowledge Representation Aspect</th> \n<th>Example Use Case in Code/System Analysis</th> \n<th>Key Benefit for the LLM</th> \n</tr> \n</thead> \n<tbody> \n<tr> \n<td> \n<strong>Flowchart</strong> (<code>graph TD</code>, <code>LR</code>, etc.)</td> \n<td><strong>Temporal Flow, Process Logic, Decision Trees.</strong></td> \n<td>Mapping the execution path of a complex function, describing a deployment pipeline, or outlining a user sign-up workflow.</td> \n<td>Easily extracts <strong>process order</strong> and <strong>conditional logic</strong> (e.g., <em>if</em> statement paths) due to explicit node-to-node connections.</td> \n</tr> \n<tr> \n<td> \n<strong>Sequence Diagram</strong> (<code>sequenceDiagram</code>)</td> \n<td><strong>Temporal Interaction, API Calls, Message Passing.</strong></td> \n<td>Documenting the step-by-step interaction between microservices, a client-server authentication handshake, or the order of event emission.</td> \n<td>Clearly defines the <strong>order of events</strong> over time and the <strong>participating entities</strong> (lifelines), which is excellent for tracing bugs.</td> \n</tr> \n<tr> \n<td> \n<strong>Class Diagram</strong> (<code>classDiagram</code>)</td> \n<td><strong>Structure, Hierarchy, Relationships between Entities (OOP).</strong></td> \n<td>Defining the structure of a new module, documenting inheritance, or showing the composition of objects within a codebase.</td> \n<td>Unambiguously captures <strong>Object-Oriented Programming (OOP) concepts</strong> like inheritance, composition, and public/private methods.</td> \n</tr> \n<tr> \n<td> \n<strong>Entity Relationship Diagram (ERD)</strong> (<code>erDiagram</code>)</td> \n<td><strong>Data Structure, Relationship between Data Models.</strong></td> \n<td>Describing the schema of a database, defining the relationship between data entities (e.g., User, Order, Product) in a system.</td> \n<td>Provides a structured map of <strong>data entities</strong> and their <strong>cardinality</strong> (one-to-many, etc.), essential for data-driven applications.</td> \n</tr> \n<tr> \n<td> \n<strong>State Diagram</strong> (<code>stateDiagram-v2</code>)</td> \n<td><strong>State Transitions, Finite State Machines.</strong></td> \n<td>Documenting the lifecycle of an object (e.g., Order status: Draft $\\rightarrow$ Pending $\\rightarrow$ Shipped $\\rightarrow$ Delivered) or a UI component's behavior.</td> \n<td>Offers clear logic on <strong>valid transitions</strong> and the <strong>events</strong> that trigger them, perfect for analyzing complex object lifecycles.</td> \n</tr> \n</tbody> \n</table></div> \n \n \n \n \n<p><strong>Note that this article is NOT the single response of a prompt that I gave to an LLM.</strong><br><br> \nIt was written by combining and structuring aspects  of various interactions I had with Gemini about<br> \nusing Mermaids in prompting, defining initial context in GEMINI.md, and also about guard railing an LLM in the MCP Service I am working on.<br><br> \n<strong>My motivation to share this approach</strong> simply is because I think it is something genuine and new and might help you in your daily work with LLMs.</p>",
      "summary": "Spoiler Alert: This article is about using Mermaid diagrams as a method of re-enforcement in LLM prompting and not about the fary tail The Little Mermaid written by Danish author Hans Christian Anderson. \n \n \n \n \n \n   \n   \n  Summary \n \n \nYou probably have seen a lot of Mermaid Diagrams already without even realizing that what is powering them is a formal description language which is very well understood by LLMs. \nMermaid syntax is very good for describing formal, structural, or temporal informa",
      "publishedAt": "2025-12-02T13:18:00.000Z",
      "author": "Christoph Grimm",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 10.4552762574266,
      "ingestedAt": "2025-12-02T14:44:03.179Z",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "observability",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a8aa36",
      "title": "AI Killed the Full-Stack Roadmap. Here's the New System.",
      "url": "https://dev.to/thinkaddict/ai-killed-the-full-stack-roadmap-heres-the-new-system-38ba",
      "content": "<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fpollinations.ai%2Fp%2FSynthwave%2520vector%2520icon%2520of%2520a%2520neural%2520network%2520roadmap%252C%2520glowing%2520neon%2520lines.%2520Text%2520%2522FUTURE%2520STACK%2522%2520in%2520retro%2520pixel%2520font%2520at%2520bottom.%252C%2520retro-futuristic%2520vector%2520art%252C%2520synthwave%2520style%252C%2520neon%2520gradient%2520colors%252C%2520dark%2520background%252C%2520high%2520quality%252C%25208k%252C%2520dribbble%2520style%3Fwidth%3D1280%26height%3D720%26seed%3D988079%26model%3Dflux%26nologo%3Dtrue\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fpollinations.ai%2Fp%2FSynthwave%2520vector%2520icon%2520of%2520a%2520neural%2520network%2520roadmap%252C%2520glowing%2520neon%2520lines.%2520Text%2520%2522FUTURE%2520STACK%2522%2520in%2520retro%2520pixel%2520font%2520at%2520bottom.%252C%2520retro-futuristic%2520vector%2520art%252C%2520synthwave%2520style%252C%2520neon%2520gradient%2520colors%252C%2520dark%2520background%252C%2520high%2520quality%252C%25208k%252C%2520dribbble%2520style%3Fwidth%3D1280%26height%3D720%26seed%3D988079%26model%3Dflux%26nologo%3Dtrue\" alt=\"Cover\" width=\"1024\" height=\"576\"></a></p> \n \n<h1> \n   \n   \n  AI Killed the Full-Stack Roadmap. Here's the New System. \n</h1> \n \n<h2>The Paradox: You're Climbing a Ladder That's Already Obsolete</h2> \n \n<p>You’ve seen the charts. The endless tree of technologies you’re supposed to learn to call yourself “full-stack.” HTML, CSS, JavaScript, then React, then Node.js, then SQL, then NoSQL, then Docker, then Kubernetes… it never ends. But here’s the brutal truth: <b>you’re preparing for a world that no longer exists.</b> While you’re memorizing syntax, AI is building entire applications from a single prompt.</p> \n \n<p>The game has changed. The value is no longer in being a human code compiler. The value has shifted from writing the lines to architecting the system. Your old roadmap is a trap, designed to make you a commodity in a world that now values unique insight above all.</p> \n \n<h2>The Analysis: From Code Writer to System Director</h2> \n \n<p>The fundamental shift is this: we are moving from an era of *information retrieval* to an era of *intelligence augmentation*. Previously, a developer's worth was tied to their ability to recall specific library functions or boilerplate patterns. Stuck? You’d search Stack Overflow. Now, you ask an AI co-pilot, and it generates the code instantly.</p> \n \n<blockquote>The best developers are no longer code writers; they are system architects who speak human, machine, and now, AI.</blockquote> \n \n<p>This terrifies those who built their identity on being a syntax wizard. But it should liberate you. It frees up your cognitive bandwidth to focus on what truly matters: the architecture, the user experience, the business logic, and the security of the system as a whole. Chasing the hot new JavaScript framework is a losing battle. The frameworks will come and go, churned out faster than ever with AI assistance. Your ability to think, however, is timeless.</p> \n \n<h3>The New Skill Stack: Principles Over Patterns</h3> \n \n<ul> \n  <li> \n<b>Deep Fundamentals:</b> AI can write a React component, but it can’t explain the fundamental trade-offs between server-side rendering and client-side rendering for your specific use case. You must.</li> \n  <li> \n<b>System Design:</b> How do services communicate? How does data flow? How do you ensure scalability and resilience? This is the blueprint. AI is the construction crew.</li> \n  <li> \n<b>AI Literacy:</b> You don't need to build foundational models, but you must know how to wield them. This means mastering prompt engineering, understanding API integrations, and knowing the limitations of the tools.</li> \n</ul> \n \n<h2>The System: The AI-Proof Developer Roadmap</h2> \n \n<p>Stop collecting technologies. Start building a system for thinking. This is the new path forward.</p> \n \n<p><b>1. Master the First Principles:</b> Don't just learn Express.js; master the HTTP protocol. Don't just learn PostgreSQL; master relational database theory. Principles are durable. Frameworks are fleeting.</p> \n \n<p><b>2. Think Like an Architect, Act Like a Director:</b> Spend 80% of your time on whiteboarding flows, defining data models, and planning the user journey. Use AI to generate the boilerplate and initial code. Your role is to be the editor, the curator, the director who says “no, that’s not quite right, try it this way.”</p> \n \n<blockquote>Stop chasing frameworks. Start mastering principles. AI can write boilerplate; it can't replicate deep understanding.</blockquote> \n \n<p><b>3. Build Your Leverage Stack:</b> Your stack is no longer just MERN or LAMP. It’s your brain, layered with your communication skills, your design sense, and your ability to leverage AI tools. This is your unique, uncopyable advantage. Forget the roadmap that everyone else is following. The real path is building a skill set so unique and principled that you can’t be easily replaced—by a human or a machine.</p> \n \n \n \n \n<h3> \n   \n   \n  🚀 Upgrade Your Mindset \n</h3> \n \n<p><strong><a href=\"https://t.me/ThinkAddictGlobal\">👉 JOIN THE SYSTEM</a></strong></p> \n \n<p><em>Visual by Think Addict System.</em></p>",
      "summary": " \n \n \n   \n   \n  AI Killed the Full-Stack Roadmap. Here's the New System. \n \n \nThe Paradox: You're Climbing a Ladder That's Already Obsolete \n \nYou’ve seen the charts. The endless tree of technologies you’re supposed to learn to call yourself “full-stack.” HTML, CSS, JavaScript, then React, then Node.js, then SQL, then NoSQL, then Docker, then Kubernetes… it never ends. But here’s the brutal truth: you’re preparing for a world that no longer exists. While you’re memorizing syntax, AI is building ",
      "publishedAt": "2025-12-02T13:12:22.000Z",
      "author": "Think Addict",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "security_incident",
      "score": 9.954623929869403,
      "ingestedAt": "2025-12-02T14:44:03.180Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a8aa3a",
      "title": "Day 12: Python Programming",
      "url": "https://dev.to/aruna_arun_0cda4eb425bb0f/day-12-python-programming-a3",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwd6qybklyc95kgs9zm56.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p><strong>Advanced Python Collections (List, Tuple, Set, Dictionary)</strong></p>  \n  \n<p><strong>PART-1: LIST – Advanced Concepts</strong></p>  \n  \n<p><strong>1. List Comprehension</strong><br>  \nShort way to create lists.<br>  \nsquares = [x*x for x in range(1, 6)]<br>  \nprint(squares)<br>  \n<strong>With condition:</strong><br>  \neven = [x for x in range(10) if x % 2 == 0]</p>  \n  \n<p><strong>2. Nested List Comprehension</strong><br>  \nmatrix = [[i*j for j in range(1, 4)] for i in range(1, 4)]</p>  \n  \n<p><strong>3.List Slicing</strong><br>  \nnums = [1,2,3,4,5,6]<br>  \nprint(nums[1:5])<br>  \nprint(nums[::-1])   # reverse</p>  \n  \n<p><strong>4.Important List Methods</strong><br>  \nappend()<br>  \nextend()<br>  \ninsert()<br>  \nremove()<br>  \npop()<br>  \nclear()<br>  \nsort()<br>  \nreverse()<br>  \ncount()<br>  \nindex()<br>  \n<strong>Example</strong>:<br>  \nitems = [5, 2, 9, 1]<br>  \nitems.sort()      # ascending<br>  \nitems.sort(reverse=True)   # descending</p>  \n  \n<p><strong>5.Cloning a List</strong><br>  \na = [1, 2, 3]<br>  \nb = a[:]<br>  \nc = a.copy()</p>  \n  \n<p><strong>PART-2: TUPLE – Advanced Concepts</strong><br>  \n<strong>1.Tuple Unpacking</strong><br>  \na, b, c = (10, 20, 30)</p>  \n  \n<p><strong>2.Tuple with * (Extended Unpacking)</strong><br>  \na, *b = (1, 2, 3, 4, 5)</p>  \n  \n<p><strong>3.Why Tuple Is Faster?</strong></p>  \n  \n<ul>  \n<li>Immutable → stored in continuous memory</li>  \n<li>Faster iteration than list</li>  \n<li>Used for fixed data (lat/long, config)</li>  \n</ul>  \n  \n<p><strong>4.Convert List to Tuple</strong><br>  \nt = tuple([1, 2, 3])</p>  \n  \n<p><strong>PART-3: SET – Advanced Concepts</strong></p>  \n  \n<p><strong>1.Set Operations</strong><br>  \nA = {1, 2, 3}<br>  \nB = {3, 4, 5}</p>  \n  \n<p>print(A | B)   # union<br>  \nprint(A &amp; B)   # intersection<br>  \nprint(A - B)   # difference</p>  \n  \n<p><strong>✔2.Remove Duplicates from List Using Set</strong><br>  \nlst = [1,2,2,3,3,4]<br>  \nunique = list(set(lst))</p>  \n  \n<p><strong>3.Add &amp; Remove Elements</strong><br>  \ns = {1,2,3}<br>  \ns.add(4)<br>  \ns.discard(2)<br>  \ns.remove(3)   # error if 3 not exists</p>  \n  \n<p><strong>4.Set Comprehension</strong><br>  \ns = {x*x for x in range(5)}</p>  \n  \n<p><strong>PART-4: DICTIONARY – Advanced Concepts</strong></p>  \n  \n<p><strong>1.Dictionary Comprehension</strong><br>  \nsquares = {x: x*x for x in range(5)}<br>  \n<strong>2.Looping Through Dictionary</strong><br>  \nd = {\"a\":1, \"b\":2, \"c\":3}<br>  \nfor key in d:<br>  \n    print(key, d[key])<br>  \nfor k, v in d.items():<br>  \n    print(k, v)</p>  \n  \n<p><strong>3.Merging Two Dictionaries</strong><br>  \nd1 = {\"a\": 1, \"b\": 2}<br>  \nd2 = {\"c\": 3}<br>  \nmerged = {**d1, **d2}</p>  \n  \n<p><strong>4.Dictionary Methods</strong><br>  \nget()<br>  \nitems()<br>  \nkeys()<br>  \nvalues()<br>  \npop()<br>  \npopitem()<br>  \nupdate()<br>  \nclear()</p>  \n  \n<p><strong>5.Using Dictionary as Counter</strong><br>  \ndata = \"aabbccc\"<br>  \ncount = {}<br>  \nfor ch in data:<br>  \n    count[ch] = count.get(ch, 0) + 1<br>  \nprint(count)</p>  \n  \n<p><strong>PART-5: Collections Module (Important for Interviews)</strong><br>  \nfrom collections import Counter, defaultdict, deque, OrderedDict</p>  \n  \n<p><strong>1.Counter</strong><br>  \nCounts frequency of elements.<br>  \nfrom collections import Counter<br>  \nprint(Counter(\"aabbbcccc\"))</p>  \n  \n<p><strong>2.defaultdict</strong><br>  \nAutomatically creates default values if key missing.<br>  \nfrom collections import defaultdict<br>  \nd = defaultdict(int)<br>  \nd[\"a\"] += 1<br>  \nd[\"b\"] += 2<br>  \nprint(d)</p>  \n  \n<p><strong>3.deque</strong><br>  \nFast append/pop from both ends.<br>  \nfrom collections import deque<br>  \nq = deque([1,2,3])<br>  \nq.appendleft(0)<br>  \nq.append(4)<br>  \nprint(q)</p>  \n  \n<p><strong>4.OrderedDict</strong><br>  \nRemembers insertion order<br>  \n(Python 3.7+ dict already preserves order)</p>  \n  \n<p><strong>PART-6: Interview Programs Using Collections</strong></p>  \n  \n<p><strong>1.Find the most repeated element</strong><br>  \nfrom collections import Counter<br>  \nprint(Counter([1,2,2,3,3,3]).most_common(1))</p>  \n  \n<p><strong>2.Reverse a list without using reverse()</strong><br>  \nlst = [1,2,3]<br>  \nprint(lst[::-1])</p>  \n  \n<p><strong>3.Merge two lists into dictionary</strong><br>  \nkeys = [\"name\", \"age\"]<br>  \nvalues = [\"Arun\", 22]<br>  \nd = dict(zip(keys, values))</p>  \n  \n<p><strong>4.Remove duplicates while preserving order</strong><br>  \nresult = []<br>  \nfor x in [1,2,2,3,1]:<br>  \n    if x not in result:<br>  \n        result.append(x)</p>  \n  \n<p><strong>5.Convert dictionary to list of tuples</strong><br>  \nd = {\"a\":1, \"b\":2}<br>  \nprint(list(d.items()))</p>",
      "summary": "Advanced Python Collections (List, Tuple, Set, Dictionary)  \n  \nPART-1: LIST – Advanced Concepts  \n  \n1. List Comprehension  \nShort way to create lists.  \nsquares = [x*x for x in range(1, 6)]  \nprint(squares)  \nWith condition:  \neven = [x for x in range(10) if x % 2 == 0]  \n  \n2. Nested List Comprehension  \nmatrix = [[i*j for j in range(1, 4)] for i in range(1, 4)]  \n  \n3.List Slicing  \nnums = [1,2,3,4,5,6]  \nprint(nums[1:5])  \nprint(nums[::-1])   # reverse  \n  \n4.Important List Methods  \nappend",
      "publishedAt": "2025-12-02T13:11:50.000Z",
      "author": "Aruna Arun",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 7.963488466810737,
      "ingestedAt": "2025-12-02T14:44:03.180Z",
      "tags": [
        "code_review",
        "retrieval",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a8aa3c",
      "title": "DebateMaster-AI",
      "url": "https://dev.to/abbasmir12/debatemaster-ai-2n8b",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F899i6wcyq1lwcban3sxc.jpg\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>Debating is an art – it’s not just what you say, but <em>how</em> you say it. Imagine having a personal AI coach that not only challenges your arguments in real-time but also tracks your performance, gives constructive feedback, and helps you grow as a debater. That’s exactly what I built for this Hackathon: <strong>DebateMaster AI</strong>.</p>  \n  \n<h2>  \n    \n    \n  What is DebateMaster-AI?  \n</h2>  \n  \n<p>DebateMaster-I is a <strong>real-time debate coaching platform</strong> that leverages <strong>Google's Gemini Live API</strong> for bidirectional voice streaming. Unlike traditional debate apps, this platform is designed to <strong>analyze your performance on the fly</strong>, track your improvement, and help you refine your skills through AI-powered insights.</p>  \n  \n<p>Here’s how it works:</p>  \n  \n<ol>  \n<li>Enter a debate topic, for example: <em>\"Impact of Pollution on Living Organisms\"</em>  \n</li>  \n<li>Choose your debate style: <strong>Coach mode</strong> for supportive feedback or <strong>Fierce mode</strong> for challenging opposition.</li>  \n<li>Configure your Gemini API key, hit <strong>Start Debate</strong>, and the AI engages you in natural conversation.</li>  \n</ol>  \n  \n<p>During the debate, the interface shows live transcription, audio visualization, and a running timer. When the debate ends, you receive a <strong>comprehensive performance report</strong> including:</p>  \n  \n<ul>  \n<li>  \n<strong>Vocabulary Richness</strong> – tracks your lexical diversity.</li>  \n<li>  \n<strong>Confidence Level</strong> – analyzes speech patterns and delivery.</li>  \n<li>  \n<strong>Argument Strength</strong> – evaluates logical structure and evidence.</li>  \n<li>  \n<strong>Response Time</strong> – measures how quickly you counter arguments.</li>  \n<li>  \n<strong>Engagement Score</strong> – tracks active participation.</li>  \n</ul>  \n  \n<p>The AI also generates <strong>personalized suggestions</strong> to improve your debating skills.</p>  \n  \n<h2>  \n    \n    \n  Tracking Progress and Persona Analysis  \n</h2>  \n  \n<p>DebateMaster AI isn’t just a one-time experience. The <strong>Activity tab</strong> visualizes your progress through score trends, heatmaps, and session history. Additionally, the <strong>Persona system</strong> assigns a debate archetype – Strategist, Orator, Analyst, or Diplomat – showing your strengths and evolution over time. Achievements like <em>Vocabulary Master</em> or <em>Debate Marathon</em> gamify the learning experience.</p>  \n  \n<h2>  \n    \n    \n  How Kiro IDE Made This Possible  \n</h2>  \n  \n<p>The real star behind the scenes is <strong>Kiro IDE</strong>. I used:</p>  \n  \n<ul>  \n<li>  \n<strong>Multi-Spec Development</strong> – I split the project into four independent specs: core debate functionality, activity view, persona system, and UI redesign. This allowed parallel development without conflicts.</li>  \n<li>  \n<strong>Steering Documents</strong> – ensured consistent styling, architecture, and API usage across features.</li>  \n<li>  \n<strong>Agent Hooks</strong> – automated repetitive tasks like linting, grammar checking, and README updates, saving hours of manual work.</li>  \n<li>  \n<strong>Model Context Protocol (MCP)</strong> – integrated live documentation for Gemini API, TypeScript, React, and Tailwind, so Kiro always had the latest references.</li>  \n</ul>  \n  \n<p>This workflow accelerated development, improved code quality, and let me focus on building the best user experience rather than boilerplate tasks.</p>  \n  \n<h2>  \n    \n    \n  Why DebateMaster AI Matters  \n</h2>  \n  \n<p>Whether you’re a student preparing for competitions, a professional improving public speaking, or someone who wants to sharpen critical thinking, DebateMaster AI offers a <strong>complete coaching platform</strong>. In just six weeks, the combination of AI-assisted development and Kiro IDE helped me turn a complex idea into a polished, fully functional app.</p>  \n  \n<h2>  \n    \n    \n  Try It Out  \n</h2>  \n  \n<p>DebateMaster AI is more than a hackathon project – it’s a learning platform. Check out the <a href=\"https://github.com/abbasmir12/debatemasterai\">repository</a> and give it a try. Your next debate could be your best one yet!</p>",
      "summary": "Debating is an art – it’s not just what you say, but how you say it. Imagine having a personal AI coach that not only challenges your arguments in real-time but also tracks your performance, gives constructive feedback, and helps you grow as a debater. That’s exactly what I built for this Hackathon: DebateMaster AI.  \n  \n  \n    \n    \n  What is DebateMaster-AI?  \n  \n  \nDebateMaster-I is a real-time debate coaching platform that leverages Google's Gemini Live API for bidirectional voice streaming.",
      "publishedAt": "2025-12-02T13:11:40.000Z",
      "author": "Mir Abbas",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 12.940561775967685,
      "ingestedAt": "2025-12-02T14:44:03.180Z",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a76994",
      "title": "Cyber Startup Frenetik Launches with Patented Deception Technology That Bets Against the AI Arms Race",
      "url": "https://devops.com/cyber-startup-frenetik-launches-with-patented-deception-technology-that-bets-against-the-ai-arms-race/",
      "content": "<div><img width=\"1200\" height=\"720\" src=\"https://devops.com/wp-content/uploads/2025/12/frenetik_17640073059qwMUZNORw.jpg\" alt=\"\" style=\"margin-bottom:0px;\"></div><img width=\"150\" height=\"150\" src=\"https://devops.com/wp-content/uploads/2025/12/frenetik_17640073059qwMUZNORw-150x150.jpg\" alt=\"\">Bethesda, USA / Maryland, 2nd December 2025, CyberNewsWire",
      "summary": "Bethesda, USA / Maryland, 2nd December 2025, CyberNewsWire",
      "publishedAt": "2025-12-02T13:00:56.000Z",
      "author": "cybernewswire",
      "source": "rss",
      "feedName": "DevOps.com",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 7.461734976200367,
      "ingestedAt": "2025-12-02T14:44:03.180Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a6d325",
      "title": "[ADS] Please verify your email address",
      "url": "https://www.inoreader.com/article/3a9c6e76b2ad2cda",
      "content": "<div class=\"email_is_html\"><div>                                            <div>             <table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" height=\"100%\" width=\"100%\" style=\"background-color: #E0E0E0\">                 <tr>                     <td align=\"center\" valign=\"top\">                         <table border=\"0\" cellpadding=\"10\" cellspacing=\"0\" width=\"600\">                             <tr>                                 <td align=\"center\" valign=\"top\">                                     <table border=\"0\" cellpadding=\"20\" cellspacing=\"0\" width=\"100%\">                                              </table>                                 </td>                             </tr>                             <tr>                                 <td align=\"center\" valign=\"top\">                                     <table border=\"0\" cellpadding=\"20\" cellspacing=\"0\" width=\"100%\" style=\"background-color: #ffffff\">                                         <tr>                                             <td align=\"center\" valign=\"top\" background=\"https://ui.adsabs.harvard.edu/styles/img/background.jpg\" style=\"width: 100%; background-color: #150E35\">                                                 <img src=\"https://ui.adsabs.harvard.edu/styles/img/ads_logo.png\" alt=\"Astrophysics Data System\" style=\"width: 70%; color: #ffffff; font-size: 34px; font-family: sans-serif\" />                                              </td>                                         </tr>                                         <tr>                                             <td align=\"left\" valign=\"top\">                                                 <p style=\"font-family: sans-serif; font-size: 14px; font-weight: normal; margin: 0; margin-bottom: 15px\">Hi,</p>  <p style=\"font-family: sans-serif; font-size: 14px; font-weight: normal; margin: 0; margin-bottom: 15px\">Welcome to the new <a href=\"https://ui.adsabs.harvard.edu\" target=\"_blank\" rel=\"noreferrer\">NASA ADS</a>! To finish setting up your account,  please confirm your email address:</p>  <p style=\"font-family: sans-serif; font-size: 14px; font-weight: normal; margin: 0; margin-bottom: 15px\"><a href=\"https://ui.adsabs.harvard.edu/#user/account/verify/register/NjQxMjQ.aS7oqw.Sh9AdkeKgliy-85Nb2uyE8eM6DY\" target=\"_blank\" rel=\"noreferrer\">https://ui.adsabs.harvard.edu/#user/account/verify/register/NjQxMjQ.aS7oqw.Sh9AdkeKgliy-85Nb2uyE8eM6DY</a></p>  <p style=\"font-family: sans-serif; font-size: 14px; font-weight: normal; margin: 0; margin-bottom: 15px\">This link is only valid for the next 24 hours.</p>  <p style=\"font-family: sans-serif; font-size: 14px; font-weight: normal; margin: 0; margin-bottom: 15px\">If you didn't request this, you can safely ignore this email.</p>  <p style=\"font-family: sans-serif; font-size: 14px; font-weight: normal; margin: 0; margin-bottom: 15px\">- the ADS team</p>                                             </td>                                         </tr>                                     </table>                                 </td>                             </tr>                             <tr>                                 <td align=\"center\" valign=\"top\">                                     <table border=\"0\" cellpadding=\"20\" cellspacing=\"0\" width=\"100%\" style=\"color: #999999; font-size: 12px; text-align: center; font-family: sans-serif\">                                         <tr>                                             <td align=\"center\" valign=\"top\">                                                 <p> This message was sent to sjads@ino.to. </p>                                                 <p> © SAO/NASA <a href=\"https://ui.adsabs.harvard.edu\" target=\"_blank\" rel=\"noreferrer\">Astrophysics Data System</a> <br /> 60 Garden Street <br /> Cambridge, MA</p>                                             </td>                                         </tr>                                     </table>                                 </td>                             </tr>                         </table>                     </td>                 </tr>             </table>         </div>     </div></div>",
      "summary": "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ",
      "publishedAt": "2025-12-02T13:25:03.000Z",
      "author": " ",
      "source": "rss",
      "feedName": "sjads",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.494133303307472,
      "ingestedAt": "2025-12-02T14:44:03.180Z",
      "tags": []
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a51ce1",
      "title": "Comment on New research: How to make time for the work that matters by Hybrid Work Habits: Your Success Guide - Serenity7Wellness.com",
      "url": "https://www.atlassian.com/blog/distributed-work/calendar-redesign-experiment#comment-24917",
      "content": "<p><img src=\"https://www.atlassian.com/blog/wp-content/uploads/2023/12/blog_hero_img_calendar-experiment-how-to-find-time_b@2x-scaled.jpg\" alt=\"blog_hero_img_calendar-experiment-how-to\"></p><p>[…] Encourage open communication about mental health. […]</p>",
      "summary": "[…] Encourage open communication about mental health. […]",
      "publishedAt": "2025-12-02T13:02:30.000Z",
      "author": "Hybrid Work Habits: Your Success Guide - Serenity7Wellness.com",
      "source": "rss",
      "feedName": "Comments for Atlassian Blog Work Life",
      "sourceType": "engineering_blog",
      "contentType": "thought_leadership",
      "score": 5.969851889916683,
      "ingestedAt": "2025-12-02T14:44:03.180Z",
      "tags": [
        "retrieval",
        "ide",
        "Tech Company Blogs"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a487db",
      "title": "CraftStory Video Gen 🎥, AIGA NY Rebrand 🎨, AI Music Tools 🎶",
      "url": "https://www.inoreader.com/article/3a9c6e76b2af7824",
      "content": "<div class=\"email_is_html\"><div><div>    <div style=\"display: none; max-height: 0px; overflow: hidden\">CraftStory emerged from stealth with $2 million in funding and technology that generates realistic, human-centric videos up to 5 minutes long ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ </div>  <div style=\"display: none; max-height: 0px; overflow: hidden\"> <br /> </div>  <table align=\"center\"><tbody><tr><td valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"600\"><tbody><tr><td>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td>  <table width=\"100%\"><tbody><tr><td>    <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" style=\"margin-top: 0px\" width=\"100%\"><tbody><tr><td style=\"padding: 0px\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div style=\"text-align: center\"> <span style=\"margin-right: 0px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fdesign%3Futm_source=tldrdesign/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/iQn8hDjlUyleHts0-ki9GxF9_n7jJWhb4JxkgDQsf28=434\" rel=\"noreferrer\" target=\"_blank\"><span>Sign Up</span></a> |<span style=\"margin-right: 2px; margin-left: 2px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2Faudiences%2Fdesign-professionals%2F%3Futm_source=tldrdesign%26utm_medium=newsletter%26utm_campaign=advertisetopnav/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/3I2OmIKdjSvyAYoVW330IVs6tCDot4ziFQs8aA2Swqs=434\" rel=\"noreferrer\" target=\"_blank\"><span>Advertise</span></a></span>|<span style=\"margin-left: 2px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa.tldrnewsletter.com%2Fweb-version%3Fep=1%26lc=8cb71674-9621-11f0-8387-a3daa84d9496%26p=9c2dbf30-cf2f-11f0-87fd-e70d5dfa173c%26pt=campaign%26t=1764681043%26s=d5e30719682843406bfa01212f26d6c96da099c315860ce8a0d914106faafe5d/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/X_X_ECsgXy1fb2e_8_U5kqAVBlb6z3fg2AfppyEgEwY=434\" target=\"_blank\" rel=\"noreferrer\"><span>View Online</span></a></span> <br /> </span></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"text-align: center\"><span style=\"--darkreader-inline-color: #3db3ff; color: rgb(51, 175, 255) !important; font-size: 30px\">T</span><span style=\"font-size: 30px\"><span style=\"color: rgb(232, 192, 96) !important; --darkreader-inline-color: #e8c163; font-size: 30px\">L</span><span style=\"color: rgb(101, 195, 173) !important; --darkreader-inline-color: #6ec7b2; font-size: 30px\">D</span></span><span style=\"--darkreader-inline-color: #dd6e6e; color: rgb(220, 107, 107) !important; font-size: 30px\">R</span> <br /> </td></tr></tbody></table>  <br />  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\"> <div style=\"text-align: center\">  <h1><strong>TLDR Design <span>2025-12-02</span></strong></h1> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr> <tr><td>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 0px\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">📱</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>News &amp; Trends</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fventurebeat.com%2Fai%2Fopencv-founders-launch-ai-video-startup-to-take-on-openai-and-google%3Futm_source=tldrdesign/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/1gzVs30mNveSNyDq6ZzJuw-HUU7-7Rz6K7rLobb3suE=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>OpenCV Founders Launch AI Video Startup to Take on OpenAI and Google (9 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     CraftStory, founded by the creators of OpenCV, emerged from stealth with $2 million in funding and technology that generates realistic, human-centric videos up to 5 minutes long. The startup uses a parallelized diffusion architecture that runs multiple algorithms simultaneously across the entire video duration, trained on proprietary high-quality footage rather than internet-scraped data. CraftStory targets enterprise customers needing longer training videos and product demonstrations, offering a video-to-video system that animates still images using driving videos as motion templates.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.creativeboom.com%2Fnews%2Faiga-ny-unveils-new-logo-and-strategic-direction-rooted-in-community-and-new-yorks-creative-energy%2F%3Futm_source=tldrdesign/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/6wc7Rtis4uuCD5bLJfKRuq3vdSVP7XjNDqNdM35OP-I=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>AIGA NY unveils new identity rooted in community and New York's creative energy (4 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     AIGA NY has launched its first major rebrand in nearly 20 years, introducing a new logo and strategy that emphasises community, visibility, and open dialogue. Designed by Christopher Guerrero, the identity creates a &quot;town square&quot; of negative space around the classic AIGA box and uses colours inspired by New York's streets. The chapter positions itself as a civic hub for designers and has formalised three pillars—elevating diverse voices, celebrating design excellence, and advocating for stronger industry standards—as the refresh rolls out across its platforms.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstability.ai%2Fnews%2Fwarner-music-group-and-stability-ai-join-forces-to-build-next-gen-tools%3Futm_source=tldrdesign/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/QcV8WVu3Sg86m8reB51Oqg8DPV63bg0hhHEZUv3RzGk=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Warner Music Group and Stability AI Join Forces to Build The Next Generation Of Responsible AI Tools For Music Creation (3 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Warner Music Group and Stability AI announced a partnership to develop responsible AI music-creation tools using ethically trained models that protect creators' rights while enabling new forms of artistic expression. The collaboration will involve working directly with artists to shape professional-grade tools that enhance creative processes without compromising quality or artistic control. Stability AI's commercially safe generative audio models, trained exclusively on licensed data, will form the foundation for these artist-friendly tools.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🚀</span></div> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Opinions &amp; Tutorials</strong></h1> </div> </div> </td></tr></tbody></table>   <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2Fcreativity-meets-precision-with-gemini-3-pro-with-nano-banana%2F%3Futm_source=tldrdesign/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/EoMXb2IT_Iyh91QYSSMvSvrvYrcGbtpOj83UGzKE51k=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Creativity Meets Precision with Google's Nano Banana Pro (5 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Google's Gemini 3 Pro with Nano Banana now integrates across Figma's products, enabling designers to generate style variations and make precise edits while preserving brand consistency and visual coherence. The model excels at retaining design elements like palette, texture, and composition when creating new directions, and handles challenging tasks like adapting illustrations for dark mode or updating profile photos to match existing styles. Users on paid Figma plans can access Nano Banana Pro across all Figma products for contextual image generation and refinement.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhvpandya.com%2Fexposure%3Futm_source=tldrdesign/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/9A5U8-rENRAN0ms3tbql4Jkd5fmJk4MzzgYqUUt7inM=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Exposure (3 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Designers limit their effectiveness by spending too much time with other designers rather than engaging with product managers, engineers, sales teams, and customers who possess crucial context on user needs and business realities. Increased exposure to diverse perspectives reveals insights that pure design skills cannot, enabling designers to create solutions that actually solve the right problems. The most creative and effective designers aren't just technically skilled, they actively seek knowledge from people who understand different aspects of the product, market, and user context.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.creativebloq.com%2Fdesign%2Fproduct-design%2Fapple-has-accidentally-changed-the-future-of-smartphone-design-with-the-iphone-air%3Futm_source=tldrdesign/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/NEfjJLTZv5jeUJnTAe_IATTDNXIFqzU8YKWFJFgjBFs=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Apple just accidentally changed the future of smartphone design (3 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     The iPhone Air, despite its sleek design, has struggled with poor sales and hardware compromises like limited battery life, a single camera, and mono speakers, prompting several manufacturers—including Xiaomi, Vivo, and Samsung—to cancel or freeze their own ultra-thin phone projects. This suggests Apple's latest ultra-slim design may be influencing the wider industry away from pursuing similar devices, highlighting how even a visually striking gadget can have unintended consequences when practicality falls short.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>   <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">💻</span></div> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Launches &amp; Tools</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmediapet.ai%2F%3Futm_source=tldrdesign/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/zv7Ks1HnHFyFT-1QnyRc0uXk8dJh_GE8cSvPSdXaNUE=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Create Professional Video Ads in Minutes (Website)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Turn your ideas into high-converting video ads instantly. No editing skills required. Just describe your business and let AI do the rest.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sprites.ai%2F%3Futm_source=tldrdesign/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/D-1KTb0PveqjD8v88Ld1vs84CRzHqUmvzZYo4cmCD5k=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Your Personal AI Marketing Agent (Website)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Automate customer acquisition tasks with AI marketing tools. Build workflows, generate content, analyze data, and scale your growth.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa11ysupport.io%2F%3Futm_source=tldrdesign/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/goNnho8vRBps8Kii_ccBvlCnA2dc-1WhMEUbBT9BTq8=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Accessibility Support (Website)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     A community-driven website that aims to help inform developers about what code features (roles, states, properties, elements, etc.) are supported by assistive technologies, and what that support looks like.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>   <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🎁</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><strong><h1>Miscellaneous</h1></strong></div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.antonsten.com%2Farticles%2Fthe-hidden-cost-of-shipping-too-fast%2F%3Futm_source=tldrdesign/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/HuTuPNOPgzxyJscQ17hpwdJGxdbKWw94vh4Lc7-OlHo=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>The Hidden Cost of Shipping Too Fast (3 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     In many startups, teams prioritize speed without establishing shared clarity about the problem being solved, leading to opinion-based decision-making, weakened craft, and months of repair work that negate any initial time savings. When clarity is absent, senior voices dominate decisions, team energy shifts from momentum to maintenance, and people lower their standards, choosing easier approaches rather than better ones. The most effective teams distinguish between speed (mere movement) and velocity (movement with direction), pausing to align on &quot;What problem am I solving, and for whom?&quot; before building anything.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FiR4txe/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/zI794IUh9C-xYggZ-oVBGvyo0mowzdqGGAxTkkaIls0=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>UI and UX Design Trends for 2026: What Founders and Designers Need to Know (6 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     The digital design landscape in 2026 prioritizes outcome-driven UX where usability, accessibility, and measurable business results trump experimental aesthetics. Key shifts include UX teams proving tangible business impact, large conferences declining in favor of continuous internal learning, a return to familiar design conventions, regional trends influencing global design, and new platforms like Apple Vision Pro demanding new interface standards. AI integration has become indispensable in modern workflows, while visual design emphasizes cleaner, softer, more accessible interfaces with larger typography and thoughtful contrast.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.itsnicethat.com%2Farticles%2Fmichael-jeter-stripe-black-friday-cyber-monday-advertising-event-project-011225%3Futm_source=tldrdesign/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/iA3bz7paF-KCFIA9Bfebn1RKmYoQTw7DIdo1a8tdLFw=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Stripe's miniature, motorised city is a model marvel and tribute to the hidden world of commerce (2 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     For Black Friday, Cyber Monday, and Giving Tuesday, Stripe created &quot;Stripe City,&quot; an eight-foot-wide miniature city with over 100 figures, motorised elements, and interactive live data displays, designed to visualise the complex flow of global commerce. Handcrafted from paper, foam, and acrylics, the city combines physical diorama charm with digital interactivity, offering a playful yet ambitious celebration of Stripe's infrastructure and the companies that rely on it, while livestreaming the experience for viewers during the busy financial weekend.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>   <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">⚡</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Quick Links</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.yankodesign.com%2F2025%2F11%2F25%2Fadidas-purechill-the-sculptural-shoe-redefining-recovery%2F%3Futm_source=tldrdesign/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/gq3YpY2ly6VnsqKakUPbsyt5R7xW9zsUpK9dCeMSZvs=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Adidas Purechill: The Sculptural Shoe Redefining Recovery (3 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     The Adidas Purechill Runner is a $75 recovery shoe featuring sculptural, single-piece EVA foam construction with 360-degree ventilation through grooved patterns and perforations.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.creativebloq.com%2Fdesign%2Fbranding%2Fgen-alpha-is-already-transforming-branding%3Futm_source=tldrdesign/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/an9oFq5UdpNMyPyeZHogR-IH0BGn4oNapO0bujG80yw=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Gen Alpha is Already Transforming Branding (5 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Gen Alpha is reshaping brand loyalty through participation rather than passive consumption.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.creativebloq.com%2Fdesign%2Flogos-icons%2Flogo-endorsed-by-king-charles-caught-in-royally-messy-design-dispute%3Futm_source=tldrdesign/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/DplnWAhHTB7FR7fsV6rvVpPYuvuzSPyeBfct88j7xOw=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Logo endorsed by King Charles caught in messy design dispute (4 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Severn &amp; Wye Smokery faces complaints that its fly-fisherman logo may mislead customers into thinking its salmon is wild-caught, though the company insists labels clearly state it's farmed.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>   <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"left\" style=\"word-break: break-word; vertical-align: top; padding: 5px 10px\">  <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Love TLDR? Tell your friends and get rewards! </p> </td></tr> <tr><td style=\"padding: 0px 10px 15px\"> <div> Share your referral link below with friends to get free TLDR swag! </div> </td></tr> <tr><td align=\"left\" style=\"padding: 10px\"> <div> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frefer.tldr.tech%2F06c87d8d%2F4/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/_SWsH6Hzo12dChJOyftjIwXBOmY0tWVB8Il3syudl4g=434\" style=\"color: #464ba4; text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">https://refer.tldr.tech/06c87d8d/4</a> </div> </td></tr> <tr></tr> <tr><td align=\"left\" style=\"padding: 5px 10px\"> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhub.sparklp.co%2Fsub_0090fe55b862%2F4/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/GSosZXFgecNR9lPf490YHPw2T3G0zeD8D_Y_Sca9qeo=434\" style=\"font-size: 16px; line-height: 1.6; padding: 10px 0; display: inline-block; text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\"><span style=\"mso-text-raise: 13pt; text-decoration: underline\">Track your referrals here.</span></a> </td></tr></tbody></table>     <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"left\" style=\"word-break: break-word; vertical-align: top; padding: 5px 10px\">  <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Want to advertise in TLDR? 📰 </p> <div style=\"margin-top: 10px\"> If your company is interested in reaching an audience of design professionals and decision makers, you may want to <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2Faudiences%2Fdesign-professionals%2F%3Futm_source=tldrdesign%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/uBedFEo-o6fYYwxxsUyZjQ9mZPUaDverBuG1xun5XJY=434\" target=\"_blank\" rel=\"noreferrer\"><strong><span>advertise with us</span></strong></a>. </div> <br />    <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Want to work at TLDR? 💼 </p> <div style=\"margin-top: 10px\"> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/rsOwZ4hZUXpM4yP4C0fYjK_HHx38SGlHokBZalS8zLA=434\" rel=\"noreferrer\" style=\"color: #0000EE; text-decoration: underline\" target=\"_blank\"><strong>Apply here</strong></a> or send a friend's resume to <a href=\"mailto:jobs@tldr.tech\" style=\"color: #0000EE; text-decoration: underline\" onclick=\"return rcmail.command('compose','jobs@tldr.tech',this)\" rel=\"noreferrer\">jobs@tldr.tech</a> and get $1k if we hire them! </div> <br />  <div> If you have any comments or feedback, just respond to this email! <br /> <br /> Thanks for reading, <br /> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fin%2Fhellojaelee%2F/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/qSnBJl996jHJa3OTxnNu_22YKESKR5eGTUAY6wVVz7M=434\" target=\"_blank\" rel=\"noreferrer\"><span>Jae Lee</span></a>, <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fin%2Fmatejlatin%2F/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/HO-gVygmMCUxlVcYBlrg6IPEsmiLRQKrFDIq43HPBkM=434\" target=\"_blank\" rel=\"noreferrer\"><span>Matej Latin</span></a> &amp; <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fin%2Fralph-brinker%2F/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/qavHkZtgvvx9CvZCypRiC76Z1C24Qtxn7AcZPynDlXI=434\" target=\"_blank\" rel=\"noreferrer\"><span>Ralph Brinker</span></a> <br /> <br /> </div> <br /> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fdesign%2Fmanage%3Femail=tldrai90%2540ino.to/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/VVfNpGwuGlb8OFR8viT718moVDbbBImEJp3f6I2VM5s=434\" target=\"_blank\" rel=\"noreferrer\">Manage your subscriptions</a> to our other newsletters on tech, startups, and programming. Or if TLDR Design isn't for you, please <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa.tldrnewsletter.com%2Funsubscribe%3Fep=1%26l=e1c4e253-3e90-11ed-9a32-0241b9615763%26lc=8cb71674-9621-11f0-8387-a3daa84d9496%26p=9c2dbf30-cf2f-11f0-87fd-e70d5dfa173c%26pt=campaign%26pv=4%26spa=1764680485%26t=1764681043%26s=4ba523cc60cc22016a6df4c4c772ff8979c84ef6274613f7aef2a222576b460e/1/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/u7L3KngPnWDiuXml6vhVjd1CwyGHXtBKKpaSD59LKFc=434\" target=\"_blank\" rel=\"noreferrer\">unsubscribe</a>. <br /> </div> </td></tr></tbody></table>  </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table>   <img src=\"http://tracking.tldrnewsletter.com/CI0/0100019adf2fce45-656bfef9-e0a9-42e6-a79e-7ac6b8449fcb-000000/j7sg_DSiXQRmbhTYSUakVXNfDzy0e37F4DbeOonuDeI=434\" style=\"display: none; width: 1px; height: 1px\" /> </div></div></div>",
      "summary": "    CraftStory emerged from stealth with $2 million in funding and technology that generates realistic, human-centric videos up to 5 minutes long ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌                     Sign Up |Advertise|View Online     TLDR           TLDR Design 2025-12-02              📱       News &amp; Trends                                                                                                                     ",
      "publishedAt": "2025-12-02T13:10:46.000Z",
      "author": "TLDR Design ",
      "source": "rss",
      "feedName": "TLDR",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 10.451525598646096,
      "ingestedAt": "2025-12-02T14:44:03.181Z",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a3ef1a",
      "title": "Enterprise DAL Final: Automated User Auditing and Architectural Retrospective",
      "url": "https://dev.to/gigaherz/enterprise-dal-final-automated-user-auditing-and-architectural-retrospective-3jpc",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2ealhjh6a6j2d9zpbhwt.webp\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>This post marks the end of our <a href=\"https://byteaether.github.io/series/enterprise-dal/\">series</a> on constructing an enterprise-grade Data Access Layer (DAL) in C# with Linq2Db. Our goal was to create a DAL that is secure and resilient by automatically handling crucial cross-cutting concerns.</p>  \n  \n<h2>  \n    \n    \n  Implementing Automated User Auditing  \n</h2>  \n  \n<p>To achieve compliance and enhance debugging, we implement <strong>Automated User Auditing</strong>. This feature automatically populates <code>CreatedByUserId</code> and <code>ModifiedByUserId</code> fields with the current user's unique identifier (<code>Ulid</code>).</p>  \n  \n<p>The implementation is integrated into our existing interface-driven architecture:</p>  \n  \n<ol>  \n<li> <strong>Contracts:</strong> New interfaces (<code>IUserCreatable</code>, <code>IUserModifiable</code>) define the required properties.</li>  \n<li> <strong>Automation:</strong> Our custom scaffolding ensures that any entity table with <code>created_by_user_id</code>/<code>modified_by_user_id</code> columns automatically implements these interfaces.</li>  \n<li> <strong>Injection Logic:</strong> We update our <code>CrudExtensions</code> to inspect the entity on create/modify operations. If the entity implements the auditing interface, the current <code>UserId</code> from the request-scoped context is injected before the database operation executes.</li>  \n</ol>  \n  \n<p><strong>Crucial detail for Architects:</strong> We detail a necessary, advanced technique using <code>(LinqToDB.Internal.Linq).Internals.GetDataContext(source)</code> to correctly inject the <code>ModifiedByUserId</code> user ID during <strong>fluent batch UPDATE</strong> operations (<code>IUpdatable&lt;T&gt;</code>). This ensures auditing integrity across all modification types.</p>  \n  \n<h2>  \n    \n    \n  Verdict: 100% Goal Achievement  \n</h2>  \n  \n<p>The final architecture successfully automated every requirement initially set for a perfect enterprise DAL:</p>  \n  \n<ul>  \n<li>  \n<strong>Ubiquitous Filtering:</strong> Soft-Delete, Multi-Tenancy, and Row-Level Security are enforced by a highly composable global query filter system, transparently generating the correct SQL joins and <code>WHERE</code> clauses for <em>all</em> read operations.</li>  \n<li>  \n<strong>Projected Security:</strong> The use of Linq2Db’s projection capabilities allows for complex security and tenancy rules to be resolved contextually from related entities, a key differentiator.</li>  \n<li>  \n<strong>Auditing:</strong> Timestamp and User auditing are handled automatically on creation and modification.</li>  \n</ul>  \n  \n<p>This robust DAL abstracts away security and compliance boilerplate, letting developers focus purely on business logic.</p>  \n  \n  \n  \n  \n<p><strong>For the full implementation details, including source code and an in-depth explanation of the global query filter system,</strong> <a href=\"https://byteaether.github.io/2025/building-an-enterprise-data-access-layer-automated-user-auditing-and-series-wrap-up/\">read the complete article on my blog</a>.</p>",
      "summary": "This post marks the end of our series on constructing an enterprise-grade Data Access Layer (DAL) in C# with Linq2Db. Our goal was to create a DAL that is secure and resilient by automatically handling crucial cross-cutting concerns.  \n  \n  \n    \n    \n  Implementing Automated User Auditing  \n  \n  \nTo achieve compliance and enhance debugging, we implement Automated User Auditing. This feature automatically populates CreatedByUserId and ModifiedByUserId fields with the current user's unique identi",
      "publishedAt": "2025-12-02T13:00:00.000Z",
      "author": "GigAHerZ",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 13.430501149202941,
      "ingestedAt": "2025-12-02T14:44:03.181Z",
      "tags": [
        "code_review",
        "ide",
        "governance",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a3ef1f",
      "title": "How Quesby Handles SEO (Without Plugins, Frameworks, or Runtime Code)",
      "url": "https://dev.to/quesby/how-quesby-handles-seo-without-plugins-frameworks-or-runtime-code-251",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fngssuuo64wqleft7z6xf.webp\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><h2>  \n    \n    \n  Automatic Metadata, Open Graph, and JSON-LD — Eleventy Style  \n</h2>  \n  \n<p>Most Eleventy setups start the same way: every project re-implements SEO from scratch.<br><br>  \nAnother <code>base.njk</code>, another batch of <code>&lt;meta&gt;</code> tags, another copy-pasted JSON-LD snippet from schema.org.</p>  \n  \n<p>It’s boring, error-prone, and inconsistent across projects.</p>  \n  \n<p><strong>Quesby solves this the only sane way: one SEO model, one source of truth, generated at build time.</strong><br><br>  \nNo plugins.<br><br>  \nNo runtime.<br><br>  \nNo React components disguised as SEO helpers.</p>  \n  \n<p>This is how it works.</p>  \n  \n  \n<h2>  \n    \n    \n  The Idea in 10 Seconds  \n</h2>  \n  \n<p>Quesby takes:<br>  \n</p>  \n  \n<div>  \n<pre><code>site.json + frontmatter → SEO model → complete &lt;head&gt; + JSON-LD  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Everything happens during the Eleventy build.<br><br>  \nTemplates stay clean.<br><br>  \nSEO stays consistent.</p>  \n  \n<p>In <code>base.njk</code>, it looks like this:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>{%</span><span>-</span> <span>set</span> <span>seoModel</span> <span>=</span> <span>page</span> <span>| </span><span>seoModel</span><span>(</span><span>site</span><span>,</span> <span>pageData</span><span>)</span> <span>-</span><span>%}</span>  \n<span>{{</span> <span>seoModel</span> <span>| </span><span>seoHeadHtml</span><span>(</span><span>site</span><span>)</span> <span>| </span><span>safe</span> <span>}}</span>  \n<span>{{</span> <span>seoModel</span> <span>| </span><span>seoJsonLd</span><span>(</span><span>site</span><span>)</span> <span>| </span><span>safe</span> <span>}}</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>That’s the entire integration surface.</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  Site-Level Defaults (One File)  \n</h2>  \n  \n<p>Global SEO config lives in:<br>  \n</p>  \n  \n<div>  \n<pre><code>src/_data/site.json  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Example:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>{</span><span>  \n  </span><span>\"name\"</span><span>:</span><span> </span><span>\"Quesby\"</span><span>,</span><span>  \n  </span><span>\"url\"</span><span>:</span><span> </span><span>\"https://quesby.dev\"</span><span>,</span><span>  \n  </span><span>\"description\"</span><span>:</span><span> </span><span>\"A modern Eleventy boilerplate\"</span><span>,</span><span>  \n  </span><span>\"socialImage\"</span><span>:</span><span> </span><span>\"/assets/images/og-default.jpg\"</span><span>,</span><span>  \n  </span><span>\"twitter\"</span><span>:</span><span> </span><span>\"@quesby\"</span><span>,</span><span>  \n  </span><span>\"language\"</span><span>:</span><span> </span><span>\"en-US\"</span><span>  \n</span><span>}</span><span>  \n</span></code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Quesby uses this for:</p>  \n  \n<ul>  \n<li>default description</li>  \n<li>default Open Graph image</li>  \n<li>canonical URL generation</li>  \n<li>Twitter Card data</li>  \n<li>JSON-LD publisher info</li>  \n</ul>  \n  \n<p>You define it once.<br><br>  \nEvery page inherits it automatically.</p>  \n  \n  \n<h2>  \n    \n    \n  Frontmatter: Minimal When You Want, Detailed When You Need  \n</h2>  \n  \n<p>Simple page:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>---</span>  \n<span>title</span><span>:</span> <span>\"</span><span>My</span><span> </span><span>Page\"</span>  \n<span>description</span><span>:</span> <span>\"</span><span>Short</span><span> </span><span>summary.\"</span>  \n<span>---</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Full blog post:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>---</span>  \n<span>title</span><span>:</span> <span>\"</span><span>How</span><span> </span><span>to</span><span> </span><span>Build</span><span> </span><span>Better</span><span> </span><span>Websites\"</span>  \n<span>seoTitle</span><span>:</span> <span>\"</span><span>Complete</span><span> </span><span>Guide</span><span> </span><span>to</span><span> </span><span>Modern</span><span> </span><span>Web</span><span> </span><span>Development\"</span>  \n<span>description</span><span>:</span> <span>\"</span><span>Learn</span><span> </span><span>the</span><span> </span><span>principles</span><span> </span><span>behind</span><span> </span><span>fast,</span><span> </span><span>accessible</span><span> </span><span>websites.\"</span>  \n<span>image</span><span>:</span> <span>\"</span><span>/assets/images/cover.jpg\"</span>  \n<span>postType</span><span>:</span> <span>\"</span><span>article\"</span>  \n<span>author</span><span>:</span> <span>\"</span><span>John</span><span> </span><span>Doe\"</span>  \n<span>date</span><span>:</span> <span>\"</span><span>2024-01-15\"</span>  \n<span>---</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Quesby applies fallback rules:</p>  \n  \n<ul>  \n<li>Title: <code>`seoTitle` → `title` → `site.name`</code>  \n</li>  \n<li>Description: <code>`description` → `site.description`</code>  \n</li>  \n<li>Image: <code>`image` → `site.socialImage`</code>  \n</li>  \n</ul>  \n  \n<p>You don’t repeat yourself.<br><br>  \nPages don’t need boilerplate.</p>  \n  \n  \n<h2>  \n    \n    \n  Automatic Head Metadata  \n</h2>  \n  \n<p>From the SEO model, Quesby generates:</p>  \n<h3>  \n    \n    \n  Basic SEO  \n</h3>  \n  \n<ul>  \n<li><code>&lt;title&gt;</code></li>  \n<li><code>&lt;meta name=\"description\"&gt;</code></li>  \n<li><code>&lt;link rel=\"canonical\"&gt;</code></li>  \n<li>  \n<code>&lt;meta name=\"robots\"&gt;</code> (with <code>noindex</code> support)</li>  \n</ul>  \n<h3>  \n    \n    \n  Open Graph  \n</h3>  \n  \n<ul>  \n<li><code>og:type</code></li>  \n<li><code>og:title</code></li>  \n<li><code>og:description</code></li>  \n<li><code>og:url</code></li>  \n<li><code>og:image</code></li>  \n</ul>  \n<h3>  \n    \n    \n  Twitter Cards  \n</h3>  \n  \n<ul>  \n<li>  \n<code>`twitter:card`</code> (<code>`summary_large_image`</code>)</li>  \n<li><code>`twitter:site`</code></li>  \n<li><code>`twitter:title`</code></li>  \n<li><code>`twitter:description`</code></li>  \n<li><code>`twitter:image`</code></li>  \n</ul>  \n  \n<p>No manual tags.<br><br>  \nNo plugin configuration.<br><br>  \nNo React component SEO helper nonsense.</p>  \n  \n  \n<h2>  \n    \n    \n  JSON-LD Structured Data  \n</h2>  \n  \n<p>Quesby also outputs JSON-LD for:</p>  \n<h3>  \n    \n    \n  Blog posts (<code>BlogPosting</code>)  \n</h3>  \n  \n<ul>  \n<li>headline  \n</li>  \n<li>description  \n</li>  \n<li>image  \n</li>  \n<li>author  \n</li>  \n<li>publisher  \n</li>  \n<li>datePublished  \n</li>  \n<li>dateModified  \n</li>  \n</ul>  \n<h3>  \n    \n    \n  Site (<code>WebSite</code>)  \n</h3>  \n  \n<ul>  \n<li>name  \n</li>  \n<li>url  \n</li>  \n<li>description  \n</li>  \n<li>publisher  \n</li>  \n</ul>  \n  \n<p>Rendered via:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>{{</span> <span>seoModel</span> <span>| </span><span>seoJsonLd</span><span>(</span><span>site</span><span>)</span> <span>| </span><span>safe</span> <span>}}</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Again: static, not runtime.</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  Sitemap, Robots, and Noindex Integration  \n</h2>  \n  \n<p>Quesby integrates SEO beyond the <code>&lt;head&gt;</code>:</p>  \n  \n<ul>  \n<li>Sitemap generated from the <code>`sitemap`</code> collection  \n</li>  \n<li>Pages with <code>`noindex`</code> or <code>`eleventyExcludeFromCollections`</code> are auto-excluded  \n</li>  \n<li>  \n<code>`robots.txt`</code> generated automatically and linked to the sitemap  \n</li>  \n</ul>  \n  \n<p>Everything stays consistent with the same SEO model.</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  Opt-Out When Needed  \n</h2>  \n  \n<p>For special pages:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>---</span>  \n<span>seoDisableCoreHead</span><span>:</span> <span>true</span>  \n<span>seoDisableCoreJsonLd</span><span>:</span> <span>true</span>  \n<span>---</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>You can disable core SEO on a per-page basis and write your own tags.<br><br>  \nNo lock-in.</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  Why This Matters for Eleventy Developers  \n</h2>  \n  \n<p>If you build Eleventy sites regularly, you already know the pain:</p>  \n  \n<ul>  \n<li>copy-pasting head tags  \n</li>  \n<li>rewriting JSON-LD  \n</li>  \n<li>fixing Open Graph mismatches  \n</li>  \n<li>forgetting canonical URLs  \n</li>  \n<li>duplicating logic across pages  \n</li>  \n</ul>  \n  \n<p>Quesby removes all of that.</p>  \n  \n<ul>  \n<li>One config file  \n</li>  \n<li>One model  \n</li>  \n<li>One consistent output  \n</li>  \n<li>Zero runtime JavaScript  \n</li>  \n<li>Zero framework dependencies  \n</li>  \n</ul>  \n  \n<p>HTML-first, static-first, and boring in all the right ways.</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  Final Thoughts  \n</h2>  \n  \n<p>If you like Eleventy but don’t want to rebuild SEO infrastructure for every project, Quesby gives you:</p>  \n  \n<ul>  \n<li>strong defaults  \n</li>  \n<li>static, predictable output  \n</li>  \n<li>extensibility when needed  \n</li>  \n<li>zero client-side bloat  \n</li>  \n</ul>  \n  \n<p>It’s a lightweight alternative to plugin-heavy or framework-heavy approaches, built specifically for people who prefer clean HTML and simple pipelines.</p>  \n  \n<p>More info:<br><br>  \n<a href=\"https://quesby.dev\">https://quesby.dev</a></p>",
      "summary": "  \n    \n    \n  Automatic Metadata, Open Graph, and JSON-LD — Eleventy Style  \n  \n  \nMost Eleventy setups start the same way: every project re-implements SEO from scratch.  \nAnother base.njk, another batch of &lt;meta&gt; tags, another copy-pasted JSON-LD snippet from schema.org.  \n  \nIt’s boring, error-prone, and inconsistent across projects.  \n  \nQuesby solves this the only sane way: one SEO model, one source of truth, generated at build time.  \nNo plugins.  \nNo runtime.  \nNo React components d",
      "publishedAt": "2025-12-02T13:00:00.000Z",
      "author": "Quesby",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "general",
      "score": 6.96396355884597,
      "ingestedAt": "2025-12-02T14:44:03.181Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a3ef20",
      "title": "Building Intelligent AI Agents with MongoDB Atlas: A Bidirectional Data Flow Architecture",
      "url": "https://dev.to/pash10g/building-intelligent-ai-agents-with-mongodb-atlas-a-bidirectional-data-flow-architecture-2obl",
      "content": "<p>As AI agents become increasingly sophisticated, the way applications interact with databases is fundamentally changing. Gone are the days of simple CRUD operations and static queries. Modern AI-powered applications require a <strong>bidirectional data flow</strong> where:</p> \n \n<ol> \n<li> \n<strong>Agents feed from the database</strong> - Using semantic search and retrieval-augmented generation (RAG) to access relevant data</li> \n<li> \n<strong>Agents feed back to the database</strong> - Storing conversation context, user interactions, and learned preferences</li> \n<li> \n<strong>Agents transform the UI</strong> - Dynamically updating search filters, results, and interface elements based on natural language understanding</li> \n</ol> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5tyxm32nsyc9ok9zvbzf.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5tyxm32nsyc9ok9zvbzf.png\" alt=\"Application\" width=\"800\" height=\"436\"></a></p> \n \n<p>In this article, I'll walk you through a production-ready rental property search application that demonstrates how MongoDB Atlas's Document Model and Vector Search capabilities make this bidirectional agent-database architecture not just possible, but elegant and performant.</p> \n \n<blockquote> \n<p>Looking for realistic sample data? All of the screenshots and demos below use the 6k-listing Airbnb dataset that MongoDB published on Hugging Face: <a href=\"https://huggingface.co/datasets/MongoDB/airbnb_embeddings\">https://huggingface.co/datasets/MongoDB/airbnb_embeddings</a>. The repo ships with <code>seed-hf-airbnb-data.js</code>, which downloads that dataset, loads it into Atlas (including the vector field), and makes the entire experience turnkey.</p> \n</blockquote> \n \n<h2> \n   \n   \n  Why MongoDB Atlas is Perfect for AI Agent Applications \n</h2> \n \n<p>Before diving into the code, let's understand why MongoDB Atlas stands out for agent-based architectures:</p> \n \n<h3> \n   \n   \n  1. <strong>Flexible Document Model</strong> \n</h3> \n \n<p>AI agents work with diverse, semi-structured data - user conversations, property details, embeddings, and metadata. MongoDB's document model handles this naturally without rigid schemas:<br> \n</p> \n \n<div> \n<pre><code><span>{</span> \n  <span>\"</span><span>_id</span><span>\"</span><span>:</span> <span>ObjectId</span><span>(</span><span>\"</span><span>...</span><span>\"</span><span>),</span> \n  <span>\"</span><span>sessionId</span><span>\"</span><span>:</span> <span>\"</span><span>user-session-123</span><span>\"</span><span>,</span> \n  <span>\"</span><span>userId</span><span>\"</span><span>:</span> <span>ObjectId</span><span>(</span><span>\"</span><span>...</span><span>\"</span><span>),</span> \n  <span>\"</span><span>messages</span><span>\"</span><span>:</span> <span>[</span> \n    <span>{</span> \n      <span>\"</span><span>role</span><span>\"</span><span>:</span> <span>\"</span><span>user</span><span>\"</span><span>,</span> \n      <span>\"</span><span>content</span><span>\"</span><span>:</span> <span>\"</span><span>Find me a 2BR in Manhattan under $200</span><span>\"</span><span>,</span> \n      <span>\"</span><span>timestamp</span><span>\"</span><span>:</span> <span>ISODate</span><span>(</span><span>\"</span><span>2024-01-15T10:30:00Z</span><span>\"</span><span>),</span> \n      <span>\"</span><span>metadata</span><span>\"</span><span>:</span> <span>{</span> \n        <span>\"</span><span>context</span><span>\"</span><span>:</span> <span>{</span> <span>\"</span><span>filters</span><span>\"</span><span>:</span> <span>{</span> <span>\"</span><span>bedrooms</span><span>\"</span><span>:</span> <span>2</span><span>,</span> <span>\"</span><span>location</span><span>\"</span><span>:</span> <span>\"</span><span>New York</span><span>\"</span> <span>}</span> <span>}</span> \n      <span>}</span> \n    <span>},</span> \n    <span>{</span> \n      <span>\"</span><span>role</span><span>\"</span><span>:</span> <span>\"</span><span>assistant</span><span>\"</span><span>,</span> \n      <span>\"</span><span>content</span><span>\"</span><span>:</span> <span>\"</span><span>I found 15 properties matching your criteria...</span><span>\"</span><span>,</span> \n      <span>\"</span><span>timestamp</span><span>\"</span><span>:</span> <span>ISODate</span><span>(</span><span>\"</span><span>2024-01-15T10:30:05Z</span><span>\"</span><span>),</span> \n      <span>\"</span><span>metadata</span><span>\"</span><span>:</span> <span>{</span> \n        <span>\"</span><span>tool_calls_made</span><span>\"</span><span>:</span> <span>1</span><span>,</span> \n        <span>\"</span><span>search_performed</span><span>\"</span><span>:</span> <span>true</span><span>,</span> \n        <span>\"</span><span>rental_ids</span><span>\"</span><span>:</span> <span>[</span><span>123</span><span>,</span> <span>456</span><span>,</span> <span>789</span><span>]</span> \n      <span>}</span> \n    <span>}</span> \n  <span>],</span> \n  <span>\"</span><span>metadata</span><span>\"</span><span>:</span> <span>{</span> \n    <span>\"</span><span>totalMessages</span><span>\"</span><span>:</span> <span>2</span><span>,</span> \n    <span>\"</span><span>lastActivity</span><span>\"</span><span>:</span> <span>ISODate</span><span>(</span><span>\"</span><span>2024-01-15T10:30:05Z</span><span>\"</span><span>)</span> \n  <span>}</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  2. <strong>Native Vector Search</strong> \n</h3> \n \n<p>Atlas Vector Search enables semantic understanding at the database layer. No need for external vector databases or complex integrations:<br> \n</p> \n \n<div> \n<pre><code><span>{</span> \n  <span>$vectorSearch</span><span>:</span> <span>{</span> \n    <span>index</span><span>:</span> <span>\"</span><span>rental_vector_search</span><span>\"</span><span>,</span> \n    <span>path</span><span>:</span> <span>\"</span><span>text_embeddings</span><span>\"</span><span>,</span> \n    <span>queryVector</span><span>:</span> <span>[</span><span>0.1234</span><span>,</span> <span>-</span><span>0.5678</span><span>,</span> <span>...],</span> <span>// 1536-dimensional embedding</span> \n    <span>numCandidates</span><span>:</span> <span>100</span><span>,</span> \n    <span>limit</span><span>:</span> <span>10</span><span>,</span> \n    <span>filter</span><span>:</span> <span>{</span> \n      <span>\"</span><span>address.market</span><span>\"</span><span>:</span> <span>{</span> <span>$eq</span><span>:</span> <span>\"</span><span>New York</span><span>\"</span> <span>},</span> \n      <span>\"</span><span>price</span><span>\"</span><span>:</span> <span>{</span> <span>$lte</span><span>:</span> <span>200</span> <span>},</span> \n      <span>\"</span><span>bedrooms</span><span>\"</span><span>:</span> <span>{</span> <span>$gte</span><span>:</span> <span>2</span> <span>}</span> \n    <span>}</span> \n  <span>}</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  3. <strong>Rich Querying and Aggregations</strong> \n</h3> \n \n<p>MongoDB's aggregation pipeline lets you combine vector search with traditional filters, scoring, and transformations in a single operation.</p> \n \n<h3> \n   \n   \n  4. <strong>Unified Platform</strong> \n</h3> \n \n<p>Store embeddings, conversation history, user profiles, and application data in one database. No data synchronization headaches.</p> \n \n<h2> \n   \n   \n  Architecture Overview: The Bidirectional Data Flow \n</h2> \n \n<p>Our rental search application demonstrates three key data flows:<br> \n</p> \n \n<div> \n<pre><code>┌─────────────────────────────────────────────────────────────┐ \n│                         User Interface                       │ \n│              (Natural Language + Dynamic Filters)            │ \n└───────────────────────┬─────────────────────────────────────┘ \n                        │ \n                        ▼ \n┌─────────────────────────────────────────────────────────────┐ \n│                    OpenAI Agents SDK                         │ \n│              (GPT-5-mini with Custom Tools)                 │ \n└───────┬───────────────────────────┬─────────────────────────┘ \n        │                           │ \n        │ ① Agents Feed FROM DB     │ ② Agents Feed TO DB \n        ▼                           ▼ \n┌─────────────────────┐     ┌──────────────────────────────┐ \n│  Vector Search      │     │  Conversation Storage        │ \n│  • Embeddings       │     │  • Chat History             │ \n│  • Semantic Query   │     │  • User Context             │ \n│  • Filters          │     │  • Search Metadata          │ \n└─────────────────────┘     └──────────────────────────────┘ \n        │                           │ \n        └───────────┬───────────────┘ \n                    ▼ \n        ③ Agents Transform UI \n    ┌──────────────────────────┐ \n    │  • Update Search Filters │ \n    │  • Display Results       │ \n    │  • Modify Interface      │ \n    └──────────────────────────┘ \n</code></pre> \n \n</div> \n \n \n \n<p>Let's explore each flow in detail.</p> \n \n \n \n \n<h2> \n   \n   \n  Flow 1: Agents Feed FROM the Database (RAG Pattern) \n</h2> \n \n<p>The first and most critical flow is how agents access relevant data to answer user queries. This is the classic <strong>Retrieval-Augmented Generation (RAG)</strong> pattern.</p> \n \n<h3> \n   \n   \n  Step 1: Vector Embeddings as Data Foundation \n</h3> \n \n<p>Every rental property in our database includes a 1536-dimensional embedding generated from its description, amenities, and location:<br> \n</p> \n \n<div> \n<pre><code><span>{</span> \n  <span>\"</span><span>_id</span><span>\"</span><span>:</span> <span>12345</span><span>,</span> \n  <span>\"</span><span>name</span><span>\"</span><span>:</span> <span>\"</span><span>Luxury Manhattan Loft</span><span>\"</span><span>,</span> \n  <span>\"</span><span>description</span><span>\"</span><span>:</span> <span>\"</span><span>Stunning 2-bedroom loft in heart of SoHo...</span><span>\"</span><span>,</span> \n  <span>\"</span><span>property_type</span><span>\"</span><span>:</span> <span>\"</span><span>Loft</span><span>\"</span><span>,</span> \n  <span>\"</span><span>price</span><span>\"</span><span>:</span> <span>175</span><span>,</span> \n  <span>\"</span><span>bedrooms</span><span>\"</span><span>:</span> <span>2</span><span>,</span> \n  <span>\"</span><span>address</span><span>\"</span><span>:</span> <span>{</span> \n    <span>\"</span><span>market</span><span>\"</span><span>:</span> <span>\"</span><span>New York</span><span>\"</span><span>,</span> \n    <span>\"</span><span>neighbourhood</span><span>\"</span><span>:</span> <span>\"</span><span>SoHo</span><span>\"</span><span>,</span> \n    <span>\"</span><span>country</span><span>\"</span><span>:</span> <span>\"</span><span>United States</span><span>\"</span> \n  <span>},</span> \n  <span>\"</span><span>amenities</span><span>\"</span><span>:</span> <span>[</span><span>\"</span><span>WiFi</span><span>\"</span><span>,</span> <span>\"</span><span>Kitchen</span><span>\"</span><span>,</span> <span>\"</span><span>Elevator</span><span>\"</span><span>,</span> <span>\"</span><span>Gym</span><span>\"</span><span>],</span> \n  <span>\"</span><span>text_embeddings</span><span>\"</span><span>:</span> <span>[</span><span>0.023</span><span>,</span> <span>-</span><span>0.145</span><span>,</span> <span>0.891</span><span>,</span> <span>...],</span> <span>// ← Generated from OpenAI</span> \n  <span>\"</span><span>review_scores</span><span>\"</span><span>:</span> <span>{</span> \n    <span>\"</span><span>review_scores_rating</span><span>\"</span><span>:</span> <span>95</span> \n  <span>}</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>Key Insight</strong>: Embeddings are stored alongside the data they represent, eliminating the need for separate vector stores and JOIN operations.</p> \n \n<h3> \n   \n   \n  Step 2: Agent Tool Definition \n</h3> \n \n<p>Using the OpenAI Agents SDK, we define a <code>searchRentals</code> tool that the agent can invoke:<br> \n</p> \n \n<div> \n<pre><code><span>import</span> <span>{</span> <span>Agent</span><span>,</span> <span>tool</span> <span>}</span> <span>from</span> <span>'</span><span>@openai/agents</span><span>'</span><span>;</span> \n<span>import</span> <span>{</span> <span>z</span> <span>}</span> <span>from</span> <span>'</span><span>zod</span><span>'</span><span>;</span> \n \n<span>this</span><span>.</span><span>searchRentalsTool</span> <span>=</span> <span>tool</span><span>({</span> \n  <span>name</span><span>:</span> <span>'</span><span>searchRentals</span><span>'</span><span>,</span> \n  <span>description</span><span>:</span> <span>\"</span><span>'Search for rental properties using semantic search based on user preferences.',</span><span>\"</span> \n  <span>parameters</span><span>:</span> <span>z</span><span>.</span><span>object</span><span>({</span> \n    <span>query</span><span>:</span> <span>z</span><span>.</span><span>string</span><span>().</span><span>describe</span><span>(</span><span>'</span><span>Natural language search query</span><span>'</span><span>),</span> \n    <span>filters</span><span>:</span> <span>z</span><span>.</span><span>object</span><span>({</span> \n      <span>min_price</span><span>:</span> <span>z</span><span>.</span><span>number</span><span>().</span><span>nullable</span><span>().</span><span>optional</span><span>(),</span> \n      <span>max_price</span><span>:</span> <span>z</span><span>.</span><span>number</span><span>().</span><span>nullable</span><span>().</span><span>optional</span><span>(),</span> \n      <span>min_bedrooms</span><span>:</span> <span>z</span><span>.</span><span>number</span><span>().</span><span>nullable</span><span>().</span><span>optional</span><span>(),</span> \n      <span>location</span><span>:</span> <span>z</span><span>.</span><span>string</span><span>().</span><span>nullable</span><span>().</span><span>optional</span><span>(),</span> \n      <span>superhost_only</span><span>:</span> <span>z</span><span>.</span><span>boolean</span><span>().</span><span>nullable</span><span>().</span><span>optional</span><span>()</span> \n    <span>}).</span><span>nullable</span><span>().</span><span>optional</span><span>(),</span> \n    <span>limit</span><span>:</span> <span>z</span><span>.</span><span>number</span><span>().</span><span>default</span><span>(</span><span>5</span><span>)</span> \n  <span>}),</span> \n  <span>execute</span><span>:</span> <span>this</span><span>.</span><span>handleSearchRentals</span><span>.</span><span>bind</span><span>(</span><span>this</span><span>)</span> \n<span>});</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>What makes this powerful</strong>: The agent understands the tool's capabilities through the description and parameter schema, deciding when and how to invoke it based on user intent.</p> \n \n<h3> \n   \n   \n  Step 3: Hybrid Search Implementation \n</h3> \n \n<p>When the agent invokes the tool, we perform a <strong>hybrid search</strong> combining vector similarity with traditional filters:<br> \n</p> \n \n<div> \n<pre><code><span>async</span> <span>hybridSearch</span><span>(</span><span>queryText</span><span>,</span> <span>filters</span> <span>=</span> <span>{},</span> <span>limit</span> <span>=</span> <span>10</span><span>)</span> <span>{</span> \n  <span>// Generate query embedding</span> \n  <span>const</span> <span>queryEmbedding</span> <span>=</span> <span>await</span> <span>this</span><span>.</span><span>generateEmbedding</span><span>(</span><span>queryText</span><span>);</span> \n \n  <span>// Build vector search pipeline</span> \n  <span>const</span> <span>pipeline</span> <span>=</span> <span>[</span> \n    <span>{</span> \n      <span>$vectorSearch</span><span>:</span> <span>{</span> \n        <span>index</span><span>:</span> <span>\"</span><span>rental_vector_search</span><span>\"</span><span>,</span> \n        <span>path</span><span>:</span> <span>\"</span><span>text_embeddings</span><span>\"</span><span>,</span> \n        <span>queryVector</span><span>:</span> <span>queryEmbedding</span><span>,</span> \n        <span>numCandidates</span><span>:</span> <span>100</span><span>,</span> \n        <span>limit</span><span>:</span> <span>limit</span><span>,</span> \n        <span>filter</span><span>:</span> <span>{</span> \n          <span>// Combine semantic + structured filters</span> \n          <span>\"</span><span>address.market</span><span>\"</span><span>:</span> <span>filters</span><span>.</span><span>location</span> <span>?</span> <span>{</span> <span>$eq</span><span>:</span> <span>filters</span><span>.</span><span>location</span> <span>}</span> <span>:</span> <span>undefined</span><span>,</span> \n          <span>\"</span><span>price</span><span>\"</span><span>:</span> <span>{</span> \n            <span>$gte</span><span>:</span> <span>filters</span><span>.</span><span>min_price</span> <span>||</span> <span>0</span><span>,</span> \n            <span>$lte</span><span>:</span> <span>filters</span><span>.</span><span>max_price</span> <span>||</span> <span>999999</span> \n          <span>},</span> \n          <span>\"</span><span>bedrooms</span><span>\"</span><span>:</span> <span>{</span> <span>$gte</span><span>:</span> <span>filters</span><span>.</span><span>min_bedrooms</span> <span>||</span> <span>0</span> <span>},</span> \n          <span>\"</span><span>host.host_is_superhost</span><span>\"</span><span>:</span> <span>filters</span><span>.</span><span>superhost_only</span> <span>?</span> <span>{</span> <span>$eq</span><span>:</span> <span>true</span> <span>}</span> <span>:</span> <span>undefined</span> \n        <span>}</span> \n      <span>}</span> \n    <span>},</span> \n    <span>{</span> \n      <span>$project</span><span>:</span> <span>{</span> \n        <span>name</span><span>:</span> <span>1</span><span>,</span> \n        <span>description</span><span>:</span> <span>\"</span><span>1,</span><span>\"</span> \n        <span>property_type</span><span>:</span> <span>1</span><span>,</span> \n        <span>price</span><span>:</span> <span>1</span><span>,</span> \n        <span>bedrooms</span><span>:</span> <span>1</span><span>,</span> \n        <span>\"</span><span>address.market</span><span>\"</span><span>:</span> <span>1</span><span>,</span> \n        <span>\"</span><span>address.country</span><span>\"</span><span>:</span> <span>1</span><span>,</span> \n        <span>score</span><span>:</span> <span>{</span> <span>$meta</span><span>:</span> <span>\"</span><span>vectorSearchScore</span><span>\"</span> <span>}</span> <span>// ← Similarity score</span> \n      <span>}</span> \n    <span>}</span> \n  <span>];</span> \n \n  <span>return</span> <span>await</span> <span>collection</span><span>.</span><span>aggregate</span><span>(</span><span>pipeline</span><span>).</span><span>toArray</span><span>();</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>MongoDB's Superpower Here</strong>:</p> \n \n<ul> \n<li>Vector search and traditional filters execute in a <strong>single database query</strong> \n</li> \n<li>No post-processing, no multiple round-trips</li> \n<li>Results are sorted by semantic relevance (cosine similarity)</li> \n</ul> \n \n<h3> \n   \n   \n  Step 4: Agent Processes and Responds \n</h3> \n \n<p>The agent receives structured results and generates a natural language response:<br> \n</p> \n \n<div> \n<pre><code><span>async</span> <span>handleSearchRentals</span><span>({</span> <span>query</span><span>,</span> <span>filters</span><span>,</span> <span>limit</span> <span>})</span> <span>{</span> \n  <span>const</span> <span>results</span> <span>=</span> <span>await</span> <span>vectorSearchService</span><span>.</span><span>hybridSearch</span><span>(</span><span>query</span><span>,</span> <span>filters</span><span>,</span> <span>limit</span><span>);</span> \n \n  <span>// Format for agent consumption</span> \n  <span>const</span> <span>formattedResults</span> <span>=</span> <span>results</span><span>.</span><span>map</span><span>((</span><span>rental</span><span>,</span> <span>index</span><span>)</span> <span>=&gt;</span> <span>({</span> \n    <span>rank</span><span>:</span> <span>index</span> <span>+</span> <span>1</span><span>,</span> \n    <span>id</span><span>:</span> <span>rental</span><span>.</span><span>_id</span><span>,</span> \n    <span>name</span><span>:</span> <span>rental</span><span>.</span><span>name</span><span>,</span> \n    <span>price</span><span>:</span> <span>rental</span><span>.</span><span>price</span><span>,</span> \n    <span>bedrooms</span><span>:</span> <span>rental</span><span>.</span><span>bedrooms</span><span>,</span> \n    <span>location</span><span>:</span> <span>`</span><span>${</span><span>rental</span><span>.</span><span>address</span><span>.</span><span>neighbourhood</span><span>}</span><span>, </span><span>${</span><span>rental</span><span>.</span><span>address</span><span>.</span><span>country</span><span>}</span><span>`</span><span>,</span> \n    <span>rating</span><span>:</span> <span>(</span><span>rental</span><span>.</span><span>review_scores</span><span>.</span><span>review_scores_rating</span> <span>/</span> <span>20</span><span>).</span><span>toFixed</span><span>(</span><span>1</span><span>),</span> \n    <span>similarity_score</span><span>:</span> <span>rental</span><span>.</span><span>score</span><span>.</span><span>toFixed</span><span>(</span><span>3</span><span>)</span> \n  <span>}));</span> \n \n  <span>return</span> <span>JSON</span><span>.</span><span>stringify</span><span>({</span> \n    <span>total_found</span><span>:</span> <span>results</span><span>.</span><span>length</span><span>,</span> \n    <span>query_used</span><span>:</span> <span>query</span><span>,</span> \n    <span>results</span><span>:</span> <span>formattedResults</span> \n  <span>});</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>The Result</strong>: User asks \"Find me a cozy apartment in Barcelona for under €150\" → Agent extracts intent → Searches MongoDB with semantic understanding → Returns relevant properties.</p> \n \n \n \n \n<h2> \n   \n   \n  Flow 2: Agents Feed TO the Database (Context Persistence) \n</h2> \n \n<p>What makes AI agents truly intelligent is <strong>memory</strong>. Every interaction teaches the system about user preferences and context. MongoDB's document model makes this persistence natural.</p> \n \n<h3> \n   \n   \n  Conversation Storage Pattern \n</h3> \n \n \n \n<div> \n<pre><code><span>export</span> <span>class</span> <span>ConversationModel</span> <span>{</span> \n  <span>static</span> <span>async</span> <span>addMessage</span><span>(</span><span>sessionId</span><span>,</span> <span>role</span><span>,</span> <span>content</span><span>,</span> <span>metadata</span> <span>=</span> <span>{},</span> <span>userId</span> <span>=</span> <span>null</span><span>)</span> <span>{</span> \n    <span>const</span> <span>message</span> <span>=</span> <span>{</span> \n      <span>id</span><span>:</span> <span>new</span> <span>ObjectId</span><span>().</span><span>toString</span><span>(),</span> \n      <span>role</span><span>,</span> <span>// 'user' or 'assistant'</span> \n      <span>content</span><span>,</span> \n      <span>timestamp</span><span>:</span> <span>new</span> <span>Date</span><span>(),</span> \n      <span>metadata</span><span>:</span> <span>{</span> \n        <span>...</span><span>metadata</span><span>,</span> \n        <span>userId</span><span>:</span> <span>userId</span> <span>||</span> <span>null</span><span>,</span> \n        <span>isAuthenticated</span><span>:</span> <span>userId</span> <span>!==</span> <span>null</span> \n      <span>}</span> \n    <span>};</span> \n \n    <span>// Upsert pattern: Create conversation if not exists</span> \n    <span>await</span> <span>collection</span><span>.</span><span>updateOne</span><span>(</span> \n      <span>{</span> <span>sessionId</span> <span>},</span> \n      <span>{</span> \n        <span>$push</span><span>:</span> <span>{</span> <span>messages</span><span>:</span> <span>message</span> <span>},</span> \n        <span>$inc</span><span>:</span> <span>{</span> <span>'</span><span>metadata.totalMessages</span><span>'</span><span>:</span> <span>1</span> <span>},</span> \n        <span>$set</span><span>:</span> <span>{</span> \n          <span>updatedAt</span><span>:</span> <span>new</span> <span>Date</span><span>(),</span> \n          <span>'</span><span>metadata.lastActivity</span><span>'</span><span>:</span> <span>new</span> <span>Date</span><span>()</span> \n        <span>},</span> \n        <span>$setOnInsert</span><span>:</span> <span>{</span> \n          <span>userId</span><span>:</span> <span>userId</span><span>,</span> \n          <span>createdAt</span><span>:</span> <span>new</span> <span>Date</span><span>()</span> \n        <span>}</span> \n      <span>},</span> \n      <span>{</span> <span>upsert</span><span>:</span> <span>true</span> <span>}</span> \n    <span>);</span> \n  <span>}</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>Why This Matters</strong>:</p> \n \n<ul> \n<li> \n<strong>Upsert Pattern</strong>: Create conversation on first message, append to existing ones</li> \n<li> \n<strong>Nested Documents</strong>: Messages are embedded in conversation, no JOINs needed</li> \n<li> \n<strong>Atomic Updates</strong>: <code>$push</code>, <code>$inc</code>, <code>$set</code> operations are atomic and efficient</li> \n<li> \n<strong>Rich Metadata</strong>: Store context about tool calls, search results, user state</li> \n</ul> \n \n<h3> \n   \n   \n  Storing Agent Metadata \n</h3> \n \n<p>After the agent responds, we capture what it did:<br> \n</p> \n \n<div> \n<pre><code><span>// Store assistant response with rich metadata</span> \n<span>await</span> <span>ConversationModel</span><span>.</span><span>addMessage</span><span>(</span><span>sessionId</span><span>,</span> <span>'</span><span>assistant</span><span>'</span><span>,</span> <span>response</span><span>.</span><span>message</span><span>,</span> <span>{</span> \n  <span>tool_calls_made</span><span>:</span> <span>response</span><span>.</span><span>toolCalls</span><span>?.</span><span>length</span> <span>||</span> <span>0</span><span>,</span> \n  <span>has_rental_results</span><span>:</span> <span>response</span><span>.</span><span>metadata</span><span>?.</span><span>search_performed</span> <span>||</span> <span>false</span><span>,</span> \n  <span>search_metadata</span><span>:</span> <span>{</span> \n    <span>query</span><span>:</span> <span>response</span><span>.</span><span>metadata</span><span>.</span><span>search_query</span><span>,</span> \n    <span>filters_applied</span><span>:</span> <span>response</span><span>.</span><span>metadata</span><span>.</span><span>search_filters</span><span>,</span> \n    <span>rental_ids</span><span>:</span> <span>response</span><span>.</span><span>metadata</span><span>.</span><span>rental_ids</span> <span>// ← IDs of returned properties</span> \n  <span>},</span> \n  <span>timestamp</span><span>:</span> <span>new</span> <span>Date</span><span>().</span><span>toISOString</span><span>()</span> \n<span>},</span> <span>userId</span><span>);</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>The Power</strong>: Later queries can reference previous searches, compare properties, or recall user preferences - all because we stored structured metadata alongside conversational content.</p> \n \n<h3> \n   \n   \n  User Activity Tracking \n</h3> \n \n<p>MongoDB's flexible schema lets us track diverse user actions:<br> \n</p> \n \n<div> \n<pre><code><span>await</span> <span>UserModel</span><span>.</span><span>updateActivity</span><span>(</span><span>userId</span><span>,</span> <span>{</span> \n  <span>$push</span><span>:</span> <span>{</span> \n    <span>activity_log</span><span>:</span> <span>{</span> \n      <span>action</span><span>:</span> <span>'</span><span>search_performed</span><span>'</span><span>,</span> \n      <span>timestamp</span><span>:</span> <span>new</span> <span>Date</span><span>(),</span> \n      <span>details</span><span>:</span> <span>{</span> \n        <span>query</span><span>:</span> <span>userMessage</span><span>,</span> \n        <span>results_count</span><span>:</span> <span>results</span><span>.</span><span>length</span><span>,</span> \n        <span>filters_used</span><span>:</span> <span>filters</span> \n      <span>}</span> \n    <span>}</span> \n  <span>},</span> \n  <span>$inc</span><span>:</span> <span>{</span> <span>'</span><span>stats.total_searches</span><span>'</span><span>:</span> <span>1</span> <span>},</span> \n  <span>$set</span><span>:</span> <span>{</span> <span>'</span><span>stats.last_search_date</span><span>'</span><span>:</span> <span>new</span> <span>Date</span><span>()</span> <span>}</span> \n<span>});</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>Real-World Use Case</strong>: Build personalized recommendations, identify power users, analyze search patterns - all from this rich behavioral data.</p> \n \n \n \n \n<h2> \n   \n   \n  Flow 3: Agents Transform the UI (Dynamic Interface Updates) \n</h2> \n \n<p>The most magical aspect of agent-database integration is when the agent's understanding directly manipulates the user interface.</p> \n \n<h3> \n   \n   \n  The Metadata Bridge \n</h3> \n \n<p>When an agent performs a search, it returns not just conversational text, but structured metadata:<br> \n</p> \n \n<div> \n<pre><code><span>{</span> \n  <span>success</span><span>:</span> <span>true</span><span>,</span> \n  <span>message</span><span>:</span> <span>\"</span><span>I found 15 great properties in Barcelona under €150...</span><span>\"</span><span>,</span> \n  <span>metadata</span><span>:</span> <span>{</span> \n    <span>search_performed</span><span>:</span> <span>true</span><span>,</span> \n    <span>search_query</span><span>:</span> <span>\"</span><span>cozy apartment in Barcelona under €150</span><span>\"</span><span>,</span> \n    <span>search_filters</span><span>:</span> <span>{</span> \n      <span>location</span><span>:</span> <span>\"</span><span>Barcelona</span><span>\"</span><span>,</span> \n      <span>max_price</span><span>:</span> <span>150</span><span>,</span> \n      <span>property_type</span><span>:</span> <span>\"</span><span>Apartment</span><span>\"</span> \n    <span>},</span> \n    <span>rental_ids</span><span>:</span> <span>[</span><span>12345</span><span>,</span> <span>12346</span><span>,</span> <span>12347</span><span>,</span> <span>...]</span> \n  <span>}</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  Frontend Integration \n</h3> \n \n<p>The UI watches for this metadata and reacts:<br> \n</p> \n \n<div> \n<pre><code><span>async</span> <span>function</span> <span>sendMessage</span><span>()</span> <span>{</span> \n  <span>const</span> <span>response</span> <span>=</span> <span>await</span> <span>fetch</span><span>(</span><span>'</span><span>/chat</span><span>'</span><span>,</span> <span>{</span> \n    <span>method</span><span>:</span> <span>'</span><span>POST</span><span>'</span><span>,</span> \n    <span>body</span><span>:</span> <span>JSON</span><span>.</span><span>stringify</span><span>({</span> \n      <span>message</span><span>:</span> <span>userInput</span><span>,</span> \n      <span>context</span><span>:</span> <span>{</span> \n        <span>current_search</span><span>:</span> <span>searchBar</span><span>.</span><span>value</span><span>,</span> \n        <span>filters</span><span>:</span> <span>getCurrentFilters</span><span>()</span> \n      <span>}</span> \n    <span>})</span> \n  <span>});</span> \n \n  <span>const</span> <span>data</span> <span>=</span> <span>await</span> <span>response</span><span>.</span><span>json</span><span>();</span> \n \n  <span>// Display conversational response</span> \n  <span>displayMessage</span><span>(</span><span>data</span><span>.</span><span>message</span><span>);</span> \n \n  <span>// Check if agent performed a search</span> \n  <span>if </span><span>(</span><span>data</span><span>.</span><span>metadata</span><span>.</span><span>search_performed</span><span>)</span> <span>{</span> \n    <span>// ① Update UI filters based on agent's understanding</span> \n    <span>updateFiltersUI</span><span>(</span><span>data</span><span>.</span><span>metadata</span><span>.</span><span>search_filters</span><span>);</span> \n \n    <span>// ② Fetch and display the rental results</span> \n    <span>const</span> <span>rentals</span> <span>=</span> <span>await</span> <span>fetchRentalsByIds</span><span>(</span><span>data</span><span>.</span><span>metadata</span><span>.</span><span>rental_ids</span><span>);</span> \n    <span>displayRentals</span><span>(</span><span>rentals</span><span>);</span> \n \n    <span>// ③ Update URL and browser history</span> \n    <span>updateURLParams</span><span>(</span><span>data</span><span>.</span><span>metadata</span><span>.</span><span>search_filters</span><span>);</span> \n  <span>}</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>User Experience</strong>:<br> \n</p> \n \n<div> \n<pre><code>User: \"Show me 2 bedroom apartments in Manhattan under $200\" \n         ↓ \nAgent: [Understands intent, extracts filters, searches MongoDB] \n         ↓ \nUI: ✨ Location dropdown changes to \"New York\" \n    ✨ Bedrooms filter updates to \"2+\" \n    ✨ Price slider moves to \"$0-$200\" \n    ✨ Results grid displays matching properties \n    ✨ Chat shows: \"I found 15 properties matching your criteria...\" \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  Bidirectional Filter Sync \n</h3> \n \n<p>The genius is that filters work both ways:</p> \n \n<ol> \n<li> \n<strong>Manual Filter → Agent Context</strong>: User adjusts UI filters → Passed to agent in next message</li> \n<li> \n<strong>Agent Understanding → UI Filters</strong>: Agent extracts intent from natural language → Updates UI filters \n</li> \n</ol> \n \n<div> \n<pre><code><span>// Sending filter context to agent</span> \n<span>const</span> <span>chatPayload</span> <span>=</span> <span>{</span> \n  <span>message</span><span>:</span> <span>userInput</span><span>,</span> \n  <span>context</span><span>:</span> <span>{</span> \n    <span>filters</span><span>:</span> <span>{</span> \n      <span>location</span><span>:</span> <span>locationDropdown</span><span>.</span><span>value</span><span>,</span> \n      <span>min_price</span><span>:</span> <span>priceSlider</span><span>.</span><span>min</span><span>,</span> \n      <span>max_price</span><span>:</span> <span>priceSlider</span><span>.</span><span>max</span><span>,</span> \n      <span>bedrooms</span><span>:</span> <span>bedroomFilter</span><span>.</span><span>value</span> \n    <span>}</span> \n  <span>}</span> \n<span>};</span> \n \n<span>// Agent enhances message with current filter state</span> \n<span>if </span><span>(</span><span>context</span><span>.</span><span>filters</span> <span>&amp;&amp;</span> <span>Object</span><span>.</span><span>keys</span><span>(</span><span>context</span><span>.</span><span>filters</span><span>).</span><span>length</span> <span>&gt;</span> <span>0</span><span>)</span> <span>{</span> \n  <span>enhancedMessage</span> <span>+=</span> <span>` Current filters: </span><span>${</span><span>formatFilters</span><span>(</span><span>context</span><span>.</span><span>filters</span><span>)}</span><span>`</span><span>;</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>Why This Works</strong>: MongoDB stores both the agent's understanding (in conversation metadata) and the current UI state (in user preferences), creating a single source of truth.</p> \n \n \n \n \n<h2> \n   \n   \n  Advanced Patterns: Going Beyond Basic RAG \n</h2> \n \n<h3> \n   \n   \n  Pattern 1: Saved Rentals with Agent Integration \n</h3> \n \n<p>Users can save favorite properties, and the agent accesses this data:<br> \n</p> \n \n<div> \n<pre><code><span>this</span><span>.</span><span>getSavedRentalsTool</span> <span>=</span> <span>tool</span><span>({</span> \n  <span>name</span><span>:</span> <span>'</span><span>getSavedRentals</span><span>'</span><span>,</span> \n  <span>description</span><span>:</span> <span>'</span><span>Get the user</span><span>\\'</span><span>s saved rental properties for comparison and recommendations.</span><span>'</span><span>,</span> \n  <span>parameters</span><span>:</span> <span>z</span><span>.</span><span>object</span><span>({</span> \n    <span>includeDetails</span><span>:</span> <span>z</span><span>.</span><span>boolean</span><span>().</span><span>default</span><span>(</span><span>false</span><span>)</span> \n  <span>}),</span> \n  <span>execute</span><span>:</span> <span>async </span><span>({</span> <span>includeDetails</span> <span>})</span> <span>=&gt;</span> <span>{</span> \n    <span>const</span> <span>savedRentals</span> <span>=</span> <span>await</span> <span>UserModel</span><span>.</span><span>getSavedRentals</span><span>(</span><span>userId</span><span>);</span> \n \n    <span>if </span><span>(</span><span>includeDetails</span><span>)</span> <span>{</span> \n      <span>// Fetch full property data using rental IDs</span> \n      <span>const</span> <span>detailedRentals</span> <span>=</span> <span>await</span> <span>Promise</span><span>.</span><span>all</span><span>(</span> \n        <span>savedRentals</span><span>.</span><span>map</span><span>(</span><span>saved</span> <span>=&gt;</span> <span>RentalModel</span><span>.</span><span>findById</span><span>(</span><span>saved</span><span>.</span><span>rental_id</span><span>))</span> \n      <span>);</span> \n      <span>return</span> <span>JSON</span><span>.</span><span>stringify</span><span>(</span><span>detailedRentals</span><span>);</span> \n    <span>}</span> \n \n    <span>return</span> <span>JSON</span><span>.</span><span>stringify</span><span>(</span><span>savedRentals</span><span>);</span> \n  <span>}</span> \n<span>});</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>User Experience</strong>:<br> \n</p> \n \n<div> \n<pre><code>User: \"Compare my saved properties in terms of price and location\" \n         ↓ \nAgent: [Calls getSavedRentals with includeDetails=true] \n         ↓ \nMongoDB: Returns full property documents \n         ↓ \nAgent: \"Here's a comparison of your 3 saved properties: \n        1. Manhattan Loft ($175/night) - SoHo, great for nightlife \n        2. Barcelona Apartment (€120/night) - Gothic Quarter, historic charm \n        3. Sydney Studio ($140/night) - Bondi, beach vibes \n \n        The Barcelona option offers the best value, while Manhattan is ideal \n        if you prioritize being in the center of the action.\" \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  Pattern 2: Context-Aware Property Details \n</h3> \n \n<p>When a user views a property, that context is passed to the agent:<br> \n</p> \n \n<div> \n<pre><code><span>const</span> <span>chatPayload</span> <span>=</span> <span>{</span> \n  <span>message</span><span>:</span> <span>\"</span><span>Tell me about the neighborhood</span><span>\"</span><span>,</span> \n  <span>context</span><span>:</span> <span>{</span> \n    <span>current_property</span><span>:</span> <span>{</span> \n      <span>id</span><span>:</span> <span>12345</span><span>,</span> \n      <span>name</span><span>:</span> <span>\"</span><span>Luxury Manhattan Loft</span><span>\"</span><span>,</span> \n      <span>location</span><span>:</span> <span>{</span> <span>market</span><span>:</span> <span>\"</span><span>New York</span><span>\"</span><span>,</span> <span>neighbourhood</span><span>:</span> <span>\"</span><span>SoHo</span><span>\"</span> <span>},</span> \n      <span>features</span><span>:</span> <span>{</span> <span>bedrooms</span><span>:</span> <span>2</span><span>,</span> <span>price</span><span>:</span> <span>175</span> <span>}</span> \n    <span>}</span> \n  <span>}</span> \n<span>};</span> \n</code></pre> \n \n</div> \n \n \n \n<p>The agent receives this context and provides targeted advice:<br> \n</p> \n \n<div> \n<pre><code><span>if </span><span>(</span><span>context</span><span>.</span><span>current_property</span><span>)</span> <span>{</span> \n  <span>const</span> <span>property</span> <span>=</span> <span>context</span><span>.</span><span>current_property</span><span>;</span> \n  <span>enhancedMessage</span> <span>+=</span> <span>` User is currently viewing: \"</span><span>${</span><span>property</span><span>.</span><span>name</span><span>}</span><span>\" in </span><span>${</span><span>property</span><span>.</span><span>location</span><span>.</span><span>neighbourhood</span><span>}</span><span>`</span><span>;</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>Agent Response</strong>: \"SoHo is one of Manhattan's most vibrant neighborhoods, known for its cast-iron architecture, upscale boutiques, and art galleries. You'll be walking distance from great restaurants and nightlife. At $175/night for a 2-bedroom, this is competitive for the area.\"</p> \n \n<h3> \n   \n   \n  Pattern 3: Hybrid Search with Scoring \n</h3> \n \n<p>Combine vector similarity with business logic:<br> \n</p> \n \n<div> \n<pre><code><span>const</span> <span>pipeline</span> <span>=</span> <span>[</span> \n  <span>{</span> \n    <span>$vectorSearch</span><span>:</span> <span>{</span> \n      <span>index</span><span>:</span> <span>\"</span><span>rental_vector_search</span><span>\"</span><span>,</span> \n      <span>path</span><span>:</span> <span>\"</span><span>text_embeddings</span><span>\"</span><span>,</span> \n      <span>queryVector</span><span>:</span> <span>queryEmbedding</span><span>,</span> \n      <span>numCandidates</span><span>:</span> <span>100</span><span>,</span> \n      <span>limit</span><span>:</span> <span>50</span> <span>// Get more candidates for scoring</span> \n    <span>}</span> \n  <span>},</span> \n  <span>{</span> \n    <span>$addFields</span><span>:</span> <span>{</span> \n      <span>vector_score</span><span>:</span> <span>{</span> <span>$meta</span><span>:</span> <span>\"</span><span>vectorSearchScore</span><span>\"</span> <span>},</span> \n      <span>rating_score</span><span>:</span> <span>{</span> \n        <span>$divide</span><span>:</span> <span>[</span><span>\"</span><span>$review_scores.review_scores_rating</span><span>\"</span><span>,</span> <span>100</span><span>]</span> \n      <span>},</span> \n      <span>superhost_bonus</span><span>:</span> <span>{</span> \n        <span>$cond</span><span>:</span> <span>[</span><span>\"</span><span>$host.host_is_superhost</span><span>\"</span><span>,</span> <span>0.1</span><span>,</span> <span>0</span><span>]</span> \n      <span>}</span> \n    <span>}</span> \n  <span>},</span> \n  <span>{</span> \n    <span>$addFields</span><span>:</span> <span>{</span> \n      <span>final_score</span><span>:</span> <span>{</span> \n        <span>$add</span><span>:</span> <span>[</span> \n          <span>{</span> <span>$multiply</span><span>:</span> <span>[</span><span>\"</span><span>$vector_score</span><span>\"</span><span>,</span> <span>0.6</span><span>]</span> <span>},</span>      <span>// 60% semantic relevance</span> \n          <span>{</span> <span>$multiply</span><span>:</span> <span>[</span><span>\"</span><span>$rating_score</span><span>\"</span><span>,</span> <span>0.3</span><span>]</span> <span>},</span>      <span>// 30% ratings</span> \n          <span>{</span> <span>$multiply</span><span>:</span> <span>[</span><span>\"</span><span>$superhost_bonus</span><span>\"</span><span>,</span> <span>0.1</span><span>]</span> <span>}</span>    <span>// 10% superhost boost</span> \n        <span>]</span> \n      <span>}</span> \n    <span>}</span> \n  <span>},</span> \n  <span>{</span> \n    <span>$sort</span><span>:</span> <span>{</span> <span>final_score</span><span>:</span> <span>-</span><span>1</span> <span>}</span> \n  <span>},</span> \n  <span>{</span> \n    <span>$limit</span><span>:</span> <span>10</span> \n  <span>}</span> \n<span>];</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>Result</strong>: Properties ranked by a combination of semantic relevance, user ratings, and business rules - all computed in MongoDB.</p> \n \n \n \n \n<h2> \n   \n   \n  MongoDB Atlas Setup for Production \n</h2> \n \n<h3> \n   \n   \n  1. Vector Search Index Configuration \n</h3> \n \n \n \n<div> \n<pre><code><span>{</span><span> \n  </span><span>\"fields\"</span><span>:</span><span> </span><span>[</span><span> \n    </span><span>{</span><span> \n      </span><span>\"numDimensions\"</span><span>:</span><span> </span><span>1536</span><span>,</span><span> \n      </span><span>\"path\"</span><span>:</span><span> </span><span>\"text_embeddings\"</span><span>,</span><span> \n      </span><span>\"similarity\"</span><span>:</span><span> </span><span>\"cosine\"</span><span>,</span><span> \n      </span><span>\"type\"</span><span>:</span><span> </span><span>\"vector\"</span><span> \n    </span><span>},</span><span> \n    </span><span>{</span><span> \n      </span><span>\"path\"</span><span>:</span><span> </span><span>\"property_type\"</span><span>,</span><span> \n      </span><span>\"type\"</span><span>:</span><span> </span><span>\"filter\"</span><span> \n    </span><span>},</span><span> \n    </span><span>{</span><span> \n      </span><span>\"path\"</span><span>:</span><span> </span><span>\"address.market\"</span><span>,</span><span> \n      </span><span>\"type\"</span><span>:</span><span> </span><span>\"filter\"</span><span> \n    </span><span>},</span><span> \n    </span><span>{</span><span> \n      </span><span>\"path\"</span><span>:</span><span> </span><span>\"price\"</span><span>,</span><span> \n      </span><span>\"type\"</span><span>:</span><span> </span><span>\"filter\"</span><span> \n    </span><span>},</span><span> \n    </span><span>{</span><span> \n      </span><span>\"path\"</span><span>:</span><span> </span><span>\"bedrooms\"</span><span>,</span><span> \n      </span><span>\"type\"</span><span>:</span><span> </span><span>\"filter\"</span><span> \n    </span><span>},</span><span> \n    </span><span>{</span><span> \n      </span><span>\"path\"</span><span>:</span><span> </span><span>\"host.host_is_superhost\"</span><span>,</span><span> \n      </span><span>\"type\"</span><span>:</span><span> </span><span>\"filter\"</span><span> \n    </span><span>}</span><span> \n  </span><span>]</span><span> \n</span><span>}</span><span> \n</span></code></pre> \n \n</div> \n \n \n \n<p><strong>Key Points</strong>:</p> \n \n<ul> \n<li> \n<code>vector</code> field for semantic search</li> \n<li> \n<code>filter</code> fields for structured filtering</li> \n<li>Cosine similarity for 1536-dim OpenAI embeddings</li> \n</ul> \n \n<h3> \n   \n   \n  2. Supporting Indexes \n</h3> \n \n \n \n<div> \n<pre><code><span>// Conversation history lookup</span> \n<span>db</span><span>.</span><span>conversations</span><span>.</span><span>createIndex</span><span>({</span> <span>\"</span><span>sessionId</span><span>\"</span><span>:</span> <span>1</span> <span>});</span> \n<span>db</span><span>.</span><span>conversations</span><span>.</span><span>createIndex</span><span>({</span> <span>\"</span><span>userId</span><span>\"</span><span>:</span> <span>1</span><span>,</span> <span>\"</span><span>metadata.lastActivity</span><span>\"</span><span>:</span> <span>-</span><span>1</span> <span>});</span> \n \n<span>// User activity queries</span> \n<span>db</span><span>.</span><span>users</span><span>.</span><span>createIndex</span><span>({</span> <span>\"</span><span>username</span><span>\"</span><span>:</span> <span>1</span> <span>},</span> <span>{</span> <span>unique</span><span>:</span> <span>true</span> <span>});</span> \n<span>db</span><span>.</span><span>users</span><span>.</span><span>createIndex</span><span>({</span> <span>\"</span><span>saved_rentals.rental_id</span><span>\"</span><span>:</span> <span>1</span> <span>});</span> \n \n<span>// Rental property queries</span> \n<span>db</span><span>.</span><span>rentals</span><span>.</span><span>createIndex</span><span>({</span> <span>\"</span><span>address.market</span><span>\"</span><span>:</span> <span>1</span><span>,</span> <span>\"</span><span>price</span><span>\"</span><span>:</span> <span>1</span> <span>});</span> \n<span>db</span><span>.</span><span>rentals</span><span>.</span><span>createIndex</span><span>({</span> <span>\"</span><span>bedrooms</span><span>\"</span><span>:</span> <span>1</span><span>,</span> <span>\"</span><span>accommodates</span><span>\"</span><span>:</span> <span>1</span> <span>});</span> \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  3. Aggregation Pipeline Optimization \n</h3> \n \n<p><strong>Use <code>$project</code> early</strong> to reduce data transfer:<br> \n</p> \n \n<div> \n<pre><code><span>{</span> \n  <span>$vectorSearch</span><span>:</span> <span>{</span> <span>/* ... */</span> <span>}</span> \n<span>},</span> \n<span>{</span> \n  <span>$project</span><span>:</span> <span>{</span> \n    <span>name</span><span>:</span> <span>1</span><span>,</span> \n    <span>price</span><span>:</span> <span>1</span><span>,</span> \n    <span>bedrooms</span><span>:</span> <span>1</span><span>,</span> \n    <span>\"</span><span>address.market</span><span>\"</span><span>:</span> <span>1</span><span>,</span> \n    <span>score</span><span>:</span> <span>{</span> <span>$meta</span><span>:</span> <span>\"</span><span>vectorSearchScore</span><span>\"</span> <span>}</span> \n    <span>// Only fetch what you need</span> \n  <span>}</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n \n \n \n<h2> \n   \n   \n  Performance Considerations \n</h2> \n \n<h3> \n   \n   \n  Embedding Generation Strategy \n</h3> \n \n \n \n<div> \n<pre><code><span>// Cache embeddings at data ingestion</span> \n<span>async</span> <span>function</span> <span>seedRental</span><span>(</span><span>rental</span><span>)</span> <span>{</span> \n  <span>const</span> <span>embeddingText</span> <span>=</span> <span>`</span><span>${</span><span>rental</span><span>.</span><span>name</span><span>}</span><span>. </span><span>${</span><span>rental</span><span>.</span><span>description</span><span>}</span><span>. \n    Located in </span><span>${</span><span>rental</span><span>.</span><span>address</span><span>.</span><span>market</span><span>}</span><span>, </span><span>${</span><span>rental</span><span>.</span><span>address</span><span>.</span><span>country</span><span>}</span><span>. \n    </span><span>${</span><span>rental</span><span>.</span><span>property_type</span><span>}</span><span> with </span><span>${</span><span>rental</span><span>.</span><span>bedrooms</span><span>}</span><span> bedrooms. \n    Amenities: </span><span>${</span><span>rental</span><span>.</span><span>amenities</span><span>.</span><span>join</span><span>(</span><span>'</span><span>, </span><span>'</span><span>)}</span><span>.`</span><span>;</span> \n \n  <span>rental</span><span>.</span><span>text_embeddings</span> <span>=</span> <span>await</span> <span>generateEmbedding</span><span>(</span><span>embeddingText</span><span>);</span> \n \n  <span>await</span> <span>db</span><span>.</span><span>rentals</span><span>.</span><span>insertOne</span><span>(</span><span>rental</span><span>);</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>Never generate embeddings at query time</strong> - pre-compute and store them.</p> \n \n<h3> \n   \n   \n  Conversation History Management \n</h3> \n \n \n \n<div> \n<pre><code><span>// Limit conversation history to last 20 messages</span> \n<span>const</span> <span>conversation</span> <span>=</span> <span>await</span> <span>collection</span><span>.</span><span>findOne</span><span>(</span> \n  <span>{</span> <span>sessionId</span> <span>},</span> \n  <span>{</span> \n    <span>projection</span><span>:</span> <span>{</span> \n      <span>messages</span><span>:</span> <span>{</span> <span>$slice</span><span>:</span> <span>-</span><span>20</span> <span>},</span> <span>// Only get last 20</span> \n      <span>metadata</span><span>:</span> <span>1</span> \n    <span>}</span> \n  <span>}</span> \n<span>);</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>Why</strong>: Sending entire conversation history to LLMs is expensive. Recent context is usually sufficient.</p> \n \n<h3> \n   \n   \n  Connection Pooling \n</h3> \n \n \n \n<div> \n<pre><code><span>const</span> <span>client</span> <span>=</span> <span>new</span> <span>MongoClient</span><span>(</span><span>uri</span><span>,</span> <span>{</span> \n  <span>maxPoolSize</span><span>:</span> <span>50</span><span>,</span> \n  <span>minPoolSize</span><span>:</span> <span>10</span><span>,</span> \n  <span>maxIdleTimeMS</span><span>:</span> <span>30000</span> \n<span>});</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>Production Tip</strong>: Pool size should match expected concurrent users/requests.</p> \n \n \n \n \n<h2> \n   \n   \n  Security Best Practices \n</h2> \n \n<h3> \n   \n   \n  1. User-Scoped Data Access \n</h3> \n \n \n \n<div> \n<pre><code><span>// NEVER trust client-provided userId</span> \n<span>const</span> <span>userId</span> <span>=</span> <span>await</span> <span>verifyJWT</span><span>(</span><span>authToken</span><span>);</span> \n \n<span>// All queries scoped to authenticated user</span> \n<span>const</span> <span>savedRentals</span> <span>=</span> <span>await</span> <span>db</span><span>.</span><span>users</span><span>.</span><span>findOne</span><span>(</span> \n  <span>{</span> <span>_id</span><span>:</span> <span>ObjectId</span><span>(</span><span>userId</span><span>)</span> <span>},</span> \n  <span>{</span> <span>projection</span><span>:</span> <span>{</span> <span>saved_rentals</span><span>:</span> <span>1</span> <span>}</span> <span>}</span> \n<span>);</span> \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  2. Input Sanitization \n</h3> \n \n \n \n<div> \n<pre><code><span>// Validate and sanitize before DB operations</span> \n<span>const</span> <span>filters</span> <span>=</span> <span>{</span> \n  <span>min_price</span><span>:</span> <span>Math</span><span>.</span><span>max</span><span>(</span><span>0</span><span>,</span> <span>parseInt</span><span>(</span><span>filters</span><span>.</span><span>min_price</span><span>)</span> <span>||</span> <span>0</span><span>),</span> \n  <span>max_price</span><span>:</span> <span>Math</span><span>.</span><span>min</span><span>(</span><span>10000</span><span>,</span> <span>parseInt</span><span>(</span><span>filters</span><span>.</span><span>max_price</span><span>)</span> <span>||</span> <span>10000</span><span>),</span> \n  <span>location</span><span>:</span> <span>sanitizeString</span><span>(</span><span>filters</span><span>.</span><span>location</span><span>)</span> \n<span>};</span> \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  3. Rate Limiting \n</h3> \n \n \n \n<div> \n<pre><code><span>// Track API usage per user</span> \n<span>await</span> <span>db</span><span>.</span><span>users</span><span>.</span><span>updateOne</span><span>(</span> \n  <span>{</span> <span>_id</span><span>:</span> <span>userId</span> <span>},</span> \n  <span>{</span> \n    <span>$inc</span><span>:</span> <span>{</span> <span>'</span><span>rate_limits.api_calls_today</span><span>'</span><span>:</span> <span>1</span> <span>},</span> \n    <span>$set</span><span>:</span> <span>{</span> <span>'</span><span>rate_limits.last_call</span><span>'</span><span>:</span> <span>new</span> <span>Date</span><span>()</span> <span>}</span> \n  <span>}</span> \n<span>);</span> \n</code></pre> \n \n</div> \n \n \n \n \n \n \n<h2> \n   \n   \n  Real-World Results: What We Achieved \n</h2> \n \n<h3> \n   \n   \n  Performance Metrics \n</h3> \n \n<ul> \n<li> \n<strong>Average search latency</strong>: 150-300ms (embedding generation + vector search + formatting)</li> \n<li> \n<strong>Vector search alone</strong>: 50-80ms for 5,000+ properties</li> \n<li> \n<strong>Conversation storage</strong>: &lt;10ms per message (upsert with indexing)</li> \n<li> \n<strong>Concurrent users</strong>: Tested up to 100 simultaneous chat sessions</li> \n</ul> \n \n<h3> \n   \n   \n  User Experience Wins \n</h3> \n \n<ul> \n<li> \n<strong>Natural language accuracy</strong>: 90%+ intent extraction on first try</li> \n<li> \n<strong>Filter synchronization</strong>: Seamless bidirectional updates</li> \n<li> \n<strong>Context retention</strong>: Agent remembers previous searches and user preferences</li> \n<li> \n<strong>Multi-turn conversations</strong>: Supports complex, multi-step property searches</li> \n</ul> \n \n<h3> \n   \n   \n  Developer Experience \n</h3> \n \n<ul> \n<li> \n<strong>Single database</strong>: No data synchronization between vector DB and app DB</li> \n<li> \n<strong>Unified query language</strong>: MongoDB aggregation for everything</li> \n<li> \n<strong>Flexible schema</strong>: Add new metadata fields without migrations</li> \n<li> \n<strong>Rich ecosystem</strong>: Works with Mongoose, native driver, Prisma, etc.</li> \n</ul> \n \n \n \n \n<h2> \n   \n   \n  Lessons Learned &amp; Best Practices \n</h2> \n \n<h3> \n   \n   \n  1. Design Your Document Schema for Agent Access \n</h3> \n \n \n \n<div> \n<pre><code><span>// ❌ Bad: Deeply nested, agent can't navigate</span> \n<span>{</span> \n  <span>\"</span><span>data</span><span>\"</span><span>:</span> <span>{</span> \n    <span>\"</span><span>property_info</span><span>\"</span><span>:</span> <span>{</span> \n      <span>\"</span><span>details</span><span>\"</span><span>:</span> <span>{</span> \n        <span>\"</span><span>location</span><span>\"</span><span>:</span> <span>{</span> <span>...</span> <span>}</span> \n      <span>}</span> \n    <span>}</span> \n  <span>}</span> \n<span>}</span> \n \n<span>// ✅ Good: Flat, predictable structure</span> \n<span>{</span> \n  <span>\"</span><span>name</span><span>\"</span><span>:</span> <span>\"</span><span>...</span><span>\"</span><span>,</span> \n  <span>\"</span><span>address</span><span>\"</span><span>:</span> <span>{</span> <span>\"</span><span>market</span><span>\"</span><span>:</span> <span>\"</span><span>...</span><span>\"</span><span>,</span> <span>\"</span><span>country</span><span>\"</span><span>:</span> <span>\"</span><span>...</span><span>\"</span> <span>},</span> \n  <span>\"</span><span>price</span><span>\"</span><span>:</span> <span>150</span><span>,</span> \n  <span>\"</span><span>bedrooms</span><span>\"</span><span>:</span> <span>2</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  2. Include Both Structured and Unstructured Data \n</h3> \n \n \n \n<div> \n<pre><code><span>{</span> \n  <span>\"</span><span>name</span><span>\"</span><span>:</span> <span>\"</span><span>Cozy Manhattan Loft</span><span>\"</span><span>,</span> \n  <span>\"</span><span>description</span><span>\"</span><span>:</span> <span>\"</span><span>Full natural language description...</span><span>\"</span><span>,</span> <span>// ← For embeddings</span> \n  <span>\"</span><span>property_type</span><span>\"</span><span>:</span> <span>\"</span><span>Loft</span><span>\"</span><span>,</span>                              <span>// ← For filtering</span> \n  <span>\"</span><span>bedrooms</span><span>\"</span><span>:</span> <span>2</span><span>,</span>                                        <span>// ← For filtering</span> \n  <span>\"</span><span>amenities</span><span>\"</span><span>:</span> <span>[</span><span>\"</span><span>WiFi</span><span>\"</span><span>,</span> <span>\"</span><span>Kitchen</span><span>\"</span><span>],</span>                     <span>// ← For filtering</span> \n  <span>\"</span><span>text_embeddings</span><span>\"</span><span>:</span> <span>[...]</span>                              <span>// ← For vector search</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  3. Store Agent Metadata Richly \n</h3> \n \n \n \n<div> \n<pre><code><span>// Don't just store the conversation</span> \n<span>{</span> \n  <span>\"</span><span>role</span><span>\"</span><span>:</span> <span>\"</span><span>assistant</span><span>\"</span><span>,</span> \n  <span>\"</span><span>content</span><span>\"</span><span>:</span> <span>\"</span><span>I found 5 properties...</span><span>\"</span> \n<span>}</span> \n \n<span>// Store what the agent DID</span> \n<span>{</span> \n  <span>\"</span><span>role</span><span>\"</span><span>:</span> <span>\"</span><span>assistant</span><span>\"</span><span>,</span> \n  <span>\"</span><span>content</span><span>\"</span><span>:</span> <span>\"</span><span>I found 5 properties...</span><span>\"</span><span>,</span> \n  <span>\"</span><span>metadata</span><span>\"</span><span>:</span> <span>{</span> \n    <span>\"</span><span>tool_calls</span><span>\"</span><span>:</span> <span>[</span><span>\"</span><span>searchRentals</span><span>\"</span><span>],</span> \n    <span>\"</span><span>filters_applied</span><span>\"</span><span>:</span> <span>{</span> <span>\"</span><span>location</span><span>\"</span><span>:</span> <span>\"</span><span>New York</span><span>\"</span><span>,</span> <span>\"</span><span>max_price</span><span>\"</span><span>:</span> <span>200</span> <span>},</span> \n    <span>\"</span><span>rental_ids</span><span>\"</span><span>:</span> <span>[</span><span>123</span><span>,</span> <span>456</span><span>],</span> \n    <span>\"</span><span>user_satisfied</span><span>\"</span><span>:</span> <span>true</span> <span>// Track based on follow-up</span> \n  <span>}</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  4. Optimize for Agent Token Limits \n</h3> \n \n \n \n<div> \n<pre><code><span>// Return concise summaries to the agent</span> \n<span>const</span> <span>formattedResults</span> <span>=</span> <span>results</span><span>.</span><span>map</span><span>(</span><span>r</span> <span>=&gt;</span> <span>({</span> \n  <span>id</span><span>:</span> <span>r</span><span>.</span><span>_id</span><span>,</span> \n  <span>name</span><span>:</span> <span>r</span><span>.</span><span>name</span><span>,</span> \n  <span>price</span><span>:</span> <span>r</span><span>.</span><span>price</span><span>,</span> \n  <span>location</span><span>:</span> <span>`</span><span>${</span><span>r</span><span>.</span><span>address</span><span>.</span><span>market</span><span>}</span><span>, </span><span>${</span><span>r</span><span>.</span><span>address</span><span>.</span><span>country</span><span>}</span><span>`</span><span>,</span> \n  <span>bedrooms</span><span>:</span> <span>r</span><span>.</span><span>bedrooms</span> \n  <span>// Skip description, images, etc. - retrieve on-demand</span> \n<span>}));</span> \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  5. Enable Agent Self-Discovery \n</h3> \n \n \n \n<div> \n<pre><code><span>// Provide tools for agents to explore data</span> \n<span>this</span><span>.</span><span>exploreDataTool</span> <span>=</span> <span>tool</span><span>({</span> \n  <span>name</span><span>:</span> <span>'</span><span>exploreAvailableMarkets</span><span>'</span><span>,</span> \n  <span>description</span><span>:</span> <span>'</span><span>Get list of available cities/markets in the database</span><span>'</span><span>,</span> \n  <span>execute</span><span>:</span> <span>async </span><span>()</span> <span>=&gt;</span> <span>{</span> \n    <span>const</span> <span>markets</span> <span>=</span> <span>await</span> <span>db</span><span>.</span><span>rentals</span><span>.</span><span>distinct</span><span>(</span><span>'</span><span>address.market</span><span>'</span><span>);</span> \n    <span>return</span> <span>JSON</span><span>.</span><span>stringify</span><span>(</span><span>markets</span><span>);</span> \n  <span>}</span> \n<span>});</span> \n</code></pre> \n \n</div> \n \n \n \n \n \n \n<h2> \n   \n   \n  The Future: What's Next for Agent-Database Integration \n</h2> \n \n<h3> \n   \n   \n  1. Agent-Driven Schema Evolution \n</h3> \n \n<p>Imagine agents that suggest new fields based on user queries:<br> \n</p> \n \n<div> \n<pre><code>Agent: \"I notice users frequently ask about 'pet-friendly' properties, \n        but this field doesn't exist. Should I add it to the schema?\" \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  2. Semantic Caching \n</h3> \n \n<p>MongoDB could cache embedding+filter combinations:<br> \n</p> \n \n<div> \n<pre><code><span>{</span> \n  <span>\"</span><span>query_hash</span><span>\"</span><span>:</span> <span>\"</span><span>sha256(...)</span><span>\"</span><span>,</span> \n  <span>\"</span><span>embedding</span><span>\"</span><span>:</span> <span>[...],</span> \n  <span>\"</span><span>filters</span><span>\"</span><span>:</span> <span>{</span> <span>\"</span><span>location</span><span>\"</span><span>:</span> <span>\"</span><span>New York</span><span>\"</span> <span>},</span> \n  <span>\"</span><span>cached_results</span><span>\"</span><span>:</span> <span>[...],</span> \n  <span>\"</span><span>valid_until</span><span>\"</span><span>:</span> <span>ISODate</span><span>(</span><span>\"</span><span>2024-01-15T12:00:00Z</span><span>\"</span><span>)</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  3. Multi-Agent Coordination \n</h3> \n \n<p>Different specialized agents sharing the same MongoDB instance:</p> \n \n<ul> \n<li> \n<strong>Search Agent</strong>: Finds properties</li> \n<li> \n<strong>Booking Agent</strong>: Handles reservations</li> \n<li> \n<strong>Recommendation Agent</strong>: Suggests based on history</li> \n<li>All coordinating through shared conversation and user state</li> \n</ul> \n \n<h3> \n   \n   \n  4. Continuous Learning from Feedback \n</h3> \n \n \n \n<div> \n<pre><code><span>// User indicates result quality</span> \n<span>{</span> \n  <span>\"</span><span>search_query</span><span>\"</span><span>:</span> <span>\"</span><span>cozy apartment in Barcelona</span><span>\"</span><span>,</span> \n  <span>\"</span><span>results_shown</span><span>\"</span><span>:</span> <span>[</span><span>123</span><span>,</span> <span>456</span><span>,</span> <span>789</span><span>],</span> \n  <span>\"</span><span>user_clicked</span><span>\"</span><span>:</span> <span>456</span><span>,</span>        <span>// Implicit feedback</span> \n  <span>\"</span><span>user_saved</span><span>\"</span><span>:</span> <span>[</span><span>456</span><span>],</span>        <span>// Strong signal</span> \n  <span>\"</span><span>user_booked</span><span>\"</span><span>:</span> <span>456</span>          <span>// Conversion</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p>Use this data to fine-tune embeddings or ranking algorithms.</p> \n \n \n \n \n<h2> \n   \n   \n  Conclusion: MongoDB Atlas as the Foundation for Intelligent Applications \n</h2> \n \n<p>Building AI agents that truly understand and serve users requires more than just a language model. You need a database that:</p> \n \n<p>✅ <strong>Stores semantic understanding</strong> (vectors) alongside structured data (filters)<br> \n✅ <strong>Handles dynamic, evolving schemas</strong> (conversations, metadata, user context)<br> \n✅ <strong>Enables bidirectional data flow</strong> (agents read, write, and transform)<br> \n✅ <strong>Performs at scale</strong> (millisecond searches across thousands of documents)<br> \n✅ <strong>Provides a unified platform</strong> (no juggling multiple databases)</p> \n \n<p>MongoDB Atlas delivers all of this with its Document Model and Vector Search capabilities. As we've seen in this rental search application:</p> \n \n<ol> \n<li> \n<strong>Agents feed FROM the database</strong> using semantic vector search combined with traditional filters</li> \n<li> \n<strong>Agents feed TO the database</strong> by storing rich conversation context and metadata</li> \n<li> \n<strong>Agents transform the UI</strong> through structured metadata that synchronizes with interface elements</li> \n</ol> \n \n<p>This bidirectional architecture represents the future of AI-powered applications. And MongoDB Atlas makes it not just possible, but elegant, performant, and production-ready.</p> \n \n \n<h2> \n   \n   \n  Try It Yourself \n</h2> \n \n<p>The complete code for this project is available on GitHub: <a href=\"https://github.com/mongodb-developer/mongodb-openai-agentic-rentals\">mongodb-openai-agentic-rentals</a></p> \n \n<p><strong>Quick Start</strong>:<br> \n</p> \n \n<div> \n<pre><code>git clone https://github.com/mongodb-developer/mongodb-openai-agentic-rentals.git \n \n<span>cd </span>mongodb-openai-agentic-rentals \nbun <span>install</span> \n<span># Configure .env with your MongoDB Atlas URI and OpenAI API key</span> \nnode seed-hf-airbnb-data.js \nbun start \n<span># Visit http://localhost:5000/index.html</span> \n</code></pre> \n \n</div> \n \n \n \n<p><strong>What to explore</strong>:</p> \n \n<ol> \n<li>Try natural language queries: \"Find me a beachfront property in Sydney\"</li> \n<li>Watch the UI filters update automatically</li> \n<li>Check the MongoDB conversation collection to see stored context</li> \n<li>Examine the aggregation pipelines in <code>src/services/vector-search.service.js</code> \n</li> \n<li>Extend the agent with new tools in <code>src/agents/rental-rag-agent.js</code> \n</li> \n</ol> \n \n \n \n \n<h2> \n   \n   \n  Additional Resources \n</h2> \n \n<ul> \n<li><a href=\"https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-overview/\">MongoDB Atlas Vector Search Documentation</a></li> \n<li><a href=\"https://openai.github.io/openai-agents-js/\">OpenAI Agents SDK</a></li> \n<li><a href=\"https://www.mongodb.com/docs/manual/core/aggregation-pipeline/\">MongoDB Aggregation Pipeline</a></li> \n<li><a href=\"https://www.mongodb.com/docs/manual/core/data-modeling-introduction/\">Document Model Design Best Practices</a></li> \n</ul> \n \n \n \n \n<p><strong>About the Author</strong>: Pavel Duchovny is a Developer Advocate at MongoDB, passionate about helping developers build intelligent, scalable applications. Connect on <a href=\"https://twitter.com/mongodb\">Twitter</a> or <a href=\"https://linkedin.com/company/mongodb\">LinkedIn</a>.</p> \n \n \n \n \n<p><em>Have questions or feedback? Open an issue on the GitHub repo or reach out to the MongoDB Developer Community.</em></p>",
      "summary": "As AI agents become increasingly sophisticated, the way applications interact with databases is fundamentally changing. Gone are the days of simple CRUD operations and static queries. Modern AI-powered applications require a bidirectional data flow where: \n \n \n \nAgents feed from the database - Using semantic search and retrieval-augmented generation (RAG) to access relevant data \n \nAgents feed back to the database - Storing conversation context, user interactions, and learned preferences \n \nAgen",
      "publishedAt": "2025-12-02T12:55:48.000Z",
      "author": "Pash10g",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 16.908959833723408,
      "ingestedAt": "2025-12-02T14:44:03.183Z",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "observability",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a3ef29",
      "title": "Pixlore — A Web-to-Figma Engine That Bridges UI, Code, and Product Workflows",
      "url": "https://dev.to/doong_yee/pixlore-a-web-to-figma-engine-that-bridges-ui-code-and-product-workflows-5a6h",
      "content": "<p>As front-end engineers and product teams, we often need to understand, audit, or replicate UI patterns from existing web interfaces — and doing that manually in Figma can feel… inefficient.</p> \n \n<p>Rebuilding layout grids, reconstructing spacing, extracting semantics from CSS, mapping design tokens — none of this is <em>hard</em>, but it’s definitely <strong>repetitive</strong>.</p> \n \n<p><a href=\"https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202\">Pixlore</a> is a Figma plugin we’ve been working on that aims to eliminate this repetitive layer.<br> \nIt converts <strong>live webpages or HTML</strong> into a clean, editable Figma structure that mirrors how modern front-end systems are built.</p> \n \n<p>Here’s why it’s become part of many dev/design workflows 👇</p> \n \n<h3> \n   \n   \n  1. Structurally Accurate Reverse Engineering \n</h3> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fl0810poulc6y5t9h4c0p.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fl0810poulc6y5t9h4c0p.png\" alt=\"\" width=\"800\" height=\"450\"></a></p> \n \n<p><a href=\"https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202\">Pixlore</a> doesn’t just screenshot, it <strong>parses DOM → styles → layout → constraints</strong>, then reconstructs a Figma tree that matches:</p> \n \n<ul> \n<li>Auto Layout structure</li> \n<li>Flexbox / CSS grid relationships</li> \n<li>Component grouping + layer hierarchy</li> \n<li>Typography scale + color tokens</li> \n<li>Spacing rules inferred from CSS</li> \n</ul> \n \n<p>Developers like this because the resulting Figma file is:</p> \n \n<ul> \n<li>predictable</li> \n<li>debuggable</li> \n<li>clean enough to hand over</li> \n<li>consistent with how UI code is actually structured</li> \n</ul> \n \n<p>It’s basically a <strong>DOM → Figma AST translator</strong>.</p> \n \n<h3> \n   \n   \n  2. AI Editing: Modify Layouts Using Natural Language \n</h3> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fs3n7kf65tlgyf1jy1qb7.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fs3n7kf65tlgyf1jy1qb7.png\" alt=\"\" width=\"800\" height=\"450\"></a></p> \n \n<p>This is where dev–design collaboration gets interesting.</p> \n \n<p><a href=\"https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202\">Pixlore</a> includes a conversational AI that can modify generated layouts:</p> \n \n<ul> \n<li>“Make the grid 4 columns with 24px gutter.”</li> \n<li>“Replace all images with gray placeholders.”</li> \n<li>“Normalize spacing to an 8px scale.”</li> \n<li>“Apply a neutral color theme.”</li> \n<li>“Convert this section into reusable components.”</li> \n</ul> \n \n<p>Think of it as using ChatGPT &amp; Gemini3 — <em>but embedded directly inside Figma</em> and acting only on selected nodes.</p> \n \n<p>It reduces the designer’s manual work and gives developers more consistent layouts to work with.</p> \n \n<h3> \n   \n   \n  3. Extension Capture for Dynamic / Authenticated Pages \n</h3> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg7pfr76udh3hiuagiavk.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg7pfr76udh3hiuagiavk.png\" alt=\"\" width=\"800\" height=\"450\"></a></p> \n \n<p>Developers often deal with:</p> \n \n<ul> \n<li>Internal dashboards</li> \n<li>Login-protected apps</li> \n<li>Infinite scroll</li> \n<li>Hydrated UI states</li> \n</ul> \n \n<p><a href=\"https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202\">Pixlore</a>’s browser extension lets you capture the rendered HTML/DOM from those states and port them into Figma.</p> \n \n<p>Great for:</p> \n \n<ul> \n<li>UX teardown</li> \n<li>Competitor audits</li> \n<li>Rebuilding internal tools</li> \n<li>Mapping legacy UI before refactoring</li> \n</ul> \n \n<h3> \n   \n   \n  4. Interaction Pattern Extraction \n</h3> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxnfwegtkv8s268mjmmnn.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxnfwegtkv8s268mjmmnn.png\" alt=\"\" width=\"800\" height=\"450\"></a></p> \n \n<p><a href=\"https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202\">Pixlore</a> can annotate the exported design with detected interaction logic:</p> \n \n<ul> \n<li>hover / active states</li> \n<li>expandable sections</li> \n<li>menu navigation flow</li> \n<li>button behavior</li> \n<li>scroll-triggered elements</li> \n</ul> \n \n<p>This helps dev teams communicate functional requirements without manually documenting everything.</p> \n \n \n \n \n<h3> \n   \n   \n  Why Dev Teams Use It \n</h3> \n \n<p>In real workflows, devs report using <a href=\"https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202\">Pixlore</a> for:</p> \n \n<h4> \n   \n   \n  <strong>UI audits</strong> \n</h4> \n \n<p>Quickly understand the structure of a new interface before implementing a component library or refactor.</p> \n \n<h4> \n   \n   \n  <strong>Design–Dev alignment</strong> \n</h4> \n \n<p>Designers start with accurate structural drafts → developers get clearer specs.</p> \n \n<h4> \n   \n   \n  <strong>Rapid prototyping</strong> \n</h4> \n \n<p>Build a prototype or redesign using a real-world reference instead of starting from a blank frame.</p> \n \n<h4> \n   \n   \n  <strong>Onboarding</strong> \n</h4> \n \n<p>New teammates can learn product UI principles by reverse-engineering existing screens.</p> \n \n \n \n \n<h3> \n   \n   \n  Simple Pricing, Fast ROI \n</h3> \n \n<p>Compared to other tools doing similar web-to-Figma extraction, <a href=\"https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202\">Pixlore</a> aims to be:</p> \n \n<ul> \n<li>⚡️ <strong>Faster</strong> \n</li> \n<li>📐 <strong>More accurate</strong> \n</li> \n<li>🪙 <strong>Significantly more affordable</strong> \n</li> \n</ul> \n \n<p>Teams use the free tier to validate the workflow, then upgrade after confirming it reduces manual rebuild time.</p> \n \n \n \n \n<h3> \n   \n   \n  🚀 <a href=\"https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202\">Try It</a> (Free) \n</h3> \n \n<p>If your dev/design workflow involves analyzing or replicating existing interfaces, this might save you hours.</p> \n \n<p>I’m also collecting feedback from developers:</p> \n \n<ul> \n<li>Does the reconstructed layout match your expectations?</li> \n<li>What would make it more useful in your workflow?</li> \n<li>Should we export back to code in future versions?</li> \n</ul> \n \n<p>Happy to chat in comments or DMs, always looking to improve the tool for real-world engineering use cases.</p>",
      "summary": "As front-end engineers and product teams, we often need to understand, audit, or replicate UI patterns from existing web interfaces — and doing that manually in Figma can feel… inefficient. \n \nRebuilding layout grids, reconstructing spacing, extracting semantics from CSS, mapping design tokens — none of this is hard, but it’s definitely repetitive. \n \nPixlore is a Figma plugin we’ve been working on that aims to eliminate this repetitive layer. \nIt converts live webpages or HTML into a clean, edi",
      "publishedAt": "2025-12-02T12:48:53.000Z",
      "author": "Doong Yee",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "pricing_business",
      "score": 9.445883281862125,
      "ingestedAt": "2025-12-02T14:44:03.183Z",
      "tags": [
        "code_review",
        "ide",
        "governance",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a3ef30",
      "title": "Authenticating Users in Astro & React Apps with Better Auth",
      "url": "https://dev.to/isnan__h/authenticating-users-in-astro-react-apps-with-better-auth-3loe",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvea9213biytatuykfyr6.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>This post was originally published in <a href=\"https://www.ishan-manandhar.com.np/\">my website here</a>.</p>  \n  \n<p>Authentication is a critical foundation for every contemporary web application, but establishing it is often one of the most significant, time-intensive hurdles developers encounter. The solution? Introducing Better Auth API: a framework-independent authentication service designed to revolutionize user management implementation. This detailed tutorial will guide you through creating a fully functional, full-stack application, showcasing the efficiency and ease of Better Auth, powered by Astro for optimal performance.</p>  \n  \n<h3>  \n    \n    \n  Why Better Auth?  \n</h3>  \n  \n<p>Traditional auth solutions often require extensive boilerplate code and complex configuration. Better Auth offers:</p>  \n  \n<ul>  \n<li>Zero boilerplate setup</li>  \n<li>Type-safe APIs with full TypeScript support</li>  \n<li>Multiple provider support (email/password, OAuth, passkeys)</li>  \n<li>Framework-agnostic design that works with Astro, React, and beyond</li>  \n</ul>  \n  \n<p>Better Auth automatically handles Astro's unique server-client boundary, providing authentication that works across both environments. Better Auth provides a unified API that adapts to your stack—whether you're using React, Vue, Svelte, or vanilla JavaScript on the frontend, and Node.js, Bun, Deno or any framekworks on javascript ecosystem on the backend. </p>  \n  \n<h3>  \n    \n    \n  Setting up Astro  \n</h3>  \n  \n<p>Astro's hybrid architecture (server and client components) works seamlessly with Better Auth. Better Auth requires a database to persist user and session data and leverages Server-Side Rendering (SSR) to manage sessions via cookies.</p>  \n  \n<p>🛠️ Prerequisites</p>  \n  \n<ul>  \n<li>A new Astro project (e.g., initialized with pnpm create astro@latest).</li>  \n<li>A database (e.g., PostgreSQL or MongoDB) configured and ready.</li>  \n<li>An SSR adapter for Astro (e.g., Cloudflare, Vercel, Netlify, or Node) added to your project.  \n</li>  \n</ul>  \n  \n<div>  \n<pre><code>pnpm create astro@latest  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<h3>  \n    \n    \n  Create a Neon (Serverless Postgres) app  \n</h3>  \n  \n<p>If you do not have one already, create a Neon (Serverless Postgres) project. </p>  \n  \n<p>Save your connection details including your password. Here is the steps involved:</p>  \n  \n<ul>  \n<li>Navigate to the Projects page in the (Neon Console)[<a href=\"https://neon.com/\">https://neon.com/</a>].</li>  \n<li>Click New Project.</li>  \n<li>Specify your project settings and click Create Project.</li>  \n<li>Create a .env file in the root directory of your Astro project, and define a DATABASE_URL key with the connection string obtained as the value.</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  Adding dependencies  \n</h3>  \n  \n  \n  \n<div>  \n<pre><code>pnpm i better-auth pg @types/pg @astrojs/cloudflare @astrojs/react @tailwindcss/vite react tailwindcss wrangler  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<ul>  \n<li>better-auth — Authentication toolkit providing server/client auth APIs for modern web apps.</li>  \n<li>pg — PostgreSQL client for Node.js used to connect and query Postgres databases.</li>  \n<li>@types/pg — TypeScript type definitions for the pg (PostgreSQL) library.</li>  \n<li>@astrojs/cloudflare — Astro adapter for deploying your Astro app to Cloudflare Workers.</li>  \n<li>@astrojs/react — Enables using React components inside an Astro project.</li>  \n<li>  \n<a href=\"https://dev.to/tailwindcss\">@tailwindcss</a>/vite — Integrates Tailwind CSS directly into Astro/Vite for fast builds and hot reload.</li>  \n<li>react — UI library for building interactive components inside Astro.</li>  \n<li>tailwindcss — Utility-first CSS framework used for styling your Astro + React project.</li>  \n<li>wrangler — Cloudflare’s CLI tool for building and deploying Workers and Worker-based apps.</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  Adding environment variables  \n</h3>  \n  \n  \n  \n<div>  \n<pre><code><span>BETTER_AUTH_SECRET</span><span>=</span><span>\"a-32-character-secret\"</span>  \n<span>DATABASE_URL</span><span>=</span><span>\"your database connection string\"</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>You can generate a secret with the code snippet below<br>  \n</p>  \n  \n<div>  \n<pre><code><span>node</span> <span>-</span><span>e</span> <span>\"</span><span>console.log(require('crypto').randomBytes(32).toString('base64'))</span><span>\"</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Astro gives you access to Vite’s built-in environment variables support and includes some default environment variables for your project that allow you to access configuration values for your current project.</p>  \n  \n<h3>  \n    \n    \n  Adding better-auth  \n</h3>  \n  \n<p>Better Auth comes with first class support for Astro.</p>  \n  \n<p>We will be creating two utility files, src/auth.ts and src/auth-client.ts to access Better Auth on the server side and client side, respectively.</p>  \n  \n<p>In the src/auth-client.ts (code below), we are instantiating a new better-auth instance to be used in the client side interactions.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>//src/auth-client.ts</span>  \n<span>import</span> <span>{</span> <span>createAuthClient</span> <span>}</span> <span>from</span> <span>\"</span><span>better-auth/react</span><span>\"</span>  \n<span>export</span> <span>const</span> <span>{</span> <span>useSession</span><span>,</span> <span>authClient</span> <span>}</span> <span>=</span> <span>createAuthClient</span><span>()</span>   \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>In src/auth.ts (code below), you are going to use a Pool to connect to your Postgres instance to persist the sessions and user object in the database, and enable email and password authentication. Astro uses Vite’s built-in support for environment variables, which are statically replaced at build time, and lets you use any of its methods to work with them.</p>  \n  \n<p>We use import.meta.env.DATABASE_URL to access environment variables inside Astro.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>//src/auth.ts</span>  \n<span>import</span> <span>pkg</span> <span>from</span> <span>'</span><span>pg</span><span>'</span>  \n<span>import</span> <span>{</span> <span>betterAuth</span> <span>}</span> <span>from</span> <span>'</span><span>better-auth</span><span>'</span>  \n  \n<span>const</span> <span>{</span> <span>Pool</span> <span>}</span> <span>=</span> <span>pkg</span>  \n  \n<span>export</span> <span>const</span> <span>auth</span> <span>=</span> <span>betterAuth</span><span>({</span>  \n  <span>emailAndPassword</span><span>:</span> <span>{</span> <span>enabled</span><span>:</span> <span>true</span> <span>},</span>  \n  <span>database</span><span>:</span> <span>new</span> <span>Pool</span><span>({</span> <span>connectionString</span><span>:</span> <span>import</span><span>.</span><span>meta</span><span>.</span><span>env</span><span>.</span><span>DATABASE_URL</span> <span>})</span>  \n<span>})</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<h3>  \n    \n    \n  Generating schema and API routes  \n</h3>  \n  \n<p>To create the schema per our configuration defined in src/auth.ts file in your database automatically, execute the command below:<br>  \n</p>  \n  \n<div>  \n<pre><code>npx @better-auth/cli migrate  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Let's define an API route to allow you to authenticate users.</p>  \n  \n<p>Better Auth does the heavy lifting of creating and managing the logic of validating credentials, creating (or updating) the user and relevant session objects in the database. You just need to create a catch-all api route in your Astro project as follows in the src/pages/api/auth/[...all].ts file:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>import</span> <span>{</span> <span>auth</span> <span>}</span> <span>from</span> <span>'</span><span>@/auth</span><span>'</span>  \n<span>import</span> <span>type</span> <span>{</span> <span>APIRoute</span> <span>}</span> <span>from</span> <span>'</span><span>astro</span><span>'</span>  \n  \n<span>export</span> <span>const</span> <span>ALL</span><span>:</span> <span>APIRoute</span> <span>=</span> <span>async </span><span>(</span><span>ctx</span><span>)</span> <span>=&gt;</span> <span>{</span>  \n  <span>return</span> <span>auth</span><span>.</span><span>handler</span><span>(</span><span>ctx</span><span>.</span><span>request</span><span>)</span>  \n<span>}</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<h3>  \n    \n    \n  Intercept all incoming requests using Astro middleware  \n</h3>  \n  \n<p>To make sure that each request maintains a user and session information is accessible over the server-side endpoints, and in .astro pages during server-side rendering, you are going create a middleware that uses Better Auth to decode/encode a user session from the cookie.</p>  \n  \n<p>Create a file middleware.ts in the src directory with the following code:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>import</span> <span>{</span> <span>auth</span> <span>}</span> <span>from</span> <span>'</span><span>@/auth</span><span>'</span>  \n<span>import</span> <span>{</span> <span>defineMiddleware</span> <span>}</span> <span>from</span> <span>'</span><span>astro:middleware</span><span>'</span>  \n  \n<span>export</span> <span>const</span> <span>onRequest</span> <span>=</span> <span>defineMiddleware</span><span>(</span><span>async </span><span>(</span><span>context</span><span>,</span> <span>next</span><span>)</span> <span>=&gt;</span> <span>{</span>  \n  <span>const</span> <span>isAuthed</span> <span>=</span> <span>await</span> <span>auth</span><span>.</span><span>api</span><span>.</span><span>getSession</span><span>({</span>  \n    <span>headers</span><span>:</span> <span>context</span><span>.</span><span>request</span><span>.</span><span>headers</span><span>,</span>  \n  <span>})</span>  \n  <span>if </span><span>(</span><span>isAuthed</span><span>)</span> <span>{</span>  \n    <span>context</span><span>.</span><span>locals</span><span>.</span><span>user</span> <span>=</span> <span>isAuthed</span><span>.</span><span>user</span>  \n    <span>context</span><span>.</span><span>locals</span><span>.</span><span>session</span> <span>=</span> <span>isAuthed</span><span>.</span><span>session</span>  \n  <span>}</span> <span>else</span> <span>{</span>  \n    <span>context</span><span>.</span><span>locals</span><span>.</span><span>user</span> <span>=</span> <span>null</span>  \n    <span>context</span><span>.</span><span>locals</span><span>.</span><span>session</span> <span>=</span> <span>null</span>  \n  <span>}</span>  \n  <span>return</span> <span>next</span><span>()</span>  \n<span>})</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>This will make sure to mark both user and session as null by default, and assign the respective values obtained from the database using the relevant cookies from the request. This allows you to always know the correct state of user authentication.</p>  \n  \n<h3>  \n    \n    \n  Creating Index Page  \n</h3>  \n  \n  \n  \n<div>  \n<pre><code><span>---</span>  \n<span>import</span> <span>\"</span><span>@/styles/global.css</span><span>\"</span><span>;</span>  \n<span>import</span> <span>DashboardPage</span> <span>from</span> <span>\"</span><span>@/components/Dashboard</span><span>\"</span><span>;</span>  \n<span>import</span> <span>Layout</span> <span>from</span> <span>\"</span><span>@/layouts/Layout.astro</span><span>\"</span><span>;</span>  \n  \n<span>if </span><span>(</span><span>!</span><span>Astro</span><span>.</span><span>locals</span><span>.</span><span>user</span><span>?.</span><span>id</span><span>)</span> <span>return</span> <span>Astro</span><span>.</span><span>redirect</span><span>(</span><span>\"</span><span>/signin</span><span>\"</span><span>);</span>  \n<span>---</span>  \n  \n<span>&lt;</span><span>html</span> <span>lang</span><span>=</span><span>\"</span><span>en</span><span>\"</span><span>&gt;</span>  \n  <span>&lt;</span><span>head</span><span>&gt;</span>  \n    <span>&lt;</span><span>meta</span> <span>charset</span><span>=</span><span>\"</span><span>utf-8</span><span>\"</span> <span>/&gt;</span>  \n    <span>&lt;</span><span>meta</span> <span>name</span><span>=</span><span>\"</span><span>viewport</span><span>\"</span> <span>content</span><span>=</span><span>\"</span><span>width=device-width</span><span>\"</span> <span>/&gt;</span>  \n  <span>&lt;</span><span>/head</span><span>&gt;  \n</span>  <span>&lt;</span><span>body</span> <span>class</span><span>=</span><span>\"</span><span>container mx-auto p-4</span><span>\"</span><span>&gt;</span>  \n    <span>&lt;</span><span>Layout</span><span>&gt;</span>  \n      <span>&lt;</span><span>DashboardPage</span> <span>client</span><span>:</span><span>load</span> <span>/&gt;</span>  \n      <span>&lt;</span><span>Layout</span> <span>/&gt;</span>  \n    <span>&lt;</span><span>/Layout</span><span>&gt;  \n</span>  <span>&lt;</span><span>/body</span><span>&gt;  \n</span><span>&lt;</span><span>/html</span><span>&gt;  \n</span></code></pre>  \n  \n</div>  \n  \n  \n  \n<p>This Redirects user to the sign in page if the user is not authenticated.<br>  \nShows the information stored in the user object pertaining to the authenticated user.</p>  \n  \n<p>Let's create a react component for our dashboard page.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>import</span> <span>{</span> <span>useEffect</span> <span>}</span> <span>from</span> <span>\"</span><span>react</span><span>\"</span><span>;</span>  \n<span>import</span> <span>{</span> <span>authClient</span><span>,</span> <span>useSession</span> <span>}</span> <span>from</span> <span>\"</span><span>@/auth-client</span><span>\"</span><span>;</span>  \n<span>import</span> <span>\"</span><span>@/styles/global.css</span><span>\"</span><span>;</span>  \n  \n<span>export</span> <span>default</span> <span>function</span> <span>DashboardPage</span><span>()</span> <span>{</span>  \n    <span>const</span> <span>{</span>  \n        <span>data</span><span>:</span> <span>session</span><span>,</span>  \n        <span>isPending</span><span>,</span>   \n        <span>error</span><span>,</span>   \n        <span>refetch</span>   \n    <span>}</span> <span>=</span> <span>useSession</span><span>()</span>  \n  \n  <span>useEffect</span><span>(()</span> <span>=&gt;</span> <span>{</span>  \n    <span>// If not logged in → redirect to signin</span>  \n    <span>if </span><span>(</span><span>!</span><span>isPending</span> <span>&amp;&amp;</span> <span>!</span><span>session</span><span>?.</span><span>user</span><span>?.</span><span>id</span><span>)</span> <span>{</span>  \n      <span>window</span><span>.</span><span>location</span><span>.</span><span>href</span><span>=</span><span>'</span><span>/</span><span>'</span>  \n    <span>}</span>  \n  <span>},</span> <span>[</span><span>session</span><span>,</span> <span>isPending</span><span>]);</span>  \n  \n  <span>const</span> <span>handleSignOut</span> <span>=</span> <span>async </span><span>():</span> <span>Promise</span><span>&lt;</span><span>void</span><span>&gt;</span> <span>=&gt;</span> <span>{</span>  \n    <span>await</span> <span>authClient</span><span>.</span><span>signOut</span><span>();</span>  \n      <span>window</span><span>.</span><span>location</span><span>.</span><span>href</span> <span>=</span> <span>'</span><span>/signin</span><span>'</span>  \n  <span>};</span>  \n  \n  <span>if </span><span>(</span><span>isPending</span><span>)</span> <span>{</span>  \n    <span>return</span> <span>&lt;</span><span>p</span><span>&gt;</span><span>Loading</span><span>...</span><span>&lt;</span><span>/p&gt;</span><span>;  \n</span>  <span>}</span>  \n  \n  <span>if </span><span>(</span><span>!</span><span>session</span><span>?.</span><span>user</span><span>)</span> <span>{</span>  \n    <span>return</span> <span>null</span><span>;</span>   \n  <span>}</span>  \n  \n  <span>return </span><span>(</span>  \n    <span>&lt;</span><span>div</span> <span>className</span><span>=</span><span>\"</span><span>container mx-auto py-4</span><span>\"</span><span>&gt;</span>  \n      <span>&lt;</span><span>h1</span> <span>className</span><span>=</span><span>\"</span><span>font-bold text-xl</span><span>\"</span><span>&gt;</span>  \n        <span>Hello</span> <span>{</span><span>session</span><span>.</span><span>user</span><span>.</span><span>name</span> <span>??</span> <span>\"</span><span>User</span><span>\"</span><span>}</span>  \n      <span>&lt;</span><span>/h1</span><span>&gt;  \n</span>  \n      <span>&lt;</span><span>h2</span> <span>className</span><span>=</span><span>\"</span><span>text-green-500 font-bold text-3xl</span><span>\"</span><span>&gt;</span>  \n        <span>You</span><span>&amp;</span><span>apos</span><span>;</span><span>ve</span> <span>authenticated</span> <span>successfully</span><span>!</span>  \n      <span>&lt;</span><span>/h2</span><span>&gt;  \n</span>  \n      <span>&lt;</span><span>pre</span> <span>className</span><span>=</span><span>\"</span><span>mt-4 bg-gray-100 p-4 rounded text-sm</span><span>\"</span><span>&gt;</span>  \n        <span>{</span><span>JSON</span><span>.</span><span>stringify</span><span>(</span><span>session</span><span>.</span><span>user</span><span>,</span> <span>null</span><span>,</span> <span>2</span><span>)}</span>  \n      <span>&lt;</span><span>/pre</span><span>&gt;  \n</span>  \n      <span>&lt;</span><span>button</span>  \n        <span>onClick</span><span>=</span><span>{</span><span>handleSignOut</span><span>}</span>  \n        <span>className</span><span>=</span><span>\"</span><span>mt-4 px-4 py-2 bg-red-600 text-white rounded</span><span>\"</span>  \n      <span>&gt;</span>  \n        <span>Sign</span> <span>Out</span>  \n      <span>&lt;</span><span>/button</span><span>&gt;  \n</span>    <span>&lt;</span><span>/div</span><span>&gt;  \n</span>  <span>);</span>  \n<span>}</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<h3>  \n    \n    \n  Creating Signup  \n</h3>  \n  \n<p>We created a signup page inside the src/pages and imported a React component and used a client:load directive so its hydrated in client. This helps to load and hydrate the component JavaScript immediately on page load.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>---</span>  \n<span>import</span> <span>SignupForm</span> <span>from</span> <span>'</span><span>@/components/SignupForm</span><span>'</span>   \n<span>import</span> <span>Layout</span> <span>from</span> <span>'</span><span>@/layouts/Layout.astro</span><span>'</span>  \n<span>---</span>  \n  \n<span>&lt;</span><span>html</span> <span>lang</span><span>=</span><span>\"</span><span>en</span><span>\"</span><span>&gt;</span>  \n  <span>&lt;</span><span>head</span><span>&gt;</span>  \n    <span>&lt;</span><span>meta</span> <span>charset</span><span>=</span><span>\"</span><span>utf-8</span><span>\"</span> <span>/&gt;</span>  \n    <span>&lt;</span><span>meta</span> <span>name</span><span>=</span><span>\"</span><span>viewport</span><span>\"</span> <span>content</span><span>=</span><span>\"</span><span>width=device-width</span><span>\"</span> <span>/&gt;</span>  \n  <span>&lt;</span><span>/head</span><span>&gt;  \n</span>  <span>&lt;</span><span>body</span><span>&gt;</span>  \n    <span>&lt;</span><span>Layout</span><span>&gt;</span>  \n      <span>&lt;</span><span>SignupForm</span> <span>client</span><span>:</span><span>load</span><span>/&gt;</span>  \n    <span>&lt;</span><span>Layout</span> <span>/&gt;</span>  \n  <span>&lt;</span><span>/body</span><span>&gt;  \n</span><span>&lt;</span><span>/html</span><span>&gt;  \n</span></code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Also we will make a signup form component which will be a simple form to create a user inside of our Neon database.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>import</span> <span>React</span><span>,</span> <span>{</span> <span>useState</span> <span>}</span> <span>from</span> <span>\"</span><span>react</span><span>\"</span><span>;</span>  \n<span>import</span> <span>type</span> <span>{</span> <span>FormEvent</span> <span>}</span> <span>from</span> <span>\"</span><span>react</span><span>\"</span><span>;</span>  \n<span>import</span> <span>{</span> <span>authClient</span> <span>}</span> <span>from</span> <span>\"</span><span>@/auth-client</span><span>\"</span><span>;</span>  \n  \n<span>export</span> <span>default</span> <span>function</span> <span>SignupPage</span><span>():</span> <span>React</span><span>.</span><span>JSX</span><span>.</span><span>Element</span> <span>{</span>  \n  <span>const</span> <span>[</span><span>isLoading</span><span>,</span> <span>setIsLoading</span><span>]</span> <span>=</span> <span>useState</span><span>&lt;</span><span>boolean</span><span>&gt;</span><span>(</span><span>false</span><span>);</span>  \n  <span>const</span> <span>[</span><span>error</span><span>,</span> <span>setError</span><span>]</span> <span>=</span> <span>useState</span><span>&lt;</span><span>string</span><span>&gt;</span><span>(</span><span>\"\"</span><span>);</span>  \n  \n  <span>const</span> <span>handleSubmit</span> <span>=</span> <span>async </span><span>(</span><span>e</span><span>:</span> <span>FormEvent</span><span>&lt;</span><span>HTMLFormElement</span><span>&gt;</span><span>):</span> <span>Promise</span><span>&lt;</span><span>void</span><span>&gt;</span> <span>=&gt;</span> <span>{</span>  \n    <span>e</span><span>.</span><span>preventDefault</span><span>();</span>  \n    <span>setIsLoading</span><span>(</span><span>true</span><span>);</span>  \n    <span>setError</span><span>(</span><span>\"\"</span><span>);</span>  \n  \n    <span>const</span> <span>formData</span> <span>=</span> <span>new</span> <span>FormData</span><span>(</span><span>e</span><span>.</span><span>currentTarget</span><span>);</span>  \n  \n    <span>const</span> <span>name</span> <span>=</span> <span>(</span><span>formData</span><span>.</span><span>get</span><span>(</span><span>\"</span><span>name</span><span>\"</span><span>)</span> <span>as</span> <span>string</span><span>)</span> <span>??</span> <span>\"\"</span><span>;</span>  \n    <span>const</span> <span>email</span> <span>=</span> <span>(</span><span>formData</span><span>.</span><span>get</span><span>(</span><span>\"</span><span>email</span><span>\"</span><span>)</span> <span>as</span> <span>string</span><span>)</span> <span>??</span> <span>\"\"</span><span>;</span>  \n    <span>const</span> <span>password</span> <span>=</span> <span>(</span><span>formData</span><span>.</span><span>get</span><span>(</span><span>\"</span><span>password</span><span>\"</span><span>)</span> <span>as</span> <span>string</span><span>)</span> <span>??</span> <span>\"\"</span><span>;</span>  \n  \n    <span>const</span> <span>response</span> <span>=</span> <span>await</span> <span>authClient</span><span>.</span><span>signUp</span><span>.</span><span>email</span><span>({</span>  \n      <span>name</span><span>,</span>  \n      <span>email</span><span>,</span>  \n      <span>password</span><span>,</span>  \n    <span>});</span>  \n  \n    <span>if </span><span>(</span><span>!</span><span>response</span><span>.</span><span>error</span><span>)</span> <span>{</span>  \n      <span>window</span><span>.</span><span>location</span><span>.</span><span>href</span> <span>=</span> <span>'</span><span>/</span><span>'</span>  \n    <span>}</span> <span>else</span> <span>{</span>  \n      <span>setError</span><span>(</span><span>response</span><span>.</span><span>error</span><span>.</span><span>message</span> <span>??</span> <span>\"</span><span>Signup failed</span><span>\"</span><span>);</span>  \n    <span>}</span>  \n  \n    <span>setIsLoading</span><span>(</span><span>false</span><span>);</span>  \n  <span>};</span>  \n  \n  <span>return </span><span>(</span>  \n    <span>&lt;</span><span>div</span> <span>className</span><span>=</span><span>\"</span><span>min-h-screen flex flex-col items-center justify-center p-4</span><span>\"</span><span>&gt;</span>  \n      <span>&lt;</span><span>h1</span> <span>className</span><span>=</span><span>\"</span><span>font-bold text-3xl py-8</span><span>\"</span><span>&gt;</span>  \n        <span>Sign</span> <span>up</span>  \n      <span>&lt;</span><span>/h1</span><span>&gt;  \n</span>      <span>&lt;</span><span>form</span>  \n        <span>onSubmit</span><span>=</span><span>{</span><span>handleSubmit</span><span>}</span>  \n        <span>className</span><span>=</span><span>\"</span><span>flex flex-col gap-3 w-full max-w-sm</span><span>\"</span>  \n      <span>&gt;</span>  \n        <span>&lt;</span><span>input</span>  \n          <span>required</span>  \n          <span>type</span><span>=</span><span>\"</span><span>text</span><span>\"</span>  \n          <span>name</span><span>=</span><span>\"</span><span>name</span><span>\"</span>  \n          <span>placeholder</span><span>=</span><span>\"</span><span>Name</span><span>\"</span>  \n          <span>className</span><span>=</span><span>\"</span><span>border p-2 rounded</span><span>\"</span>  \n        <span>/&gt;</span>  \n  \n        <span>&lt;</span><span>input</span>  \n          <span>required</span>  \n          <span>type</span><span>=</span><span>\"</span><span>email</span><span>\"</span>  \n          <span>name</span><span>=</span><span>\"</span><span>email</span><span>\"</span>  \n          <span>placeholder</span><span>=</span><span>\"</span><span>Email</span><span>\"</span>  \n          <span>className</span><span>=</span><span>\"</span><span>border p-2 rounded</span><span>\"</span>  \n        <span>/&gt;</span>  \n  \n        <span>&lt;</span><span>input</span>  \n          <span>required</span>  \n          <span>type</span><span>=</span><span>\"</span><span>password</span><span>\"</span>  \n          <span>name</span><span>=</span><span>\"</span><span>password</span><span>\"</span>  \n          <span>placeholder</span><span>=</span><span>\"</span><span>Password</span><span>\"</span>  \n          <span>className</span><span>=</span><span>\"</span><span>border p-2 rounded</span><span>\"</span>  \n        <span>/&gt;</span>  \n  \n        <span>&lt;</span><span>button</span>  \n          <span>type</span><span>=</span><span>\"</span><span>submit</span><span>\"</span>  \n          <span>disabled</span><span>=</span><span>{</span><span>isLoading</span><span>}</span>  \n          <span>className</span><span>=</span><span>\"</span><span>p-2 bg-blue-600 text-white rounded</span><span>\"</span>  \n        <span>&gt;</span>  \n          <span>{</span><span>isLoading</span> <span>?</span> <span>\"</span><span>Signing up...</span><span>\"</span> <span>:</span> <span>\"</span><span>Sign up</span><span>\"</span><span>}</span>  \n        <span>&lt;</span><span>/button</span><span>&gt;  \n</span>  \n        <span>{</span><span>error</span> <span>&amp;&amp;</span> <span>&lt;</span><span>p</span> <span>className</span><span>=</span><span>\"</span><span>text-red-500 text-sm</span><span>\"</span><span>&gt;</span><span>{</span><span>error</span><span>}</span><span>&lt;</span><span>/p&gt;</span><span>}  \n</span>      <span>&lt;</span><span>/form</span><span>&gt;  \n</span>  \n      <span>&lt;</span><span>p</span> <span>className</span><span>=</span><span>\"</span><span>mt-4</span><span>\"</span><span>&gt;</span>  \n        <span>Already</span> <span>have</span> <span>an</span> <span>account</span><span>?{</span><span>\"</span><span> </span><span>\"</span><span>}</span>  \n        <span>&lt;</span><span>a</span> <span>href</span><span>=</span><span>\"</span><span>/signin</span><span>\"</span> <span>className</span><span>=</span><span>\"</span><span>underline</span><span>\"</span><span>&gt;</span>  \n          <span>Sign</span> <span>in</span>  \n        <span>&lt;</span><span>/a</span><span>&gt;  \n</span>      <span>&lt;</span><span>/p</span><span>&gt;  \n</span>    <span>&lt;</span><span>/div</span><span>&gt;  \n</span>  <span>);</span>  \n<span>}</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<h3>  \n    \n    \n  Creating Signin  \n</h3>  \n  \n<p>Let's also create a signin page so that we can verify user exists in our DB.</p>  \n  \n<p>In the frontmatter we check if user is available we redirect to main route.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>---</span>  \n<span>import</span> <span>Layout</span> <span>from</span> <span>\"</span><span>@/layouts/Layout.astro</span><span>\"</span>  \n<span>import</span> <span>SigninForm</span> <span>from</span> <span>\"</span><span>@/components/SigninForm</span><span>\"</span>  \n  \n<span>if </span><span>(</span><span>Astro</span><span>.</span><span>locals</span><span>.</span><span>user</span><span>?.</span><span>id</span><span>)</span> <span>return</span> <span>Astro</span><span>.</span><span>redirect</span><span>(</span><span>'</span><span>/</span><span>'</span><span>)</span>  \n<span>---</span>  \n  \n<span>&lt;</span><span>html</span> <span>lang</span><span>=</span><span>\"</span><span>en</span><span>\"</span><span>&gt;</span>  \n  <span>&lt;</span><span>head</span><span>&gt;</span>  \n    <span>&lt;</span><span>meta</span> <span>charset</span><span>=</span><span>\"</span><span>utf-8</span><span>\"</span> <span>/&gt;</span>  \n    <span>&lt;</span><span>meta</span> <span>name</span><span>=</span><span>\"</span><span>viewport</span><span>\"</span> <span>content</span><span>=</span><span>\"</span><span>width=device-width</span><span>\"</span> <span>/&gt;</span>  \n  <span>&lt;</span><span>/head</span><span>&gt;  \n</span>  <span>&lt;</span><span>body</span><span>&gt;</span>  \n    <span>&lt;</span><span>Layout</span><span>&gt;</span>  \n      <span>&lt;</span><span>SigninForm</span> <span>client</span><span>:</span><span>load</span><span>/&gt;</span>  \n    <span>&lt;</span><span>Layout</span> <span>/&gt;</span>  \n  <span>&lt;</span><span>/body</span><span>&gt;  \n</span><span>&lt;</span><span>/html</span><span>&gt;  \n</span></code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Same like before we will create a form submission so that we can login the user and redirect to our desired route.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>import</span> <span>type</span> <span>{</span> <span>FormEvent</span> <span>}</span> <span>from</span> <span>\"</span><span>react</span><span>\"</span><span>;</span>  \n<span>import</span> <span>{</span> <span>useState</span> <span>}</span> <span>from</span> <span>\"</span><span>react</span><span>\"</span><span>;</span>  \n<span>import</span> <span>{</span> <span>authClient</span> <span>}</span> <span>from</span> <span>\"</span><span>@/auth-client</span><span>\"</span><span>;</span>  \n  \n<span>export</span> <span>default</span> <span>function</span> <span>SigninPage</span><span>()</span> <span>{</span>  \n  <span>const</span> <span>[</span><span>isLoading</span><span>,</span> <span>setIsLoading</span><span>]</span> <span>=</span> <span>useState</span><span>&lt;</span><span>boolean</span><span>&gt;</span><span>(</span><span>false</span><span>);</span>  \n  <span>const</span> <span>[</span><span>error</span><span>,</span> <span>setError</span><span>]</span> <span>=</span> <span>useState</span><span>&lt;</span><span>string</span><span>&gt;</span><span>(</span><span>\"\"</span><span>);</span>  \n  \n  <span>const</span> <span>handleSubmit</span> <span>=</span> <span>async </span><span>(</span><span>e</span><span>:</span> <span>FormEvent</span><span>&lt;</span><span>HTMLFormElement</span><span>&gt;</span><span>):</span> <span>Promise</span><span>&lt;</span><span>void</span><span>&gt;</span> <span>=&gt;</span> <span>{</span>  \n    <span>e</span><span>.</span><span>preventDefault</span><span>();</span>  \n    <span>setIsLoading</span><span>(</span><span>true</span><span>);</span>  \n    <span>setError</span><span>(</span><span>\"\"</span><span>);</span>  \n  \n    <span>const</span> <span>formData</span> <span>=</span> <span>new</span> <span>FormData</span><span>(</span><span>e</span><span>.</span><span>currentTarget</span><span>);</span>  \n  \n    <span>const</span> <span>email</span> <span>=</span> <span>(</span><span>formData</span><span>.</span><span>get</span><span>(</span><span>\"</span><span>email</span><span>\"</span><span>)</span> <span>as</span> <span>string</span><span>)</span> <span>??</span> <span>\"\"</span><span>;</span>  \n    <span>const</span> <span>password</span> <span>=</span> <span>(</span><span>formData</span><span>.</span><span>get</span><span>(</span><span>\"</span><span>password</span><span>\"</span><span>)</span> <span>as</span> <span>string</span><span>)</span> <span>??</span> <span>\"\"</span><span>;</span>  \n  \n    <span>const</span> <span>response</span> <span>=</span> <span>await</span> <span>authClient</span><span>.</span><span>signIn</span><span>.</span><span>email</span><span>({</span>  \n      <span>email</span><span>,</span>  \n      <span>password</span><span>,</span>  \n    <span>});</span>  \n  \n    <span>if </span><span>(</span><span>!</span><span>response</span><span>.</span><span>error</span><span>)</span> <span>{</span>  \n      <span>window</span><span>.</span><span>location</span><span>.</span><span>href</span> <span>=</span> <span>'</span><span>/</span><span>'</span><span>;</span>  \n    <span>}</span> <span>else</span> <span>{</span>  \n      <span>setError</span><span>(</span><span>response</span><span>.</span><span>error</span><span>.</span><span>message</span> <span>??</span> <span>\"</span><span>Sign-in failed</span><span>\"</span><span>);</span>  \n    <span>}</span>  \n  \n    <span>setIsLoading</span><span>(</span><span>false</span><span>);</span>  \n  <span>};</span>  \n  \n  <span>return </span><span>(</span>  \n    <span>&lt;</span><span>div</span> <span>className</span><span>=</span><span>\"</span><span>min-h-screen flex flex-col items-center justify-center p-4</span><span>\"</span><span>&gt;</span>  \n      <span>&lt;</span><span>h1</span> <span>className</span><span>=</span><span>\"</span><span>font-bold text-3xl py-8</span><span>\"</span><span>&gt;</span>  \n        <span>Sign</span> <span>in</span>  \n      <span>&lt;</span><span>/h1</span><span>&gt;  \n</span>      <span>&lt;</span><span>form</span>  \n        <span>onSubmit</span><span>=</span><span>{</span><span>handleSubmit</span><span>}</span>  \n        <span>className</span><span>=</span><span>\"</span><span>flex flex-col gap-3 w-full max-w-sm</span><span>\"</span>  \n      <span>&gt;</span>  \n        <span>&lt;</span><span>input</span>  \n          <span>required</span>  \n          <span>type</span><span>=</span><span>\"</span><span>email</span><span>\"</span>  \n          <span>name</span><span>=</span><span>\"</span><span>email</span><span>\"</span>  \n          <span>placeholder</span><span>=</span><span>\"</span><span>Email</span><span>\"</span>  \n          <span>className</span><span>=</span><span>\"</span><span>border p-2 rounded</span><span>\"</span>  \n        <span>/&gt;</span>  \n  \n        <span>&lt;</span><span>input</span>  \n          <span>required</span>  \n          <span>type</span><span>=</span><span>\"</span><span>password</span><span>\"</span>  \n          <span>name</span><span>=</span><span>\"</span><span>password</span><span>\"</span>  \n          <span>placeholder</span><span>=</span><span>\"</span><span>Password</span><span>\"</span>  \n          <span>className</span><span>=</span><span>\"</span><span>border p-2 rounded</span><span>\"</span>  \n        <span>/&gt;</span>  \n  \n        <span>&lt;</span><span>button</span>  \n          <span>type</span><span>=</span><span>\"</span><span>submit</span><span>\"</span>  \n          <span>disabled</span><span>=</span><span>{</span><span>isLoading</span><span>}</span>  \n          <span>className</span><span>=</span><span>\"</span><span>p-2 bg-blue-600 text-white rounded</span><span>\"</span>  \n        <span>&gt;</span>  \n          <span>{</span><span>isLoading</span> <span>?</span> <span>\"</span><span>Signing in...</span><span>\"</span> <span>:</span> <span>\"</span><span>Sign In</span><span>\"</span><span>}</span>  \n        <span>&lt;</span><span>/button</span><span>&gt;  \n</span>  \n        <span>{</span><span>error</span> <span>&amp;&amp;</span> <span>&lt;</span><span>p</span> <span>className</span><span>=</span><span>\"</span><span>text-red-500 text-sm</span><span>\"</span><span>&gt;</span><span>{</span><span>error</span><span>}</span><span>&lt;</span><span>/p&gt;</span><span>}  \n</span>      <span>&lt;</span><span>/form</span><span>&gt;  \n</span>  \n      <span>&lt;</span><span>p</span> <span>className</span><span>=</span><span>\"</span><span>mt-4</span><span>\"</span><span>&gt;</span>  \n        <span>Don</span><span>&amp;</span><span>apos</span><span>;</span><span>t</span> <span>have</span> <span>an</span> <span>account</span><span>?{</span><span>\"</span><span> </span><span>\"</span><span>}</span>  \n        <span>&lt;</span><span>a</span> <span>href</span><span>=</span><span>\"</span><span>/signup</span><span>\"</span> <span>className</span><span>=</span><span>\"</span><span>underline</span><span>\"</span><span>&gt;</span>  \n          <span>Sign</span> <span>up</span>  \n        <span>&lt;</span><span>/a</span><span>&gt;  \n</span>      <span>&lt;</span><span>/p</span><span>&gt;  \n</span>    <span>&lt;</span><span>/div</span><span>&gt;  \n</span>  <span>);</span>  \n<span>}</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>We enabled user authentication via credentials method with the help of Better Auth in an Astro application. You have also gained some experience with using middleware in Astro, and understanding how it can help you build dynamic user interfaces.</p>  \n  \n<h3>  \n    \n    \n  Conclusion  \n</h3>  \n  \n<p>Whether we're building a content-focused Astro site or a dynamic React application, Better Auth provides the secure, modern authentication solution you need without the typical complexity.</p>  \n  \n<p>Better Auth's greatest strength might be its developer experience. With automatic API route generation in Astro and React hooks that provide immediate user state, you can implement complete auth flows in minutes rather than days.</p>  \n  \n<p>Have a question, feedback or simply wish to contact me privately? Shoot me a DM and I'll do my best to get back to you.</p>  \n  \n<p>You can find the complete source code in <a href=\"https://github.com/isNan909/astro-betterauth\">Github Link here</a>.</p>  \n  \n<p>Thank you!</p>",
      "summary": "This post was originally published in my website here.  \n  \nAuthentication is a critical foundation for every contemporary web application, but establishing it is often one of the most significant, time-intensive hurdles developers encounter. The solution? Introducing Better Auth API: a framework-independent authentication service designed to revolutionize user management implementation. This detailed tutorial will guide you through creating a fully functional, full-stack application, showcasing",
      "publishedAt": "2025-12-02T12:40:13.000Z",
      "author": "ishan",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 9.938761491080673,
      "ingestedAt": "2025-12-02T14:44:03.184Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a3ef3b",
      "title": "Security Holes in MCP Servers and How To Plug Them",
      "url": "https://dev.to/thenjdevopsguy/security-holes-in-mcp-servers-and-how-to-plug-them-d61",
      "content": "<p>Model Context Protocol (MCP), has officially hit one year old as of November 25th and although there have been some amazing innovations within MCP, one issue still persists - the gaping security hole. This is no secret as just about every organization is talking about it. The long-running joke so far has been “The S in MCP stands for security”.</p> \n \n<p>Aside from prompt injections, MCP Server security is arguably the biggest issue in the AI security world right now.</p> \n \n<p>In this blog post, you’ll learn how to fix the gaps.</p> \n \n<h2> \n   \n   \n  Prerequisites \n</h2> \n \n<p>To follow along with this blog post, you should have:</p> \n \n<ol> \n<li>A Kubernetes cluster running (it can be local).</li> \n<li>Kubernetes Gateway API CRDs installed, which you can find <a href=\"https://gateway-api.sigs.k8s.io/guides/\">here</a>.</li> \n</ol> \n \n<h2> \n   \n   \n  Why Security Matters For MCP \n</h2> \n \n<p>There are two forms of MCP servers:</p> \n \n<ol> \n<li><code>stdio</code></li> \n<li><code>StreamableHTTP</code></li> \n</ol> \n \n<p><code>stdio</code> (the communication method) stands for standard input/output. When using standard input/output, you’re typically targeting a pre-built MCP Server that’s written in, typically, Python or JS (Go is up and coming in this space) that’s called to locally via a command like <code>uvx</code> or <code>npx</code> (depending on how the MCP Server is built). It’s not a standard library download (like a <code>pip install</code>), but instead stored in cache (e.g - <code>~/.local/share/uv</code>).</p> \n \n<p><code>StreamableHTTP</code> is an external (or even internal) server that you’re reaching out to that’s not cached locally. A good example of this is the GitHub Copilot MCP Server. It’s an MCP Server that you’re reaching out to over the HTTP protocol instead of via a local cache.</p> \n \n<p>💡 There was another option called SSE which you may see, but it is now deprecated as of June 2025.</p> \n \n<p>Regardless of which option you choose, there are many security holes.</p> \n \n<p>One (of the many) problem with <code>stdio</code> MCP Servers is that you can't run them through a Gateway. That means no AuthN/Z, no rate limiting, and no tool control. The MCP Servers are effectively open and usable by anyone in an organization unless you’re manually locking each computer down with Claude Desktop configurations (which, spoiler alert: no ones doing).</p> \n \n<p>With Streamable HTTP, you’re in the dark. You’re connecting Agents to some black box running in someones datacenter with who knows what (if any) security protocols, and even if there are security protocols, that doesn’t help from an overall AuthN/Z perspective for your organization. There’s also no way to even test the security without a proper pentest, which wouldn’t be legal without explicit permission from the organization hosting the MCP Server. The only way to do it would be to put an Agent in front of the MCP Server, but then you're not actually securing the MCP Server, you're securing the Agent.</p> \n \n<p>As Model Context Protocol stands right now, there’s only one true way to secure it - with a proper AI Gateway.</p> \n \n<p>Solo Enterprise For Agentgateway implements everything from locking down tools to proper user and system-level authentication to MCP Servers with or without Agents. In the following sections, you’ll see how to configure security for MCP.</p> \n \n<h2> \n   \n   \n  Deploy An MCP Server and Agentgateway \n</h2> \n \n<p>The first step is to deploy an MCP Server and a Gateway via agentgateway enterprise so we not only have an MCP Server to test with, but a proper AI gateway to secure our MCP connectivity.</p> \n \n<ol> \n<li>Create a new Kubernetes Deployment pointing to the test MCP Server that is containerized. You’ll also see a Service that gets deployed so the Gateway can properly connect to it. \n</li> \n</ol> \n \n<div> \n<pre><code><span>kubectl</span> <span>apply</span> <span>-</span><span>f</span> <span>-</span> <span>&lt;</span><span>&lt;</span><span>EOF</span> \n<span>apiVersion</span><span>:</span> <span>apps</span><span>/</span><span>v1</span> \n<span>kind</span><span>:</span> <span>Deployment</span> \n<span>metadata</span><span>:</span> \n  <span>name</span><span>:</span> <span>mcp-website-fetcher</span> \n  <span>namespace</span><span>:</span> <span>default</span> \n<span>spec</span><span>:</span> \n  <span>selector</span><span>:</span> \n    <span>matchLabels</span><span>:</span> \n      <span>app</span><span>:</span> <span>mcp-website-fetcher</span> \n  <span>template</span><span>:</span> \n    <span>metadata</span><span>:</span> \n      <span>labels</span><span>:</span> \n        <span>app</span><span>:</span> <span>mcp-website-fetcher</span> \n    <span>spec</span><span>:</span> \n      <span>containers</span><span>:</span> \n      <span>-</span> <span>name</span><span>:</span> <span>mcp-website-fetcher</span> \n        <span>image</span><span>:</span> <span>ghcr</span><span>.</span><span>io</span><span>/</span><span>peterj</span><span>/</span><span>mcp-website-fetcher</span><span>:</span><span>main</span> \n        <span>imagePullPolicy</span><span>:</span> <span>Always</span> \n<span>---</span> \n<span>apiVersion</span><span>:</span> <span>v1</span> \n<span>kind</span><span>:</span> <span>Service</span> \n<span>metadata</span><span>:</span> \n  <span>name</span><span>:</span> <span>mcp-website-fetcher</span> \n  <span>namespace</span><span>:</span> <span>default</span> \n<span>spec</span><span>:</span> \n  <span>selector</span><span>:</span> \n    <span>app</span><span>:</span> <span>mcp-website-fetcher</span> \n  <span>ports</span><span>:</span> \n  <span>-</span> <span>port</span><span>:</span> <span>80</span> \n    <span>targetPort</span><span>:</span> <span>8000</span> \n    <span>appProtocol</span><span>:</span> <span>kgateway</span><span>.</span><span>dev</span><span>/</span><span>mcp</span> \n<span>EOF</span> \n</code></pre> \n \n</div> \n \n \n \n<ol> \n<li>Deploy the Backend so agentgateway knows what to route to. In this case, it’s routing to the MCP Server service that you deployed in step 1. \n</li> \n</ol> \n \n<div> \n<pre><code><span>kubectl</span> <span>apply</span> <span>-</span><span>f</span> <span>-</span> <span>&lt;</span><span>&lt;</span><span>EOF</span> \n<span>apiVersion</span><span>:</span> <span>gateway</span><span>.</span><span>kgateway</span><span>.</span><span>dev</span><span>/</span><span>v1alpha1</span> \n<span>kind</span><span>:</span> <span>Backend</span> \n<span>metadata</span><span>:</span> \n  <span>name</span><span>:</span> <span>mcp-backend</span> \n  <span>namespace</span><span>:</span> <span>gloo-system</span> \n<span>spec</span><span>:</span> \n  <span>type</span><span>:</span> <span>MCP</span> \n  <span>mcp</span><span>:</span> \n    <span>targets</span><span>:</span> \n    <span>-</span> <span>name</span><span>:</span> <span>mcp-target</span> \n      <span>static</span><span>:</span> \n        <span>host</span><span>:</span> <span>mcp-website-fetcher</span><span>.</span><span>default</span><span>.</span><span>svc</span><span>.</span><span>cluster</span><span>.</span><span>local</span> \n        <span>port</span><span>:</span> <span>80</span> \n        <span>protocol</span><span>:</span> <span>StreamableHTTP</span> \n<span>EOF</span> \n</code></pre> \n \n</div> \n \n \n \n<ol> \n<li>Deploy the Gateway using the agentgateway enterprise class. This Gateway is what will be used for MCP Inspector to connect to (more on Inspector coming up). \n</li> \n</ol> \n \n<div> \n<pre><code><span>kubectl</span> <span>apply</span> <span>-</span><span>f</span> <span>-</span> <span>&lt;</span><span>&lt;</span><span>EOF</span> \n<span>apiVersion</span><span>:</span> <span>gateway</span><span>.</span><span>networking</span><span>.</span><span>k8s</span><span>.</span><span>io</span><span>/</span><span>v1</span> \n<span>kind</span><span>:</span> <span>Gateway</span> \n<span>metadata</span><span>:</span> \n  <span>name</span><span>:</span> <span>agentgateway</span> \n  <span>namespace</span><span>:</span> <span>gloo-system</span> \n<span>spec</span><span>:</span> \n  <span>gatewayClassName</span><span>:</span> <span>agentgateway-enterprise</span> \n  <span>listeners</span><span>:</span> \n  <span>-</span> <span>name</span><span>:</span> <span>http</span> \n    <span>port</span><span>:</span> <span>8080</span> \n    <span>protocol</span><span>:</span> <span>HTTP</span> \n    <span>allowedRoutes</span><span>:</span> \n      <span>namespaces</span><span>:</span> \n        <span>from</span><span>:</span> <span>Same</span> \n<span>EOF</span> \n</code></pre> \n \n</div> \n \n \n \n<ol> \n<li>Create an HTTP route so you can route the traffic to the backend, which you created in step 3. \n</li> \n</ol> \n \n<div> \n<pre><code><span>kubectl</span> <span>apply</span> <span>-</span><span>f</span> <span>-</span> <span>&lt;</span><span>&lt;</span><span>EOF</span> \n<span>apiVersion</span><span>:</span> <span>gateway</span><span>.</span><span>networking</span><span>.</span><span>k8s</span><span>.</span><span>io</span><span>/</span><span>v1</span> \n<span>kind</span><span>:</span> <span>HTTPRoute</span> \n<span>metadata</span><span>:</span> \n  <span>name</span><span>:</span> <span>mcp-route</span> \n  <span>namespace</span><span>:</span> <span>gloo-system</span> \n<span>spec</span><span>:</span> \n  <span>parentRefs</span><span>:</span> \n  <span>-</span> <span>name</span><span>:</span> <span>agentgateway</span> \n  <span>rules</span><span>:</span> \n  <span>-</span> <span>backendRefs</span><span>:</span> \n    <span>-</span> <span>name</span><span>:</span> <span>mcp-backend</span> \n      <span>group</span><span>:</span> <span>gateway</span><span>.</span><span>kgateway</span><span>.</span><span>dev</span> \n      <span>kind</span><span>:</span> <span>Backend</span> \n<span>EOF</span> \n</code></pre> \n \n</div> \n \n \n \n<ol> \n<li>Capture the Gateway ALB IP in an environment variable to be used when connecting to the MCP Server. If you’re running this locally, you can do a <code>port-forward</code> on the Gateway and use <a href=\"http://localhost\"><code>localhost</code></a> within the IP address section when connecting to it. \n</li> \n</ol> \n \n<div> \n<pre><code><span>export</span> <span>GATEWAY_IP</span><span>=</span><span>$</span><span>(</span><span>kubectl</span> <span>get</span> <span>svc</span> <span>agentgateway</span> <span>-</span><span>n</span> <span>gloo</span><span>-</span><span>system</span> <span>-</span><span>o</span> <span>jsonpath</span><span>=</span><span>'</span><span>{.status.loadBalancer.ingress[0].ip}</span><span>'</span><span>)</span> \n<span>echo</span> <span>$GATEWAY_IP</span> \n</code></pre> \n \n</div> \n \n \n \n<ol> \n<li>Open MCP Inspector to connect to the MCP Server. \n</li> \n</ol> \n \n<div> \n<pre><code><span>npx</span> <span>modelcontextprotocol</span><span>/</span><span>inspector</span><span>#</span><span>0.16</span><span>.</span><span>2</span> \n</code></pre> \n \n</div> \n \n \n \n<p>The URL to put into MCP Inspector is: <code>http://YOUR_ALB_LB_IP:8080/mcp</code> or if you’re running locally, <code>http://localhost:8080/mcp</code></p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvc0yf8ylrnhjeny31q8z.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvc0yf8ylrnhjeny31q8z.png\" alt=\"\" width=\"800\" height=\"351\"></a></p> \n \n<p>You’re now connected to the MCP Server via Inspector (the MCP client), but as you can see, it’s fully open. There’s no security at all. In the next section, that’ll be fixed.</p> \n \n<h2> \n   \n   \n  Secure MCP Server Auth \n</h2> \n \n<p>With a properly deployed MCP Server and agentgateway in front of it, let’s begin the journey of securing MCP Server connectivity. The first step is to enable token-based authentication. In this case, you’ll use a JWT token.</p> \n \n<ol> \n<li>Add in a traffic policy for auth based on a JWT token. \n</li> \n</ol> \n \n<div> \n<pre><code><span>kubectl</span> <span>apply</span> <span>-</span><span>f</span><span>-</span> <span>&lt;</span><span>&lt;</span><span>EOF</span> \n<span>apiVersion</span><span>:</span> <span>gloo</span><span>.</span><span>solo</span><span>.</span><span>io</span><span>/</span><span>v1alpha1</span> \n<span>kind</span><span>:</span> <span>GlooTrafficPolicy</span> \n<span>metadata</span><span>:</span> \n  <span>name</span><span>:</span> <span>jwt</span> \n  <span>namespace</span><span>:</span> <span>gloo-system</span> \n<span>spec</span><span>:</span> \n  <span>targetRefs</span><span>:</span> \n    <span>-</span> <span>group</span><span>:</span> <span>gateway</span><span>.</span><span>networking</span><span>.</span><span>k8s</span><span>.</span><span>io</span> \n      <span>kind</span><span>:</span> <span>Gateway</span> \n      <span>name</span><span>:</span> <span>agentgateway</span> \n  <span>glooJWT</span><span>:</span> \n    <span>beforeExtAuth</span><span>:</span> \n      <span>providers</span><span>:</span> \n        <span>selfminted</span><span>:</span> \n          <span>issuer</span><span>:</span> <span>solo</span><span>.</span><span>io</span> \n          <span>jwks</span><span>:</span> \n            <span>local</span><span>:</span> \n              <span>key</span><span>:</span> <span>'{\"keys\":[{\"kty\":\"RSA\",\"kid\":\"solo-public-key-001\",\"use\":\"sig\",\"alg\":\"RS256\",\"n\":\"AOfIaJMUm7564sWWNHaXt_hS8H0O1Ew59-nRqruMQosfQqa7tWne5lL3m9sMAkfa3Twx0LMN_7QqRDoztvV3Wa_JwbMzb9afWE-IfKIuDqkvog6s-xGIFNhtDGBTuL8YAQYtwCF7l49SMv-GqyLe-nO9yJW-6wIGoOqImZrCxjxXFzF6mTMOBpIODFj0LUZ54QQuDcD1Nue2LMLsUvGa7V1ZHsYuGvUqzvXFBXMmMS2OzGir9ckpUhrUeHDCGFpEM4IQnu-9U8TbAJxKE5Zp8Nikefr2ISIG2Hk1K2rBAc_HwoPeWAcAWUAR5tWHAxx-UXClSZQ9TMFK850gQGenUp8\",\"e\":\"AQAB\"}]}'</span> \n<span>EOF</span> \n</code></pre> \n \n</div> \n \n \n \n<ol> \n<li>Save the token for auth via MCP Inspector. \n</li> \n</ol> \n \n<div> \n<pre><code><span>eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InNvbG8tcHVibGljLWtleS0wMDEifQ</span><span>.</span><span>eyJpc3MiOiJzb2xvLmlvIiwib3JnIjoic29sby5pbyIsInN1YiI6ImJvYiIsInRlYW0iOiJvcHMiLCJleHAiOjIwNzQyNzQ5NTQsImxsbXMiOnsibWlzdHJhbGFpIjpbIm1pc3RyYWwtbGFyZ2UtbGF0ZXN0Il19fQ</span><span>.</span><span>GF_uyLpZSTT1DIvJeO_eish1WDjMaS4BQSifGQhqPRLjzu3nXtPkaBRjceAmJi9gKZYAzkT25MIrT42ZIe3bHilrd1yqittTPWrrM4sWDDeldnGsfU07DWJHyboNapYR</span><span>-</span><span>KZGImSmOYshJlzm1tT_Bjt3</span><span>-</span><span>RK3OBzYi90_wl0dyAl9D7wwDCzOD4MRGFpoMrws_OgVrcZQKcadvIsH8figPwN4mK1U_1mxuL08RWTu92xBcezEO4CdBaFTUbkYN66Y2vKSTyPCxg3fLtg1mvlzU1</span><span>-</span><span>Wgm2xZIiPiarQHt6Uq7v9ftgzwdUBQM1AYLvUVhCN6XkkR9OU3p0OXiqEDjAxcg</span> \n</code></pre> \n \n</div> \n \n \n \n<ol> \n<li>Try to reconnect to the MCP Server and you’ll see an error similar to the below: \n</li> \n</ol> \n \n<div> \n<pre><code><span>Connection</span> <span>Error</span> <span>-</span> <span>Check</span> <span>if</span> <span>your</span> <span>MCP</span> <span>server</span> <span>is</span> <span>running</span> <span>and</span> <span>proxy</span> <span>token</span> <span>is</span> <span>correct</span> \n</code></pre> \n \n</div> \n \n \n \n<ol> \n<li>Within MCP Inspector, click on <strong>Authentication</strong> and add in the following:</li> \n<li>Header Name: <strong>Authorization</strong> \n</li> \n<li>Bearer Token: Bobs Token from step 2</li> \n</ol> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ffus385h83g5xo9n07hfg.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ffus385h83g5xo9n07hfg.png\" alt=\"\" width=\"384\" height=\"644\"></a></p> \n \n<p>You should now be able to connect to the MCP Server successfully.</p> \n \n<p>With proper auth set up, you now know that not just anyone can use your agentgateway to connect to an MCP Server. This allows you to ensure that the traffic you’re observing from an AuthN/Z perspective is valid in comparison to the “anyone can do whatever they want” nature of MCP Servers without agentgateway in place.</p> \n \n<h2> \n   \n   \n  Locking Down MCP Tool Lists \n</h2> \n \n<p>The final step is to specify what MCP Tools are available. One of the main issues for organizations is they want to use Tools within an MCP Server, but not all Tools. For example, maybe a person or an AI Agent connecting to an MCP Server only needs the ability to view/list/get resources (like a readonly Agent), but with the current architecture out of the box available for MCP, that’s not doable.</p> \n \n<p>However, with traffic policies via agentgateway, it is.</p> \n \n<ol> \n<li>Create a policy that specifies no tools available for use. This will help in testing the ability to lock down tools via the Gateway. \n</li> \n</ol> \n \n<div> \n<pre><code><span>kubectl</span> <span>apply</span> <span>-</span><span>f</span><span>-</span> <span>&lt;</span><span>&lt;</span><span>EOF</span> \n<span>apiVersion</span><span>:</span> <span>gateway</span><span>.</span><span>kgateway</span><span>.</span><span>dev</span><span>/</span><span>v1alpha1</span> \n<span>kind</span><span>:</span> <span>TrafficPolicy</span> \n<span>metadata</span><span>:</span> \n  <span>name</span><span>:</span> <span>jwt-rbac</span> \n  <span>namespace</span><span>:</span> <span>gloo-system</span> \n<span>spec</span><span>:</span> \n  <span>targetRefs</span><span>:</span> \n    <span>-</span> <span>group</span><span>:</span> <span>gateway</span><span>.</span><span>kgateway</span><span>.</span><span>dev</span> \n      <span>kind</span><span>:</span> <span>Backend</span> \n      <span>name</span><span>:</span> <span>mcp-backend</span> \n  <span>rbac</span><span>:</span> \n    <span>policy</span><span>:</span> \n      <span>matchExpressions</span><span>:</span> \n        <span>-</span> <span>'mcp.tool.name == \"\"'</span> \n<span>EOF</span> \n</code></pre> \n \n</div> \n \n \n \n<ol> \n<li>Disconnect and reconnect via the MCP Inspector and you should see no tools available.</li> \n</ol> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxf7ay60kp3tgiijb26j1.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxf7ay60kp3tgiijb26j1.png\" alt=\"\" width=\"800\" height=\"457\"></a></p> \n \n<ol> \n<li>Update the policy to include the <strong>fetch</strong> tool. \n</li> \n</ol> \n \n<div> \n<pre><code><span>kubectl</span> <span>apply</span> <span>-</span><span>f</span><span>-</span> <span>&lt;</span><span>&lt;</span><span>EOF</span> \n<span>apiVersion</span><span>:</span> <span>gateway</span><span>.</span><span>kgateway</span><span>.</span><span>dev</span><span>/</span><span>v1alpha1</span> \n<span>kind</span><span>:</span> <span>TrafficPolicy</span> \n<span>metadata</span><span>:</span> \n  <span>name</span><span>:</span> <span>jwt-rbac</span> \n  <span>namespace</span><span>:</span> <span>gloo-system</span> \n<span>spec</span><span>:</span> \n  <span>targetRefs</span><span>:</span> \n    <span>-</span> <span>group</span><span>:</span> <span>gateway</span><span>.</span><span>kgateway</span><span>.</span><span>dev</span> \n      <span>kind</span><span>:</span> <span>Backend</span> \n      <span>name</span><span>:</span> <span>mcp-backend</span> \n  <span>rbac</span><span>:</span> \n    <span>policy</span><span>:</span> \n      <span>matchExpressions</span><span>:</span> \n        <span>-</span> <span>'mcp.tool.name == \"fetch\"'</span> \n<span>EOF</span> \n</code></pre> \n \n</div> \n \n \n \n<ol> \n<li>Reconnect to the MCP Server via Inspector and you’ll now see the tool available.</li> \n</ol> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F1bjmw9ea661dyxwo1hb8.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F1bjmw9ea661dyxwo1hb8.png\" alt=\"\" width=\"800\" height=\"351\"></a></p> \n \n<h2> \n   \n   \n  Conclusion \n</h2> \n \n<p>With all of the security concerns around MCP Servers, it reminds us of a very important aspect of cyber security - it’s not about trying to block all bad actors, it’s about mitigating as much risk as possible. That should be the goal for every organization and with these implementations, your MCP security posture should be in a much better place.</p>",
      "summary": "Model Context Protocol (MCP), has officially hit one year old as of November 25th and although there have been some amazing innovations within MCP, one issue still persists - the gaping security hole. This is no secret as just about every organization is talking about it. The long-running joke so far has been “The S in MCP stands for security”. \n \nAside from prompt injections, MCP Server security is arguably the biggest issue in the AI security world right now. \n \nIn this blog post, you’ll learn",
      "publishedAt": "2025-12-02T12:35:43.000Z",
      "author": "Michael Levan",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 17.88577787720273,
      "ingestedAt": "2025-12-02T14:44:03.184Z",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "testing",
        "governance",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a3de65",
      "title": "A series of vignettes from my childhood and early career",
      "url": "https://www.jasonscheirer.com/weblog/vignettes/",
      "content": "<p>Article URL: <a href=\"https://www.jasonscheirer.com/weblog/vignettes/\">https://www.jasonscheirer.com/weblog/vignettes/</a></p> \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46120549\">https://news.ycombinator.com/item?id=46120549</a></p> \n<p>Points: 39</p> \n<p># Comments: 18</p>",
      "summary": "Article URL: https://www.jasonscheirer.com/weblog/vignettes/ \nComments URL: https://news.ycombinator.com/item?id=46120549 \nPoints: 39 \n# Comments: 18",
      "publishedAt": "2025-12-02T12:28:34.000Z",
      "author": "absqueued",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4899529653092944,
      "ingestedAt": "2025-12-02T14:44:03.184Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a39c9e",
      "title": "‘The biggest decision yet’: Jared Kaplan on allowing AI to train itself - The Guardian",
      "url": "https://news.google.com/rss/articles/CBMitgFBVV95cUxQR3htZDNHeEE0aWl3cGMtbU1mVnV4TDY4QjllanlEODAwSFdfYWZybVFXdjFWVmxpeGZvVGlwUENwd1JjU1VXcW5tMVYxRGVTclB1MnFKcnMzVDRxWTNHRnBnT1RFNEl2UjJYRkdpcU1hb2JFNkt1ZWF2a1hNLThYNVRmU1I4eTRrVTFSSTlMTDFxRnNZUE5PNVZsM01QeFo5TDYwb0dOb3JYdkEtZlRoWFB4aDdLQQ?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMitgFBVV95cUxQR3htZDNHeEE0aWl3cGMtbU1mVnV4TDY4QjllanlEODAwSFdfYWZybVFXdjFWVmxpeGZvVGlwUENwd1JjU1VXcW5tMVYxRGVTclB1MnFKcnMzVDRxWTNHRnBnT1RFNEl2UjJYRkdpcU1hb2JFNkt1ZWF2a1hNLThYNVRmU1I4eTRrVTFSSTlMTDFxRnNZUE5PNVZsM01QeFo5TDYwb0dOb3JYdkEtZlRoWFB4aDdLQQ?oc=5\">‘The biggest decision yet’: Jared Kaplan on allowing AI to train itself</a>  The Guardian",
      "summary": "‘The biggest decision yet’: Jared Kaplan on allowing AI to train itself  The Guardian",
      "publishedAt": "2025-12-02T12:37:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4905763729742991,
      "ingestedAt": "2025-12-02T14:44:03.184Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a39ca0",
      "title": "Why Anthropic’s Claude Is the Co-Founder of the Year - Inc.com",
      "url": "https://news.google.com/rss/articles/CBMilgFBVV95cUxORWVsaTJZZzQ1NkZsVEEtUlk0WFVnQXZjcHM2Z0hlTlN6QXF2NzJYdU12RWpRRlRKVWdSdXNLbjVXSWM5TGRxVEp6MHBHUGxCeXNHZV9CMU1kRHBpOXZIN0FIbExtTV84dTBqaGRnUEZuQnhHOEp1SncxOVZNMlNPdHVYaXVMOHZuaGdXVWY4UjlYN2ZYSnc?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMilgFBVV95cUxORWVsaTJZZzQ1NkZsVEEtUlk0WFVnQXZjcHM2Z0hlTlN6QXF2NzJYdU12RWpRRlRKVWdSdXNLbjVXSWM5TGRxVEp6MHBHUGxCeXNHZV9CMU1kRHBpOXZIN0FIbExtTV84dTBqaGRnUEZuQnhHOEp1SncxOVZNMlNPdHVYaXVMOHZuaGdXVWY4UjlYN2ZYSnc?oc=5\">Why Anthropic’s Claude Is the Co-Founder of the Year</a>  Inc.com",
      "summary": "Why Anthropic’s Claude Is the Co-Founder of the Year  Inc.com",
      "publishedAt": "2025-12-02T12:07:25.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.465171993993628,
      "ingestedAt": "2025-12-02T14:44:03.184Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a39ca5",
      "title": "AI agents pose immediate threat to smart contract security, Anthropic says - theblock.co",
      "url": "https://news.google.com/rss/articles/CBMieEFVX3lxTE50Y3NQS05qaW5nUExHVlR0RWxoY2htcHVvNUpESUt3b0xGWEEwOWRyZzk3U1hvYi1oMEJNTDNTbjlQQTE4Njk5RlJ5cHAxU3VfdU5udGxma203ZWllUXNVS0dZVzNWQkNSQlNkdkplVmZjeDVHdjU5NtIBfkFVX3lxTFBlZEljLUpJampUZU42VV9iRlBJWllFNnJUR0tLbFM1M3FmaEJCM0RVR05pNFBEM3RtREg4OERzWDRoQnRsVFhobEpGUVlJNE1jS3RZZDZpWl9mY2JhZUdwZldvaXR2Y2VrcWJYeWRpcWkxUU5zVThwcXFGVURWZw?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMieEFVX3lxTE50Y3NQS05qaW5nUExHVlR0RWxoY2htcHVvNUpESUt3b0xGWEEwOWRyZzk3U1hvYi1oMEJNTDNTbjlQQTE4Njk5RlJ5cHAxU3VfdU5udGxma203ZWllUXNVS0dZVzNWQkNSQlNkdkplVmZjeDVHdjU5NtIBfkFVX3lxTFBlZEljLUpJampUZU42VV9iRlBJWllFNnJUR0tLbFM1M3FmaEJCM0RVR05pNFBEM3RtREg4OERzWDRoQnRsVFhobEpGUVlJNE1jS3RZZDZpWl9mY2JhZUdwZldvaXR2Y2VrcWJYeWRpcWkxUU5zVThwcXFGVURWZw?oc=5\">AI agents pose immediate threat to smart contract security, Anthropic says</a>  theblock.co",
      "summary": "AI agents pose immediate threat to smart contract security, Anthropic says  theblock.co",
      "publishedAt": "2025-12-02T08:37:49.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 10.310974042424592,
      "ingestedAt": "2025-12-02T14:44:03.184Z",
      "tags": [
        "agents",
        "Coding Agent Product Updates",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09a39ca8",
      "title": "Anthropic's New Claude Opus 4.5 AI Model Is Designed for Coding and Office Work - CNET",
      "url": "https://news.google.com/rss/articles/CBMixgFBVV95cUxOLWdienVVTHNHRktFQzVwczRtN1dCamFYZGY4ZndVZ2FlUDdrTlJZajdqQkdjNndkNl9hRUtDNEg2aVRSNi11TGdiSVhlUnZxOUtpNTl3YS1ocklSUWVBcEFFaGtSdzEtYlFKLThxamFmZENxX2hWOTRxYXNQczBfMGxHMGtHSGdDOWNUSjBHcllBR0hSVy1kMG95SC1xRFRqdWxIUV9mdk1ldW5rRy1EcHVkYVotZjRaY1k2R05OcWFrMm5iM2c?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMixgFBVV95cUxOLWdienVVTHNHRktFQzVwczRtN1dCamFYZGY4ZndVZ2FlUDdrTlJZajdqQkdjNndkNl9hRUtDNEg2aVRSNi11TGdiSVhlUnZxOUtpNTl3YS1ocklSUWVBcEFFaGtSdzEtYlFKLThxamFmZENxX2hWOTRxYXNQczBfMGxHMGtHSGdDOWNUSjBHcllBR0hSVy1kMG95SC1xRFRqdWxIUV9mdk1ldW5rRy1EcHVkYVotZjRaY1k2R05OcWFrMm5iM2c?oc=5\">Anthropic's New Claude Opus 4.5 AI Model Is Designed for Coding and Office Work</a>  CNET",
      "summary": "Anthropic's New Claude Opus 4.5 AI Model Is Designed for Coding and Office Work  CNET",
      "publishedAt": "2025-11-24T19:00:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 0.8579000459028766,
      "ingestedAt": "2025-12-02T14:44:03.184Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099ee943",
      "title": "I Built an AI That Finds Every Article Similar to Your Text — Here’s How It Works",
      "url": "https://dev.to/stokry/i-built-an-ai-that-finds-every-article-similar-to-your-text-heres-how-it-works-bhb",
      "content": "<p>Most search engines will show you pages that <em>mention</em> the same keywords.</p> \n \n<p>But what if you want to find articles that express the <strong>same ideas</strong>, even if they’re written completely differently?</p> \n \n<p>That’s the problem that pushed me into building something new.</p> \n \n<p>A few months ago, I needed to verify whether parts of my content were being reused online. Google wasn’t helpful — keyword search missed half of the semantically similar pages. Rephrased paragraphs were completely invisible.</p> \n \n<p>So I built my own engine.</p> \n \n<p>Today, I’m excited to introduce <strong><a href=\"https://nooth.dev/\">Nooth</a></strong>, an AI-powered system that takes <em>any text you enter</em> and searches the web for content with similar themes, ideas, and semantic meaning.</p> \n \n<p>Drop in a paragraph → get URLs, similarity percentages, and deep analytical insights.</p> \n \n<p>Here’s how it works under the hood.</p> \n \n<h2> \n   \n   \n  <strong>🚀 The Idea</strong> \n</h2> \n \n<p>Traditional search relies on keyword overlap.</p> \n \n<p>If two texts don’t share the same words, the engine assumes they’re unrelated.</p> \n \n<p>But humans don’t think that way.</p> \n \n<p>We recognize similarity through <strong>meaning</strong> — the structure of ideas, not characters.</p> \n \n<p>So the goal was simple:</p> \n \n<blockquote> \n<p><strong>Build an engine that understands what your text is about, then find everything on the internet that expresses the same ideas.</strong></p> \n</blockquote> \n \n<p>That meant two pieces were crucial:</p> \n \n<ol> \n<li><p><strong>A way to measure meaning</strong></p></li> \n<li><p><strong>A way to scan the web, extract clean text, and compare it</strong></p></li> \n</ol> \n \n<h2> \n   \n   \n  <strong>🧠 Step 1: Turning Text Into Meaning (Embeddings)</strong> \n</h2> \n \n<p>At the core of Nooth is a semantic model that converts text into embeddings — numerical vectors that represent meaning.</p> \n \n<p>Example:</p> \n \n<blockquote> \n<p>“How to speed up your Node.js API”</p> \n \n<p>and</p> \n \n<p>“Optimizing performance in a JavaScript backend”</p> \n</blockquote> \n \n<p>These look different, but in vector space, their embeddings are neighbors.</p> \n \n<p>Every time you paste text into Nooth:</p> \n \n<ul> \n<li><p>it cleans and normalizes it</p></li> \n<li><p>creates sentence-level embeddings</p></li> \n<li><p>composes them into a unified “semantic fingerprint”</p></li> \n</ul> \n \n<p>That fingerprint becomes the reference point for the search.</p> \n \n<h2> \n   \n   \n  <strong>🌐 Step 2: Finding Content Across the Web</strong> \n</h2> \n \n<p>Nooth crawls and indexes content using:</p> \n \n<ul> \n<li><p>lightweight scrapers</p></li> \n<li><p>structured metadata</p></li> \n<li><p>boilerplate removal</p></li> \n<li><p>language detection</p></li> \n<li><p>canonical URL resolution</p></li> \n<li><p>and chunking for long-form content</p></li> \n</ul> \n \n<p>The goal isn’t to store the entire HTML — only the <strong>meaningful part</strong>:</p> \n \n<p>titles, headings, paragraphs, and contextual metadata.</p> \n \n<p>Each chunk gets its own embedding.</p> \n \n<p>That allows highly precise, paragraph-level similarity detection.</p> \n \n \n \n \n<h2> \n   \n   \n  <strong>📊 Step 3: Computing Semantic Similarity</strong> \n</h2> \n \n<p>Once all candidates are scraped, Nooth performs:</p> \n \n<ul> \n<li><p>cosine similarity</p></li> \n<li><p>conceptual overlap scoring</p></li> \n<li><p>syntactic variance weighting</p></li> \n<li><p>partial-match scoring (for rewritten content)</p></li> \n</ul> \n \n<p>This combination is what lets Nooth detect even heavily rephrased articles.</p> \n \n<p>It doesn’t just ask:</p> \n \n<p><strong>“Do these texts share words?”</strong></p> \n \n<p>It asks:</p> \n \n<p><strong>“Do these texts express the same thing?”</strong></p> \n \n \n \n \n<h2> \n   \n   \n  <strong>🔍 Step 4: Presenting Clear Evidence</strong> \n</h2> \n \n<p>The final result looks like this (example flow):</p> \n \n<ol> \n<li><p>You paste a paragraph</p></li> \n<li><p>Engine breaks it into meaning blocks</p></li> \n<li><p>It fetches semantically closest content across the web</p></li> \n<li><p>You get:</p></li> \n</ol> \n \n<div> \n<pre><code>-   URL \n \n-   domain \n \n-   similarity percentage \n \n-   highlighted overlapping ideas \n \n-   reasoning on _why_ the match is close \n</code></pre> \n \n</div> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxc7fbzsdrckb2qtfmd1s.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxc7fbzsdrckb2qtfmd1s.png\" alt=\"Ui Nooth\" width=\"800\" height=\"489\"></a></p> \n \n<p>This makes it useful for:</p> \n \n<ul> \n<li><p>discovering who’s copying your content</p></li> \n<li><p>researching related topics</p></li> \n<li><p>finding competitors</p></li> \n<li><p>understanding what else exists on the same theme</p></li> \n<li><p>generating new ideas based on existing clusters</p></li> \n</ul> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsyj9aklbsfzco6g1pq9x.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsyj9aklbsfzco6g1pq9x.png\" alt=\"UI NOOTH\" width=\"800\" height=\"748\"></a></p> \n \n \n \n \n<h2> \n   \n   \n  <strong>🧭 Why I Built It</strong> \n</h2> \n \n<p>I didn’t want another Google-like keyword search.</p> \n \n<p>I wanted a lens into the <strong>semantic web</strong> — the world of ideas, not strings.</p> \n \n<p>Whether you’re a researcher, writer, builder, or SEO nerd, sometimes you need to find content that’s <em>conceptually</em> similar, not textually identical.</p> \n \n<p>Nooth solves that by combining:</p> \n \n<ul> \n<li><p>modern embeddings</p></li> \n<li><p>optimized scraping</p></li> \n<li><p>semantic search</p></li> \n<li><p>scoring heuristics</p></li> \n<li><p>and clean UI on top of everything</p></li> \n</ul> \n \n<p>It shows you what’s related on an idea level, not just word level.</p> \n \n \n \n \n<h2> \n   \n   \n  <strong>🧪 Try It Out</strong> \n</h2> \n \n<p>If you want to test it:</p> \n \n<p>👉 Paste any text.</p> \n \n<p>👉 See what the web holds that’s semantically connected.</p> \n \n<p>👉 Explore clusters of related ideas you didn’t even know existed.</p> \n \n<p><a href=\"https://nooth.dev/\">https://nooth.dev/</a></p> \n \n \n \n \n<h2> \n   \n   \n  <strong>🔮 What’s Next</strong> \n</h2> \n \n<p>I’m currently working on:</p> \n \n<ul> \n<li><p>deeper plagiarism detection</p></li> \n<li><p>topic extraction</p></li> \n<li><p>content clustering</p></li> \n<li><p>semantic timelines</p></li> \n<li><p>alerts when new similar content appears</p></li> \n<li><p>full API for developers</p></li> \n</ul> \n \n<p>If any of this sounds interesting, I’d love feedback.</p> \n \n<p>Thanks for reading — and welcome to the world of semantic discovery.</p>",
      "summary": "Most search engines will show you pages that mention the same keywords. \n \nBut what if you want to find articles that express the same ideas, even if they’re written completely differently? \n \nThat’s the problem that pushed me into building something new. \n \nA few months ago, I needed to verify whether parts of my content were being reused online. Google wasn’t helpful — keyword search missed half of the semantically similar pages. Rephrased paragraphs were completely invisible. \n \nSo I built my",
      "publishedAt": "2025-12-02T12:34:19.000Z",
      "author": "Stokry",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "thought_leadership",
      "score": 6.458304608035026,
      "ingestedAt": "2025-12-02T14:44:03.185Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099ee94e",
      "title": "Getting Started with Nop: How to Creatively Extend GraphQL",
      "url": "https://dev.to/canonical/getting-started-with-nop-how-to-creatively-extend-graphql-5e7c",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fua8xqp8153db4qk3d1ul.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>The Nop platform does not use common GraphQL open-source libraries such as <code>graphql-java</code>; instead, it implements the NopGraphQL engine from scratch. The NopGraphQL engine introduces many novel implementation approaches that broaden GraphQL’s scope of application and enhance its practicality.</p>  \n  \n<p>For detailed documentation, see <a href=\"https://gitee.com/canonical-entropy/nop-entropy/blob/master/docs-en/dev-guide/graphql/index.md\">graphql/index.md</a></p>  \n  \n<h2>  \n    \n    \n  I. Simplifying GraphQL Queries with Fragment Definitions  \n</h2>  \n  \n<p>GraphQL requires the frontend to specify the fields to be returned, which can be cumbersome when there are many fields. In such cases, you can use the Fragment feature of the GraphQL language to define common field sets and reference these fragments in queries to simplify them.</p>  \n  \n<h3>  \n    \n    \n  1.1 Add selection definitions in XMeta, prefixed with <code>F_</code>  \n</h3>  \n  \n<p>Each backend service object in the Nop platform has an associated XMeta metadata model file, where you can augment GraphQL types with additional metadata.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>&lt;meta&gt;</span>  \n  <span>&lt;selections&gt;</span>  \n    <span>&lt;selection</span> <span>id=</span><span>\"F_defaults\"</span><span>&gt;</span>  \n      userId, userName, status, relatedRoleList{ roleName}  \n    <span>&lt;/selection&gt;</span>  \n  <span>&lt;/selections&gt;</span>  \n<span>&lt;/meta&gt;</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<ul>  \n<li>By convention, only fragment definitions prefixed with <code>F_</code> are accessible from the frontend. Selections have other uses as well.</li>  \n<li>If <code>F_defaults</code> is not configured, it will be automatically inferred based on all non-lazy fields of the GraphQL type. If explicitly specified, the specified content is used.</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  1.2 Reference fragments in frontend queries  \n</h3>  \n  \n<p>When invoking backend services via GraphQL, you can use fragments:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>query</span><span>{</span><span>  \n   </span><span>NopAuthUser__findList</span><span>{</span><span>  \n     </span><span>...</span><span>F_defaults</span><span>,</span><span> </span><span>groupMappings</span><span>{...</span><span>F_defaults</span><span>}</span><span>  \n   </span><span>}</span><span>  \n</span><span>}</span><span>  \n</span></code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Alternatively, when calling backend services via REST, use the <code>@selection</code> parameter to reference fragments:<br>  \n</p>  \n  \n<div>  \n<pre><code>/r/NopAuthUser__findList?@selection=...F_defaults,groupMappings  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<ul>  \n<li>When using REST, if <code>@selection</code> is not provided, it is equivalent to returning <code>F_defaults</code>.</li>  \n</ul>  \n  \n<p><strong>Under REST, if the selection only specifies object-level fields, it will be automatically expanded to nested levels.</strong></p>  \n  \n<h2>  \n    \n    \n  II. Simplify Tree-structured Queries with the <code>@TreeChildren</code> Directive  \n</h2>  \n  \n<p>For retrieving tree structures such as organization trees or menu trees, NopGraphQL provides an extended syntax via the directive mechanism to directly express recursive data fetching, for example:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>query</span><span> </span><span>{</span><span>  \n    </span><span>NopAuthDept_findList</span><span>{</span><span>  \n        </span><span>value</span><span>:</span><span> </span><span>id</span><span>  \n        </span><span>label</span><span>:</span><span> </span><span>displayName</span><span>  \n        </span><span>children</span><span> </span><span>@TreeChildren</span><span>(</span><span>max</span><span>:</span><span>5</span><span>)</span><span>  \n    </span><span>}</span><span>  \n</span><span>}</span><span>  \n</span></code></pre>  \n  \n</div>  \n  \n  \n  \n<ul>  \n<li>  \n<code>@TreeChildren(max:5)</code> indicates that, following the current level’s structure, up to 5 levels will be nested.</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  III. Map Type  \n</h2>  \n  \n<p>GraphQL is a strongly typed framework that requires all data to have explicit type definitions, which can be inconvenient in certain dynamic scenarios. For instance, sometimes you may need to return an extensible collection to the frontend.</p>  \n  \n<p>NopGraphQL introduces a special scalar type: Map, which can be used to describe dynamic data structures. For example:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>type</span><span> </span><span>QueryBean</span><span>{</span><span>  \n    </span><span>filter</span><span>:</span><span> </span><span>Map</span><span>  \n    </span><span>orderBy</span><span>:</span><span> </span><span>[</span><span>OrderFieldBean</span><span>]</span><span>  \n</span><span>}</span><span>  \n</span></code></pre>  \n  \n</div>  \n  \n  \n  \n<h2>  \n    \n    \n  IV. XMeta Metadata Model  \n</h2>  \n  \n<p>The XMeta metadata model enables many features via configuration.</p>  \n  \n<h3>  \n    \n    \n  4.1 Map to existing properties via mapToProp  \n</h3>  \n  \n  \n  \n<div>  \n<pre><code><span>&lt;prop</span> <span>name=</span><span>\"a\"</span> <span>mapToProp=</span><span>\"b.a\"</span><span>&gt;</span>  \n<span>&lt;/prop&gt;</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<ul>  \n<li>The mapToProp attribute can specify an alias for an existing property. When the frontend accesses property a, it actually retrieves the a property on the associated object b.</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  4.2 Specify computed expressions directly via getter  \n</h3>  \n  \n<p>In NopGraphQL, you can introduce dynamically computed fields in a BizModel service class via the <code>@BizLoader</code> annotation.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>@BizModel</span><span>(</span><span>\"LoginApi\"</span><span>)</span>  \n<span>public</span> <span>class</span> <span>LoginApiBizModelDelta</span> <span>{</span>  \n    <span>@BizLoader</span><span>(</span><span>autoCreateField</span> <span>=</span> <span>true</span><span>,</span> <span>forType</span> <span>=</span> <span>LoginResult</span><span>.</span><span>class</span><span>)</span>  \n    <span>@LazyLoad</span>  \n    <span>public</span> <span>String</span> <span>location</span><span>(</span><span>@ContextSource</span> <span>LoginResult</span> <span>result</span><span>,</span>  \n                           <span>IServiceContext</span> <span>context</span><span>)</span> <span>{</span>  \n        <span>return</span> <span>\"loc:\"</span> <span>+</span> <span>result</span><span>.</span><span>getUserInfo</span><span>().</span><span>getUserId</span><span>();</span>  \n    <span>}</span>  \n<span>}</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>For lightweight computed expressions, defining service functions may be overly complex; in such cases, you can define them directly in XMeta via a getter expression:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>&lt;prop</span> <span>name=</span><span>\"myValue\"</span><span>&gt;</span>  \n  <span>&lt;getter&gt;</span>  \n    return entity.name + 'Ext'  \n  <span>&lt;/getter&gt;</span>  \n<span>&lt;/prop&gt;</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<h3>  \n    \n    \n  4.3 Field-level access control  \n</h3>  \n  \n<p>In the xmeta file, you can specify <code>auth</code> settings for a <code>prop</code>:<br>  \n</p>  \n  \n<div>  \n<pre><code>  \n<span>&lt;prop</span> <span>name=</span><span>\"xx\"</span><span>&gt;</span>  \n    <span>&lt;auth</span> <span>permissions=</span><span>\"NopAuthUser:query\"</span> <span>roles=</span><span>\"admin\"</span> <span>for=</span><span>\"read\"</span><span>/&gt;</span>  \n    <span>&lt;auth</span> <span>permissions=</span><span>\"NopAuthUser:mutation\"</span> <span>roles=</span><span>\"hr\"</span> <span>for=</span><span>\"write\"</span><span>/&gt;</span>  \n<span>&lt;/prop&gt;</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<ul>  \n<li>This configuration enables read/write access control at the field level. <code>for=\"read\"</code> controls read access to the field, <code>for=\"write\"</code> controls write access, and <code>for=\"all\"</code> allows both read and write.</li>  \n<li>Before performing any actual actions, the NopGraphQL engine checks access permissions for each field in the selection set, ensuring you don't end up executing business operations only to discover that certain result fields are inaccessible.</li>  \n<li>For data permissions and filter criteria for related sub-tables, see <a href=\"https://dev.to/canonical/nop-getting-started-how-to-implement-complex-queries-l98\">4-complex-query.md</a>.</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  4.4 Automatically generate data dictionary text fields  \n</h3>  \n  \n<p>In business development, a common requirement is to translate backend business field values into display text according to a data dictionary configuration. In the Nop platform, during the loading phase, XMeta model files use metaprogramming to dynamically determine whether a data dictionary is configured.<br>  \nIf so, a dictionary translation field is automatically generated.<br>  \n</p>  \n  \n<div>  \n<pre><code>  \n<span>&lt;prop</span> <span>name=</span><span>\"status\"</span><span>&gt;</span>  \n    <span>&lt;schema</span> <span>type=</span><span>\"Integer\"</span> <span>dict=</span><span>\"auth/user-status\"</span><span>/&gt;</span>  \n<span>&lt;/prop&gt;</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>After metaprogramming transformation, the following field definitions are generated:<br>  \n</p>  \n  \n<div>  \n<pre><code>  \n<span>&lt;prop</span> <span>name=</span><span>\"status\"</span> <span>graphql:labelProp=</span><span>\"status_label\"</span><span>&gt;</span>  \n    <span>&lt;schema</span> <span>type=</span><span>\"Integer\"</span> <span>dict=</span><span>\"auth/user-status\"</span><span>/&gt;</span>  \n<span>&lt;/prop&gt;</span>  \n<span>&lt;prop</span> <span>name=</span><span>\"status_label\"</span> <span>internal=</span><span>\"true\"</span> <span>graphql:dictName=</span><span>\"auth/user-status\"</span>  \n      <span>graphql:dictValueProp=</span><span>\"status\"</span><span>&gt;</span>  \n    <span>&lt;schema</span> <span>type=</span><span>\"String\"</span><span>/&gt;</span>  \n<span>&lt;/prop&gt;</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<h3>  \n    \n    \n  4.5 Masked display  \n</h3>  \n  \n<p>For security reasons, some sensitive user information must not be printed to log files; when returned to the frontend for display, it also needs to be masked—only showing the first few and the last few characters,<br>  \nsuch as credit card numbers, user phone numbers, etc.</p>  \n  \n<p>You can specify a masking display pattern via <code>ui:maskPattern</code>, and when GraphQL returns field values it will automatically apply this pattern.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>&lt;prop</span> <span>name=</span><span>\"email\"</span> <span>ui:maskPattern=</span><span>\"3*4\"</span><span>&gt;</span>  \n  \n<span>&lt;/prop&gt;</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<ul>  \n<li>  \n<code>ui:maskPattern=\"3*4\"</code> means keeping the first 3 and last 4 characters, with the rest replaced by <code>*</code>.</li>  \n</ul>",
      "summary": "The Nop platform does not use common GraphQL open-source libraries such as graphql-java; instead, it implements the NopGraphQL engine from scratch. The NopGraphQL engine introduces many novel implementation approaches that broaden GraphQL’s scope of application and enhance its practicality.  \n  \nFor detailed documentation, see graphql/index.md  \n  \n  \n    \n    \n  I. Simplifying GraphQL Queries with Fragment Definitions  \n  \n  \nGraphQL requires the frontend to specify the fields to be returned, w",
      "publishedAt": "2025-12-02T12:17:12.000Z",
      "author": "canonical",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "security_incident",
      "score": 11.912905051828682,
      "ingestedAt": "2025-12-02T14:44:03.185Z",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099ee950",
      "title": "How Crypto Businesses Can Prepare for MiCA Authorization in the European Union",
      "url": "https://dev.to/aleksandr_rozental_1b7caa/how-crypto-businesses-can-prepare-for-mica-authorization-in-the-european-union-3i2h",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F46cuh99892gycvsewyf1.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>The European regulatory environment for crypto is evolving rapidly, and MiCA is the most significant development shaping how businesses operate across the European Union. Companies launching exchanges, custody solutions, OTC desks, tokenization platforms or payment systems must now comply with a unified regulatory structure that is far more demanding than the previous patchwork of national rules. Preparing for authorization requires a strategic, methodical and well-organized approach that goes far beyond producing a few basic compliance documents.</p>  \n  \n<p>MiCA introduces a new regulatory architecture designed to ensure consumer protection, market integrity and financial stability. It defines a complete framework for authorization, governance, internal controls, AML procedures, ICT security, operational resilience and oversight. Companies entering the EU market must understand that authorization is not a one-time formality but a long-term commitment to maintaining a high degree of organizational discipline.</p>  \n  \n<p>One of the most challenging aspects of preparing for authorization is realizing how interconnected the requirements are. A company cannot simply create a standalone AML policy or a single risk statement. Every procedure must align with operational reality, governance structure, staffing, technology and internal oversight. Regulators expect consistency across all documentation, and even small discrepancies between internal policies can lead to questions or delays. Many applications fail because companies assemble documents from different sources without ensuring they reflect a coherent internal system.</p>  \n  \n<p>Another common barrier arises from governance. MiCA requires a clear organizational structure, well-defined responsibilities, transparent management roles and reliable control functions. Regulators want to see real oversight, not symbolic titles. Senior management must be fit and proper, competent and actively involved in the company’s operations. Decision-making processes need to be documented and supported by actual workflows. Applicants who treat governance as a checkbox exercise often face long regulator feedback cycles.</p>  \n  \n<p>Operational resilience is another crucial area. Companies must demonstrate that they have considered incident response, business continuity, third-party dependencies, cybersecurity, data protection, operational risks and ICT vulnerabilities. Regulatory expectations include the ability to detect, manage and report incidents effectively. If a company relies heavily on outsourced technology, it must show that it remains fully responsible for oversight and control. Regulators are particularly cautious about technology providers located outside the EU or in jurisdictions with weak regulatory safeguards.</p>  \n  \n<p>AML and CTF compliance remains one of the toughest components. Companies must design robust onboarding procedures, transaction monitoring tools, risk scoring models, suspicious activity processes, record-keeping methods and continuous review mechanisms. The Travel Rule must be fully integrated into operational systems. Regulators want proof that a company can detect high-risk behavior and respond appropriately. Many applicants underestimate the technical complexity of AML implementation and rely too heavily on third-party KYC tools without building internal procedures that reflect their business model.</p>  \n  \n<p>ICT and cybersecurity requirements are heavily influenced by the Digital Operational Resilience Act. Although MiCA and DORA are separate frameworks, they interact closely. Applicants must show that their systems are secure, resilient and regularly tested. Documentation must cover access controls, encryption, monitoring, incident detection, penetration testing, data protection and disaster recovery. Companies need to provide evidence that their systems are not only designed securely but are maintained securely through ongoing operational processes.</p>  \n  \n<p>Another area that often leads to rejections is the lack of consistency between business plans and internal controls. If a company claims it will offer a large range of activities but provides minimal staffing or insufficient capital, regulators will immediately question the feasibility of the business model. Financial projections must be realistic, risk-aware and aligned with the resources described in the application. Underestimating capital or staffing needs can significantly delay the authorization process.</p>  \n  \n<p>Despite the complexity of authorization, companies that prepare early and treat the process seriously can navigate it successfully. Clear documentation, consistent internal controls, realistic planning and structured governance significantly increase the chances of approval. It is also essential to maintain open communication with the regulator, respond quickly to feedback and update policies when required. Regulators appreciate transparency and professionalism, and companies that demonstrate commitment tend to complete the process more efficiently.</p>  \n  \n<p>Regulatory advisory firms like <a href=\"https://licensium.io/\">Licensium</a>  help companies manage the entire authorization journey. They assist with governance design, AML documentation, ICT frameworks, risk assessment, internal policies, application preparation and communication with regulators. This level of professional support is often decisive, especially for companies without dedicated compliance teams. By organizing the authorization process from the beginning, firms significantly reduce the risk of delays and accelerate the approval timeline.</p>  \n  \n<p>As the regulatory landscape continues to evolve, businesses that invest in compliance and operational maturity gain long-term advantages. MiCA introduces clear rules that reward stability, transparency and responsible management. Companies that adapt proactively will be better positioned to enter the European market, expand their operations and build trust with users and partners. A strong regulatory foundation is not only a legal requirement but also a strategic asset in a competitive global industry.</p>  \n  \n<p>Preparing for MiCA authorization is a complex yet achievable process. With the right structure, documentation and strategic planning, crypto businesses can meet the new standards and establish themselves as credible, compliant and resilient participants in the European market. The companies that take regulation seriously today will become the leaders of tomorrow’s digital asset ecosystem.</p>",
      "summary": "The European regulatory environment for crypto is evolving rapidly, and MiCA is the most significant development shaping how businesses operate across the European Union. Companies launching exchanges, custody solutions, OTC desks, tokenization platforms or payment systems must now comply with a unified regulatory structure that is far more demanding than the previous patchwork of national rules. Preparing for authorization requires a strategic, methodical and well-organized approach that goes f",
      "publishedAt": "2025-12-02T12:15:53.000Z",
      "author": "Aleksandr Rozental",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 17.868190552982224,
      "ingestedAt": "2025-12-02T14:44:03.185Z",
      "tags": [
        "code_review",
        "documentation",
        "ide",
        "testing",
        "observability",
        "governance",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099ee953",
      "title": "รัน Typhoon 2.5 บน Colab ฟรี: จาก 30B (ไม่ไหว) สู่ 4B \"Sweet Spot\"",
      "url": "https://dev.to/ubinix_warun/ran-typhoon-25-bn-colab-frii-cchaak-30b-aimaihw-suu-4b-sweet-spot-2jdp",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftcser2z7dvqfaxrryx18.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>สวัสดีครับ! นี่คือบทความสรุปการเดินทางของเราในการพยายามรันโมเดล Typhoon 2.5 (ทั้ง 30B และ 4B) บน Google Colab Free Tier ครับ เราได้ลองผิดลองถูกมาหลายวิธี และนี่คือบทเรียนทั้งหมดที่เราพบ ตั้งแต่ความล้มเหลวไปจนถึง Config ที่ดีที่สุดครับ</p>  \n  \n<p><a href=\"https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb0e8bfbf2b81&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fubinix-warun&amp;collection=UBINIX+WARUN&amp;collectionId=49c3a90b9f1f&amp;newsletterV3=UBINIX+WARUN&amp;newsletterV3Id=b0e8bfbf2b81\">อ่านฉบับเต็ม...</a></p>  \n  \n<h2>  \n    \n    \n  บทเรียนที่ 1: Typhoon 30B \"ใหญ่เกินไป\" สำหรับ Colab ฟรี  \n</h2>  \n  \n<p>ความฝันของเราคือการรันโมเดลเรือธง 30B แต่ความจริงก็คือ:</p>  \n  \n<ul>  \n<li><p>Ollama (บน T4 GPU): รอดแบบเฉียดฉิว! ใช้ VRAM 14.3 GB จาก 15 GB ที่มีให้ นี่คือการรัน \"จนเต็มขีดจำกัด\" ของ T4</p></li>  \n<li>  \n<p>Transformers (เขียนโค้ด Python): ล่มโดยสิ้นเชิง!</p>  \n  \n<ul>  \n<li>บน T4 (GPU): เจอปัญหา \"Disk is almost full\" (Disk 112GB ไม่พอโหลดโมเดล 60-70GB)</li>  \n<li>บน TPU (v5e-1): แก้ปัญหา Disk ได้ แต่เจอ \"Time Out\" (แค่โหลดโมเดลก็ 40 นาที จนค้าง)</li>  \n</ul>  \n  \n  \n</li>  \n  \n</ul>  \n  \n<p><a href=\"https://medium.com/super-ai-agents/%E0%B9%80%E0%B8%88%E0%B8%B2%E0%B8%B0%E0%B8%A5%E0%B8%B6%E0%B8%81-typhoon-2-5-part-1-%E0%B8%97%E0%B8%94%E0%B8%A5%E0%B8%AD%E0%B8%87%E0%B8%A3%E0%B8%B1%E0%B8%99-30b-%E0%B8%9A%E0%B8%99-colab-%E0%B8%9F%E0%B8%A3%E0%B8%B5-%E0%B8%A8%E0%B8%B6%E0%B8%81%E0%B8%9B%E0%B8%B0%E0%B8%97%E0%B8%B0-runtime-ollama-02402ff24871\">สรุป Part 1: ถ้าอยากลอง 30B บน Colab ฟรี, Ollama คือทางเดียว (แต่ก็เสี่ยงมาก) ส่วน transformers นั้น \"เป็นไปไม่ได้\"</a></p>  \n  \n<h2>  \n    \n    \n  บทเรียนที่ 2: Typhoon 4B \"คุณภาพ\" คือคำตอบ เมื่อ 30B ไม่รอด เราจึงหันมาที่ \"น้องเล็ก\" 4B แต่คำถามคือ \"จำเป็นต้องใช้ GPU T4 ไหม?\"  \n</h2>  \n  \n<ul>  \n<li>Ollama (บน CPU Runtime): รันได้! ใช้ System RAM ~3.5GB และ CPU 100%</li>  \n</ul>  \n  \n<blockquote>  \n<p>...แต่ (จุดเปลี่ยน): เมื่อทดสอบ เราพบว่า \"คุณภาพคำตอบ (Quality) ดร็อปอย่างชัดเจน\" คำตอบมักจะสั้น, เหตุผลเพี้ยน, หรือไม่ดีเท่าที่ควร</p>  \n</blockquote>  \n  \n<p><a href=\"https://medium.com/super-ai-agents/%E0%B9%80%E0%B8%88%E0%B8%B2%E0%B8%B0%E0%B8%A5%E0%B8%B6%E0%B8%81-typhoon-2-5-part-2-%E0%B8%81%E0%B8%A5%E0%B8%B1%E0%B8%9A%E0%B8%AA%E0%B8%B9%E0%B9%88%E0%B8%9E%E0%B8%B7%E0%B9%89%E0%B8%99%E0%B8%90%E0%B8%B2%E0%B8%99-%E0%B8%A3%E0%B8%B1%E0%B8%99-4b-%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-4-bit-quantization-a6e0bb262cce\">สรุป Part 2: การรันบน CPU ทำได้แค่ \"ให้ติด\" แต่ถ้าคุณต้องการ \"คุณภาพ\" ที่แท้จริง... คุณต้องใช้ T4 GPU และนี่คือวิธีที่เราทำ<br>  \n</a></p>  \n<h2>  \n    \n    \n  3. \"พระเอก\" ของงาน: 4-bit Quantization (โค้ด Transformers)  \n</h2>  \n  \n<p>นี่คือวิธีรัน 4B บน T4 GPU เพื่อให้ได้ \"คุณภาพสูงสุด\" โดยใช้ VRAM น้อยที่สุดครับ</p>  \n  \n<p>Quantization คืออะไร? คือการ \"บีบอัด\" โมเดล (เหมือนบีบไฟล์ RAW เป็น .JPG) เราใช้ 4-bit Quantization เพื่อลดขนาดโมเดลใน VRAM ลง 4 เท่า! (จาก 16-bit)</p>  \n  \n<p>นี่คือโค้ดหลักที่เราใช้:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>import</span> <span>torch</span>  \n<span>from</span> <span>transformers</span> <span>import</span> <span>AutoTokenizer</span><span>,</span> <span>AutoModelForCausalLM</span><span>,</span> <span>BitsAndBytesConfig</span>  \n  \n<span># 1. เลือกโมเดล 4B  \n</span><span>model_id</span> <span>=</span> <span>\"</span><span>scb10x/typhoon2.5-qwen3-4b</span><span>\"</span>  \n  \n<span># 2. นี่คือหัวใจสำคัญ! (เราจะเทียบ NF4 vs FP4)  \n</span><span>quantization_config</span> <span>=</span> <span>BitsAndBytesConfig</span><span>(</span>  \n    <span>load_in_4bit</span><span>=</span><span>True</span><span>,</span>  \n    <span>bnb_4bit_quant_type</span><span>=</span><span>\"</span><span>nf4</span><span>\"</span><span>,</span> <span># หรือ \"fp4\"  \n</span>    <span>bnb_4bit_compute_dtype</span><span>=</span><span>torch</span><span>.</span><span>bfloat16</span>  \n<span>)</span>  \n  \n<span># 3. โหลดโมเดล  \n</span><span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_id</span><span>)</span>  \n<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span><span>.</span><span>from_pretrained</span><span>(</span>  \n    <span>model_id</span><span>,</span>  \n    <span>quantization_config</span><span>=</span><span>quantization_config</span><span>,</span>  \n    <span>torch_dtype</span><span>=</span><span>torch</span><span>.</span><span>bfloat16</span><span>,</span>  \n    <span>device_map</span><span>=</span><span>\"</span><span>auto</span><span>\"</span>  \n<span>)</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>วิธีนี้ ทำให้เราใช้ VRAM ไปเพียง ~4.0 GB (จากการทดลองใน Part 2) และได้คุณภาพคำตอบที่ดีเยี่ยม!</p>  \n  \n<h2>  \n    \n    \n  4. บทเรียนที่ 3: Benchmark 8-bit vs 4-bit... ใครชนะ?  \n</h2>  \n  \n<p>เรารู้ว่า 4-bit ดี แต่แบบไหนดีที่สุด? เราจึงทดสอบ 3 Configs บน T4 GPU (จากผลทดลองจริง):</p>  \n  \n<h2>  \n    \n    \n  🏆 บทสรุป (The Final Verdict): Config ที่ดีที่สุด  \n</h2>  \n  \n<p>4-bit Quantization ชนะ 8-bit ขาดลอย!</p>  \n  \n<p><a href=\"https://medium.com/super-ai-agents/%E0%B9%80%E0%B8%88%E0%B8%B2%E0%B8%B0%E0%B8%A5%E0%B8%B6%E0%B8%81-typhoon-2-5-part-3-benchmark-%E0%B9%80%E0%B8%97%E0%B8%B5%E0%B8%A2%E0%B8%9A-8-bit-vs-4-bit-nf4-vs-fp4-f33e3f61417d\">สรุป Part 3: “Benchmark” เทียบ 8-bit vs 4-bit (NF4 vs FP4)</a></p>  \n  \n<ul>  \n<li>เร็วกว่า 2.5 เท่า! (12 Tok/s vs 4.6 Tok/s)</li>  \n<li>ประหยัด VRAM กว่า 35% (2.71 GB vs 4.19 GB)</li>  \n</ul>  \n  \n<p>ศึกชิงที่ 1: NF4 vs FP4</p>  \n  \n<ul>  \n<li>VRAM: ใช้เท่ากัน (2.71 GB)</li>  \n<li>Speed: FP4 เร็วกว่า NF4 เล็กน้อย (ประมาณ 3-4%)</li>  \n<li>Quality: NF4 (Normal Float) ถูกออกแบบมาเพื่อ \"รักษาคุณภาพ\" ได้ดีกว่า FP4 (Float Point) ที่เน้น \"ความเร็ว\"</li>  \n</ul>  \n  \n<p>คำแนะนำสุดท้าย: สำหรับ Google Colab T4:</p>  \n  \n<blockquote>  \n<p>4-bit NF4 (bnb_4bit_quant_type=\"nf4\") คือ \"จุดสมดุล\" (The Sweet Spot) ที่ดีที่สุด</p>  \n  \n<p>คุณจะได้โมเดลที่ ประหยัด VRAM ที่สุด (2.71 GB), เร็วมาก (11.68 Tok/s), และ รักษาคุณภาพคำตอบ ไว้ได้ใกล้เคียงต้นฉบับที่สุดครับ!</p>  \n</blockquote>",
      "summary": "สวัสดีครับ! นี่คือบทความสรุปการเดินทางของเราในการพยายามรันโมเดล Typhoon 2.5 (ทั้ง 30B และ 4B) บน Google Colab Free Tier ครับ เราได้ลองผิดลองถูกมาหลายวิธี และนี่คือบทเรียนทั้งหมดที่เราพบ ตั้งแต่ความล้มเหลวไปจนถึง Config ที่ดีที่สุดครับ  \n  \nอ่านฉบับเต็ม...  \n  \n  \n    \n    \n  บทเรียนที่ 1: Typhoon 30B \"ใหญ่เกินไป\" สำหรับ Colab ฟรี  \n  \n  \nความฝันของเราคือการรันโมเดลเรือธง 30B แต่ความจริงก็คือ:  \n  \n  \nOllama (บน T4 GPU): รอดแบบเฉียดฉิว! ใช้ VRAM 14.3 GB จาก 15 GB ที่มีให้ นี่คือการรัน \"จนเต็มขีดจ",
      "publishedAt": "2025-12-02T12:12:47.000Z",
      "author": "Warun C. ⚡",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 8.436459277269595,
      "ingestedAt": "2025-12-02T14:44:03.185Z",
      "tags": [
        "code_review",
        "agents",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099ee954",
      "title": "Stop Prompting the Hard Way: How Mellea Gives You Supercharged AI Results",
      "url": "https://dev.to/manikandan/stop-prompting-the-hard-way-how-mellea-gives-you-supercharged-ai-results-2080",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwueyp4a9b0tavjkpe0r2.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><h2>  \n    \n    \n  Introduction  \n</h2>  \n  \n<p>Prompting used to feel like magic. Sometimes the AI gave great results sometimes it didn’t. But modern AI workloads require consistency, structure, validation, and reliability — not guesswork.</p>  \n  \n<p>This is where <code>Mellea</code> steps in. </p>  \n  \n<p>Mellea is changing the way developers craft prompts by introducing structured, modular, and reusable prompt design. Instead of writing plain text prompts, Mellea lets you create smart, dynamic, and interactive prompt components.</p>  \n  \n<blockquote>  \n<p>Mellea transforms ordinary text prompts into well-defined, self-correcting AI functions that produce predictable and error-free outputs.</p>  \n</blockquote>  \n  \n<h2>  \n    \n    \n  What Is Mellea?  \n</h2>  \n  \n<p>Mellea is a Python-based framework that allows you to build <strong>prompt<br>  \nobjects</strong>, reuse them, and execute them like code. This makes prompting more consistent, scalable, and easier to maintain.</p>  \n  \n<p><code>Mellea is a runtime layer</code> that sits between your <code>application</code> and an <code>LLM</code>. It forces the AI to follow structured, rule-based, validated outputs.</p>  \n  \n<p>Think of Mellea as:</p>  \n  \n<ul>  \n<li>A <strong>compiler</strong> for your AI instructions</li>  \n<li>A <strong>validator</strong> that checks if AI output matches your schema</li>  \n<li>A <strong>controller</strong> that retries/corrects the AI when it goes off-track</li>  \n</ul>  \n<h3>  \n    \n    \n  Without Mellea vs. With Mellea  \n</h3>  \n  \n  \n<div>  \n<pre><code>  \n<span>Without</span> <span>Mellea</span><span>:</span>  \n  \n<span>Prompt</span> <span>→</span> <span>AI</span> <span>→</span> <span>(</span><span>Sometimes</span> <span>correct</span><span>,</span> <span>sometimes</span> <span>messy</span><span>)</span>  \n  \n  \n<span>With</span> <span>Mellea</span><span>:</span>  \n  \n<span>Prompt</span> <span>+</span> <span>Schema</span> <span>→</span> <span>Mellea</span> <span>→</span> <span>AI</span> <span>→</span> <span>Validate</span> <span>→</span> <span>Correct</span> <span>→</span> <span>Perfect</span> <span>Output</span>  \n  \n</code></pre>  \n  \n</div>  \n  \n<h2>  \n    \n    \n  Why Prompting Alone Is No Longer Enough  \n</h2>  \n<h3>  \n    \n    \n  ❌ Traditional Prompting Problems  \n</h3>  \n  \n<ul>  \n<li><p>AI may hallucinate</p></li>  \n<li><p>Output may not follow structure</p></li>  \n<li><p>Difficult to embed into production apps</p></li>  \n<li><p>No validation layer</p></li>  \n<li><p>High retry cost</p></li>  \n</ul>  \n<h3>  \n    \n    \n  ✔️ Mellea Solves All of These  \n</h3>  \n  \n<p>Mellea adds <strong>guaranteed structure:</strong></p>  \n  \n<ul>  \n<li><p>Enforces strict JSON schemas</p></li>  \n<li><p>Validates output</p></li>  \n<li><p>Auto-corrects when wrong</p></li>  \n<li><p>Reduces hallucinations</p></li>  \n<li><p>Makes AI outputs predictable</p></li>  \n</ul>  \n  \n<p>This is why developers say:</p>  \n  \n<blockquote>  \n<p>“Mellea makes LLMs behave like functions, not guessing machines.”</p>  \n</blockquote>  \n<h2>  \n    \n    \n  How Mellea Upgrades Prompting: A Visual Diagram  \n</h2>  \n<h3>  \n    \n    \n  Architecture Overview  \n</h3>  \n  \n  \n<div>  \n<pre><code><span>+</span><span>------------------------+</span>  \n<span>|</span>        <span>Your</span> <span>App</span>        <span>|</span>  \n<span>+</span><span>------------------------+</span>  \n            <span>|</span>  \n            <span>v</span>  \n<span>+</span><span>------------------------+</span>  \n<span>|</span>         <span>Mellea</span>         <span>|</span>  \n<span>|</span>  <span>-</span> <span>Function</span> <span>schemas</span>    <span>|</span>  \n<span>|</span>  <span>-</span> <span>Validators</span>          <span>|</span>  \n<span>|</span>  <span>-</span> <span>Auto</span><span>-</span><span>correct</span>        <span>|</span>  \n<span>+</span><span>------------------------+</span>  \n            <span>|</span>  \n            <span>v</span>  \n<span>+</span><span>------------------------+</span>  \n<span>|</span>        <span>LLM</span> <span>Model</span>       <span>|</span>  \n<span>+</span><span>------------------------+</span>  \n  \n</code></pre>  \n  \n</div>  \n  \n<h3>  \n    \n    \n  What Happens Internally?  \n</h3>  \n  \n  \n<div>  \n<pre><code>  \n<span>Input</span> <span>→</span> <span>Parse</span> <span>→</span> <span>Send</span> <span>to</span> <span>LLM</span> <span>→</span> <span>Validate</span> <span>Output</span>  \n         <span>|</span>                     <span>|</span>  \n         <span>|&lt;</span><span>-- retry/fix if needed</span>  \n  \n</code></pre>  \n  \n</div>  \n  \n  \n<blockquote>  \n<p>Mellea behaves like an AI compiler.</p>  \n</blockquote>  \n<h2>  \n    \n    \n  Real Example: Prompting Without vs With Mellea  \n</h2>  \n<h3>  \n    \n    \n  ❌ Without Mellea  \n</h3>  \n  \n  \n<div>  \n<pre><code><span>Prompt</span><span>:</span>  \n<span>\"</span><span>Extract all products from this HTML page.</span><span>\"</span>  \n  \n<span>AI</span> <span>Output</span><span>:</span>  \n<span>Maybe</span> <span>JSON</span><span>,</span> <span>maybe</span> <span>text</span><span>,</span> <span>maybe</span> <span>missing</span> <span>fields</span><span>,</span> <span>maybe</span> <span>hallucinated</span> <span>items</span><span>.</span>  \n</code></pre>  \n  \n</div>  \n  \n<h3>  \n    \n    \n  ✔️ With Mellea  \n</h3>  \n  \n  \n<div>  \n<pre><code><span>Define</span> <span>a</span> <span>function</span>  \n<span>@mellea.function</span>  \n<span>def</span> <span>extract_products</span><span>(</span><span>html</span><span>:</span> <span>str</span><span>)</span> <span>-&gt;</span> <span>List</span><span>[</span><span>Product</span><span>]:</span>  \n    <span>\"\"\"</span><span>Extract product list from HTML.</span><span>\"\"\"</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n<p>Call it<br>  \n</p>  \n  \n<div>  \n<pre><code><span>products</span> <span>=</span> <span>extract_products</span><span>(</span><span>html</span><span>)</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<h3>  \n    \n    \n  Guaranteed Output  \n</h3>  \n  \n  \n  \n<div>  \n<pre><code><span>[</span><span>  \n  </span><span>{</span><span>\"name\"</span><span>:</span><span> </span><span>\"Laptop\"</span><span>,</span><span> </span><span>\"price\"</span><span>:</span><span> </span><span>899</span><span>,</span><span> </span><span>\"availability\"</span><span>:</span><span> </span><span>true</span><span>},</span><span>  \n  </span><span>{</span><span>\"name\"</span><span>:</span><span> </span><span>\"Headphones\"</span><span>,</span><span> </span><span>\"price\"</span><span>:</span><span> </span><span>129</span><span>,</span><span> </span><span>\"availability\"</span><span>:</span><span> </span><span>false</span><span>}</span><span>  \n</span><span>]</span><span>  \n</span></code></pre>  \n  \n</div>  \n  \n  \n  \n<blockquote>  \n<p>Structured. Clean. Valid. No hallucinations. No formatting issues.</p>  \n</blockquote>  \n  \n<h2>  \n    \n    \n  Why Do We Need This Tool?  \n</h2>  \n  \n<ul>  \n<li>  Traditional prompts are unstructured and hard to debug.</li>  \n<li>  Teams struggle to reuse prompt logic across projects.</li>  \n<li>  Scaling prompts for large applications is tedious and error-prone.</li>  \n</ul>  \n  \n<p>Mellea solves all of this with a programmable prompting interface.</p>  \n  \n<h2>  \n    \n    \n  Example: Simple Prompt  \n</h2>  \n  \n  \n  \n<div>  \n<pre><code><span>from</span> <span>mellea.core</span> <span>import</span> <span>Prompt</span>  \n  \n<span>p</span> <span>=</span> <span>Prompt</span><span>(</span><span>\"</span><span>Write a poem about {topic}.</span><span>\"</span><span>)</span>  \n<span>print</span><span>(</span><span>p</span><span>(</span><span>topic</span><span>=</span><span>\"</span><span>snow</span><span>\"</span><span>))</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<h2>  \n    \n    \n  Example: Operator System  \n</h2>  \n  \n  \n  \n<div>  \n<pre><code><span>from</span> <span>mellea.core</span> <span>import</span> <span>Operator</span>  \n  \n<span>class</span> <span>Adder</span><span>(</span><span>Operator</span><span>):</span>  \n    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>y</span><span>):</span>  \n        <span>return</span> <span>x</span> <span>+</span> <span>y</span>  \n  \n<span>adder</span> <span>=</span> <span>Adder</span><span>()</span>  \n<span>print</span><span>(</span><span>adder</span><span>(</span><span>10</span><span>,</span> <span>20</span><span>))</span>  <span># Output: 30  \n</span></code></pre>  \n  \n</div>  \n  \n  \n  \n<h2>  \n    \n    \n  Real Use Case Example  \n</h2>  \n  \n  \n  \n<div>  \n<pre><code><span>from</span> <span>mellea.core</span> <span>import</span> <span>Prompt</span><span>,</span> <span>Operator</span>  \n  \n<span>class</span> <span>Summarizer</span><span>(</span><span>Operator</span><span>):</span>  \n    <span>prompt</span> <span>=</span> <span>Prompt</span><span>(</span><span>\"</span><span>Summarize the following text:  \n{text}</span><span>\"</span><span>)</span>  \n  \n<span>summary</span> <span>=</span> <span>Summarizer</span><span>()</span>  \n<span>print</span><span>(</span><span>summary</span><span>(</span><span>text</span><span>=</span><span>\"</span><span>AI is transforming the world...</span><span>\"</span><span>))</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<h2>  \n    \n    \n  Why This Tool Is Really Needed  \n</h2>  \n  \n<p><strong>1. AI is too unpredictable for production apps</strong></p>  \n  \n<p>Companies need reliability, not creativity.</p>  \n  \n<p><strong>2. Schema validation is mandatory in real workflows</strong></p>  \n  \n<p>APIs, pipelines, agents → All require structured outputs.</p>  \n  \n<p><strong>3. LLMs hallucinate and misformat</strong></p>  \n  \n<p>Mellea guards against this.</p>  \n  \n<p><strong>4. Prompts alone do not scale</strong></p>  \n  \n<p>Mellea gives a programming paradigm, not instructions.</p>  \n  \n<h2>  \n    \n    \n  What Is the Future of Mellea?  \n</h2>  \n  \n<p>Mellea will shape the future of AI development in multiple ways:</p>  \n  \n<p><strong>1. Turning AI into composable functions</strong></p>  \n  \n<p>Every AI model becomes a strict function block—like software components.</p>  \n  \n<p><strong>2. More agent frameworks will depend on Mellea-like logic</strong></p>  \n  \n<p>Agents need deterministic outputs to perform actions.</p>  \n  \n<p><strong>3. AI pipelines will become fully typed</strong></p>  \n  \n<p>Just like TypeScript changed JS, Mellea-style validation will change LLM development.</p>  \n  \n<p><strong>4. Enterprise adoption will explode</strong></p>  \n  \n<p>Compliance + Reliability = Must-have for companies</p>  \n  \n<p><strong>5. Integrated into major AI tools &amp; platforms</strong></p>  \n  \n<p>Mellea-like layers will become standard in LLM SDKs.</p>  \n  \n<h2>  \n    \n    \n  How Mellea Impacts Our Daily Routine  \n</h2>  \n  \n<p><strong>1. Faster development</strong></p>  \n  \n<p>No more debugging AI output.</p>  \n  \n<p><strong>2. Cleaner responses</strong></p>  \n  \n<p>Every prompt becomes predictable.</p>  \n  \n<p><strong>3. Better safety</strong></p>  \n  \n<p>Schemas stop AI from generating harmful or irrelevant content.</p>  \n  \n<p><strong>4. Low cognitive load</strong></p>  \n  \n<p>Developers stop writing long prompts; they define functions instead.</p>  \n  \n<p><strong>5. More reliable AI systems</strong></p>  \n  \n<p>Great for automations, bots, monitoring, summarization, extraction, and more.</p>  \n  \n<h2>  \n    \n    \n  Final Thoughts  \n</h2>  \n  \n<p>Mellea is not <strong>“just another AI tool.”</strong> It is a <strong>revolution in prompting</strong>.</p>  \n  \n<p>It transforms:</p>  \n  \n<p>❌ Messy, unpredictable prompts -&gt; ✔️ Structured, safe, validated AI functions.</p>  \n  \n<p><strong>Mellea is the future.</strong></p>  \n  \n<h2>  \n    \n    \n  🔗 References  \n</h2>  \n  \n<ul>  \n<li>  Mellea GitHub: <a href=\"https://github.com/generative-computing/mellea\">https://github.com/generative-computing/mellea</a>  \n</li>  \n<li>  Tutorial:  \n<a href=\"https://github.com/generative-computing/mellea/blob/main/docs/tutorial.md\">https://github.com/generative-computing/mellea/blob/main/docs/tutorial.md</a>  \n</li>  \n</ul>",
      "summary": "  \n    \n    \n  Introduction  \n  \n  \nPrompting used to feel like magic. Sometimes the AI gave great results sometimes it didn’t. But modern AI workloads require consistency, structure, validation, and reliability — not guesswork.  \n  \nThis is where Mellea steps in.   \n  \nMellea is changing the way developers craft prompts by introducing structured, modular, and reusable prompt design. Instead of writing plain text prompts, Mellea lets you create smart, dynamic, and interactive prompt components. ",
      "publishedAt": "2025-12-02T12:10:47.000Z",
      "author": "Manikandan Mariappan",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 14.390179334429186,
      "ingestedAt": "2025-12-02T14:44:03.185Z",
      "tags": [
        "code_review",
        "documentation",
        "agents",
        "observability",
        "governance",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099ee955",
      "title": "Snapshots, Brooms, and Arch Linux Chaos Control",
      "url": "https://dev.to/akshay_gupta/snapshots-brooms-and-arch-linux-chaos-control-hie",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frpr1si1ux5whibfov0q7.jpeg\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>Arch gives you the steering wheel <em>and</em> the broom. On my Arch setup, Limine boots a BTRFS root while Snapper snaps a before/after every time I poke <code>pacman</code>. It's gorgeous, rollbacks for days, but the snapshots stack up faster than RGB stickers unless I sweep them out. So I wrote this simple <code>cleanup.sh</code>, my tiny digital janitor.</p>  \n  \n<blockquote>  \n<p>\"I use Arch btw.\"</p>  \n</blockquote>  \n  \n<h2>  \n    \n    \n  Why I even bother  \n</h2>  \n  \n<ul>  \n<li>Pacman + Snapper hooks = instant safety net. If a wile build breaks stuff, I just reboot, pick the previous snapshot in Limine, and carry on.</li>  \n<li>I settled on <a href=\"https://cachyos.org/\">CachyOS</a> and <a href=\"https://hydeproject.pages.dev/\">HyDE</a> on top of it, after trying and experimenting with multiple setups. Now CachyOS loves performance tweaks, which means I love experimenting. Snapshots let me go full mad scientist without sweating the fallout.</li>  \n<li>Doing the cleanup myself keeps that classic Arch vibe: I decide what stays, what goes, and when my SSD gets to breathe.</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  How my cleanup script keeps me sane  \n</h2>  \n  \n<p>You can keep the below script anywhere on your system, and yes, it wants root (<code>sudo ./cleanup.sh [KEEP]</code>). Default keep count is 2, but bump it to whatever number keeps you cozy. The flow:</p>  \n  \n<ul>  \n<li>  \n<strong>Are we legit?</strong>: Bails if you're not root or if <code>btrfs</code> tools are missing.</li>  \n<li>  \n<strong>Roll call</strong>: Lists every <code>/.snapshots/&lt;id&gt;/snapshot</code>, nicely sorted so we know what's oldest.</li>  \n<li>  \n<strong>Pick favourites</strong>: Shows what stays, what goes, and then asks \"you sure?\" so you don't rage-delete your lifeline.</li>  \n<li>  \n<strong>Yeet + sync</strong>: Deletes the dusty stuff, cleans empty directories, and syncs the filesystem so everything feels fresh.</li>  \n</ul>  \n  \n<p>Slide the whole script into your toolbox:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>#!/usr/bin/env bash</span>  \n<span>set</span> <span>-euo</span> pipefail  \n  \n<span># How many latest snapshots to keep (default: 2)</span>  \n<span>KEEP</span><span>=</span><span>\"</span><span>${</span><span>1</span><span>:-</span><span>2</span><span>}</span><span>\"</span>  \n  \n<span>if</span> <span>[[</span> <span>\"</span><span>$EUID</span><span>\"</span> <span>-ne</span> 0 <span>]]</span><span>;</span> <span>then  \n  </span><span>echo</span> <span>\"Please run this script as root:\"</span>  \n  <span>echo</span> <span>\"  sudo </span><span>$0</span><span> [KEEP]\"</span>  \n  <span>exit </span>1  \n<span>fi  \n  \nif</span> <span>!</span> <span>command</span> <span>-v</span> btrfs &amp;&gt;/dev/null<span>;</span> <span>then  \n  </span><span>echo</span> <span>\"Error: 'btrfs' command not found. Is btrfs-progs installed?\"</span>  \n  <span>exit </span>1  \n<span>fi  \n  \n</span><span>echo</span> <span>\"Scanning for snapshots under /.snapshots ...\"</span>  \n<span>mapfile</span> <span>-t</span> SNAPSHOTS &lt; &lt;<span>(</span>  \n  btrfs subvolume list / <span>\\</span>  \n    | <span>awk</span> <span>'$9 ~ /^\\.snapshots\\/[0-9]+\\/snapshot$/ {print $9}'</span> <span>\\</span>  \n    | <span>sort</span> <span>-t</span><span>'/'</span> <span>-k2</span>,2n  \n<span>)</span>  \n  \n<span>TOTAL</span><span>=</span><span>${#</span><span>SNAPSHOTS</span><span>[@]</span><span>}</span>  \n  \n<span>if</span> <span>((</span> TOTAL <span>==</span> 0 <span>))</span><span>;</span> <span>then  \n  </span><span>echo</span> <span>\"No snapshots found under /.snapshots.\"</span>  \n  <span>exit </span>0  \n<span>fi  \n  \n</span><span>echo</span> <span>\"Found </span><span>$TOTAL</span><span> snapshot subvolumes:\"</span>  \n<span>printf</span> <span>'  %s\\n'</span> <span>\"</span><span>${</span><span>SNAPSHOTS</span><span>[@]</span><span>}</span><span>\"</span>  \n<span>echo  \n  \n</span><span>if</span> <span>((</span> TOTAL &lt;<span>=</span> KEEP <span>))</span><span>;</span> <span>then  \n  </span><span>echo</span> <span>\"Total snapshots (</span><span>$TOTAL</span><span>) &lt;= KEEP (</span><span>$KEEP</span><span>). Nothing to delete.\"</span>  \n  <span>exit </span>0  \n<span>fi</span>  \n  \n<span># Old ones to delete = everything except the last KEEP entries</span>  \n<span>DELETE_COUNT</span><span>=</span><span>$((</span> TOTAL <span>-</span> KEEP <span>))</span>  \n<span>TO_DELETE</span><span>=(</span> <span>\"</span><span>${</span><span>SNAPSHOTS</span><span>[@]</span>:0:DELETE_COUNT<span>}</span><span>\"</span> <span>)</span>  \n<span>TO_KEEP</span><span>=(</span> <span>\"</span><span>${</span><span>SNAPSHOTS</span><span>[@]</span>:DELETE_COUNT<span>}</span><span>\"</span> <span>)</span>  \n  \n<span>echo</span> <span>\"Will KEEP the latest </span><span>$KEEP</span><span> snapshot(s):\"</span>  \n<span>printf</span> <span>'  %s\\n'</span> <span>\"</span><span>${</span><span>TO_KEEP</span><span>[@]</span><span>}</span><span>\"</span>  \n<span>echo  \necho</span> <span>\"Will DELETE </span><span>$DELETE_COUNT</span><span> older snapshot(s):\"</span>  \n<span>printf</span> <span>'  %s\\n'</span> <span>\"</span><span>${</span><span>TO_DELETE</span><span>[@]</span><span>}</span><span>\"</span>  \n<span>echo  \n  \nread</span> <span>-rp</span> <span>\"Proceed with deletion? [y/N] \"</span> ans  \n<span>case</span> <span>\"</span><span>$ans</span><span>\"</span> <span>in  \n  </span>y|Y<span>)</span> <span>;;</span>  \n  <span>*</span><span>)</span> <span>echo</span> <span>\"Aborted.\"</span><span>;</span> <span>exit </span>0 <span>;;</span>  \n<span>esac</span>  \n  \n<span>for </span>relpath <span>in</span> <span>\"</span><span>${</span><span>TO_DELETE</span><span>[@]</span><span>}</span><span>\"</span><span>;</span> <span>do  \n  </span><span>fullpath</span><span>=</span><span>\"/</span><span>$relpath</span><span>\"</span>  \n  <span>echo</span> <span>\"Deleting subvolume: </span><span>$fullpath</span><span>\"</span>  \n  btrfs subvolume delete <span>\"</span><span>$fullpath</span><span>\"</span>  \n  \n  <span># Try to remove the parent directory (/.snapshots/&lt;id&gt;) if it is empty</span>  \n  <span>parent</span><span>=</span><span>\"/</span><span>$(</span><span>dirname</span> <span>\"</span><span>$relpath</span><span>\"</span><span>)</span><span>\"</span>  \n  <span>if </span><span>rmdir</span> <span>\"</span><span>$parent</span><span>\"</span> 2&gt;/dev/null<span>;</span> <span>then  \n    </span><span>echo</span> <span>\"Removed empty directory: </span><span>$parent</span><span>\"</span>  \n  <span>fi  \ndone  \n  \n</span><span>echo</span> <span>\"Syncing filesystem...\"</span>  \nbtrfs filesystem <span>sync</span> /  \n  \n<span>echo</span> <span>\"Cleanup complete.\"</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<h2>  \n    \n    \n  Why this feels 100% Arch  \n</h2>  \n  \n<ul>  \n<li>I'm the one dialling in how many snapshots survive, not some mystery cron job.</li>  \n<li>Limine keeps every snapshot bootable but leaves the housekeeping to me, which is exactly how I like it.</li>  \n<li>No silent \"optimization\" services, just a bash script, a terminal prompt, and the knowledge my restore points are ones I actually care about.</li>  \n</ul>  \n  \n<p>Next time you finish spicy upgrade on your Arch system, kick back for a second and run <code>cleanup.sh</code>. Five seconds of sweeping, and boom: your Arch universe stays tidy, intentional, and totally yours.</p>",
      "summary": "Arch gives you the steering wheel and the broom. On my Arch setup, Limine boots a BTRFS root while Snapper snaps a before/after every time I poke pacman. It's gorgeous, rollbacks for days, but the snapshots stack up faster than RGB stickers unless I sweep them out. So I wrote this simple cleanup.sh, my tiny digital janitor.  \n  \n  \n\"I use Arch btw.\"  \n  \n  \n  \n    \n    \n  Why I even bother  \n  \n  \n  \nPacman + Snapper hooks = instant safety net. If a wile build breaks stuff, I just reboot, pick t",
      "publishedAt": "2025-12-02T12:07:10.000Z",
      "author": "Akshay Gupta",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "general",
      "score": 6.945736957098553,
      "ingestedAt": "2025-12-02T14:44:03.186Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099ee956",
      "title": "Mock Elements: The Unsung Heroes of UI Design",
      "url": "https://dev.to/zopdev/mock-elements-the-unsung-heroes-of-ui-design-ocd",
      "content": "<p>When crafting user interfaces, designers and developers often need to present visuals, forms, and scripts before real data is available. Mock elements, placeholders and fictional samples, make this possible with both clarity and tradition. Below are some of the most famous placeholders and their backstories.</p> \n \n<h2> \n   \n   \n  The Role of Mock Elements in UI Design \n</h2> \n \n<p>Mock elements act as stand-ins for actual content, helping designers and clients focus on visual structure, usability, and layout, undistracted by real-world data or identities. They keep prototypes relevant, flexible, and safe for sharing and testing while enabling early feedback and iteration.</p> \n \n \n \n \n<h2> \n   \n   \n  Lorem Ipsum \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F1tpcwp3o79ahuesfcc1e.jpeg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F1tpcwp3o79ahuesfcc1e.jpeg\" alt=\"Picture depicting Lorem Ipsum\" width=\"605\" height=\"958\"></a></p> \n \n<p><strong>History:</strong> The “lorem ipsum” text originated from a scrambled section of Cicero’s 1st-century Latin treatise “de finibus bonorum et malorum.” Letraset further popularized it in the 1960s for type samples, and desktop publishing in the 1980s cemented its role.<br><br> \n<strong>Significance &amp; Usage:</strong> Designers use lorem ipsum to fill text fields and paragraphs in UI mockups, allowing everyone to focus strictly on layout and typographic choices instead of content. Its meaningless Latin roots prevent storyline distraction—an effect called “greeking”—and help judge graphical hierarchy.</p> \n \n \n \n \n<h2> \n   \n   \n  Alice and Bob \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Feyzo7wyb36egild22upc.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Feyzo7wyb36egild22upc.png\" alt=\"Picture depicting Alice &amp; Bob\" width=\"800\" height=\"450\"></a></p> \n \n<p><strong>History:</strong> “Alice and Bob” became canonical in cryptography with the seminal 1978 RSA paper. Previously, researchers used impersonal A and B, but the friendlier names made technical papers more accessible and memorable.<br><br> \n<strong>Significance &amp; Usage:</strong> In UI scenarios, Alice and Bob typically represent communicating users, especially in chat apps, email forms, and privacy demos. The names are proxies for any generic pair, lending emotional resonance and clarity without revealing real identities.</p> \n \n \n \n \n<h2> \n   \n   \n  John Doe \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9olszuzlla5ng6huaf5x.jpeg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9olszuzlla5ng6huaf5x.jpeg\" alt=\"Picture depicting John Doe\" width=\"800\" height=\"466\"></a></p> \n \n<p><strong>History:</strong> “John Doe” traces back to English common law and the 14th century, where it was used in fictional legal actions concerning land ownership. Over time, it became the go-to name for unknown, average, or anonymous male persons.<br><br> \n<strong>Significance &amp; Usage:</strong> In web forms, legal demos, and healthcare mock interfaces, “John Doe” identifies hypothetical users without associating with actual people. It’s also vital in survey prototypes, error screens, and contact lists where anonymity is essential.</p> \n \n \n \n \n<h2> \n   \n   \n  Acme Corp \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fur74bh19im1nrnmdf4q2.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fur74bh19im1nrnmdf4q2.png\" alt=\"Picture depicting Acme Corp\" width=\"200\" height=\"200\"></a></p> \n \n<p><strong>History:</strong> While some real companies bear the name, “Acme Corporation” gained fame as a fictional firm supplying comedic gadgets to Wile E. Coyote in Looney Tunes. Over decades, it’s become a generic brand for UI and legal mockups.<br><br> \n<strong>Significance &amp; Usage:</strong> In interface design, Acme Corp is the archetypal placeholder company for demo dashboards, example invoices, and onboarding flows. It’s safely neutral and universally understood as a stand-in, avoiding legal risk or confusion.</p> \n \n \n \n \n<h2> \n   \n   \n  123 Main Street, Anytown, USA \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2ek930b9z03cq2hflmlq.jpeg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2ek930b9z03cq2hflmlq.jpeg\" alt=\"Picture depicting 123 Main Street, Anytown, USA\" width=\"800\" height=\"311\"></a></p> \n \n<p><strong>History:</strong> By convention, “123 Main Street, Anytown, USA” is the default fake address for software demos, documentation, and forms. Its universal, harmless wording means everyone recognizes it as not real, helping avoid data privacy issues.<br><br> \n<strong>Significance &amp; Usage:</strong> Sample address fields, test registrations, and validation scripts often use this structure to illustrate required formats or fill space during development.</p> \n \n \n \n \n<h2> \n   \n   \n  555-1234 \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fezz9oxxklmagt6wvg76p.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fezz9oxxklmagt6wvg76p.png\" alt=\"Picture depicting 555-1234\" width=\"800\" height=\"448\"></a></p> \n \n<p><strong>History:</strong> The “555” prefix for phone numbers got standardized in the USA in the 1950s mostly for directory assistance, but was popularized as a movie/TV placeholder since real numbers could provoke prank calling. By the 1990s, the North American Numbering Plan designated 555-x prefixes for fiction, minimizing accidental real-world connections.<br><br> \n<strong>Significance &amp; Usage:</strong> Designers use 555-1234 for form validation, contact demos, and UI prototypes to guarantee sample numbers are safe and never misroute calls or texts.</p> \n \n \n \n \n<h2> \n   \n   \n  <a href=\"mailto:example@example.com\">example@example.com</a> \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fo3l4zzt7gucrs9fy9njd.jpeg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fo3l4zzt7gucrs9fy9njd.jpeg\" alt=\"Picture depicting exaple@example.com\" width=\"672\" height=\"254\"></a></p> \n \n<p><strong>History:</strong> The “example.com” domain is reserved for illustrative and instructional purpose only, following IETF guidelines. Likewise, “<a href=\"mailto:example@example.com\">example@example.com</a>” emerged as its logical and safe default for email placeholders.<br><br> \n<strong>Significance &amp; Usage:</strong> By using <a href=\"mailto:example@example.com\">example@example.com</a> in mockups and UI forms, developers avoid accidental emails to real users, ensure demo code is generic, and sidestep confidential data leaks.</p> \n \n \n \n \n<h2> \n   \n   \n  Conclusion \n</h2> \n \n<p>Mock elements are fundamental to safe, effective, and rapid UI design. They educate, protect, and streamline the creative process—and their legacy is deeply woven into every modern app, site, and system.</p> \n \n<p>👉 <a href=\"https://zop.dev/zopnight\">Try ZopNight today</a><br><br> \n👉 <a href=\"https://bookings.zop.dev/#/discover-zopdev\">Book a demo</a></p>",
      "summary": "When crafting user interfaces, designers and developers often need to present visuals, forms, and scripts before real data is available. Mock elements, placeholders and fictional samples, make this possible with both clarity and tradition. Below are some of the most famous placeholders and their backstories. \n \n \n   \n   \n  The Role of Mock Elements in UI Design \n \n \nMock elements act as stand-ins for actual content, helping designers and clients focus on visual structure, usability, and layout, ",
      "publishedAt": "2025-12-02T12:05:15.000Z",
      "author": "Rocktim M",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "general",
      "score": 10.417614957248617,
      "ingestedAt": "2025-12-02T14:44:03.186Z",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "ide",
        "testing",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099ee957",
      "title": "Introducing Forgetful - Shared Knowledge and Memory across Agents",
      "url": "https://dev.to/scott_raisbeck_24ea5fbc1e/introducing-forgetful-shared-knowledge-and-memory-across-agents-40o6",
      "content": "<p>I, like many of us over the last year or so, have been on an interesting journey when it comes to software development. </p> \n \n<p>From using ChatGPT to show me how to use sk-learn to build a classifier in 2022, seeing tab completion reach super saiyan level with copilot in 2023 (or was it 2024?! - It all seems like ancient history) on an engineers machine at work, using Cursor to remove the friction of copy/pasting out of chatgpt in 2024, to adopting my own form of the <a href=\"https://github.com/bmad-code-org/BMAD-METHOD?tab=readme-ov-file\">BMAD method</a> to build out some really nice pet projects using a variety of tools like Claude Code, Cursor and Codex here in 2025.</p> \n \n<p>That's my personal experience compressed into a small paragraph, possibly a similar experience had by many on here have already had. The past few years have been a hell of a ride and I am finding I am now using more AI tools than ever. </p> \n \n<p>Over that time I have enjoyed reading many an article on DEV.TO on how many of us have our own approaches to get the best in this new paradigm, coupled with the 'X AI Feature/Product.. is Insane' YouTube videos (seriously guys, please stop). </p> \n \n<p>There have been some real nuggets in this that I have found, I've already mentioned the BMAD Method (which I still use to this day, albeit a bit more lightweight). Managing context window was another, and probably my favourite of all was <a href=\"https://github.com/upstash/context7\">context7</a>.  </p> \n \n<p>Using context7 also made me realise something, the models are more accurate when the information is inside their context window and they are not having to rely on training data.</p> \n \n<p>For me the training data is almost like a necessary evil, I want them to rely on it less and less, please allow me to explain a bit further. </p> \n \n<p>Let's imagine I am looking to implement a Model Context Protocol (MCP) server. If you were looking to do this a few months back, the models straight up didn't know about MCP, so you were forced to ask it to use context7 and a web search to bring back the necessary data. </p> \n \n<p>Even today, with the models with the more recent training data, things are developing so fast in MCP, what it is likely to surface could be out of date and is more than likely not the optimal implementation pattern, so using Context7 I was able to be more confident in the approaches the Agent would suggest, especially when they could cite the sources and I could go validate them myself.  </p> \n \n<p>Like any other developer, once you solve a problem, you like to keep that pattern for other projects and over time your toolbox gets bigger and better. It should be no different with AIs, in fact, if anything it is even more important to ensure that the AIs have your taste, preferences and best practices in mind whenever they are implementing on your behalf. </p> \n \n<p>Simultaneously, I was working on my own AI agents, I've been developing agents for about six months now. As a learning exercise mostly, but I found it to actually be really fun. </p> \n \n<p>The first thing that anyone engineering an agent will realise is that you need memory. Even if it is just reinjecting the conversation history back to the agent so you can have continuity between requests. </p> \n \n<p>As you work on these you start to envisage better systems for memory. You often start to architect different types of memory, short term (a simple version being what I described in the previous paragraph), long term memory - for example some kind of automated retrieval of relevant facts/conversations that had been during previous interactions. </p> \n \n<p>There's also the breakdown of episodic (temporal and spatial context - \"I had a meeting last week about the new payroll implementation\"), semantic (facts and concepts - \"payroll is a term to describe the business process of paying employees\"), procedural (motor skills, and how to knowledge - \"to put up a shelf I first..\" &lt;- I am still to learn this one).</p> \n \n<p>Ultimately the answer usually ends up with some kind of persistence layer, a search mechanism and some service to pull it all together (maybe even some sub-agent specifically to manage retrieval and storage of memories) at inference time.</p> \n \n<p>So with this in mind I went away and vibe coded up an MCP server for agentic systems to store memories in. I did it in an evening and spent the rest of the weekend dogfooding it on systems like Claude Code, Claude Desktop, Codex and Cursor. I had built this tool for my own agents that I was building, but I actually found it really useful in my coding agents. I quickly set about encoding the code for all my projects into memories, getting encoding agents to attach documents and code snippets to sit alongside memories so that an agent querying the knowledge base could get an idea of a particular pattern, or the way something worked, just from what was in the knowledge base and dig into the code if needed. </p> \n \n<p>In this sense, the MCP server I had built became very much like my own little mini context7, well actually a bit more than that really. I could ask it to look at the code in those repos of course, but it would still lack information about design decisions, information for how we resolved an issue, preferred patterns and when to use them. It just felt like I was working in the same session across multiple projects/agents, it was really refreshing. </p> \n \n<p>I had hosted this remotely and managed to connect it to Claude Mobile via <a href=\"https://dev.to/scott_raisbeck_24ea5fbc1e/i-spent-a-week-building-oauth-plumbing-that-shouldnt-exist-1h34\">Dynamic Client Registration</a>, </p> \n \n<p>So now when I was out having a walk or something and I had an idea around something related to one of my projects, Claude had semantic understanding of it. It meant I could have meaningful conversations about it without it having to scan through code. </p> \n \n<p>I could even make decisions with Claude while out on that walk, get it to record a memory (and an implementation plan document) and then once I was back home just tell Claude Code to go ahead and implement. The same applied to using other agentic coding applications, such as Cursor, Copilot and Codex. </p> \n \n<p>I found myself logging every design decision in the memory, and then just having built in commands to fetch context for when I started out a new session. This worked across Claude Code, Codex, OpenCode and Copilot. Which has been my arsenal of AI coding tools for the past few months (could I get by with just one? Yes, but those YouTube videos I mentioned earlier.. they're unfortunately really effective - so please stop guys, I have kids to feed). </p> \n \n<p>Anyhow, it didn't take long for me to realise that I needed to build a proper version of this as I was depending on it more and more in my daily work. While it all worked fine and felt pretty robust, I didn't like the way Claude had implemented it, maybe I shouldn't care but I do. In order to understand something, I need to build it and all that good stuff, I believe dr Feinman said something along those lines, I am certainly no Richard Feinman but I can still appreciate that sentiment. </p> \n \n<p>So I set about building out a new version of it, gave it a name (Forgetful) and decided I'd make it open source to see if it would be useful for others - be it for people using coding agents (which was a use case I had found myself accidently having) to AI engineers build agents themselves. </p> \n \n<p>I did all this before checking if anyone else had built one by the way, there are several out there now, some paid and some for free. I would encourage you to check them out and see what works for you. I do see this as the next paradigm in AI, cross agent memory solutions are going to be key, especially as the AI ecosystem seems to open up more and more for third party apps on some of the big AI labs systems and I cannot tell you how much this has helped my own use of coding agents. I am working on something right now for my day job that I plan on showcasing, but that is for another post (as it's not finished) and Forgetful sits at the heart of it.</p> \n \n<p>It's not some 'I 10xd my AI workflow' hack, I don't know what kind of gains this has given me, I don't really have a way to measure it. </p> \n \n<p>I just feel more comfortable switching between projects and agents now and perhaps more importantly getting the agents more familiar with the patterns you use and  reducing the need to have the same conversation with AI's to tackle the same problem elsewhere. </p> \n \n<h2> \n   \n   \n  So What Did I Actually Build? \n</h2> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5u0mcmc5s3p5og7nz9kw.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5u0mcmc5s3p5og7nz9kw.png\" alt=\"Architecture Diagram\" width=\"800\" height=\"416\"></a></p> \n \n<p>When I sat down to build Forgetful properly, I had to make some decisions about how memories should be structured. This is where I got opinionated.</p> \n \n<p>Most memory systems are essentially vector dumps — throw everything in, embed it, retrieve by cosine similarity, hope for the best. It works, but it's messy. You end up with overlapping chunks, no clear boundaries between concepts, and retrieval that's good enough but never precise, reranking helps to a degree, but I see this as something other than just a store. </p> \n \n<p>I wanted something more like how I use Obsidian. Atomic notes. One concept per note. Links between related ideas. A graph that emerges from the connections rather than being imposed from above. I had been using Obsidian to follow the <a href=\"https://en.wikipedia.org/wiki/Zettelkasten\">Zettelkasten principle</a> — a note-taking method where each note captures exactly one idea, is self-contained enough to understand on its own, and links explicitly to related notes.</p> \n \n<p>I had stumbled across <a href=\"https://www.youtube.com/watch?v=nSh2BYJ29kY&amp;t=15s\">A video of someone implementing semantic encoding of their own Obisidan Notes</a> and then things started to come together. I did my usually back and forth with Claude when out for one of my walks, it threw up some papers around where similar concepts had already been tried: The <a href=\"https://arxiv.org/abs/2502.12110\">A-Mem paper</a> on agentic memory systems found that structured, self-organising memory significantly improves retrieval precision. </p> \n \n<p>In Forgetful, every memory must have (these are configurable as environment variables):</p> \n \n<ul> \n<li>A clear title (forced brevity — 200 char limit)</li> \n<li>Content covering one concept (~300-400 words max)</li> \n<li>Context around what the agent was doing when it created the memory</li> \n<li>Keywords and tags for additional retrieval paths</li> \n</ul> \n \n<p>This might seem restrictive, but that's the point. When an agent goes to store something, it has to think about what the atomic unit of knowledge actually is. No more dumping entire conversations or documents into memory and hoping retrieval figures it out later.</p> \n \n<h2> \n   \n   \n  Auto-Linking \n</h2> \n \n<p>Here's where it gets interesting. When you create a memory, Forgetful doesn't just store it — it finds its place in the graph. Now I would stipulate this is also configurable, I think the best practice is to have an agent dedicated to memory management, who takes in raw input and decides what memories are worth keeping, and how they should fit inside the knowledge base, and whether existing memories need updating or need to be made obsolete as a result of the new interactions. This however is not something I have built yet for my own development memory management and indeed might not be something others want to build. So as a starting point I added automated memory linking, and so far it has worked just fine. </p> \n \n<p>The process:</p> \n \n<ol> \n<li>Generate an embedding for the new memory </li> \n<li>Search for semantically similar existing memories</li> \n<li>Any memories above a 0.7 similarity threshold and cross encoder ranked get automatically linked</li> \n<li>These links are bidirectional — the graph builds itself</li> \n</ol> \n \n<p>So if I store a memory about \"choosing Stripe over PayPal for payment processing\" and I already have memories about \"PCI compliance requirements\" and \"subscription billing architecture\", Forgetful will automatically connect them. No manual linking required.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F7jte83j54yk3pcpv4kcg.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F7jte83j54yk3pcpv4kcg.png\" alt=\"Memory Auto-Linking\" width=\"593\" height=\"499\"></a></p> \n \n<p>When an agent later queries \"how should I handle payments?\", it doesn't just get the Stripe decision — it gets the linked context about compliance and billing architecture too. One-hop graph traversal is included by default, what I mean by that is, for every memory that is retrieved through search, all linked memories 1-hop away are returned as well. </p> \n \n<p>This is what I mean by \"Obsidian for AI agents\". The same way your Obsidian vault becomes more valuable as connections emerge between notes, your agent's memory becomes more useful as the knowledge graph densifies.</p> \n \n<h2> \n   \n   \n  Why Not Neo4j? \n</h2> \n \n<p>I can already hear some of you asking: \"If you're building a knowledge graph, why not use a graph database?\"</p> \n \n<p>The honest answer is I've never worked with a Graph database, I'm only familiar with the relational database such as PostgreSQL, MySQL, MSSQL etc.<br> \nSo Forgetful stores memories in PostgreSQL (or SQLite for the zero-config local experience) with pgvector for embeddings. The graph relationships are just rows in a links table. So no elegant graph theory, maybe it's something I'll consider in the future. I've architectued Forgetful in a way that I can add adapters quite easily for different types of implementation layers, hence why I can switch between Postgres and SQLite, so adding a Graph Database or even a dedicated Vector Database later down the line wouldn't involve a total re-write. </p> \n \n<p>The relational database appraoch appears to  works fine for my scale. For the access patterns agents actually use (store a memory, find related memories, traverse one hop), a relational model with proper indexing handles it without breaking a sweat. Maybe at massive scale I'd revisit this, but I'd rather ship something useful than architect for problems I don't have yet.</p> \n<h2> \n   \n   \n  Making Retrieval Actually Good \n</h2> \n \n<p>Storing memories is the easy part. Retrieval is where most systems fall down.</p> \n \n<p>A naive approach: embed the query, find the top-k most similar memories by cosine distance, return them. This works but has problems. Embedding similarity is fuzzy — semantically related doesn't always mean actually relevant to what the agent needs right now.</p> \n \n<p>Forgetful uses a multi-stage approach:</p> \n \n<p><strong>Stage 1: Dense retrieval</strong><br> \nEmbed the query, pull back candidate memories using vector similarity. Cast a wide net.</p> \n \n<p><strong>Stage 2: Cross-encoder reranking</strong><br> \nHere's the trick — when an agent searches, it provides not just the query but a <code>query_context</code> explaining <em>why</em> it's searching. \"I'm implementing payment integration\" gives different results than \"I'm debugging a checkout error\" even if both search for \"payments\". The cross-encoder uses this full context to rerank candidates.</p> \n \n<p>The cross-encoder scores each candidate against the full query context, reranks them, and the top results go back to the agent.</p> \n \n<p>Is this overkill? Maybe. But retrieval precision is everything. Returning the wrong context is worse than returning nothing — it confidently misleads the agent.</p> \n<h2> \n   \n   \n  Token Budget Management \n</h2> \n \n<p>Even with great retrieval, you can still overwhelm an agent's context window. Twenty highly relevant memories might be 15,000 tokens — and that's before the agent's actual task.</p> \n \n<p>Forgetful manages this with a configurable token budget (default 8K). Results are prioritised by:</p> \n \n<ol> \n<li>Importance score (9-10 rated memories first)</li> \n<li>Recency (newest within each importance tier)</li> \n</ol> \n \n<p>If the budget fills up, lower-priority memories get truncated. The agent always gets the most critical context without the LLM choking on input length.</p> \n<h2> \n   \n   \n  From Single Machine to Cloud \n</h2> \n \n<p>One thing I wanted to get right: Forgetful should scale with your needs.</p> \n \n<p><strong>Just trying it out?</strong><br> \n</p> \n \n<div> \n<pre><code>uvx forgetful-ai \n</code></pre> \n \n</div> \n \n \n \n<p>That's it. SQLite database, stored in your home directory, stdio transport for MCP. Zero configuration.</p> \n \n<p>The default setup runs completely offline — embeddings are generated locally using FastEmbed with the BAAI/bge-small-en-v1.5 model. No OpenAI API key required, no data leaves your machine. If you want cloud embeddings (Azure OpenAI, Google), you can configure that, but it's entirely optional.</p> \n \n<p><strong>Running it for real?</strong><br> \nDocker Compose with PostgreSQL, HTTP transport, proper authentication. Same codebase, same API, just different deployment.</p> \n \n<p><strong>Multi-device access?</strong><br> \nHost it somewhere with an endpoint, configure your MCP clients to point at it. I run mine on a VPS and connect from Claude Desktop, Claude Mobile, Cursor, and Claude Code — all hitting the same knowledge base.</p> \n \n<p>The progression should feel natural. Start local, go remote when you need to.</p> \n<h2> \n   \n   \n  What Else Is In There? \n</h2> \n \n<p>Memories are the core, but Forgetful has a few other concepts that emerged from real usage:</p> \n \n<p><strong>Entities</strong>: Concrete things — people, organisations, products, infrastructure. These can have relationships to each other (\"Jordan works_for TechFlow\") and link to memories. Useful for building out knowledge about your team, your systems, your clients.</p> \n \n<p><strong>Projects</strong>: Scope for memories. When I'm working on the e-commerce platform, I don't need memories from the trading bot project polluting my context. Project scoping keeps retrieval focused.</p> \n \n<p><strong>Documents</strong>: Sometimes you need more than 400 words. Documents store long-form content, with the expectation that you'll extract atomic memories from them that link back to the parent.</p> \n \n<p><strong>Code Artifacts</strong>: Reusable code snippets attached to memories. The agent can retrieve not just the concept but a working example.</p> \n \n<p>All of these link together. An entity can relate to memories, which belong to projects, which have associated documents and code artifacts. The graph extends beyond just memory-to-memory connections.</p> \n<h2> \n   \n   \n  The Meta-Tools Pattern \n</h2> \n \n<p>One last implementation detail that I'm quite pleased with.</p> \n \n<p>MCP clients see three tools from Forgetful:</p> \n \n<ul> \n<li> \n<code>discover_tools</code> — what's available?</li> \n<li> \n<code>how_to_use</code> — how does a specific tool work?</li> \n<li> \n<code>execute_tool</code> — run a tool with arguments</li> \n</ul> \n \n<p>Behind that facade sit 42 actual tools. The agent discovers what it needs, learns how to use it, then executes. This keeps the tool list in the agent's context minimal while still exposing full functionality.</p> \n \n<p>It's a small thing, but context window discipline matters. Every token spent on tool definitions is a token not available for actual reasoning. The numbers matter here: exposing dozens of tools with full JSON schemas would consume thousands of tokens before the agent even starts working. The three meta-tools keep context overhead minimal while still providing full access to everything Forgetful can do.</p> \n \n \n \n<p>That's Forgetful. An opinionated memory system built on Zettelkasten principles, with auto-linking, proper retrieval, and a deployment model that scales from \"just trying it\" to \"running in production\".</p> \n \n<p>If you want to try it:<br> \n</p> \n \n<div> \n<pre><code>uvx forgetful-ai \n</code></pre> \n \n</div> \n \n \n \n<p>GitHub: <a href=\"https://github.com/ScottRBK/forgetful\">github.com/ScottRBK/forgetful</a></p> \n \n<p>Discord: <a href=\"https://discord.gg/ngaUjKWkFJ\">If you are into building AI Agents or even just like talking about coding agents and AI in general head over to my discord</a></p> \n \n<p>I'd love to hear how you use it, what breaks, and what's missing. I'd imagine that this will be something i continually work on as part of agent development, so having the input of others will be most helpful! </p> \n \n<p>Next post: I'll show what I'm building on top of Forgetful for my day job — a system for semantic understanding of 250+ repositories. But that's for another time.</p>",
      "summary": "I, like many of us over the last year or so, have been on an interesting journey when it comes to software development.  \n \nFrom using ChatGPT to show me how to use sk-learn to build a classifier in 2022, seeing tab completion reach super saiyan level with copilot in 2023 (or was it 2024?! - It all seems like ancient history) on an engineers machine at work, using Cursor to remove the friction of copy/pasting out of chatgpt in 2024, to adopting my own form of the BMAD method to build out some re",
      "publishedAt": "2025-12-02T12:03:17.000Z",
      "author": "Scott Raisbeck",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 16.864969385595774,
      "ingestedAt": "2025-12-02T14:44:03.186Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "governance",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099ee95c",
      "title": "AI Regulation News Today: Global & U.S. Policy Updates",
      "url": "https://dev.to/techdecodedly/ai-regulation-news-today-global-us-policy-updates-3dcf",
      "content": "<p>AI Regulation News Today shows that around the world, AI regulation is rapidly evolving as governments race to set standards for safe and responsible AI development. A 2025 survey notes that “at least 69 countries” (including the EU) have proposed or adopted AI laws and initiatives. The European Union leads with the landmark EU Artificial Intelligence Act (Regulation 2024/1689), adopted July 2024 and entering into force August 2024 (with most provisions effective in 2026). In Asia, China issued its first generative AI rules (“Interim Measures”) for content services, while India, Singapore and others are rolling out national AI strategies and sector-specific guidelines. International bodies reinforce these efforts: the UN recently encouraged countries to adopt AI rules for “safe, secure and trustworthy” systems, and organizations like the OECD have AI Principles promoting trustworthy AI globally. </p> \n \n<p><strong>Key highlights include:</strong><br> \n• <strong>EU AI Act (2024):</strong> First-ever comprehensive AI law across the 27-member bloc. It takes a risk-based approach to AI systems and will impose fines up to 7% of global turnover for non-compliance.<br> \n• C*<em>hina’s Interim Measures:</em>* New administrative rules govern generative AI service providers in China’s digital ecosystem.<br> \n• <strong>International Frameworks:</strong> Bodies like the OECD and G7 emphasize AI ethics, and the UN’s AI resolutions call for member states to enact national AI regulations.</p> \n \n<p><strong>U.S. AI Regulation 2025: Trump’s Approach</strong><br> \n<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5vrn97ljlcjgps1b2vj0.jpg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5vrn97ljlcjgps1b2vj0.jpg\" alt=\"\" width=\"800\" height=\"433\"></a><br> \nThe United States still has no single federal AI law, relying instead on a patchwork of laws and guidelines. In early 2025 the Trump administration took a markedly different tack from the previous (Biden) administration. President Trump’s January 2025 Executive Order “Removing Barriers to American Leadership in AI” revoked many of Biden’s AI directives. Trump’s order calls for all agencies to rescind policies seen as hindering U.S. AI dominance. <br> \nIn July 2025 the administration also released “America’s AI Action Plan”, outlining 90+ actions to boost AI innovation and leadership. This plan has a pro‑innovation, deregulatory bent – contrasting with the EU’s risk-based model and even some state AI laws (like Colorado’s AI Act) that focus on preventing bias. <br> \nMeanwhile, Congress is considering various AI bills, most aiming to issue voluntary guidelines or create new agencies. In practice, U.S. companies navigate a maze of rules: current federal laws (e.g. consumer protection, aviation or defense statutes) apply in limited ways, and agencies like the FTC and FCC are adapting existing mandates to cover AI.</p> \n \n<p><strong>Key points in U.S. federal AI policy include:</strong><br> \n<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frjljznym6euwvnz9g3t4.jpg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frjljznym6euwvnz9g3t4.jpg\" alt=\"\" width=\"800\" height=\"433\"></a></p> \n \n<ul> \n<li> \n<strong>Trump’s 2025 EO (“Removing Barriers”):</strong> Signaled a permissive, growth-focused stance. It rescinds Biden’s Oct 2023 AI EO (Safe, Secure &amp; Trustworthy AI) and directs agencies to withdraw any “obstacles” to AI development.</li> \n<li> \n<strong>American AI Action Plan (July 2025)</strong>: Lists over 90 federal initiatives to secure U.S. AI leadership. It emphasizes export of AI tech, infrastructure upgrades, and incentives for industry.</li> \n<li> \n<strong>No Comprehensive Law Yet:</strong> Developers still operate under existing statutes. As one legal update notes, without federal AI rules “developers and deployers of AI systems will operate in an increasing patchwork of state and local laws”. Federal lawmakers to date favor voluntary standards (for example, promoting AI safety research and transparency) rather than stringent mandates, to avoid stifling innovation.</li> \n</ul> \n \n<p><strong>State AI Laws in the U.S.</strong><br> \nAt the state level, Ai legislative activity is surging. Dozens of states have introduced AI bills, leading to a fragmented state-by-state regulatory landscape.** For example:**<br> \n• <strong>Colorado:</strong> In May 2024 Colorado passed the nation’s first AI Act, effective Feb 2026. It requires developers/deployers of “high-risk” AI systems to use reasonable care to protect consumers from “algorithmic discrimination” (unlawful bias) in areas like hiring, credit, healthcare, etc...<br> \n• <strong>California:</strong> In 2024 California legislators drafted dozens of AI-related bills on topics like transparency of AI-generated content, rights of people depicted in AI media, data privacy, and banning deceptive deepfakes. These add to the U.S. regulatory patchwork. (A White &amp; Case analysis notes CA’s laws “aim to impose wide-ranging obligations” on AI developers, covering everything from safety reporting to content disclosures.)<br> \n• <strong>Other States:</strong> Over 45 states considered AI measures in 2024, and 31 enacted related laws or resolutions. For instance, Utah created an AI Policy Act, New York and Illinois are moving data/biometric laws with AI provisions, and many states have task forces or guidelines.</p> \n \n<p><strong>Frequently Asked Questions</strong><br> \n<strong>Q: How is the U.S. handling AI regulation in 2025?</strong><br> \n<strong>A:</strong> As of 2025 the U.S. has no single AI law. The Trump administration has prioritized innovation over restriction. A January 2025 Executive Order (“Removing Barriers to American Leadership in AI”) rescinded many of the Biden administration’s AI safety directives. In July 2025 the White House released an AI Action Plan with 90+ measures to boost U.S. AI leadership. At the same time, Congress has debated AI bills (mostly setting guidelines), and agencies like the FTC continue to use existing laws (e.g. anti-discrimination rules) to police AI. In practice, companies must comply with a mix of existing laws and voluntary standards, and pay close attention to state regulations.<br> \n<strong>Q: What is the EU AI Act?</strong><br> \n<strong>A:</strong> The EU Artificial Intelligence Act is the world’s first comprehensive AI law. Published in the EU Official Journal on July 12, 2024, it creates a risk-based framework for AI in all member states. High-risk AI systems (e.g. in healthcare, transport, law enforcement) will face strict requirements, while prohibited AI uses (like undetectable manipulative techniques) are banned. The law took effect in August 2024, with most rules enforceable by August 2026. It also imposes penalties up to €35 million or 7% of global turnover for violations.<br> \n<strong>Q: How many countries have AI regulations?</strong><br> \n<strong>A:</strong> By early 2025, many nations are moving on AI governance. One analysis found “at least 69 countries have proposed over 1000 AI-related policy initiatives and legal frameworks”. This includes data protection rules adapted for AI, special AI ethics laws, and government strategies. So far, major economies (EU, China, U.S.) and dozens of others (India, Canada, Australia, Brazil, etc.) have some AI rules or guidelines in place or in the works.<br> \n<strong>Q: Which U.S. states have their own AI laws?</strong><br> \n<strong>A:</strong> Several states are active. Colorado passed a landmark AI Act in 2024, targeting bias: it requires impact assessments and care to avoid “algorithmic discrimination” by high-risk AI systems. California has introduced many AI bills (e.g. requiring disclosures on AI-generated content) and broader AI/transparency laws. Utah, New York, and Illinois, among others, have new laws or regulations affecting AI use (from autonomous vehicles to biometric data). In 2024 over 30 states enacted AI-related laws or resolutions, so companies should track the state landscape carefully.<br> \n<strong>Q: What did President Trump’s 2025 AI executive order do?</strong><br> \n<strong>A:</strong> President Trump’s Jan 2025 EO titled “Removing Barriers to American Leadership in Artificial Intelligence” reversed many Biden-era AI policies. It revokes Biden’s Oct 2023 AI EO and directs federal agencies to rescind any rules or guidance seen as stifling innovation. The order explicitly emphasizes maintaining U.S. global AI dominance and calls for a new AI Action Plan (published in July 2025). In practice, it signals a shift from the prior administration’s risk- and safety-focused approach toward a deregulation stance.</p> \n \n<p><strong>Conclusion: Why AI Regulation News Today Matters More Than Ever</strong><br> \nThe rise of AI regulation is no longer a distant policy discussion—it’s a pressing reality shaping our digital lives, economies, and futures. From u.s. ai regulation 2025 under the Trump administration’s innovation-first stance, to a growing network of state ai laws like those in Colorado and California, the legal landscape is shifting fast. Meanwhile, international frameworks and ai regulations around the world 2025—like the EU AI Act or China’s content rules—are setting powerful precedents.<br> \nWhether you’re a business leader deploying AI tools, a developer writing algorithms, or a consumer curious about artificial intelligence laws and regulations, staying informed is critical. These laws affect how your data is used, how prices are set, and what kind of technologies get released (or banned). Understanding the regulatory direction can help you act responsibly, innovate ethically, and protect both user trust and long-term success.</p> \n \n<p>At TechDecodedly, we’re committed to delivering accurate, human-readable updates on AI regulations around the world, U.S. AI action plans, and the future of tech policy. Subscribe, follow, and keep reading—we’ll help you make sense of it all.</p>",
      "summary": "AI Regulation News Today shows that around the world, AI regulation is rapidly evolving as governments race to set standards for safe and responsible AI development. A 2025 survey notes that “at least 69 countries” (including the EU) have proposed or adopted AI laws and initiatives. The European Union leads with the landmark EU Artificial Intelligence Act (Regulation 2024/1689), adopted July 2024 and entering into force August 2024 (with most provisions effective in 2026). In Asia, China issued ",
      "publishedAt": "2025-12-02T12:00:49.000Z",
      "author": "Techdecodedly",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 15.87097034638796,
      "ingestedAt": "2025-12-02T14:44:03.187Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "governance",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099ee95e",
      "title": "Beyond `console.log('Great Product!')`: Architecting B2B Testimonials That Actually Convert",
      "url": "https://dev.to/michaelaiglobal/beyond-consoleloggreat-product-architecting-b2b-testimonials-that-actually-convert-2fnh",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fog715azb60vxmqxk1op6.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>As developers, we’re masters of systems, logic, and architecture. We build complex applications, orchestrate data pipelines, and design elegant APIs. So why, when it comes to marketing our own products, do we often settle for testimonials that are the equivalent of a <code>console.log('They have great customer service!')</code>?</p>  \n  \n<p>Flat, generic quotes are the null pointers of B2B marketing. They point to nothing, carry no emotional weight, and fail to build the trust needed for another technical team to adopt your tool. </p>  \n  \n<p>It’s time to stop collecting quotes and start architecting stories. Let’s refactor our approach to testimonials by treating them not as static strings, but as compelling narratives with a clear structure.</p>  \n  \n<h2>  \n    \n    \n  The Testimonial Monomyth: Deconstructing the Hero's Journey  \n</h2>  \n  \n<p>Every great story, from Star Wars to The Matrix, follows a predictable pattern: The Hero's Journey. Your customer is that hero. Your product isn't the hero—it's the lightsaber, the red pill, the powerful tool that enables their transformation.</p>  \n  \n<p>By mapping your customer's experience to this narrative arc, you move from a bland statement to an authentic, emotional journey that other developers can see themselves in. </p>  \n  \n<p>Instead of a flat data object, you create a structured narrative.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>// The OLD way: Low-information testimonial</span>  \n<span>const</span> <span>flatTestimonial</span> <span>=</span> <span>{</span>  \n  <span>customer</span><span>:</span> <span>\"</span><span>Jane Doe, Lead Engineer @ Acme Corp</span><span>\"</span><span>,</span>  \n  <span>quote</span><span>:</span> <span>\"</span><span>This product saved us a lot of time. We highly recommend it.</span><span>\"</span><span>,</span>  \n  <span>rating</span><span>:</span> <span>5</span>  \n<span>};</span>  \n  \n<span>// The NEW way: A structured story arc</span>  \n<span>const</span> <span>heroJourneyTestimonial</span> <span>=</span> <span>{</span>  \n  <span>hero</span><span>:</span> <span>\"</span><span>Jane Doe, Lead Engineer @ Acme Corp</span><span>\"</span><span>,</span>  \n  <span>act_1_the_problem</span><span>:</span> <span>{</span>  \n    <span>ordinary_world</span><span>:</span> <span>\"</span><span>We were manually deploying microservices, spending 10 hours a week in YAML hell.</span><span>\"</span><span>,</span>  \n    <span>call_to_adventure</span><span>:</span> <span>\"</span><span>Our monolithic CI/CD pipeline finally broke during a critical release.</span><span>\"</span>  \n  <span>},</span>  \n  <span>act_2_the_solution</span><span>:</span> <span>{</span>  \n    <span>meeting_the_mentor</span><span>:</span> <span>\"</span><span>A colleague mentioned your platform in a Slack channel.</span><span>\"</span><span>,</span>  \n    <span>crossing_the_threshold</span><span>:</span> <span>\"</span><span>We signed up for a trial and integrated our first service in 30 minutes.</span><span>\"</span><span>,</span>  \n    <span>tests_and_allies</span><span>:</span> <span>\"</span><span>The biggest challenge was migrating legacy configs, but the support docs and our dedicated support engineer (our 'ally') were amazing.</span><span>\"</span>  \n  <span>},</span>  \n  <span>act_3_the_transformation</span><span>:</span> <span>{</span>  \n    <span>the_reward</span><span>:</span> <span>\"</span><span>We've automated 95% of our deployment process, cutting release times from hours to minutes.</span><span>\"</span><span>,</span>  \n    <span>the_elixir</span><span>:</span> <span>\"</span><span>Now, our dev team focuses on building features, not fighting the pipeline. We're the heroes who brought real velocity back to the org.</span><span>\"</span>  \n  <span>}</span>  \n<span>};</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>See the difference? The second example isn't just a review; it's a relatable struggle with a triumphant resolution. It provides context, detail, and an emotional payoff.</p>  \n  \n<h2>  \n    \n    \n  The <code>getStory()</code> Function: An API for Authenticity  \n</h2>  \n  \n<p>You can't get a story if you don't ask for one. Stop sending out surveys with a single text field asking for \"feedback.\" Instead, engage your power users in a conversation designed to extract the narrative. Think of it as an API call where the payload is their journey.</p>  \n  \n<h3>  \n    \n    \n  Endpoint: <code>GET /customer-story</code>  \n</h3>  \n  \n<p>The key is to ask open-ended questions that prompt storytelling. Ditch the old script and try these prompts instead.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>// Deprecated questions</span>  \n<span>const</span> <span>oldQuestions</span> <span>=</span> <span>[</span>  \n  <span>\"</span><span>What do you like about our product?</span><span>\"</span><span>,</span>  \n  <span>\"</span><span>How has our product helped you?</span><span>\"</span><span>,</span>  \n  <span>\"</span><span>Would you recommend us to a friend?</span><span>\"</span>  \n<span>];</span>  \n  \n<span>// The new, story-driven prompts</span>  \n<span>const</span> <span>newQuestions</span> <span>=</span> <span>[</span>  \n  <span>\"</span><span>Can you describe your workflow *before* you found us? Paint a picture of the frustration.</span><span>\"</span><span>,</span> <span>// Establishes the 'Ordinary World'</span>  \n  <span>\"</span><span>What was the specific event that made you realize you needed a new solution?</span><span>\"</span><span>,</span> <span>// The 'Call to Adventure'</span>  \n  <span>\"</span><span>Walk me through the 'aha!' moment you had while using the tool for the first time.</span><span>\"</span><span>,</span> <span>// 'Crossing the Threshold'</span>  \n  <span>\"</span><span>What was the single biggest hurdle you overcame using our platform?</span><span>\"</span><span>,</span> <span>// 'The Ordeal'</span>  \n  <span>\"</span><span>Tell me about a specific metric or outcome that changed. How did that make your team look?</span><span>\"</span> <span>// 'The Reward / The Elixir'</span>  \n<span>];</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>These questions guide the customer to tell their own hero story, naturally and authentically.</p>  \n  \n<h2>  \n    \n    \n  The Medium: Best Practices for Video Testimonials  \n</h2>  \n  \n<p>While a written story is powerful, seeing and hearing your customer hero tell it themselves is unbeatable. But this doesn't mean you need a Hollywood budget. For a technical audience, authenticity trumps production value every time.</p>  \n  \n<h3>  \n    \n    \n  Authenticity &gt; Polish  \n</h3>  \n  \n<p>A slightly grainy Zoom recording where an engineer is genuinely excited about solving a problem is infinitely more credible than a C-level executive reading a script from a teleprompter in a sterile office. Don't be afraid of remote recordings; they feel real and immediate.</p>  \n  \n<h3>  \n    \n    \n  B-Roll is Your Context Layer  \n</h3>  \n  \n<p>This is where you connect the story to the product. While the customer talks about their YAML hell, show a screen recording of their confusing old config files. When they describe their 'aha!' moment, show their cursor clicking the exact button in your UI. This visual evidence grounds the narrative in reality and makes it more compelling for other builders.</p>  \n  \n<h3>  \n    \n    \n  Use Jump Cuts to Your Advantage  \n</h3>  \n  \n<p>No one wants to watch someone ramble for five minutes. Be ruthless in your editing. Use jump cuts to clip out pauses, filler words (\"um,\" \"ah\"), and rambling tangents. The result is a high-energy, information-dense story that respects the viewer's time—something every developer appreciates.</p>  \n  \n<h2>  \n    \n    \n  From User to Hero  \n</h2>  \n  \n<p>Your customers aren't just entries in a database. They are the heroes who fought back against technical debt, inefficient workflows, and broken pipelines. Your product was their secret weapon.</p>  \n  \n<p>When you stop asking for quotes and start seeking out these stories, you do more than just collect marketing assets. You humanize your brand, build deep emotional connections, and create authentic marketing that actually resonates with the most skeptical audience out there: other developers.</p>  \n  \n<p>Originally published at <a href=\"https://getmichaelai.com/blog/from-happy-customer-to-hero-the-art-of-storytelling-in-b2b-t\">https://getmichaelai.com/blog/from-happy-customer-to-hero-the-art-of-storytelling-in-b2b-t</a></p>",
      "summary": "As developers, we’re masters of systems, logic, and architecture. We build complex applications, orchestrate data pipelines, and design elegant APIs. So why, when it comes to marketing our own products, do we often settle for testimonials that are the equivalent of a console.log('They have great customer service!')?  \n  \nFlat, generic quotes are the null pointers of B2B marketing. They point to nothing, carry no emotional weight, and fail to build the trust needed for another technical team to a",
      "publishedAt": "2025-12-02T12:00:36.000Z",
      "author": "Michael",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 8.431362381191262,
      "ingestedAt": "2025-12-02T14:44:03.187Z",
      "tags": [
        "code_review",
        "documentation",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099ee960",
      "title": "The Great Equaliser",
      "url": "https://dev.to/rawveg/the-great-equaliser-20em",
      "content": "<p>The corner shop that predicts your shopping habits better than Amazon. The local restaurant that automates its supply chain with the precision of McDonald's. The one-person consultancy that analyses data like McKinsey. These scenarios aren't science fiction—they're the emerging reality as artificial intelligence democratises tools once exclusive to corporate giants. But as small businesses gain access to enterprise-grade capabilities, a fundamental question emerges: will AI truly level the playing field, or simply redraw the battle lines in ways we're only beginning to understand?</p> \n \n<h2> \n   \n   \n  The New Arsenal \n</h2> \n \n<p>Walk into any high street business today and you'll likely encounter AI working behind the scenes. The local bakery uses machine learning to optimise flour orders. The independent bookshop employs natural language processing to personalise recommendations. The neighbourhood gym deploys computer vision to monitor equipment usage and predict maintenance needs. What was once the exclusive domain of Fortune 500 companies—sophisticated data analytics, predictive modelling, automated customer service—is now available as a monthly subscription.</p> \n \n<p>This transformation represents more than just technological advancement; it's a fundamental shift in the economic architecture. According to research from the Brookings Institution, AI functions as a \"wide-ranging\" technology that redefines how information is integrated, data is analysed, and decisions are made across every aspect of business operations. Unlike previous technological waves that primarily affected specific industries or functions, AI's impact cuts across all sectors simultaneously.</p> \n \n<p>The democratisation happens through cloud computing platforms that package complex AI capabilities into user-friendly interfaces. A small retailer can now access the same customer behaviour prediction algorithms that power major e-commerce platforms. A local manufacturer can implement quality control systems that rival those of industrial giants. The barriers to entry—massive computing infrastructure, teams of data scientists, years of algorithm development—have largely evaporated.</p> \n \n<p>Consider the transformation in customer relationship management. Where large corporations once held decisive advantages through expensive CRM systems and dedicated analytics teams, small businesses can now deploy AI-powered tools that automatically segment customers, predict purchasing behaviour, and personalise marketing messages. The playing field appears more level than ever before.</p> \n \n<p>Yet this apparent equalisation masks deeper complexities. Access to tools doesn't automatically translate to competitive advantage, and the same AI systems that empower small businesses also amplify the capabilities of their larger competitors. The question isn't whether AI will reshape local economies—it already is. The question is whether this reshaping will favour David or Goliath.</p> \n \n<h2> \n   \n   \n  Local Economies in Flux \n</h2> \n \n<p>Much like the corner shop discovering it can compete with retail giants through predictive analytics, local economies are experiencing transformations that challenge traditional assumptions about scale and proximity. The impact unfolds in unexpected ways. Traditional advantages—proximity to customers, personal relationships, intimate market knowledge—suddenly matter less when AI can predict consumer behaviour with precision. Simultaneously, new advantages emerge for businesses that can harness these tools effectively.</p> \n \n<p>Small businesses often possess inherent agility that larger corporations struggle to match. They can implement new AI systems faster, pivot strategies more quickly, and adapt to local market conditions with greater flexibility. A family-owned restaurant can adjust its menu based on AI-analysed customer preferences within days, while a chain restaurant might need months to implement similar changes across its corporate structure.</p> \n \n<p>The \"tele-everything\" environment accelerated by AI adoption fundamentally alters the value of physical presence. Local businesses that once relied primarily on foot traffic and geographical convenience must now compete with online-first enterprises that leverage AI to deliver personalised experiences regardless of location. This shift doesn't necessarily disadvantage local businesses, but it forces them to compete on new terms.</p> \n \n<p>Some local economies are experiencing a renaissance as AI enables small businesses to serve global markets. A craftsperson in rural Wales can now use AI-powered tools to identify international customers, optimise pricing strategies, and manage complex supply chains that were previously beyond their capabilities. The local becomes global, but the global also becomes intensely local as AI enables mass customisation and hyper-personalised services.</p> \n \n<p>The transformation extends beyond individual businesses to entire economic ecosystems. Local suppliers, service providers, and complementary businesses must all adapt to new AI-driven demands and capabilities. A local accounting firm might find its traditional bookkeeping services automated away, but discover new opportunities in helping businesses implement and optimise AI systems. The ripple effects create new interdependencies and collaborative possibilities that reshape entire commercial districts.</p> \n \n<h2> \n   \n   \n  The Corporate Response \n</h2> \n \n<p>Large corporations aren't passive observers in this transformation. They're simultaneously benefiting from the same AI democratisation while developing strategies to maintain their competitive advantages. The result is an arms race where both small businesses and corporations are rapidly adopting AI capabilities, but with vastly different resources and strategic approaches.</p> \n \n<p>Corporate advantages in the AI era often centre on data volume and variety. While small businesses can access sophisticated AI tools, large corporations possess vast datasets that can train more accurate and powerful models. A multinational retailer has purchase data from millions of customers across diverse markets, enabling AI insights that a local shop with hundreds of customers simply cannot match. This data advantage compounds over time, as larger datasets enable more sophisticated AI models, which generate better insights, which attract more customers, which generate more data.</p> \n \n<p>Scale also provides advantages in AI implementation. Corporations can afford dedicated AI teams, custom algorithm development, and integration across multiple business functions. They can experiment with cutting-edge technologies, absorb the costs of failed implementations, and iterate rapidly towards optimal solutions. Small businesses, despite having access to AI tools, often lack the resources for such comprehensive adoption.</p> \n \n<p>However, corporate size can also become a liability. Large organisations often struggle with legacy systems, bureaucratic decision-making processes, and resistance to change. A small business can implement a new AI-powered inventory management system in weeks, while a corporation might need years to navigate internal approvals, system integrations, and change management processes. The very complexity that enables corporate scale can inhibit the rapid adaptation that AI environments reward.</p> \n \n<p>The competitive dynamics become particularly complex in markets where corporations and small businesses serve similar customer needs. AI enables both to offer increasingly sophisticated services, but the nature of competition shifts from traditional factors like price and convenience to new dimensions like personalisation depth, prediction accuracy, and automated service quality. A local financial advisor equipped with AI-powered portfolio analysis tools might compete effectively with major investment firms, not on the breadth of services, but on the depth of personal attention combined with sophisticated analytical capabilities.</p> \n \n<h2> \n   \n   \n  New Forms of Inequality \n</h2> \n \n<p>The promise of AI democratisation comes with a darker counterpart: the emergence of new forms of inequality that may prove more entrenched than those they replace. While AI tools become more accessible, the skills, knowledge, and resources required to use them effectively remain unevenly distributed.</p> \n \n<p>Digital literacy emerges as a critical factor determining who benefits from AI democratisation. Small business owners who can understand and implement AI systems gain significant advantages over those who cannot. This creates a new divide not based on access to capital or technology, but on the ability to comprehend and leverage complex digital tools. The gap between AI-savvy and AI-naive businesses may prove wider than traditional competitive gaps.</p> \n \n<p>A significant portion of technology experts express concern about AI's societal impact. Research from the Pew Research Centre indicates that many experts believe the tech-driven future will worsen life for most people, specifically citing \"greater inequality\" as a major outcome. This pessimism stems partly from AI's potential to replace human workers while concentrating benefits among those who own and control AI systems.</p> \n \n<p>The productivity gains from AI create a paradox for small businesses. While these tools can dramatically increase efficiency and capability, they also reduce the need for human employees. A small business that once employed ten people might accomplish the same work with five people and sophisticated AI systems. The business becomes more competitive, but contributes less to local employment and economic circulation. This labour-saving potential of AI creates a fundamental tension between business efficiency and community economic health.</p> \n \n<p>Geographic inequality also intensifies as AI adoption varies significantly across regions. Areas with strong digital infrastructure, educated populations, and supportive business environments see rapid AI adoption among local businesses. Rural or economically disadvantaged areas lag behind, creating growing gaps in local economic competitiveness. The digital divide evolves into an AI divide with potentially more severe consequences.</p> \n \n<p>Access to data becomes another source of inequality. While AI tools are democratised, the data required to train them effectively often isn't. Businesses in data-rich environments—urban areas with dense customer interactions, regions with strong digital adoption, markets with sophisticated tracking systems—can leverage AI more effectively than those in data-poor environments. This creates a new form of resource inequality where information, rather than capital or labour, becomes the primary determinant of competitive advantage.</p> \n \n<p>The emergence of these inequalities is particularly concerning because they compound existing disadvantages. Businesses that already struggle with traditional competitive factors—limited capital, poor locations, outdated infrastructure—often find themselves least equipped to navigate AI adoption successfully. The democratisation of AI tools doesn't automatically democratise the benefits if the underlying capabilities to use them remain concentrated.</p> \n \n<h2> \n   \n   \n  The Skills Revolution \n</h2> \n \n<p>The AI transformation demands new skills that don't align neatly with traditional business education or experience. Small business owners must become part technologist, part data analyst, part strategic planner in ways that previous generations never required. This skills revolution creates opportunities for some while leaving others behind.</p> \n \n<p>Traditional business skills—relationship building, local market knowledge, operational efficiency—remain important but are no longer sufficient. Success increasingly requires understanding how to select appropriate AI tools, interpret outputs, and integrate digital systems with human processes. The learning curve is steep, and not everyone can climb it effectively. A successful restaurant owner with decades of experience in food service and customer relations might struggle to understand machine learning concepts or data analytics principles necessary to leverage AI-powered inventory management or customer prediction systems.</p> \n \n<p>Educational institutions struggle to keep pace with the rapidly evolving skill requirements. Business schools that taught traditional management principles find themselves scrambling to incorporate AI literacy into curricula. Vocational training programmes designed for traditional trades must now include digital components. The mismatch between educational offerings and business needs creates gaps that some entrepreneurs can bridge while others cannot.</p> \n \n<p>Generational differences compound the skills challenge. Younger business owners who grew up with digital technology often adapt more quickly to AI tools, while older entrepreneurs with decades of experience may find the transition more difficult. This creates potential for generational turnover in local business leadership as AI adoption becomes essential for competitiveness. However, the relationship isn't simply age-based—some older business owners embrace AI enthusiastically while some younger ones struggle with its complexity.</p> \n \n<p>The skills revolution also affects employees within small businesses. Workers must adapt to AI-augmented roles, learning to collaborate with systems rather than simply performing traditional tasks. Some thrive in this environment, developing hybrid human-AI capabilities that make them more valuable. Others struggle with the transition, potentially facing displacement or reduced relevance. A retail employee who learns to work with AI-powered inventory systems and customer analytics becomes more valuable, while one who resists such integration may find their role diminished.</p> \n \n<p>The pace of change in required skills creates ongoing challenges. AI capabilities evolve rapidly, meaning that skills learned today may become obsolete within years. This demands a culture of continuous learning that many small businesses struggle to maintain while managing day-to-day operations. The businesses that succeed are often those that can balance immediate operational needs with ongoing skill development.</p> \n \n<h2> \n   \n   \n  Redefining Competition \n</h2> \n \n<p>Just as the local restaurant now competes on supply chain optimisation rather than just food quality, AI doesn't just change the tools of competition; it fundamentally alters what businesses compete on. Traditional competitive factors like price, location, and product quality remain important, but new dimensions emerge that can overwhelm traditional advantages.</p> \n \n<p>Prediction capability becomes a key competitive differentiator. Businesses that can accurately forecast customer needs, market trends, and operational requirements gain significant advantages over those relying on intuition or historical patterns. A local retailer that predicts seasonal demand fluctuations can optimise inventory and pricing in ways that traditional competitors cannot match. This predictive capability extends beyond simple forecasting to understanding complex patterns in customer behaviour, market dynamics, and operational efficiency.</p> \n \n<p>Personalisation depth emerges as another competitive battlefield. AI enables small businesses to offer individually customised experiences that were previously impossible at their scale. A neighbourhood coffee shop can remember every customer's preferences, predict their likely orders, and adjust recommendations based on weather, time of day, and purchasing history. This level of personalisation can compete effectively with larger chains that offer consistency but less individual attention.</p> \n \n<p>Speed of adaptation becomes crucial as market conditions change rapidly. Businesses that can quickly adjust strategies, modify offerings, and respond to new opportunities gain advantages over slower competitors. AI systems that continuously monitor market conditions and automatically adjust business parameters enable small businesses to be more responsive than larger organisations with complex decision-making hierarchies. A small online retailer can adjust pricing in real-time based on competitor analysis and demand patterns, while a large corporation might need weeks to implement similar changes.</p> \n \n<p>Data quality and integration emerge as competitive moats. Businesses that collect clean, comprehensive data and integrate it effectively across all operations can leverage AI more powerfully than those with fragmented or poor-quality information. This creates incentives for better data management practices but also advantages businesses that start with superior data collection capabilities. A small business that systematically tracks customer interactions, inventory movements, and operational metrics can build AI capabilities that larger competitors with poor data practices cannot match.</p> \n \n<p>The redefinition of competition extends to entire business models. AI enables new forms of value creation that weren't previously possible at small business scale. A local service provider might develop AI-powered tools that become valuable products in their own right. A neighbourhood retailer might create data insights that benefit other local businesses. Competition evolves from zero-sum battles over market share to more complex ecosystems of value creation and exchange.</p> \n \n<p>Customer expectations also shift as AI capabilities become more common. Businesses that don't offer AI-enabled features—personalised recommendations, predictive service, automated support—may appear outdated compared to competitors that do. This creates pressure for AI adoption not just for operational efficiency, but for customer satisfaction and retention.</p> \n \n<h2> \n   \n   \n  The Network Effect \n</h2> \n \n<p>As AI adoption spreads across local economies, network effects emerge that can either amplify competitive advantages or create new forms of exclusion. Businesses that adopt AI early and effectively often find their advantages compound over time, while those that lag behind face increasingly difficult catch-up challenges.</p> \n \n<p>Data network effects prove particularly powerful. Businesses that collect more customer data can train better AI models, which provide superior service, which attracts more customers, which generates more data. This virtuous cycle can quickly separate AI-successful businesses from their competitors in ways that traditional competitive dynamics rarely achieved. A local delivery service that uses AI to optimise routes and predict demand can provide faster, more reliable service, attracting more customers and generating more data to further improve its AI systems.</p> \n \n<p>Partnership networks also evolve around AI capabilities. Small businesses that can effectively integrate AI systems often find new collaboration opportunities with other AI-enabled enterprises. They can share data insights, coordinate supply chains, and develop joint offerings that leverage combined AI capabilities. Businesses that cannot participate in these AI-enabled networks risk isolation from emerging collaborative opportunities.</p> \n \n<p>Platform effects emerge as AI tools become more sophisticated and interconnected. Businesses that adopt compatible AI systems can more easily integrate with suppliers, customers, and partners who use similar technologies. This creates pressure for standardisation around particular AI platforms, potentially disadvantaging businesses that choose different or incompatible systems. A small manufacturer that uses AI systems compatible with its suppliers' inventory management can achieve seamless coordination, while one using incompatible systems faces integration challenges.</p> \n \n<p>The network effects extend beyond individual businesses to entire local economic ecosystems. Regions where many businesses adopt AI capabilities can develop supportive infrastructure, shared expertise, and collaborative advantages that attract additional AI-enabled enterprises. Areas that lag in AI adoption may find themselves increasingly isolated from broader economic networks. Cities that develop strong AI business clusters can offer shared resources, talent pools, and collaborative opportunities that individual businesses in less developed areas cannot access.</p> \n \n<p>Knowledge networks become particularly important as AI implementation requires ongoing learning and adaptation. Businesses in areas with strong AI adoption can share experiences, learn from each other's successes and failures, and collectively develop expertise that benefits the entire local economy. This creates positive feedback loops where AI success breeds more AI success, but also means that areas that fall behind may find it increasingly difficult to catch up.</p> \n \n<h2> \n   \n   \n  Global Reach, Local Impact \n</h2> \n \n<p>AI democratisation enables small businesses to compete in global markets while simultaneously making global competition more intense at the local level. This paradox creates both opportunities and threats for local economies in ways that previous technological waves didn't achieve.</p> \n \n<p>A small manufacturer in Manchester can now use AI to identify customers in markets they never previously accessed, optimise international shipping routes, and manage currency fluctuations with sophisticated algorithms. The barriers to global commerce—language translation, market research, logistics coordination—diminish significantly when AI tools handle these complexities automatically. Machine learning systems can analyse global market trends, identify emerging opportunities, and even handle customer service in multiple languages, enabling small businesses to operate internationally with capabilities that previously required large multinational operations.</p> \n \n<p>However, this global reach works in both directions. Local businesses that once competed primarily with nearby enterprises now face competition from AI-enabled businesses anywhere in the world. A local graphic design firm competes not just with other local designers, but with AI-augmented freelancers from dozens of countries who can deliver similar services at potentially lower costs. The protective barriers of geography and local relationships diminish when AI enables remote competitors to offer personalised, efficient service regardless of physical location.</p> \n \n<p>The globalisation of competition through AI creates pressure for local businesses to find defensible advantages that global competitors cannot easily replicate. Physical presence, local relationships, and regulatory compliance become more valuable when other competitive factors can be matched by distant AI-enabled competitors. A local accountant might compete with global AI-powered tax preparation services by offering face-to-face consultation and deep knowledge of local regulations that remote competitors cannot match.</p> \n \n<p>Cultural and regulatory differences provide some protection for local businesses, but AI's ability to adapt to local preferences and navigate regulatory requirements reduces these natural barriers. A global e-commerce platform can use AI to automatically adjust its offerings for local tastes, comply with regional regulations, and even communicate in local dialects or cultural contexts. This erosion of natural competitive barriers forces local businesses to compete more directly on service quality, innovation, and efficiency rather than relying on geographic or cultural advantages.</p> \n \n<p>The global competition enabled by AI also creates opportunities for specialisation and niche market development. Small businesses can use AI to identify and serve highly specific customer segments globally, rather than trying to serve broad local markets. A craftsperson specialising in traditional techniques can use AI to find customers worldwide who value their specific skills, creating sustainable businesses around expertise that might not support a local market.</p> \n \n<p>International collaboration becomes more feasible as AI tools handle communication, coordination, and logistics challenges. Small businesses can participate in global supply chains, joint ventures, and collaborative projects that were previously accessible only to large corporations. This creates opportunities for local businesses to access global resources, expertise, and markets while maintaining their local identity and operations.</p> \n \n<h2> \n   \n   \n  Policy and Regulatory Responses \n</h2> \n \n<p>Governments and regulatory bodies are beginning to recognise the transformative potential of AI democratisation and its implications for local economies. Policy responses vary significantly across jurisdictions, creating a patchwork of approaches that may determine which regions benefit most from AI-enabled economic transformation.</p> \n \n<p>Some governments focus on ensuring broad access to AI tools and training, recognising that digital divides could become AI divides with severe economic consequences. Public funding for AI education, infrastructure development, and small business support programmes aims to prevent the emergence of AI-enabled inequality between different economic actors and regions. The European Union's Digital Single Market strategy includes provisions for supporting small business AI adoption, while countries like Singapore have developed comprehensive AI governance frameworks that include support for small and medium enterprises.</p> \n \n<p>Competition policy faces new challenges as AI blurs traditional boundaries between markets and competitive advantages. Regulators must determine whether AI democratisation genuinely increases competition or whether it creates new forms of market concentration that require intervention. The complexity of AI systems makes it difficult to assess competitive impacts using traditional regulatory frameworks. When a few large technology companies provide the AI platforms that most small businesses depend on, questions arise about whether this creates new forms of economic dependency that require regulatory attention.</p> \n \n<p>Data governance emerges as a critical policy area affecting small business competitiveness. Regulations that restrict data collection or sharing may inadvertently disadvantage small businesses that rely on AI tools requiring substantial data inputs. Conversely, policies that enable broader data access might help level the playing field between small businesses and large corporations with extensive proprietary datasets. The General Data Protection Regulation in Europe, for example, affects how small businesses can collect and use customer data for AI applications, potentially limiting their ability to compete with larger companies that have more resources for compliance.</p> \n \n<p>Privacy and security regulations create compliance burdens that affect small businesses differently than large corporations. While AI tools can help automate compliance processes, the underlying regulatory requirements may still favour businesses with dedicated legal and technical resources. Policy makers must balance privacy protection with the need to avoid creating insurmountable barriers for small business AI adoption.</p> \n \n<p>International coordination becomes increasingly important as AI-enabled businesses operate across borders more easily. Differences in AI regulation, data governance, and digital trade policies between countries can create competitive advantages or disadvantages for businesses in different jurisdictions. Small businesses with limited resources to navigate complex international regulatory environments may find themselves at a disadvantage compared to larger enterprises with dedicated compliance teams.</p> \n \n<p>The pace of AI development often outstrips regulatory responses, creating uncertainty for businesses trying to plan AI investments and implementations. Regulatory frameworks developed for traditional business models may not adequately address the unique challenges and opportunities created by AI adoption. This regulatory lag can create both opportunities for early adopters and risks for businesses that invest in AI capabilities that later face regulatory restrictions.</p> \n \n<h2> \n   \n   \n  The Human Element \n</h2> \n \n<p>Despite AI's growing capabilities, human factors remain crucial in determining which businesses succeed in the AI-enabled economy. The interaction between human creativity, judgement, and relationship-building skills with AI capabilities often determines competitive outcomes more than pure technological sophistication.</p> \n \n<p>Small businesses often possess advantages in human-AI collaboration that larger organisations struggle to match. The close relationships between owners, employees, and customers in small businesses enable more nuanced understanding of how AI tools should be deployed and customised. A local business owner who knows their customers personally can guide AI systems more effectively than distant corporate algorithms. This intimate knowledge allows for AI implementations that enhance rather than replace human insights and relationships.</p> \n \n<p>Trust and relationships become more valuable, not less, as AI capabilities proliferate. Customers who feel overwhelmed by purely digital interactions may gravitate towards businesses that combine AI efficiency with human warmth and understanding. Small businesses that successfully blend AI capabilities with personal service can differentiate themselves from purely digital competitors. A local bank that uses AI for fraud detection and risk assessment while maintaining personal relationships with customers can offer security and efficiency alongside human understanding and flexibility.</p> \n \n<p>The human element also affects AI implementation success within businesses. Small business owners who can effectively communicate AI benefits to employees, customers, and partners are more likely to achieve successful adoption than those who treat AI as a purely technical implementation. Change management skills become as important as technical capabilities in determining AI success. Employees who understand how AI tools enhance their work rather than threaten their jobs are more likely to use these tools effectively and contribute to successful implementation.</p> \n \n<p>Ethical considerations around AI use create opportunities for small businesses to differentiate themselves through more responsible AI deployment. While large corporations may face pressure to maximise AI efficiency regardless of broader impacts, small businesses with strong community ties may choose AI implementations that prioritise local employment, customer privacy, or social benefit alongside business objectives. This ethical positioning can become a competitive advantage in markets where customers value responsible business practices.</p> \n \n<p>The human element extends to customer experience design and service delivery. AI can handle routine tasks and provide data insights, but human creativity and empathy remain essential for understanding customer needs, designing meaningful experiences, and building lasting relationships. Small businesses that use AI to enhance human capabilities rather than replace them often achieve better customer satisfaction and loyalty than those that pursue purely automated solutions.</p> \n \n<p>Creativity and innovation in AI application often come from human insights about customer needs, market opportunities, and operational challenges. Small business owners who understand their operations intimately can identify AI applications that larger competitors might miss. This human insight into business operations and customer needs becomes a source of competitive advantage in AI implementation.</p> \n \n<h2> \n   \n   \n  Future Trajectories \n</h2> \n \n<p>The trajectory of AI democratisation and its impact on local economies remains uncertain, with multiple possible futures depending on technological development, policy choices, and market dynamics. Understanding these potential paths helps businesses and policymakers prepare for different scenarios.</p> \n \n<p>One trajectory leads towards genuine democratisation where AI tools become so accessible and easy to use that most small businesses can compete effectively with larger enterprises on AI-enabled capabilities. In this scenario, local economies flourish as small businesses leverage AI to serve global markets while maintaining local roots and relationships. The corner shop truly does compete with Amazon, not by matching its scale, but by offering superior personalisation and local relevance powered by AI insights.</p> \n \n<p>An alternative trajectory sees AI democratisation creating new forms of concentration where a few AI platform providers control the tools that all businesses depend on. Small businesses gain access to AI capabilities but become dependent on platforms controlled by large technology companies, potentially creating new forms of economic subjugation rather than liberation. In this scenario, the democratisation of AI tools masks a concentration of control over the underlying infrastructure and algorithms that determine business success.</p> \n \n<p>A third possibility involves fragmentation where AI adoption varies dramatically across regions, industries, and business types, creating a complex patchwork of AI-enabled and traditional businesses. This scenario might preserve diversity in business models and competitive approaches but could also create significant inequalities between different economic actors and regions. Some areas become AI-powered economic hubs while others remain trapped in traditional competitive dynamics.</p> \n \n<p>The speed of AI development affects all these trajectories. Rapid advancement might favour businesses and regions that can adapt quickly while leaving others behind. Slower, more gradual development might enable broader adoption and more equitable outcomes but could also delay beneficial transformations in productivity and capability. The current pace of AI development, particularly in generative AI capabilities, suggests that rapid change is more likely than gradual evolution.</p> \n \n<p>International competition adds another dimension to these trajectories. Countries that develop strong AI capabilities and supportive regulatory frameworks may see their local businesses gain advantages over those in less developed AI ecosystems. China's rapid advancement in AI innovation, as documented by the Information Technology and Innovation Foundation, demonstrates how national AI strategies can affect local business competitiveness on a global scale.</p> \n \n<p>The role of human-AI collaboration will likely determine which trajectory emerges. Research from the Pew Research Centre suggests that the most positive outcomes occur when AI enhances human capabilities rather than simply replacing them. Local economies that successfully integrate AI tools with human skills and relationships may achieve better outcomes than those that pursue purely technological solutions.</p> \n \n<h2> \n   \n   \n  Preparing for Transformation \n</h2> \n \n<p>The AI transformation of local economies is not a distant future possibility but a current reality that businesses, policymakers, and communities must navigate actively. Success in this environment requires understanding both the opportunities and risks while developing strategies that leverage AI capabilities while preserving human and community values.</p> \n \n<p>Small businesses must develop AI literacy not as a technical specialisation but as a core business capability. This means understanding what AI can and cannot do, how to select appropriate tools, and how to integrate AI systems with existing operations and relationships. The learning curve is steep, but the costs of falling behind may be steeper. Business owners need to invest time in understanding AI capabilities, experimenting with available tools, and developing strategies for gradual implementation that builds on their existing strengths.</p> \n \n<p>Local communities and policymakers must consider how to support AI adoption while preserving the diversity and character that make local economies valuable. This might involve public investment in digital infrastructure, education programmes, or support for businesses struggling with AI transition. The goal should be enabling beneficial transformation rather than simply accelerating technological adoption. Communities that proactively address AI adoption challenges are more likely to benefit from the opportunities while mitigating the risks.</p> \n \n<p>The democratisation of AI represents both the greatest opportunity and the greatest challenge facing local economies in generations. It promises to level competitive playing fields that have favoured large corporations for decades while threatening to create new forms of inequality that could be more entrenched than those they replace. The outcome will depend not on the technology itself, but on how wisely we deploy it in service of human and community flourishing.</p> \n \n<p>Collaboration between businesses, educational institutions, and government agencies becomes essential for successful AI adoption. Small businesses need access to training, technical support, and financial resources to implement AI effectively. Educational institutions must adapt curricula to include AI literacy alongside traditional business skills. Government agencies must develop policies that support beneficial AI adoption while preventing harmful concentration of power or exclusion of vulnerable businesses.</p> \n \n<p>The transformation requires balancing efficiency gains with social and economic values. While AI can dramatically improve business productivity and competitiveness, communities must consider the broader impacts on employment, social cohesion, and economic diversity. The most successful AI adoptions are likely to be those that enhance human capabilities and community strengths rather than simply replacing them with automated systems.</p> \n \n<p>As we stand at this inflection point, the choices made by individual businesses, local communities, and policymakers will determine whether AI democratisation fulfils its promise of economic empowerment or becomes another force for concentration and inequality. The technology provides the tools; wisdom in their application will determine the results.</p> \n \n<p>The corner shop that predicts your needs, the restaurant that optimises its operations, the consultancy that analyses like a giant—these are no longer future possibilities but present realities. The question is no longer whether AI will transform local economies, but whether that transformation will create the more equitable and prosperous future that its democratisation promises. The answer lies not in the algorithms themselves, but in the human choices that guide their deployment.</p> \n \n<p>Is AI levelling the field, or just redrawing the battle lines?</p> \n \n \n \n \n<h2> \n   \n   \n  References and Further Information \n</h2> \n \n<p><strong>Primary Sources:</strong></p> \n \n<p>Brookings Institution. \"How artificial intelligence is transforming the world.\" Available at: <a href=\"http://www.brookings.edu\">www.brookings.edu</a></p> \n \n<p>Pew Research Center. \"Experts Say the 'New Normal' in 2025 Will Be Far More Tech-Driven.\" Available at: <a href=\"http://www.pewresearch.org\">www.pewresearch.org</a></p> \n \n<p>Pew Research Center. \"Improvements ahead: How humans and AI might evolve together in the next decade.\" Available at: <a href=\"http://www.pewresearch.org\">www.pewresearch.org</a></p> \n \n<p>ScienceDirect. \"Opinion Paper: 'So what if ChatGPT wrote it?' Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy.\" Available at: <a href=\"http://www.sciencedirect.com\">www.sciencedirect.com</a></p> \n \n<p>ScienceDirect. \"AI revolutionizing industries worldwide: A comprehensive overview of artificial intelligence applications across diverse sectors.\" Available at: <a href=\"http://www.sciencedirect.com\">www.sciencedirect.com</a></p> \n \n<p>Information Technology and Innovation Foundation. \"China Is Rapidly Becoming a Leading Innovator in Advanced Technologies.\" Available at: itif.org</p> \n \n<p>International Monetary Fund. \"Technological Progress, Artificial Intelligence, and Inclusive Growth.\" Available at: <a href=\"http://www.elibrary.imf.org\">www.elibrary.imf.org</a></p> \n \n<p><strong>Additional Reading:</strong></p> \n \n<p>For deeper exploration of AI's economic impacts, readers should consult academic journals focusing on technology economics, policy papers from major think tanks examining AI democratisation, and industry reports tracking small business AI adoption rates across different sectors and regions. The European Union's Digital Single Market strategy documents provide insight into policy approaches to AI adoption support, while Singapore's AI governance frameworks offer examples of comprehensive national AI strategies that include small business considerations.</p> \n \n \n \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fos7pdncawa0mgqcin0gf.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fos7pdncawa0mgqcin0gf.png\" title=\"Tim Green\" alt=\"Tim Green\"></a></p> \n \n<p><strong>Tim Green</strong> <em>UK-based Systems Theorist &amp; Independent Technology Writer</em></p> \n \n<p>Tim explores the intersections of artificial intelligence, decentralised cognition, and posthuman ethics. His work, published at <a href=\"https://smarterarticles.co.uk/\">smarterarticles.co.uk</a>, challenges dominant narratives of technological progress while proposing interdisciplinary frameworks for collective intelligence and digital stewardship.</p> \n \n<p>His writing has been featured on Ground News and shared by independent researchers across both academic and technological communities.</p> \n \n<p><strong>ORCID:</strong> <a href=\"https://orcid.org/0009-0002-0156-9795\">0009-0002-0156-9795</a><br><br> \n<strong>Email:</strong> <a href=\"mailto:tim@smarterarticles.co.uk\">tim@smarterarticles.co.uk</a></p>",
      "summary": "The corner shop that predicts your shopping habits better than Amazon. The local restaurant that automates its supply chain with the precision of McDonald's. The one-person consultancy that analyses data like McKinsey. These scenarios aren't science fiction—they're the emerging reality as artificial intelligence democratises tools once exclusive to corporate giants. But as small businesses gain access to enterprise-grade capabilities, a fundamental question emerges: will AI truly level the playi",
      "publishedAt": "2025-12-02T12:00:00.000Z",
      "author": "Tim Green",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 14.878431960972327,
      "ingestedAt": "2025-12-02T14:44:03.188Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability",
        "governance",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099dc271",
      "title": "AI Adoption Surges While Governance Lags — Report Warns of Growing Shadow Identity Risk",
      "url": "https://devops.com/ai-adoption-surges-while-governance-lags-report-warns-of-growing-shadow-identity-risk/",
      "content": "<div><img width=\"1536\" height=\"1024\" src=\"https://devops.com/wp-content/uploads/2025/12/AI-Data-Security-Report-Image-2_1764604257jUacyOMyCu.jpeg\" alt=\"\" style=\"margin-bottom:0px;\"></div><img width=\"150\" height=\"150\" src=\"https://devops.com/wp-content/uploads/2025/12/AI-Data-Security-Report-Image-2_1764604257jUacyOMyCu-150x150.jpeg\" alt=\"\">Baltimore, MD, 2nd December 2025, CyberNewsWire",
      "summary": "Baltimore, MD, 2nd December 2025, CyberNewsWire",
      "publishedAt": "2025-12-02T12:00:56.000Z",
      "author": "cybernewswire",
      "source": "rss",
      "feedName": "DevOps.com",
      "sourceType": "general",
      "contentType": "security_incident",
      "score": 9.423443169034194,
      "ingestedAt": "2025-12-02T14:44:03.188Z",
      "tags": [
        "ide",
        "governance",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099d8e4d",
      "title": "Google Introduces Nano Banana Pro with Grounded, Multimodal Image Synthesis",
      "url": "https://www.infoq.com/news/2025/12/nano-banana-pro/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global",
      "content": "<img src=\"https://res.infoq.com/news/2025/12/nano-banana-pro/en/headerimage/generatedHeaderImage-1764673167516.jpg\" alt=\"generatedHeaderImage-1764673167516.jpg\"><p>Google has released Nano Banana Pro. The system moves beyond conventional diffusion workflows by tightly coupling image generation with Gemini’s multimodal reasoning stack. The result: visuals that are not only aesthetically pleasing, but structurally, contextually, and informationally accurate.</p> <i>By Robert Krzaczyński</i>",
      "summary": "Google has released Nano Banana Pro. The system moves beyond conventional diffusion workflows by tightly coupling image generation with Gemini’s multimodal reasoning stack. The result: visuals that are not only aesthetically pleasing, but structurally, contextually, and informationally accurate. By Robert Krzaczyński",
      "publishedAt": "2025-12-02T11:52:00.000Z",
      "author": "Robert Krzaczyński",
      "source": "rss",
      "feedName": "InfoQ",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 7.436264496303309,
      "ingestedAt": "2025-12-02T14:44:03.188Z",
      "tags": [
        "code_review",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099ba658",
      "title": "Writing good docs for agents 🤖, Siri SVP retires ✖️, why use React 🤔",
      "url": "https://www.inoreader.com/article/3a9c6e76b29059a7",
      "content": "<div class=\"email_is_html\"><div><div>    <div style=\"display: none; max-height: 0px; overflow: hidden\">GitHub analyzed 2,500+ `agents.md` files and found the successful ones were specialists with clear jobs, not vague helpers ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ </div> <div style=\"display: none; max-height: 0px; overflow: hidden\"> <br /> </div>  <table align=\"center\"><tbody><tr><td valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"600\"><tbody><tr><td>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td>  <table width=\"100%\"><tbody><tr><td>    <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" style=\"margin-top: 0px\" width=\"100%\"><tbody><tr><td style=\"padding: 0px\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div style=\"text-align: center\"> <span style=\"margin-right: 0px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fdev%3Futm_source=tldrdev/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/6r4x_rQRiSJvqAwxJ6b1-xt5pHQHZ93C1dZYesm-lZQ=433\" rel=\"noreferrer\" target=\"_blank\"><span>Sign Up</span></a> |<span style=\"margin-right: 2px; margin-left: 2px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisetopnav/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/QJsEVCDPeQzRLPN0b3hqlXpVRlsE9m9PLzN2F7JE3uA=433\" rel=\"noreferrer\" target=\"_blank\"><span>Advertise</span></a></span>|<span style=\"margin-left: 2px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa.tldrnewsletter.com%2Fweb-version%3Fep=1%26lc=8cb671ec-9621-11f0-8514-3dc2c8c251bf%26p=a281c37a-cf4a-11f0-9e8a-e70d5dfa173c%26pt=campaign%26t=1764677741%26s=76349cedec85ceb1edb7b274e40fa024646deb06d65d46e43d2fc15d0ced376c/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/R0PcCLZSddOpqC06P3WFjMJxkKMwMUaAi68qsDOwinU=433\" target=\"_blank\" rel=\"noreferrer\"><span>View Online</span></a></span> <br /> </span></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"text-align: center\"><span style=\"--darkreader-inline-color: #3db3ff; color: rgb(51, 175, 255) !important; font-size: 30px\">T</span><span style=\"font-size: 30px\"><span style=\"color: rgb(232, 192, 96) !important; --darkreader-inline-color: #e8c163; font-size: 30px\">L</span><span style=\"color: rgb(101, 195, 173) !important; --darkreader-inline-color: #6ec7b2; font-size: 30px\">D</span></span><span style=\"--darkreader-inline-color: #dd6e6e; color: rgb(220, 107, 107) !important; font-size: 30px\">R</span> <br /> </td></tr></tbody></table>  <br />  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"center\" height=\"20\" style=\"vertical-align: middle !important\" valign=\"middle\" width=\"100%\"><strong style=\"vertical-align: middle !important; height: 100%\">Together With </strong> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2FzMvEri4/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/Lx-xys-f0EiVjrEn3UYt3zPsfePmF9u5CRyY_4qqzZ4=433\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://images.tldr.tech/clerk.png\" valign=\"middle\" style=\"vertical-align: middle !important; height: 100%\" alt=\"Clerk\" /></a></td></tr></tbody></table>   <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\"> <div style=\"text-align: center\">  <h1><strong> TLDR Dev <span>2025-12-02</span></strong></h1> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2FzMvEri4/2/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/uAVeD52hH6nizCEBf_1O3ag1Kq5rTJqzwr8jeMSfMBg=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Secure your backend services without the auth headaches (Sponsor)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Your microservices, background workers, and distributed systems need authentication too, but rolling your own M2M auth is a nightmare. <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2FzMvEri4/3/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/Uf_47SuADIDE98MOvlzshtPoipICSybGAUl96APCNCk=433\" rel=\"noreferrer\" target=\"_blank\"><span>Clerk's M2M tokens</span></a> are now production-ready: stable APIs, usage tracking in the dashboard, and straightforward token-based auth between any backend service. No more managing service credentials, rotating secrets, or debugging custom auth flows. Ship secure service-to-service auth in minutes, not weeks. </span> </span> </div> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr> <tr><td>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 0px\">    <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🧑‍💻</span></div> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Articles &amp; Tutorials</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot%2Fhow-to-write-a-great-agents-md-lessons-from-over-2500-repositories%2F%3Futm_source=tldrdev/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/rhlQtYbEIWnonElH8flTiumWS_GxgTChcMm0Yq7a9J4=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>How to write a great agents.md: Lessons from over 2,500 repositories (6 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     GitHub analyzed 2,500+ `agents.md` files and found the successful ones were specialists with clear jobs, not vague helpers. Good agents.md files give your agent specific commands to run, concrete code examples to follow, and explicit boundaries (like &quot;never touch these files&quot;). Start simple with one focused task like writing tests or docs, then iterate based on what breaks.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcss-tricks.com%2Fon-inheriting-and-sharing-property-values%2F%3Futm_source=tldrdev/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/xWhXBvhTXqQnrISZiG-aP47NVeWNStJ2dqAc9xfhr28=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>On Inheriting and Sharing Property Values (11 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     CSS doesn't let you set one property based on another's value (like `border-radius: height`), and there's no magic `compute()` function coming. Your best bets right now are to use CSS custom properties if you know the values, `aspect-ratio` to link width/height, `currentColor` to reuse colors, or container query units like `100cqh` to steal parent dimensions.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Faws%2Fintroducing-aws-lambda-managed-instances-serverless-simplicity-with-ec2-flexibility%2F%3Futm_source=tldrdev/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/LfJlbNRWyHV4jU4VG9_ErMwLRLPunc8_a_FuV4yXLZQ=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Introducing AWS Lambda Managed Instances: Serverless simplicity with EC2 flexibility (5 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     AWS Lambda Managed Instances is a new capability that can run AWS Lambda functions on Amazon Elastic Compute Cloud compute while maintaining serverless operational simplicity. It allows developers to access specialized compute options and optimize costs for steady-state workloads using a familiar serverless development experience. Lambda Managed Instances can be used to define how Lambda functions run on EC2 instances. Developers can just select compute profiles optimized for specific workload requirements without taking on the operational burden of managing Amazon EC2 infrastructure.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>   <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🧠</span></div> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Opinions &amp; Advice</strong></h1> </div> </div> </td></tr></tbody></table>   <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadactio.com%2Fjournal%2F22265%3Futm_source=tldrdev/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/MzPVwk3lJ97-u-JbF9bnFHiRj27kRty6HNzngzk6QBU=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Why use React? (14 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     While React makes sense as a server-side authoring tool (developers love JSX and component architecture), shipping it to the browser hurts users with unnecessary JavaScript. Frameworks like Astro let you write in React but render plain HTML to users, while Next.js defaults to the &quot;hydration&quot; pattern that sends all that server-side JavaScript to the client anyway.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fopen.substack.com%2Fpub%2Fhighgrowthengineer%2Fp%2Fmy-weekly-tech-industry-intake-routine%3Futm_source=tldrdev/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/aWdhONhDv6HMcKxJs9Re4nuXu4DcA78wMp1VmiydCng=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>My weekly tech industry intake routine as a Staff Engineer (10 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     A Staff Engineer shares his Saturday morning ritual for staying current: he batches all newsletters into a 2-3 hour block using Superhuman's split inbox (to separate articles from urgent emails) and Mailbrew (to group 60+ weekly emails into just 2 digests). His process is simple: queue interesting articles by title only, read them one by one, then immediately take action by either saving to Notion for reference or adding to Todoist as a work idea.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🚀</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Launches &amp; Tools</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2FgGLjCIO%3Futm_source=tldrdev/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/CvgAypBH7nkQI6qelp94Q7mrhaq-EXbpZlh1jo2YFVk=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Track B2B growth like you track users (Sponsor)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Your organizations are just as important as your users. Now track them the same way. Clerk's <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2FgGLjCIO/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/t5o4gyeQVL4774y4UK0pAxhlvfL82T-lPzq7W1SNMMA=433\" rel=\"noreferrer\" target=\"_blank\"><span>new Organization Growth Analytics</span></a> gives you retention, churn, and cohort analysis for every organization in your app. Daily, weekly, or monthly active orgs. Flexible filtering. Click any chart segment to drill into specific cohorts. Stop flying blind on B2B metrics. </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fabout-aws%2Fwhats-new%2F2025%2F11%2Fpreview-aws-interconnect-multicloud%2F%3Futm_source=tldrdev/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/wzDjtztkX0LLKIbIRalNermDrFkf214Y9oSl0uU25nc=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>AWS announces preview of AWS Interconnect - multicloud (1 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     AWS Interconnect - multicloud makes it easy to connect AWS networking services to other cloud service providers. It will start in preview with Google Cloud and then open up to Microsoft Azure later in 2026. The service is now available in preview in five AWS regions. Cloud service providers can easily adopt the service via a published open API package on GitHub.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Famterp%2Frad%3Futm_source=tldrdev/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/AaMqjSgHFwlupa3gWezy5S907mDg3EFotE20adZx6TE=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Rad (GitHub Repo)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Rad gives users Python-like scripting with CLI superpowers built in. It allows users to write maintainable scripts with declarative argument parsing, built-in JSON processing, and more. The project is still in early development, but it is already useful for real scripts. Rad is optimized for CLI scripting - a general-purpose language may be more appropriate for enterprise applications, high-performance computations, and projects that require specialized libraries.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>   <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🎁</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><strong><h1>Miscellaneous</h1></strong></div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fred.anthropic.com%2F2025%2Fsmart-contracts%2F%3Futm_source=tldrdev/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/QaBKt3VEaKqjUoSkc_0YIMbS6YWCaPZD5XJcSlL8GLM=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>AI agents find $4.6M in blockchain smart contract exploits (22 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Anthropic researchers evaluated AI agents' ability to exploit smart contracts on a newly created benchmark, SCONE-bench. The agents, including Claude Opus 4.5 and GPT-5, successfully exploited vulnerabilities in smart contracts, collectively amassing $4.6 million in simulated stolen funds from contracts exploited after March. In a separate simulation, the AI agents uncovered two novel zero-day vulnerabilities in recently deployed contracts, showing the technical feasibility of profitable, real-world autonomous exploitation.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Finfrequently.org%2F2025%2F11%2Fperformance-inequality-gap-2026%2F%3Futm_source=tldrdev/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/9Z-QKhjtezyIWez9cwU7vWNGTx_MdI1txY8YTYI6vao=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>The Performance Inequality Gap, 2026 (42 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Despite devices and networks improving, the performance gap between rich and poor users is still growing. For 2026, the budget is ~150KiB of HTML/CSS/fonts and ~300-350KiB of JavaScript (gzipped) to serve the global 75th percentile, but less than half of mobile sites pass Core Web Vitals because developers keep shipping way more JavaScript than reasonable.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>    <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">⚡</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Quick Links</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.apple.com%2Fnewsroom%2F2025%2F12%2Fjohn-giannandrea-to-retire-from-apple%2F%3Futm_source=tldrdev/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/NV8Z2t6NmrU9gttCQUB1gzWvgXTvBcJbguvPRuYsP8U=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>John Giannandrea to retire from Apple (7 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Apple announced that John Giannandrea will retire as senior vice president for Machine Learning and AI Strategy, and Amar Subramanya (previously part of Microsoft and ex-VP of the Gemini App at Google) will join as the new vice president of AI.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftonisagrista.com%2Fblog%2F2025%2Fgoogle-unkills-jpegxl%2F%3Futm_source=tldrdev/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/SA0hun8Tonq94TY_LwD1nEACOz1z4K3pDQ6fJt14Tos=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Google unkills JPEG XL? (6 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     After initially rejecting JPEG XL despite community support, Chromium has reversed its decision and will now support the format.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstratechery.com%2F2025%2Fgoogle-nvidia-and-openai%2F%3Futm_source=tldrdev/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/n9-VIBu6yvRtb2jw31NUuezvtn0j2CI_iWX0B3qTaxk=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Google, Nvidia, and OpenAI (20 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Google's advancements in AI with Gemini 3 and its TPU offerings are challenging the dominance of OpenAI and Nvidia, forcing them to re-evaluate their moats and strategies.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Feclecticlight.co%2F2025%2F11%2F30%2Flast-week-on-my-mac-losing-confidence%2F%3Futm_source=tldrdev/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/UX_76u-ckdsgvS3tAoe1Oh_bhAUlPMrIIVIuXtjcBoQ=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Last Week on My Mac: Losing confidence (30 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     This dev has lost confidence in macOS due to silent failures, uninformative error messages, and the resulting reliance on workarounds.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>   <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"left\" style=\"word-break: break-word; vertical-align: top; padding: 5px 10px\">  <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Love TLDR? Tell your friends and get rewards! </p> </td></tr> <tr><td style=\"padding: 0px 10px 15px\"> <div> Share your referral link below with friends to get free TLDR swag! </div> </td></tr> <tr><td align=\"left\" style=\"padding: 10px\"> <div> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frefer.tldr.tech%2F4c4dc851%2F3/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/utkTr2J_bxA0F_G6qbq30gsC7lrSqCHsc65jFI1hzzg=433\" style=\"color: #464ba4; text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">https://refer.tldr.tech/4c4dc851/3</a> </div> </td></tr> <tr></tr> <tr><td align=\"left\" style=\"padding: 5px 10px\"> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhub.sparklp.co%2Fsub_55aaf7809844%2F3/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/xiCC_5KqpKAn1uEc854BoBt-_m5kOUJd-Nn8hvyET0M=433\" style=\"font-size: 16px; line-height: 1.6; padding: 10px 0; display: inline-block; text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\"><span style=\"mso-text-raise: 13pt; text-decoration: underline\">Track your referrals here.</span></a> </td></tr></tbody></table>     <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"left\" style=\"word-break: break-word; vertical-align: top; padding: 5px 10px\">  <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Want to advertise in TLDR? 📰 </p> <div style=\"margin-top: 10px\"> If your company is interested in reaching an audience of web developers and engineering decision makers, you may want to <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/49wLzsKxlGT-gYFyDYQ4MUdY-qrbI6mMYPd8VQS8fJs=433\" target=\"_blank\" rel=\"noreferrer\"><strong><span>advertise with us</span></strong></a>. </div> <br />    <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Want to work at TLDR? 💼 </p> <div style=\"margin-top: 10px\"> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/XEwFj1wp4S5cqjYbADH8AOff4G-gnFjI_Z5_6hj9olc=433\" rel=\"noreferrer\" style=\"color: #0000EE; text-decoration: underline\" target=\"_blank\"><strong>Apply here</strong></a> or send a friend's resume to <a href=\"mailto:jobs@tldr.tech\" style=\"color: #0000EE; text-decoration: underline\" onclick=\"return rcmail.command('compose','jobs@tldr.tech',this)\" rel=\"noreferrer\">jobs@tldr.tech</a> and get $1k if we hire them! </div> <br />  <div> If you have any comments or feedback, just respond to this email! <br /> <br /> Thanks for reading, <br /> <span>Priyam Mohanty</span>, <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fin%2Fxu-jenny%2F/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/n4qI4pDCgjVGPy4d-qcxF3zSvvQH7DBwMBmZc1mO3GY=433\" target=\"_blank\" rel=\"noreferrer\"><span>Jenny Xu</span></a> &amp; <span>Ceora Ford</span> <br /> <br /> </div> <br /> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fdev%2Fmanage%3Femail=tldrai90%2540ino.to/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/EGOOcbc4vvLJ0iLOIT6U5Q6rry9t5Kr6eDhYZzyNUIQ=433\" target=\"_blank\" rel=\"noreferrer\">Manage your subscriptions</a> to our other newsletters on tech, startups, and programming. Or if TLDR Dev isn't for you, please <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa.tldrnewsletter.com%2Funsubscribe%3Fep=1%26l=e8d201ca-3e93-11ed-9a32-0241b9615763%26lc=8cb671ec-9621-11f0-8514-3dc2c8c251bf%26p=a281c37a-cf4a-11f0-9e8a-e70d5dfa173c%26pt=campaign%26pv=4%26spa=1764676905%26t=1764677741%26s=fc3fe6586fafc66300e777768f6d9636af106f1b736abf502a3f80534f4cfcd0/1/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/C8ug3qa0hAQal9cadAG19iG63sUla3OGKIIdE64I_N0=433\" target=\"_blank\" rel=\"noreferrer\">unsubscribe</a>. <br /> </div> </td></tr></tbody></table>   </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table>    <img src=\"http://tracking.tldrnewsletter.com/CI0/0100019adefd6de6-4d1e5f57-2328-45e1-995f-6dc0e0a5153c-000000/oGh7oWJrX41ywdUCklBSP3RPyV7y_pPzvVpqRyVLdpk=433\" style=\"display: none; width: 1px; height: 1px\" /> </div></div></div>",
      "summary": "    GitHub analyzed 2,500+ `agents.md` files and found the successful ones were specialists with clear jobs, not vague helpers ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌                    Sign Up |Advertise|View Online     TLDR      Together With         TLDR Dev 2025-12-02                                                                                                                     Secure your backend services without the auth ",
      "publishedAt": "2025-12-02T12:15:46.000Z",
      "author": "TLDR Dev ",
      "source": "rss",
      "feedName": "TLDR",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 16.379079846328114,
      "ingestedAt": "2025-12-02T14:44:03.188Z",
      "tags": [
        "code_review",
        "documentation",
        "agents",
        "ide",
        "observability",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099af910",
      "title": "Content cal ideas ✅, the new dashboard 📈, social connectivity 🫱🏻&zwj;🫲🏽",
      "url": "https://www.inoreader.com/article/3a9c6e76b29106ef",
      "content": "<div class=\"email_is_html\"><div><div>    <div style=\"display: none; max-height: 0px; overflow: hidden\">Strong content strategies blend category evangelism, marketing transparency, advisor collaboration, first-party data, and creative stunts ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ </div>  <div style=\"display: none; max-height: 0px; overflow: hidden\"> <br /> </div>  <table align=\"center\"><tbody><tr><td valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"600\"><tbody><tr><td>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td>  <table width=\"100%\"><tbody><tr><td>    <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" style=\"margin-top: 0px\" width=\"100%\"><tbody><tr><td style=\"padding: 0px\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div style=\"text-align: center\"> <span style=\"margin-right: 0px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fmarketing%3Futm_source=tldrmarketing/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/LDuo_2No_4Ao5B5fwBLsgvxEVACkhlKy9k8W3SXn4rs=433\" rel=\"noreferrer\" target=\"_blank\"><span>Sign Up</span></a> |<span style=\"margin-right: 2px; margin-left: 2px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%3Futm_source=tldrmarketing%26utm_medium=newsletter%26utm_campaign=advertisetopnav/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/s6lALSRN8PKtnfcYlfwOZGjV2_98w4MWn0128dBgeXs=433\" rel=\"noreferrer\" target=\"_blank\"><span>Advertise</span></a></span>|<span style=\"margin-left: 2px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa.tldrnewsletter.com%2Fweb-version%3Fep=1%26lc=8cc4c5f8-9621-11f0-9ca9-63bc73b21b46%26p=d0b7f982-cf39-11f0-87b3-cb95d47fe381%26pt=campaign%26t=1764677494%26s=09f848de9dd923a9ab9b66390ab6e58535713cfb1b577def5ff1af650c94610b/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/lDw5GYc5j2TPrem5eM4Znrwi0JGup9yP-VBBzlYaKec=433\" target=\"_blank\" rel=\"noreferrer\"><span>View Online</span></a></span> <br /> </span></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"text-align: center\"><span style=\"--darkreader-inline-color: #3db3ff; color: rgb(51, 175, 255) !important; font-size: 30px\">T</span><span style=\"font-size: 30px\"><span style=\"color: rgb(232, 192, 96) !important; --darkreader-inline-color: #e8c163; font-size: 30px\">L</span><span style=\"color: rgb(101, 195, 173) !important; --darkreader-inline-color: #6ec7b2; font-size: 30px\">D</span></span><span style=\"--darkreader-inline-color: #dd6e6e; color: rgb(220, 107, 107) !important; font-size: 30px\">R</span> <br /> </td></tr></tbody></table>  <br />  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"center\" height=\"20\" style=\"vertical-align: middle !important\" valign=\"middle\" width=\"100%\"><strong style=\"vertical-align: middle !important; height: 100%\">Together With </strong> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fasana.com%2Fcampaign%2Fmarketing%3Futm_source=tldr%26utm_medium=pd_newsletter_cev%26utm_campaign=namer_us_en_cev_signup_nltrtxt_tldr_newslettertxt-conversion_icp-pub%26utm_term=en_marketing_0_lp_mkgq3_v1long_cstm_cstm_tryfree_0x0_0_us-tldr-corp-ent-vert%26utm_content=namer_us_en_cev_signup_nltrtxt_dir_tldr_prsp_3p_icp-marketing-subscribers_all_0x0_newslettertxt-primary-120225/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/HApCtXMTplTb6D0tRGyNgdtfZB0VvXoA54AZDCwR9FQ=433\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://images.tldr.tech/asana.png\" valign=\"middle\" style=\"vertical-align: middle !important; height: 100%\" alt=\"Asana\" /></a></td></tr></tbody></table>   <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\"> <div style=\"text-align: center\">  <h1><strong>TLDR Marketing <span>2025-12-02</span></strong></h1> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fasana.com%2Fcampaign%2Fmarketing%3Futm_source=tldr%26utm_medium=pd_newsletter_cev%26utm_campaign=namer_us_en_cev_signup_nltrtxt_tldr_newslettertxt-conversion_icp-pub%26utm_term=en_marketing_0_lp_mkgq3_v1long_cstm_cstm_tryfree_0x0_0_us-tldr-corp-ent-vert%26utm_content=namer_us_en_cev_signup_nltrtxt_dir_tldr_prsp_3p_icp-marketing-subscribers_all_0x0_newslettertxt-primary-120225/2/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/jgjkL58dh9ySoCAm_5Mls2EiLhMnB7ZpiIjjnQVrwHo=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Conquer the Q4 campaign chaos with Asana (Sponsor)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Marketers everywhere are feeling the pressure: launch more campaigns, ship them faster, and do it all with fewer resources. <p></p><p>If you need a moment for a primal scream, we understand. But here's the good news: There's a way to do more with less, and it's Asana. </p><p>Asana is the work platform where human + AI collaboration comes together, helping marketing teams to plan more smoothly, speed up campaign production, and reduce busywork—without adding headcount. </p><p><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fasana.com%2Fcampaign%2Fmarketing%3Futm_source=tldr%26utm_medium=pd_newsletter_cev%26utm_campaign=namer_us_en_cev_signup_nltrtxt_tldr_newslettertxt-conversion_icp-pub%26utm_term=en_marketing_0_lp_mkgq3_v1long_cstm_cstm_tryfree_0x0_0_us-tldr-corp-ent-vert%26utm_content=namer_us_en_cev_signup_nltrtxt_dir_tldr_prsp_3p_icp-marketing-subscribers_all_0x0_newslettertxt-primary-120225/3/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/sAKGof2lS3nQ9e65oOhUocwq2vl9aGJ6Gx4rdptf7EY=433\" rel=\"noreferrer\" target=\"_blank\"><span>Learn more about how your team can boost productivity and scale campaign workflows with Asana. </span></a></p>  <p><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fasana.com%2Fcampaign%2Fmarketing%3Futm_source=tldr%26utm_medium=pd_newsletter_cev%26utm_campaign=namer_us_en_cev_signup_nltrtxt_tldr_newslettertxt-conversion_icp-pub%26utm_term=en_marketing_0_lp_mkgq3_v1long_cstm_cstm_tryfree_0x0_0_us-tldr-corp-ent-vert%26utm_content=namer_us_en_cev_signup_nltrtxt_dir_tldr_prsp_3p_icp-marketing-subscribers_all_0x0_newslettertxt-primary-120225/4/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/gWe4NQOP19OLiTywthJ3mLtBfB6Mb-Z0bfIEXsk_zaQ=433\" rel=\"noreferrer\" target=\"_blank\"><span>Try Asana for free →</span></a>   </p> </span></span></div> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr> <tr><td>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 0px\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">📱</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>News &amp; Trends</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FmzpMqj/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/gcg8u30a_qwkGuTD4_HoFmQVmzhF2jvbNi1kuiY9B20=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Gen Z Shoppers Aren't Spending Like Retailers Need Them To (5 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Gen Z is cutting holiday spending far more than other generations. They plan to reduce budgets by an average of 34% as rising rent, student loans, and everyday costs strain finances. This pullback contrasts with overall US holiday sales. Younger shoppers are hunting for deals, choosing cheaper dupes, buying secondhand, or making gifts, creating pressure for retailers who expect this group to drive nearly 20% of spending within 5 years.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.netinfluencer.com%2Fmost-instagram-users-overestimate-their-addiction-to-social-media-study-finds%2F%3Futm_source=tldrmarketing/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/G4HqohcLobN9YdWqPFr3LIma7SULsWKObD5XFfk6rLU=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Most Instagram Users Overestimate Their Addiction To Social Media Use (3 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     A large study of US Instagram users found that only 2% show clinical signs of addiction, even though 18% believe they are addicted. Most heavy use reflects habit, not addiction, yet media narratives push the addiction label and shape user perceptions. When researchers framed Instagram use as addictive, users reported lower control and more self-blame. The findings show frequent use is common, but true addiction is rare and often overstated by both users and media coverage.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🚀</span></div> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Strategies &amp; Tactics</strong></h1> </div> </div> </td></tr></tbody></table>   <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Fbrendanhufford_quiz-time-how-many-of-these-5-campaigns-activity-7401251277608886272-iJdW%3Futm_source=tldrmarketing/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/jZbn8dtkre3elwfnh5nkZ-JM5qwJhRIF72hlZooXSOE=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>5 campaigns to add to your 2026 content calendar (2 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Strong content strategies blend category evangelism, marketing transparency, advisor collaboration, first-party data, and creative stunts. Educate the market on why old methods fail, share launches and experiments, and reveal behind-the-scenes insights. Engage advisors with shareable, exclusive content, and use product and customer data to challenge trends or produce industry reports. Capture attention with playful stunts, parodies, and unexpected uses of your product.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Fkieranjflanagan_were-going-to-need-new-metrics-to-define-activity-7401259563691438083-J4ql%3Futm_source=tldrmarketing/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/s0kjlTxZDCT3RC3Sam4PCCP8xWoEIVFfmOEOe7SNkJM=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>The New Marketing Dashboard in the AI era (1 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     B2B marketing has always relied on SEO, paid ads, and automation. However, AI is reshaping those pillars. SEO is shifting into LLM visibility, paid clicks are up an estimated 50% to 100%, and budgets are moving toward harder-to-measure brand efforts. AI also unlocks automated prospecting, qualification, and seller enablement that speed deal cycles and improve close rates. To reflect this shift, teams need dashboards that track influence, demand, and revenue impact rather than legacy metrics.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🧑‍💻</span></div> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Resources &amp; Tools</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fasana.com%2Fuses%2Fcampaign-management%3Futm_source=tldr%26utm_medium=pd_newsletter_cev%26utm_campaign=namer_us_en_cev_signup_nltrtxt_tldr_newslettertxt-conversion_icp-pub%26utm_term=en_marketing_0_lp_mkgq3_v2short_cstm_cstm_tryfree_0x0_0_us-tldr-corp-ent-vert%26utm_content=namer_us_en_cev_signup_nltrtxt_dir_tldr_prsp_3p_icp-marketing-subscribers_all_0x0_newslettertxt-secondary-120225/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/P9eigW-wG_nGWleri7aXzHBeYSnunAIM5fhpxMkwcpc=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Give campaigns more bang for your buck with Asana (Sponsor)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Asana helps you keep your campaigns on track and on message for better results. <p></p><p>With Asana, you can:</p><ul><li>See the full picture across campaigns</li><li>Review and approve assets</li><li>Keep stakeholders informed</li></ul><p><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fasana.com%2Fuses%2Fcampaign-management%3Futm_source=tldr%26utm_medium=pd_newsletter_cev%26utm_campaign=namer_us_en_cev_signup_nltrtxt_tldr_newslettertxt-conversion_icp-pub%26utm_term=en_marketing_0_lp_mkgq3_v2short_cstm_cstm_tryfree_0x0_0_us-tldr-corp-ent-vert%26utm_content=namer_us_en_cev_signup_nltrtxt_dir_tldr_prsp_3p_icp-marketing-subscribers_all_0x0_newslettertxt-secondary-120225/2/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/QHArP27-bzBf8p9iqM3astuPWrzIJD4Bkn6bcqdzFOo=433\" rel=\"noreferrer\" target=\"_blank\"><span>Try Asana for free →</span></a>   </p> </span></span></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fseeblindspot.com%2Fhyperlocal%2F%3Futm_source=tldrmarketing/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/Y0234qoWcEEg_q0_VrxinDZTS54H67D0hFcofUpCCOQ=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Blindspot (Platform)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Blindspot is a fully automated digital billboard platform that lets advertisers book over 2.5 million screens worldwide. Users can select specific billboards, hours, and creative, then swap content in real time based on weather, traffic, or events. The platform provides context-aware messaging, frequency control, and transparent ROI tracking, including foot traffic, web sessions, and sales lift.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.yoghurtdigital.com.au%2Finsights%2Fchatgpt-research-shopping%3Futm_source=tldrmarketing/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/mn73ttTXzkpGdWlAgkHwLn9z9MCtljpnEa7Lz5AfIjA=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>ChatGPT Just Became a Personal Shopper. Is Your Store Ready? (4 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     ChatGPT's new Shopping Research feature transforms the AI into a personal shopper. It guides users through product discovery, comparison, and decision-making. It also collects preferences, budgets, and needs, iteratively refines recommendations, and pulls live product and retailer data. The AI evaluates product pages, ranks options, provides personalized buying guides, and explains trade-offs. This makes structured, complete, and consistent product data essential. Brands need to clean and standardize data and strengthen PDPs.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🎁</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><strong><h1>Miscellaneous</h1></strong></div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.practicalecommerce.com%2Fhow-u-s-merchants-fail-e-u-consumers%3Futm_source=tldrmarketing/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/swX535YndYLFB9BjYraUD0lOUCj9qSM2CXsIaPRSG-g=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>How US Merchants Fail EU Consumers (3 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     US merchants often misread Europe as one unified market. However, consumer habits, languages, and trust cues vary widely. Germany favors detailed specs, strict return clarity, and payment after delivery. France prioritizes native language, brand storytelling, and culturally aligned presentation. The Nordics expect seamless mobile commerce and clear sustainability details, while Southern Europe is more price sensitive and focused on reliable delivery. Merchants need to tailor payments, logistics, and branding to each country.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F12%2F01%2Famazons-ai-chatbot-rufus-drove-sales-on-black-friday%2F%3Futm_source=tldrmarketing/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/Wl5AR1Iy5-QNvtjv5QhjQubCwv07ebV3NfjkkoiUQtA=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Amazon's AI chatbot Rufus drove sales on Black Friday (2 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     AI-powered shopping sessions delivered strong results on Black Friday. Amazon sessions using Rufus doubled conversion rates compared to non-AI sessions. Broader retail data showed similar patterns, with AI-assisted shoppers 38% more likely to buy. These tools were especially influential in high-volume categories like electronics and toys.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>    <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">⚡</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Quick Links</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F9ZQ5nq/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/nzjXjT27J7VyeVbBFeGR80Epu6wZO-N5a-dALuxBXis=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>The role of social connectivity (1 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     People give the biggest sums when they feel their gifts add meaning to their own lives.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Fgeorgerterry_5-ways-to-stop-the-scroll-ugcPost-7401182738596143104-eYbH%3Futm_source=tldrmarketing/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/fjk11OF0XtH1FSDw7xHbkQIksx3doQ9h9mh-PSIaiVQ=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>5 ways to stop the scroll (2 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Tactics include using familiar faces, animals, and massive headlines.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>     <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"left\" style=\"word-break: break-word; vertical-align: top; padding: 5px 10px\">  <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Love TLDR? Tell your friends and get rewards! </p> </td></tr> <tr><td style=\"padding: 0px 10px 15px\"> <div> Share your referral link below with friends to get free TLDR swag! </div> </td></tr> <tr><td align=\"left\" style=\"padding: 10px\"> <div> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frefer.tldr.tech%2Fefeb4d94%2F5/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/j_qCI4G_k_u8WJ_iITr4RXhIdSBvU__mbQw0sFtho9c=433\" style=\"color: #464ba4; text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">https://refer.tldr.tech/efeb4d94/5</a> </div> </td></tr> <tr></tr> <tr><td align=\"left\" style=\"padding: 5px 10px\"> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhub.sparklp.co%2Fsub_4f1ca42d1a50%2F5/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/nkV2OkgdyrvJjq-yM-B9FN-mAmFiteknCXWHbFGCakE=433\" style=\"font-size: 16px; line-height: 1.6; padding: 10px 0; display: inline-block; text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\"><span style=\"mso-text-raise: 13pt; text-decoration: underline\">Track your referrals here.</span></a> </td></tr></tbody></table>     <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"left\" style=\"word-break: break-word; vertical-align: top; padding: 5px 10px\">  <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Want to advertise in TLDR? 📰 </p> <div style=\"margin-top: 10px\"> If your company is interested in reaching an audience of marketing professionals and decision makers, you may want to <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrmarketing%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/2gqwWJnsxCahd69M0qiW4V7pKwlp1c7f2e4zJzjJQoA=433\" target=\"_blank\" rel=\"noreferrer\"><strong><span>advertise with us</span></strong></a>. </div> <br />    <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Want to work at TLDR? 💼 </p> <div style=\"margin-top: 10px\"> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/RlHCTYpUSBfrEe7Z9LQJ35vyMm6wzYdRGBPh0eRsvhQ=433\" rel=\"noreferrer\" style=\"color: #0000EE; text-decoration: underline\" target=\"_blank\"><strong>Apply here</strong></a> or send a friend's resume to <a href=\"mailto:jobs@tldr.tech\" style=\"color: #0000EE; text-decoration: underline\" onclick=\"return rcmail.command('compose','jobs@tldr.tech',this)\" rel=\"noreferrer\">jobs@tldr.tech</a> and get $1k if we hire them! </div> <br />  <div> If you have any comments or feedback, just respond to this email! <br /> <br /> Thanks for reading, <br /> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fin%2Falikoh%2F/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/RSJFi0oYIf20TTlBSl77-EE6v5a8U_IzwlJ95VyvN2w=433\" target=\"_blank\" rel=\"noreferrer\"><span>Alison Koh</span></a> &amp; <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fin%2Fmaddisalmon%2F/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/53ffr_Gz9Z1sAP18SQqPrKXJwLDMJOGnvNXRSd0d_0g=433\" target=\"_blank\" rel=\"noreferrer\"><span>Maddi Salmon</span></a> <br /> <br /> </div> <br /> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fmarketing%2Fmanage%3Femail=tldrai90%2540ino.to/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/Dni9TFPBiOUPtJBG1XL6yWuYRN16lBYSQVYO_uvCk7Y=433\" target=\"_blank\" rel=\"noreferrer\">Manage your subscriptions</a> to our other newsletters on tech, startups, and programming. Or if TLDR Marketing isn't for you, please <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa.tldrnewsletter.com%2Funsubscribe%3Fep=1%26l=7dbdbdcb-3e94-11ed-9a32-0241b9615763%26lc=8cc4c5f8-9621-11f0-9ca9-63bc73b21b46%26p=d0b7f982-cf39-11f0-87b3-cb95d47fe381%26pt=campaign%26pv=4%26spa=1764676900%26t=1764677494%26s=e2b4facd798833db1c409c2b9e1d4f76f58a516b898c6424e8de1423a24c9926/1/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/OMDo39-kKvAKTMzU2v3tZG1TgV7KP3Yq4PqGNHT29y4=433\" target=\"_blank\" rel=\"noreferrer\">unsubscribe</a>. <br /> </div> </td></tr></tbody></table>  </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table>   <img src=\"http://tracking.tldrnewsletter.com/CI0/0100019adef9a572-5fdeab61-262f-476c-b959-15888174873d-000000/iy2YhtNATW909On20GdrClqwI8oBKyILa-w3MAHkI7I=433\" style=\"display: none; width: 1px; height: 1px\" /> </div></div></div>",
      "summary": "    Strong content strategies blend category evangelism, marketing transparency, advisor collaboration, first-party data, and creative stunts ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌                     Sign Up |Advertise|View Online     TLDR      Together With        TLDR Marketing 2025-12-02                                                                                                                     Conquer the Q4 campaign c",
      "publishedAt": "2025-12-02T12:11:37.000Z",
      "author": "TLDR Marketing ",
      "source": "rss",
      "feedName": "TLDR",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 10.917138996145887,
      "ingestedAt": "2025-12-02T14:44:03.189Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099a2f11",
      "title": "OpenAI Language Translation: Pros & Cons for Enterprises",
      "url": "https://dev.to/jennamitchell/openai-language-translation-pros-cons-for-enterprises-32e8",
      "content": "<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fr87yq3bmiaxy03meh6p6.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fr87yq3bmiaxy03meh6p6.png\" alt=\"\" width=\"800\" height=\"499\"></a></p> \n \n<p>Is your organization interested in leveraging OpenAI for language translation via ChatGPT or API? OpenAI and its popular generative pre-trained models offer many benefits, but there are also some potential drawbacks to consider.</p> \n \n<p>Regardless, OpenAI and it’s popular Large Language Model (LLM) ChatGPT are cutting-edge technologies that will revolutionize the language translation industry.</p> \n \n<p>In fact, it’s happening as you read this article.</p> \n \n<h2> \n   \n   \n  Background and what you’ll learn \n</h2> \n \n<p>As language industry veterans and software developers, we have first-hand experience in harnessing artificial intelligence for natural language processing. In fact, it’s our mission at Pairaphrase to make translation fast, smart and safe for multinational enterprise organizations.</p> \n \n<p>While Machine Translation has been around since the 80’s, the growing popularity of AI-powered translation services is now amplified with the buzz surrounding ChatGPT and Agentic Translation solutions. OpenAI's advanced language model offers powerful translation capabilities and productivity gains.</p> \n \n<p>However, there are pros and cons to using OpenAI for language translation. Oftentimes, the pros and cons depend on the content you intend to translate with OpenAI.</p> \n \n<p>Familiarize yourself with the following advantages and disadvantages of translating with ChatGPT/OpenAI. As a result, you can make an informed decision for your organization.</p> \n \n<h2> \n   \n   \n  Pros of Using OpenAI for Language Translation \n</h2> \n \n<h3> \n   \n   \n  Accuracy \n</h3> \n \n<p>While there have been some concerns about accuracy and hallucinations, OpenAI's language model will oftentimes produce accurate translations. This is due to its extensive training on vast amounts of pre-existing data.</p> \n \n<p>More time is needed to understand how accurate OpenAI translation is.</p> \n \n<p>There are multiple benefits to using a highly accurate translation tool to translate text. These include:</p> \n \n<p>Decent first-draft translation</p> \n \n<p>Lack of misspellings</p> \n \n<p>Less edits required</p> \n \n<h3> \n   \n   \n  Broad Language Support \n</h3> \n \n<p>If your organization needs to translate a wide range of languages and language pairs, it’s possible with OpenAI. It supports almost any language pair your organization requires for commercial use.</p> \n \n<h3> \n   \n   \n  Speed &amp; Efficiency \n</h3> \n \n<p>Its fast production of translations (compared to pure human translation) makes OpenAI suitable for real-time translation needs or situations that require high-volume translation.</p> \n \n<p>You can integrate the OpenAI API into your organization’s other software applications such as <a href=\"https://www.pairaphrase.com/blog/translation-management-systems\">Translation Management Systems</a> (TMS) and Content Management Systems (CMS). All you need is the API key.</p> \n \n<p>Alternatively, you can use translation software that comes with ChatGPT built into the application.</p> \n \n<h3> \n   \n   \n  Contextual Understanding \n</h3> \n \n<p>Want to produce coherent and contextually accurate translations? This is possible due to OpenAI's ability to comprehend and interpret the context of the text being translated.</p> \n \n<p>While this is true in many instances, it’s important to read the “cons” list in this article to grasp the nuances of this.</p> \n \n<p>OpenAI uses large language models (LLMs). This language model makes predictions based on the large amounts of data on which it’s been pre-trained. Compare this with Natural Language Processing (NLP) which uses the immediate context of your text to generate a translation.</p> \n \n<h3> \n   \n   \n  Continuous Improvement \n</h3> \n \n<p>OpenAI's language models are continuously updated and refined, ensuring that translation quality can improve over time as the model learns from new data. Continuous improvement means your organization will need to spend less hours translating as time moves forward.</p> \n \n<h3> \n   \n   \n  Ideal for Consumer-Facing Content \n</h3> \n \n<p>When you use OpenAI for language translation, you can expect it to be less “dry” than word-for-word translation. This makes it great for consumer-facing text and entertainment content. In other words, it’s ideal for transcreation.</p> \n \n<p>Whether you’re translating fictional literature, video scripts or advertising copy, OpenAI is a good option.</p> \n \n<h2> \n   \n   \n  Cons of Using OpenAI for Language Translation \n</h2> \n \n<h3> \n   \n   \n  OpenAI May Not Be Ideal for Technical Translation \n</h3> \n \n<p>Need to develop translations for technical concepts or straight-forward texts? Don’t use OpenAI for that. Based on our professional experience with various machine translation engines, we recommend Google and Microsoft’s engines for those types of translation projects.</p> \n \n<p>Google and Microsoft stick to word-for-word translations as per traditional translation methodology. It shouldn’t be treated as a replacement for traditional translation methodology.</p> \n \n<p>OpenAI is a complementary Machine Translation option, as it’s more suited for consumer-facing texts, as mentioned in the “pros” list above.</p> \n \n<p>Tip: Familiarize yourself with the best translation engine to use by content type.</p> \n \n<h3> \n   \n   \n  Non-English Translations Might Suffer \n</h3> \n \n<p>If you have a lot of content with the source language as English, OpenAI will likely generate better translations for you. This is because it’s pre-trained on mostly English due to the existence of more English language content on the internet.</p> \n \n<p>Compare this to foreign language content, of which there is naturally less abundance online. This is because there’s less volume of published content in those languages.</p> \n \n<h3> \n   \n   \n  Limited Subject Matter Expertise \n</h3> \n \n<p>While it’s trained on large datasets, be cautious when using OpenAI for translation of specialized knowledge. It will likely present knowledge gaps in your texts.</p> \n \n<p>This applies for non-technical texts, too. For example, if you were to transcreate a science fiction novel, this would involve translating scientific concepts–albeit fictional. This means even a literary translation of a fictional technical concept could be negatively affected by relying on artificial intelligence alone.</p> \n \n<h3> \n   \n   \n  Trouble with Ambiguous Text \n</h3> \n \n<p>Pay careful attention to this one if your source text includes ambiguous text made complex by cultural references, nuances or idiomatic expressions or nuances. This applies to machine translation in general (not only when you translate using OpenAI).</p> \n \n<h3> \n   \n   \n  Potential for Bias &amp; Inaccurate Data \n</h3> \n \n<p>When you translate using OpenAI, it’s important to note that its translation quality relies on the quality of the data that was used to train it. Biased or inaccurate training data may affect the accuracy and reliability of OpenAI's translations.</p> \n \n<p>This is a well-known weakness of OpenAI, and it applies for more use cases than just translation.</p> \n \n<h3> \n   \n   \n  Security &amp; Privacy Concerns \n</h3> \n \n<p>When you use OpenAI/ChatGPT, it warns you not to enter confidential information into the chatbot.</p> \n \n<p>If you’re going to translate sensitive or confidential information, make sure you use OpenAI via a translation management system that doesn’t return data to machine translation engines. This means your data won’t be sent back to OpenAI.</p> \n \n<p>Try Pairaphrase for secure ChatGPT translations.</p> \n \n<h3> \n   \n   \n  Can't Translate a Scanned PDF Document \n</h3> \n \n<p>ChatGPT itself cannot translate a scanned document on its own without help from external tools. It doesn't have the capability to extract the text from the scanned document for translation.</p> \n \n<p>However, you can accomplish this with an AI PDF translator such as Pairaphrase.</p> \n \n<p>It is translation software integrated with ChatGPT AND it includes optical character recognition (OCR). This means you can extract the text using OCR and translate your scanned document, all without leaving the Pairaphrase application.</p> \n \n<h2> \n   \n   \n  Key Takeaway \n</h2> \n \n<p>AI-assisted translation can completely transform your organization’s productivity.</p> \n \n<p>However, it’s important to always use human oversight when using AI. If you decide to use OpenAI/ChatGPT for translation, make sure you fully understand your organization’s policies on data security and quality assurance.</p> \n \n<p>And remember, you might be better off using other translation engines for technical translations.</p> \n \n<p>Dive Deeper: Explore the 17 best AI translators for enterprise teams</p> \n \n<p>Now that you understand OpenAI's translation strengths and weaknesses, we'll recommend a solution below that gives you the best of both LLM translation and non-LLM translation in a secure environment.</p> \n \n<h2> \n   \n   \n  Conclusion: \n</h2> \n \n<p>OpenAI’s translation capabilities offer impressive speed, adaptability and creativity, making them a strong option for organizations seeking to enhance productivity and improve consumer-facing content. However, these benefits come with important considerations—ranging from accuracy limitations in technical domains to privacy concerns and variability in non-English performance. With the right safeguards, human oversight, and a clear understanding of when OpenAI is (and isn’t) the best tool for the job, organizations can harness AI translation effectively. Ultimately, the strongest approach pairs LLM-powered innovation with traditional machine translation and secure translation workflows to achieve the best balance of quality, reliability and data protection.</p> \n \n<p><em>This content was originally published <a href=\"https://www.pairaphrase.com/blog/openai-language-translation\">here</a></em></p>",
      "summary": " \n \nIs your organization interested in leveraging OpenAI for language translation via ChatGPT or API? OpenAI and its popular generative pre-trained models offer many benefits, but there are also some potential drawbacks to consider. \n \nRegardless, OpenAI and it’s popular Large Language Model (LLM) ChatGPT are cutting-edge technologies that will revolutionize the language translation industry. \n \nIn fact, it’s happening as you read this article. \n \n \n   \n   \n  Background and what you’ll learn \n \n",
      "publishedAt": "2025-12-02T11:50:06.000Z",
      "author": "Jenna Mitchell",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 12.888310386919024,
      "ingestedAt": "2025-12-02T14:44:03.189Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099a2f24",
      "title": "Building a Blockchain in 2026: From-Scratch Engineering vs. Modern SDKs",
      "url": "https://dev.to/thevenice/building-a-blockchain-in-2026-from-scratch-engineering-vs-modern-sdks-34jn",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fpzadp8c025t6dbj464f2.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>The blockchain ecosystem has evolved dramatically since Bitcoin’s launch in 2009. In the early years, designing a blockchain meant implementing every component yourself: networking, consensus, block validation, mining, difficulty adjustment, peer discovery, and wallet logic. Today, developers have access to mature frameworks such as Cosmos SDK, Substrate, Polygon CDK, and the OP Stack. These frameworks abstract many of the lower-level components, allowing teams to focus on business logic instead of reinventing infrastructure.</p>  \n  \n<p>This raises an important question for engineers: <strong>Is it still worth building a blockchain entirely from scratch, like Bitcoin, or is relying on an SDK the practical approach in 2026?</strong><br>  \nThis blog breaks down the engineering reality.</p>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>1. What “from scratch” really means in blockchain engineering</strong>  \n</h1>  \n  \n<p>Many people think building a blockchain from scratch means creating new block types and hashing them. In reality, a “true from-scratch blockchain” requires implementing several subsystems:</p>  \n  \n<h3>  \n    \n    \n  <strong>1.1 Network Layer</strong>  \n</h3>  \n  \n<p>Responsible for:</p>  \n  \n<ul>  \n<li>Peer discovery</li>  \n<li>Message propagation</li>  \n<li>Gossip protocol</li>  \n<li>Maintaining network topology</li>  \n<li>Flood protection</li>  \n<li>Peer scoring</li>  \n<li>Bandwidth control</li>  \n</ul>  \n  \n<p>This layer alone can require months of engineering effort.</p>  \n  \n<h3>  \n    \n    \n  <strong>1.2 Consensus Algorithm</strong>  \n</h3>  \n  \n<p>From scratch means you implement:</p>  \n  \n<ul>  \n<li>Proof-of-Work or Proof-of-Stake logic</li>  \n<li>Block proposal, voting, and finality</li>  \n<li>Fork choice rules</li>  \n<li>Difficulty or validator rotation</li>  \n<li>Security assumptions</li>  \n<li>Recovery logic</li>  \n</ul>  \n  \n<p>Bitcoin uses PoW with a simple longest-chain rule, while modern chains often use BFT-style consensus, which is significantly more complex.</p>  \n  \n<h3>  \n    \n    \n  <strong>1.3 Mempool &amp; Transaction Lifecycle</strong>  \n</h3>  \n  \n<p>You need to define:</p>  \n  \n<ul>  \n<li>Transaction format</li>  \n<li>Validation logic</li>  \n<li>Prioritization rules</li>  \n<li>Fee market design</li>  \n<li>Relay policies</li>  \n</ul>  \n  \n<p>This becomes even more challenging if you support smart contracts.</p>  \n  \n<h3>  \n    \n    \n  <strong>1.4 State Machine</strong>  \n</h3>  \n  \n<p>Either a:</p>  \n  \n<ul>  \n<li>UTXO model (like Bitcoin)</li>  \n<li>Account model (like Ethereum)</li>  \n<li>Hybrid or custom model</li>  \n</ul>  \n  \n<p>State transition functions must be deterministic, secure, and efficient.</p>  \n  \n<h3>  \n    \n    \n  <strong>1.5 Cryptography</strong>  \n</h3>  \n  \n<p>A custom chain requires decisions about:</p>  \n  \n<ul>  \n<li>Signature schemes</li>  \n<li>Hashing algorithms</li>  \n<li>Merkle tree construction</li>  \n<li>Key management</li>  \n<li>Wallet compatibility</li>  \n</ul>  \n  \n<p>Bitcoin uses ECDSA secp256k1 and SHA-256 double hashing. Many chains today use Ed25519, BLS signatures, or STARK/STARK-friendly hash functions.</p>  \n  \n<h3>  \n    \n    \n  <strong>1.6 Storage &amp; Database Layers</strong>  \n</h3>  \n  \n<p>You must implement or integrate:</p>  \n  \n<ul>  \n<li>Block storage</li>  \n<li>Indexing</li>  \n<li>State database</li>  \n<li>Pruning</li>  \n<li>Archival modes</li>  \n</ul>  \n  \n<p>Ethereum, for example, uses LevelDB/RocksDB with complex trie structures optimized over years.</p>  \n  \n<h3>  \n    \n    \n  <strong>1.7 Node Instrumentation</strong>  \n</h3>  \n  \n<p>A production-grade blockchain node needs:</p>  \n  \n<ul>  \n<li>RPC server</li>  \n<li>WebSocket endpoints</li>  \n<li>Debug API</li>  \n<li>Logging and metrics</li>  \n<li>Prometheus integration</li>  \n<li>Monitoring and telemetry</li>  \n</ul>  \n  \n<p>Without these, external developers cannot build on your chain.</p>  \n  \n<h3>  \n    \n    \n  <strong>1.8 Wallet Infrastructure</strong>  \n</h3>  \n  \n<p>You need to provide:</p>  \n  \n<ul>  \n<li>Keypair generation</li>  \n<li>Signing utilities</li>  \n<li>Address formats</li>  \n<li>Hardware wallet integrations</li>  \n<li>Client libraries</li>  \n</ul>  \n  \n<p>Building a blockchain from scratch means building all of this.</p>  \n  \n<p>It is fundamentally a distributed systems engineering project, not a Crypto Twitter idea.</p>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>2. Why SDKs exist (and why most chains use them)</strong>  \n</h1>  \n  \n<p>Modern blockchain SDKs exist to save developers from rebuilding components that have already been solved millions of times.</p>  \n  \n<p>A framework like Cosmos SDK or Substrate already includes:</p>  \n  \n<ul>  \n<li>Peer-to-peer networking</li>  \n<li>Production-tested consensus</li>  \n<li>RPC and gRPC servers</li>  \n<li>Modular governance</li>  \n<li>IBC or cross-chain communication</li>  \n<li>Staking and slashing</li>  \n<li>WASM or EVM execution environments</li>  \n<li>State management and storage layers</li>  \n</ul>  \n  \n<p>Using an SDK provides:</p>  \n  \n<ul>  \n<li>Faster development times</li>  \n<li>Safer consensus implementation</li>  \n<li>Easier upgrades</li>  \n<li>Existing toolchains</li>  \n<li>Wallet compatibility</li>  \n<li>Easier validator bootstrapping</li>  \n</ul>  \n  \n<p>Today, more than 90% of new blockchains are built on one of these frameworks. Entire L1 ecosystems such as Osmosis, Sei, Evmos, Injective, and dYdX are built using the Cosmos SDK.</p>  \n  \n<p>The reason is simple: <strong>developers want to build chains, not reinvent distributed networking from scratch.</strong></p>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>3. The dependency misconception</strong>  \n</h1>  \n  \n<p>A common concern is:<br>  \n“If I use Cosmos SDK or Substrate, am I dependent on their codebase?”</p>  \n  \n<p>The truth is more nuanced. SDKs are <em>open-source frameworks</em>, not proprietary platforms.<br>  \nYou can:</p>  \n  \n<ul>  \n<li>Fork the code</li>  \n<li>Modify modules</li>  \n<li>Replace consensus</li>  \n<li>Build custom state machines</li>  \n<li>Write new modules in Go or Rust</li>  \n</ul>  \n  \n<p>This is no different from building a web server on top of Linux instead of writing your own operating system.<br>  \nYou are not dependent; you are leveraging battle-tested engineering.</p>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>4. When it makes sense to build from scratch</strong>  \n</h1>  \n  \n<p>Despite the complexity, there are legitimate reasons to build your own blockchain from zero.</p>  \n  \n<h3>  \n    \n    \n  <strong>4.1 You are designing new consensus</strong>  \n</h3>  \n  \n<p>For example:</p>  \n  \n<ul>  \n<li>A novel PoW algorithm</li>  \n<li>ASIC-resistant mining</li>  \n<li>New BFT variants</li>  \n<li>DAG-based systems</li>  \n<li>Order-fairness consensus like Pulp Systems or Wimble</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  <strong>4.2 You need a fully custom architecture</strong>  \n</h3>  \n  \n<p>Examples:</p>  \n  \n<ul>  \n<li>Bitcoin’s UTXO model</li>  \n<li>Nano’s block-lattice</li>  \n<li>Kadena’s braided chains</li>  \n<li>Solana’s proof-of-history timestamping</li>  \n<li>Chia’s proof-of-space</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  <strong>4.3 You want full control of networking</strong>  \n</h3>  \n  \n<p>Especially relevant for:</p>  \n  \n<ul>  \n<li>IoT blockchains</li>  \n<li>On-premises or air-gapped systems</li>  \n<li>Ultra-low-latency trading chains</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  <strong>4.4 You are building a research chain or academic project</strong>  \n</h3>  \n  \n<p>Building from scratch is the best way to understand blockchain internals at a low level.</p>  \n  \n<h3>  \n    \n    \n  <strong>4.5 You want to avoid existing technical debt</strong>  \n</h3>  \n  \n<p>Some teams believe building a new stack is better than inheriting old architectural compromises.</p>  \n  \n<p>But these are specific cases, not general needs.</p>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>5. When using an SDK or framework is the right decision</strong>  \n</h1>  \n  \n<p>For 99% of modern blockchain projects, the goal is:</p>  \n  \n<ul>  \n<li>Launching a chain that supports smart contracts</li>  \n<li>Providing cheap blockspace</li>  \n<li>Enabling developers to build apps</li>  \n<li>Creating custom tokenomics or modules</li>  \n<li>Interoperability with other ecosystems</li>  \n<li>Setting up validators and RPC infrastructure</li>  \n</ul>  \n  \n<p>If this describes your use case, writing your own P2P networking library or consensus engine provides no additional value.</p>  \n  \n<h3>  \n    \n    \n  The Priority in 2026 is:  \n</h3>  \n  \n<p>Fewer teams care about reinventing block propagation.<br>  \nThey care about:</p>  \n  \n<ul>  \n<li>throughput</li>  \n<li>tooling</li>  \n<li>developer experience</li>  \n<li>stability</li>  \n<li>interoperability</li>  \n<li>predictable upgrades</li>  \n</ul>  \n  \n<p>And these are areas where SDKs excel.</p>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>6. The engineering cost of “from scratch”</strong>  \n</h1>  \n  \n<p>If your goal is a production-grade chain, here are realistic numbers:</p>  \n  \n<h3>  \n    \n    \n  <strong>Team Requirement</strong>  \n</h3>  \n  \n<ul>  \n<li>3–6 core protocol engineers</li>  \n<li>2 distributed systems engineers</li>  \n<li>1–2 cryptographers</li>  \n<li>3 devops + SRE for network ops</li>  \n<li>2 QA engineers</li>  \n<li>1 project manager</li>  \n<li>1 documentation engineer</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  <strong>Timeframe</strong>  \n</h3>  \n  \n<ul>  \n<li>Minimum 18–36 months for mainnet</li>  \n<li>Additional 12 months for tooling, wallets, SDKs</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  <strong>Cost Breakdown</strong>  \n</h3>  \n  \n<ul>  \n<li>Engineering salaries (core protocol): $2–5M</li>  \n<li>Infrastructure (testnet &amp; mainnet): $200k+</li>  \n<li>Security audits: $300k–$1M</li>  \n<li>Maintenance: ongoing costs</li>  \n</ul>  \n  \n<p>This is why frameworks dominate.<br>  \nNot because developers are “lazy”, but because <strong>rebuilding the entire Bitcoin stack is prohibitively expensive unless you are innovating at the protocol level.</strong></p>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>7. The hybrid approach: what most serious chains do today</strong>  \n</h1>  \n  \n<p>Many successful blockchains take an in-between approach:</p>  \n  \n<ul>  \n<li>Use an SDK for networking and consensus</li>  \n<li>Replace or extend the execution environment</li>  \n<li>Write custom modules</li>  \n<li>Write a custom mempool or fee mechanism</li>  \n<li>Modify state machine logic</li>  \n<li>Add their own governance logic</li>  \n</ul>  \n  \n<p>Examples:</p>  \n  \n<ul>  \n<li>dYdX replaced CosmWasm with a custom Rust engine</li>  \n<li>Celestia uses Cosmos SDK but created a new DA layer</li>  \n<li>Aptos built a new VM but reused classical networking</li>  \n<li>Arbitrum Orbit uses OP Stack + custom proving logic</li>  \n<li>Sei modified mempool design for parallel execution</li>  \n</ul>  \n  \n<p>You avoid rewriting everything while still achieving deep differentiation.</p>  \n  \n  \n  \n  \n<h1>  \n    \n    \n  <strong>8. Conclusion</strong>  \n</h1>  \n  \n<p>Building a blockchain from scratch is technically possible and still relevant for certain categories of projects, particularly when designing novel consensus or architectures. However, in 2026, most blockchains do not need to reimplement Bitcoin’s network or Ethereum’s fundamental components. The ecosystem has matured to the point where leveraging frameworks is not a shortcut but an industry standard.</p>  \n  \n<p>If your goal is to launch a secure, scalable chain with modern tooling, smart contract support, and interoperability, using a mature SDK is the effective path.<br>  \nIf your goal is fundamental protocol innovation or academic exploration, building from scratch remains a valuable pursuit.</p>  \n  \n<p>The key question is not whether you <em>can</em> build a blockchain from scratch, but whether it is the best use of your engineering time in an ecosystem where mature, open-source frameworks already exist.</p>",
      "summary": "The blockchain ecosystem has evolved dramatically since Bitcoin’s launch in 2009. In the early years, designing a blockchain meant implementing every component yourself: networking, consensus, block validation, mining, difficulty adjustment, peer discovery, and wallet logic. Today, developers have access to mature frameworks such as Cosmos SDK, Substrate, Polygon CDK, and the OP Stack. These frameworks abstract many of the lower-level components, allowing teams to focus on business logic instead",
      "publishedAt": "2025-12-02T11:35:02.000Z",
      "author": "Prakash Pawar",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 17.832021006399046,
      "ingestedAt": "2025-12-02T14:44:03.190Z",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "ide",
        "observability",
        "governance",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099a2f2b",
      "title": "Converted all Behat WebAPIExtension step definitions to Node.js, packaged in Webship-JS",
      "url": "https://dev.to/webshipco/converted-all-behat-webapiextension-step-definitions-to-nodejs-packaged-in-webship-js-5fh0",
      "content": "<p>Behat’s <a href=\"https://github.com/Behat/WebApiExtension\">WebAPIExtension</a> was a simple and effective way to test JSON-based APIs using Gherkin steps. On July 14, 2025, the repository was archived by its owner and became read-only, meaning it’s no longer maintained, but not abandoned in purpose.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2mqtx2r007m8ec10y8oh.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2mqtx2r007m8ec10y8oh.png\" alt=\"\" width=\"800\" height=\"481\"></a></p> \n \n<p>At Webship.co, we saw great value in the extension’s structured API-testing approach. So instead of letting it fade away, we rebuilt all of its step definitions in Node.js and integrated them directly into Webship-JS.<br> \nNow, teams can use the same clear BDD style, but with modern JavaScript tooling and active support.</p> \n \n<p><strong>Why WebAPIExtension was worth reviving</strong></p> \n \n<ul> \n<li> It made API testing readable and easy to understand.</li> \n<li>    It supported setting headers, sending JSON bodies, checking status codes, and validating responses.</li> \n<li>    It helped teams describe API behavior in a simple Given/When/Then format.</li> \n</ul> \n \n<p>We kept all these strengths, and enhanced them.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsnvbnqvf6z8671lhuxk4.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsnvbnqvf6z8671lhuxk4.png\" alt=\"\" width=\"800\" height=\"550\"></a></p> \n \n<p><strong>What’s new in Webship-js</strong></p> \n \n<p>Our rebuilt API steps now support:</p> \n \n<ul> \n<li>Setting headers and request bodies</li> \n<li>Sending all HTTP methods (GET, POST, PUT, DELETE…)</li> \n<li>Validating status codes</li> \n<li>Matching JSON responses, including nested fields</li> \n<li>Checking response headers</li> \n<li>Using matcher patterns (regex, array length, JWT, etc.)</li> \n</ul> \n \n<p>*<em>Example: *</em><br> \nSends a request (POST, PUT, etc.) to the given endpoint, using the values listed in the table as the JSON request body.<br> \n</p> \n \n<div> \n<pre><code>When I send a POST request to \"/users\" with values: \n            | name  | John Doe         | \n            | email | john@example.com | \n            | age   | 30               | \n</code></pre> \n \n</div> \n \n \n \n<p>Full documentation of the steps is available here:<br> \n<a href=\"https://webship.co/docs/webship-js/1.0.x/api-step-definitions\">https://webship.co/docs/webship-js/1.0.x/api-step-definitions</a></p> \n \n<p><strong>Conclusion</strong></p> \n \n<p>Although the original WebAPIExtension is now archived, its value continues.<br> \nWith our Node.js rebuild, Webship-JS brings it back — cleaner, faster, and actively maintained.<br> \nThe same familiar BDD API experience, but modern and ready for real projects today.</p>",
      "summary": "Behat’s WebAPIExtension was a simple and effective way to test JSON-based APIs using Gherkin steps. On July 14, 2025, the repository was archived by its owner and became read-only, meaning it’s no longer maintained, but not abandoned in purpose. \n \n \n \nAt Webship.co, we saw great value in the extension’s structured API-testing approach. So instead of letting it fade away, we rebuilt all of its step definitions in Node.js and integrated them directly into Webship-JS. \nNow, teams can use the same ",
      "publishedAt": "2025-12-02T11:27:58.000Z",
      "author": "webship.co",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "general",
      "score": 8.912885729751537,
      "ingestedAt": "2025-12-02T14:44:03.190Z",
      "tags": [
        "code_review",
        "documentation",
        "ide",
        "testing",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099a2f2f",
      "title": "From Vibe Coding to Spec-Driven Development that Slashed Dev Time",
      "url": "https://dev.to/amarpreetbhatia/from-vibe-coding-to-spec-driven-development-that-slashed-dev-time-3f1f",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fk9wstkp8j5vg86wfh6dr.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><h2>  \n    \n    \n  🚀 From 'Vibe' to Victory: How Spec-Driven Development in Kiro IDE Slashed My Dev Time by 44%  \n</h2>  \n  \n<p><strong>A deep dive into building a full-featured Agile Planning app with zero WebSocket bugs.</strong></p>  \n  \n  \n  \n  \n<p>When I first used Kiro a few months ago, I was focused entirely on <strong>vibe coding</strong>. It was fun, but chaotic. Now, after experimenting with <strong>spec-driven development (SDD)</strong>, I'm completely blown away! I strongly suspect Kiro pioneered this structured approach, which was then adopted by other coding agents.</p>  \n  \n<p>This time, I built a full-featured Agile Planning application with real-time collaboration, GitHub integration, and a collaborative whiteboard. The result?</p>  \n  \n<blockquote>  \n<p>Complex features like authentication, WebSocket management, and drag-and-drop became <strong>surprisingly straightforward</strong>.</p>  \n</blockquote>  \n  \n<p>Here's exactly why the structured <strong>\"Plan, Review, Execute\"</strong> model beats \"vibe coding\" every single time, backed by concrete numbers from the project.</p>  \n  \n  \n  \n  \n<h3>  \n    \n    \n  🤯 The Two Worlds of Coding  \n</h3>  \n  \n<h4>  \n    \n    \n  What is \"Vibe Coding\"?  \n</h4>  \n  \n<p><em>You know the drill:</em></p>  \n  \n<ol>  \n<li> Open your editor.</li>  \n<li> Start typing code.</li>  \n<li> \"I'll figure it out as I go.\"</li>  \n<li> Realize you forgot crucial edge cases.</li>  \n<li> Refactor everything (or leave it messy).</li>  \n<li> Repeat.</li>  \n</ol>  \n  \n<p>It <strong>feels</strong> productive, but it's chaos in disguise. It's building a skyscraper without a blueprint.</p>  \n  \n<h4>  \n    \n    \n  Enter: Spec-Driven Development (SDD)  \n</h4>  \n  \n<p>SDD follows a rigorous, structured approach built directly into the <strong>Kiro IDE</strong>:</p>  \n  \n<ol>  \n<li> <strong>Plan:</strong> Write detailed requirements and acceptance criteria.</li>  \n<li> <strong>Review:</strong> Create a comprehensive design document (architecture, data models, properties).</li>  \n<li> <strong>Execute:</strong> Implement with clear tasks and continuous validation against the spec.</li>  \n</ol>  \n  \n<p>I used Kiro IDE, which has built-in support for this workflow, and the difference was <strong>night and day.</strong></p>  \n  \n  \n  \n  \n<h3>  \n    \n    \n  🎯 The Project: Agile Planning Tool  \n</h3>  \n  \n<p>I built a real-time collaborative planning poker application.</p>  \n  \n<p><strong><a href=\"https://github.com/amarpreetbhatia/agile-planning-tool\">View the Code: Agile Planning Tool on GitHub</a></strong></p>  \n  \n<div><table>  \n<thead>  \n<tr>  \n<th>Core Features Implemented</th>  \n<th>Tech Stack</th>  \n</tr>  \n</thead>  \n<tbody>  \n<tr>  \n<td>  \n<strong>Real-time</strong> Planning Poker</td>  \n<td>  \n<strong>Next.js 14</strong> (App Router)</td>  \n</tr>  \n<tr>  \n<td>  \n<strong>GitHub</strong> OAuth Authentication</td>  \n<td><strong>TypeScript</strong></td>  \n</tr>  \n<tr>  \n<td>  \n<strong>WebSocket</strong> Collaboration</td>  \n<td>  \n<strong>MongoDB</strong> + Mongoose</td>  \n</tr>  \n<tr>  \n<td>Drag-and-drop Story Management</td>  \n<td><strong>Socket.IO</strong></td>  \n</tr>  \n<tr>  \n<td><strong>Collaborative Whiteboard</strong></td>  \n<td>  \n<strong>Playwright</strong> (E2E Tests)</td>  \n</tr>  \n<tr>  \n<td>External Tool Embedding (Miro, Figma)</td>  \n<td><strong>Shadcn UI</strong></td>  \n</tr>  \n</tbody>  \n</table></div>  \n  \n  \n  \n  \n<h3>  \n    \n    \n  💡 1. Why Spec-Driven Development Won: Complex Features Became Simple  \n</h3>  \n  \n<p>SDD forces you to think through the entire system <em>before</em> writing the first line of code.</p>  \n  \n<h4>  \n    \n    \n  A. Authentication (GitHub OAuth)  \n</h4>  \n  \n<div><table>  \n<thead>  \n<tr>  \n<th>Vibe Coding</th>  \n<th>Spec-Driven Development</th>  \n</tr>  \n</thead>  \n<tbody>  \n<tr>  \n<td>  \n<strong>Approach:</strong> \"I'll just add GitHub login... wait, how do I handle tokens? What about session management? Encryption?\"</td>  \n<td>  \n<strong>Approach:</strong> The spec demands defining security up front.</td>  \n</tr>  \n<tr>  \n<td>  \n<strong>Result:</strong> 2 days of debugging and insecure token handling.</td>  \n<td>  \n<strong>Result:</strong> Implementation took <strong>2 hours</strong> instead of 2 days of debugging.</td>  \n</tr>  \n</tbody>  \n</table></div>  \n  \n<p><strong>Key Requirement: Authentication</strong></p>  \n  \n<blockquote>  \n<p>Acceptance Criteria:<br>  \n3. THE System <strong>SHALL</strong> store <strong>encrypted</strong> access tokens in the database.</p>  \n</blockquote>  \n  \n<h4>  \n    \n    \n  B. Real-Time Collaboration  \n</h4>  \n  \n<div><table>  \n<thead>  \n<tr>  \n<th>Vibe Coding</th>  \n<th>Spec-Driven Development</th>  \n</tr>  \n</thead>  \n<tbody>  \n<tr>  \n<td>  \n<strong>Approach:</strong> \"Let's add Socket.IO... oh wait, how do I handle disconnections? What about race conditions?\"</td>  \n<td>  \n<strong>Approach:</strong> The spec forces planning for high-availability.</td>  \n</tr>  \n<tr>  \n<td>  \n<strong>Result:</strong> Production bugs related to race conditions and dropped connections.</td>  \n<td>  \n<strong>Result:</strong> <strong>Zero production bugs</strong> related to WebSocket management.</td>  \n</tr>  \n</tbody>  \n</table></div>  \n  \n<p><strong>Key Requirement: Real-Time Updates</strong></p>  \n  \n<blockquote>  \n<p>Acceptance Criteria:<br>  \n3. WHEN connection drops, THE System <strong>SHALL</strong> attempt reconnection with <strong>exponential backoff</strong>.</p>  \n</blockquote>  \n  \n<h4>  \n    \n    \n  C. Drag-and-Drop Story Management  \n</h4>  \n  \n<div><table>  \n<thead>  \n<tr>  \n<th>Vibe Coding</th>  \n<th>Spec-Driven Development</th>  \n</tr>  \n</thead>  \n<tbody>  \n<tr>  \n<td>  \n<strong>Approach:</strong> \"I'll use a library... which one? How do I persist order? What about conflicts?\"</td>  \n<td>  \n<strong>Approach:</strong> The spec explicitly defines conflict resolution.</td>  \n</tr>  \n<tr>  \n<td>  \n<strong>Result:</strong> Janky UX and data integrity issues upon concurrent edits.</td>  \n<td>  \n<strong>Result:</strong> Smooth drag-and-drop with <strong>zero conflicts</strong>.</td>  \n</tr>  \n</tbody>  \n</table></div>  \n  \n<p><strong>Key Requirement: Story Backlog</strong></p>  \n  \n<blockquote>  \n<p>Acceptance Criteria:<br>  \n4. THE System <strong>SHALL</strong> handle concurrent reordering conflicts gracefully.</p>  \n</blockquote>  \n  \n  \n  \n  \n<h3>  \n    \n    \n  📊 2. The Numbers: A Quantitative Victory  \n</h3>  \n  \n<p>The biggest win was the dramatic reduction in debugging and refactoring time. <strong>Upfront planning saves time, period.</strong></p>  \n  \n<h4>  \n    \n    \n  Development Time Comparison  \n</h4>  \n  \n<div><table>  \n<thead>  \n<tr>  \n<th>Approach</th>  \n<th>Planning</th>  \n<th>Coding</th>  \n<th>Debugging</th>  \n<th>Refactoring</th>  \n<th><strong>Total</strong></th>  \n</tr>  \n</thead>  \n<tbody>  \n<tr>  \n<td><strong>Vibe Coding</strong></td>  \n<td>0h</td>  \n<td>40h</td>  \n<td>30h</td>  \n<td>20h</td>  \n<td><strong>90h</strong></td>  \n</tr>  \n<tr>  \n<td><strong>Spec-Driven</strong></td>  \n<td>7h</td>  \n<td>35h</td>  \n<td><strong>5h</strong></td>  \n<td><strong>3h</strong></td>  \n<td><strong>50h</strong></td>  \n</tr>  \n<tr>  \n<td><strong>Time Saved</strong></td>  \n<td></td>  \n<td></td>  \n<td></td>  \n<td></td>  \n<td><strong>40 hours (44%)</strong></td>  \n</tr>  \n</tbody>  \n</table></div>  \n  \n<h4>  \n    \n    \n  Code Quality Metrics  \n</h4>  \n  \n<div><table>  \n<thead>  \n<tr>  \n<th>Metric</th>  \n<th>Vibe Coding (Hypothetical)</th>  \n<th>Spec-Driven (Actual Project)</th>  \n</tr>  \n</thead>  \n<tbody>  \n<tr>  \n<td>Production Bugs</td>  \n<td>15+</td>  \n<td><strong>2</strong></td>  \n</tr>  \n<tr>  \n<td>Major Refactors Needed</td>  \n<td>5 major</td>  \n<td><strong>0 major</strong></td>  \n</tr>  \n<tr>  \n<td>Test Coverage</td>  \n<td>40%</td>  \n<td><strong>85%</strong></td>  \n</tr>  \n<tr>  \n<td>Documentation</td>  \n<td>Sparse and outdated</td>  \n<td>Comprehensive (The Spec IS the documentation)</td>  \n</tr>  \n</tbody>  \n</table></div>  \n  \n  \n  \n  \n<h3>  \n    \n    \n  ✍️ 3. Testing Became Natural  \n</h3>  \n  \n<p>Because every major feature had pre-written <strong>Acceptance Criteria</strong> (AC), writing E2E tests was a mere translation process.</p>  \n  \n<p>An AC like: <em>“WHEN GitHub OAuth succeeds, THE System SHALL create a user session with GitHub profile data.”</em> directly translates into a test block.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>test</span><span>(</span><span>'</span><span>should complete full estimation workflow</span><span>'</span><span>,</span> <span>async </span><span>({</span> <span>page</span> <span>})</span> <span>=&gt;</span> <span>{</span>  \n  <span>// Requirement 2.1: Create session</span>  \n  <span>await</span> <span>page</span><span>.</span><span>click</span><span>(</span><span>'</span><span>button:has-text(\"New Session\")</span><span>'</span><span>);</span>  \n  \n  <span>// Requirement 6.3: Cast vote</span>  \n  <span>await</span> <span>page</span><span>.</span><span>click</span><span>(</span><span>'</span><span>[data-testid=\"poker-card-5\"]</span><span>'</span><span>);</span>  \n  \n  <span>// Verify: Average calculated correctly</span>  \n  <span>await</span> <span>expect</span><span>(</span><span>page</span><span>.</span><span>locator</span><span>(</span><span>'</span><span>[data-testid=\"average\"]</span><span>'</span><span>))</span>  \n    <span>.</span><span>toContainText</span><span>(</span><span>'</span><span>5</span><span>'</span><span>);</span>  \n<span>});</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p><strong>Result:</strong> 20+ E2E tests written with Playwright, providing high confidence and 100% passing smoke tests before deployment.</p>  \n  \n  \n  \n  \n<h3>  \n    \n    \n  🛠️ Tools That Made It Possible  \n</h3>  \n  \n<p>The built-in SDD support in <strong>Kiro IDE</strong> was the catalyst:</p>  \n  \n<ul>  \n<li>  \n<strong>Structured Spec Creation:</strong> Used pre-built templates for requirements and design documents.</li>  \n<li>  \n<strong>Requirement Linking:</strong> Ability to link code commits and tasks directly back to the specific requirement they fulfill.</li>  \n<li>  \n<strong>Test Integration:</strong> Running tests directly from the spec view to validate acceptance criteria.</li>  \n</ul>  \n  \n  \n  \n  \n<h3>  \n    \n    \n  🛑 Common Objections Answered  \n</h3>  \n  \n<div><table>  \n<thead>  \n<tr>  \n<th>Objection</th>  \n<th>Reality</th>  \n</tr>  \n</thead>  \n<tbody>  \n<tr>  \n<td><em>\"Specs take too long to write.\"</em></td>  \n<td>  \n<strong>False:</strong> 7 hours of planning saved 40+ hours of debugging and rework. That's a huge ROI.</td>  \n</tr>  \n<tr>  \n<td><em>\"Requirements change anyway.\"</em></td>  \n<td>  \n<strong>False:</strong> Specs make changes <em>easier</em>. You know exactly which linked components need updating.</td>  \n</tr>  \n<tr>  \n<td><em>\"This only works for big projects.\"</em></td>  \n<td>  \n<strong>False:</strong> Even a quick, 30-minute spec for a small feature saves hours of rework and edge-case fixing.</td>  \n</tr>  \n</tbody>  \n</table></div>  \n  \n  \n  \n  \n<h3>  \n    \n    \n  ✅ Final Thoughts  \n</h3>  \n  \n<p>After building this project, I can't imagine going back to \"vibe coding\" for anything serious. The structure, clarity, and confidence that spec-driven development provides is invaluable.</p>  \n  \n<p>Vibe coding <strong>feels fast</strong> but is ultimately <strong>slow</strong>.<br>  \nSpec-driven development <strong>feels slow</strong> but is ultimately <strong>fast</strong>.</p>  \n  \n<p><strong>The Bottom Line:</strong> Give the \"Plan, Review, Execute\" model a try on your next project. Your future self will thank you for the robust architecture, reduced bug count, and the sheer predictability of development.</p>  \n  \n  \n  \n  \n<p><em>What's your experience with structured development approaches? Have you tried spec-driven development? Let me know in the comments!</em></p>  \n  \n<p>#agile #typescript #testing #productivity #webdev #nextjs #react #mongodb #playwright #softwaredevelopment</p>",
      "summary": "  \n    \n    \n  🚀 From 'Vibe' to Victory: How Spec-Driven Development in Kiro IDE Slashed My Dev Time by 44%  \n  \n  \nA deep dive into building a full-featured Agile Planning app with zero WebSocket bugs.  \n  \n  \n  \n  \nWhen I first used Kiro a few months ago, I was focused entirely on vibe coding. It was fun, but chaotic. Now, after experimenting with spec-driven development (SDD), I'm completely blown away! I strongly suspect Kiro pioneered this structured approach, which was then adopted by oth",
      "publishedAt": "2025-12-02T11:16:54.000Z",
      "author": "amarpreetbhatia",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 15.341545940166995,
      "ingestedAt": "2025-12-02T14:44:03.190Z",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "testing",
        "observability",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099a2f3b",
      "title": "Fundamentals of Large Language Models: Understanding LLM Architectures",
      "url": "https://dev.to/derrickryangiggs/fundamentals-of-large-language-models-understanding-llm-architectures-446j",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F0liru52os1p4ivsw7cyr.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><h2>  \n    \n    \n  What is an LLM?  \n</h2>  \n  \n<p>An LLM (Large Language Model) is fundamentally a probabilistic model that predicts distributions over vocabulary tokens. At its core, an LLM understands a fixed set of words called a <strong>vocabulary</strong> and assigns probabilities to each word appearing in a given context. </p>  \n  \n<p>The \"Large\" in LLM refers to the <strong>number of parameters</strong> the model contains. These models can have billions or even trillions of parameters, allowing them to capture complex patterns in language. While there's no universally agreed-upon threshold for what constitutes \"large,\" modern LLMs typically range from hundreds of millions to hundreds of billions of parameters.</p>  \n  \n<h3>  \n    \n    \n  Understanding Vocabulary Size  \n</h3>  \n  \n<p>Recent research has revealed that vocabulary size plays a crucial role in LLM performance, with optimal vocabulary sizes depending on the compute budget used for training. Most modern LLMs use vocabulary sizes ranging from 30,000 to 100,000 tokens, though this varies significantly:</p>  \n  \n<ul>  \n<li>  \n<strong>BERT</strong>: 30,000 tokens (WordPiece tokenization)</li>  \n<li>  \n<strong>GPT-3</strong>: ~50,000 tokens</li>  \n<li>  \n<strong>Llama 2</strong>: 32,000 tokens</li>  \n<li>  \n<strong>RoBERTa</strong>: 50,265 tokens (Byte-Pair Encoding)</li>  \n<li>  \n<strong>T5</strong>: 32,128 tokens</li>  \n</ul>  \n  \n<p>Research suggests that many existing LLMs actually use suboptimal vocabulary sizes - for example, Llama2-70B's optimal vocabulary size should have been at least 216,000 tokens, seven times larger than its actual 32,000-token vocabulary.</p>  \n  \n<h2>  \n    \n    \n  LLM Architectures: The Three Main Types  \n</h2>  \n  \n<p>Modern transformer models use one of three fundamental architectures: encoder-only, decoder-only, or encoder-decoder (sequence-to-sequence). Each architecture is optimized for different types of tasks.</p>  \n  \n<h3>  \n    \n    \n  1. Encoder-Only Models  \n</h3>  \n  \n<p>Encoders are designed to convert sequences of words into vector representations (embeddings) that can be used for various predictive modeling tasks such as classification. These models use <strong>bidirectional attention</strong>, meaning they can look at context from both directions simultaneously.</p>  \n  \n<p><strong>Key Characteristics:</strong></p>  \n  \n<ul>  \n<li>Use bidirectional attention to access all words in the input sentence</li>  \n<li>Specialized in understanding and analyzing text</li>  \n<li>Pretrained using masked language modeling objectives</li>  \n</ul>  \n  \n<p><strong>Popular Encoder Models:</strong></p>  \n  \n<ul>  \n<li><p><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: Introduced in October 2018 by Google researchers, BERT uses 12-24 layers (depending on the variant) and dramatically improved the state of the art for many NLP tasks. The base model has 110M parameters, while the large model has 340M parameters.</p></li>  \n<li><p><strong>RoBERTa (Robustly Optimized BERT Approach)</strong>: An improved version of BERT that modifies the training procedure by removing the next-sentence prediction task, using larger mini-batches, and training on 160GB of text (10x more than BERT). It has 355M parameters in its large version.</p></li>  \n<li><p><strong>DistilBERT</strong>: A distilled version that retains 95% of BERT's performance with only 60% of its parameters (66M).</p></li>  \n<li><p><strong>DeBERTa</strong>: Uses disentangled attention mechanisms for improved performance.</p></li>  \n<li><p><strong>ModernBERT</strong>: Released in 2024 as a state-of-the-art replacement for BERT, featuring an 8,192 token sequence length and significantly faster processing.</p></li>  \n</ul>  \n  \n<p><strong>Primary Use Cases:</strong></p>  \n  \n<ul>  \n<li>Text classification (sentiment analysis, topic classification)</li>  \n<li>Named entity recognition (NER)</li>  \n<li>Question answering</li>  \n<li>Semantic similarity tasks</li>  \n<li>Embedding generation for retrieval systems</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  2. Decoder-Only Models  \n</h3>  \n  \n<p>Decoders are designed to generate new text by predicting the next word in a sequence. They use masked (causal) self-attention, which only allows tokens to attend to previous tokens in the sequence, ensuring autoregressive generation.</p>  \n  \n<p><strong>Key Characteristics:</strong></p>  \n  \n<ul>  \n<li>Use unidirectional (causal) attention</li>  \n<li>Generate text one token at a time</li>  \n<li>Excel at creative text generation and completion</li>  \n</ul>  \n  \n<p><strong>Popular Decoder Models:</strong></p>  \n  \n<ul>  \n<li><p><strong>GPT Series (GPT, GPT-2, GPT-3, GPT-4)</strong>: The GPT series pioneered the decoder-only architecture, with models ranging from 117M parameters (GPT-2 small) to hundreds of billions in GPT-4. GPT models became state of the art in natural language generation starting in 2018.</p></li>  \n<li><p><strong>Llama (1, 2, 3)</strong>: Meta's open-source models that have become foundational for many smaller LLM projects, with the Llama 2 family ranging up to 70 billion parameters.</p></li>  \n<li><p><strong>Falcon</strong>: A series of open-source models trained on refined web data.</p></li>  \n<li><p><strong>BLOOM</strong>: A multilingual decoder-only model.</p></li>  \n<li><p><strong>Mistral</strong>: High-performance open-weight models with efficient architectures.</p></li>  \n</ul>  \n  \n<p><strong>Primary Use Cases:</strong></p>  \n  \n<ul>  \n<li>Text generation and completion</li>  \n<li>Conversational AI and chatbots</li>  \n<li>Creative writing assistance</li>  \n<li>Code generation</li>  \n<li>Question answering in chat format</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  3. Encoder-Decoder Models  \n</h3>  \n  \n<p>Encoder-decoder models combine both architectures, using an encoder to understand the input and a decoder to generate appropriate output text. This makes them perfect for tasks that transform one sequence into another, like translation or summarization.</p>  \n  \n<p><strong>Key Characteristics:</strong></p>  \n  \n<ul>  \n<li>Bidirectional understanding through the encoder</li>  \n<li>Autoregressive generation through the decoder</li>  \n<li>Connected via cross-attention mechanism</li>  \n<li>Ideal for sequence-to-sequence tasks</li>  \n</ul>  \n  \n<p><strong>Popular Encoder-Decoder Models:</strong></p>  \n  \n<ul>  \n<li><p><strong>T5 (Text-to-Text Transfer Transformer)</strong>: Developed by Google, T5 treats every NLP task as a text-to-text problem, ranging from 60M to 11B parameters. It uses task-specific prefixes (e.g., \"translate English to German:\", \"summarize:\") to handle different tasks with the same architecture.</p></li>  \n<li><p><strong>BART (Bidirectional and Auto-Regressive Transformers)</strong>: Developed by Facebook (Meta) in 2019, BART combines strengths of BERT and GPT. It's pretrained by corrupting text in various ways (deleting words, shuffling sentences, masking tokens) and learning to reconstruct the original.</p></li>  \n<li><p><strong>UL2</strong>: A unified framework for language understanding and generation.</p></li>  \n<li><p><strong>mT5</strong>: Multilingual variant of T5 supporting 100+ languages.</p></li>  \n</ul>  \n  \n<p><strong>Primary Use Cases:</strong></p>  \n  \n<ul>  \n<li>Machine translation</li>  \n<li>Text summarization</li>  \n<li>Question answering with context</li>  \n<li>Text simplification</li>  \n<li>Paraphrasing</li>  \n<li>Data-to-text generation</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  The Transformer Foundation  \n</h2>  \n  \n<p>All these architectures are built on the transformer architecture, which uses self-attention mechanisms to process input sequences. The original transformer was proposed in the 2017 paper \"Attention Is All You Need\" by Google researchers.</p>  \n  \n<p><strong>Key Components of Transformers:</strong></p>  \n  \n<ol>  \n<li><p><strong>Tokenization</strong>: Converting text into discrete tokens using algorithms like Byte-Pair Encoding (BPE), WordPiece, or SentencePiece.</p></li>  \n<li><p><strong>Embeddings</strong>: Converting tokens into dense vector representations.</p></li>  \n<li><p><strong>Self-Attention Layers</strong>: Mechanisms that allow each token to attend to other tokens in the sequence, capturing contextual relationships.</p></li>  \n<li><p><strong>Feed-Forward Networks</strong>: Processing representations through neural networks.</p></li>  \n<li><p><strong>Layer Normalization</strong>: Stabilizing training and improving convergence.</p></li>  \n<li><p><strong>Positional Encodings</strong>: Adding position information since attention has no inherent notion of sequence order.</p></li>  \n</ol>  \n  \n<h2>  \n    \n    \n  Choosing the Right Architecture  \n</h2>  \n  \n<p>When selecting an architecture for a specific task, consider whether you need bidirectional understanding (encoder), text generation (decoder), or sequence transformation (encoder-decoder).</p>  \n  \n<p><strong>Decision Framework:</strong></p>  \n  \n<ul>  \n<li>  \n<strong>Need deep understanding of text?</strong> → Use encoder-only models</li>  \n<li>  \n<strong>Need to generate creative or conversational text?</strong> → Use decoder-only models</li>  \n<li>  \n<strong>Need to transform one text form to another?</strong> → Use encoder-decoder models</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  Recent Trends and Future Directions  \n</h2>  \n  \n<p>Almost every major LLM since GPT-3 has adopted the decoder-only architecture due to its simplicity and effectiveness at scale. However, encoder models remain critical for tasks like retrieval-augmented generation (RAG), classification, and entity extraction, with billions of downloads per month.</p>  \n  \n<p>Recent developments include reasoning models like OpenAI's o1 and DeepSeek-R1, which generate step-by-step analysis before producing answers, achieving better performance on complex tasks.</p>  \n  \n<p>Alternative architectures like Mamba (state space models) are emerging as potential challengers to transformer dominance, offering linear-time processing for long sequences.</p>  \n  \n<p>Understanding the fundamental differences between encoder, decoder, and encoder-decoder architectures is essential for working effectively with LLMs. Each architecture serves distinct purposes:</p>  \n  \n<ul>  \n<li>  \n<strong>Encoders</strong> excel at understanding and analyzing text</li>  \n<li>  \n<strong>Decoders</strong> shine at generating creative and coherent text</li>  \n<li>  \n<strong>Encoder-decoders</strong> bridge both worlds for transformation tasks</li>  \n</ul>  \n  \n<p>As the field continues to evolve rapidly, these foundational concepts remain crucial for anyone working with or building upon large language models.</p>  \n  \n<p><em>Are you working with LLMs in your projects? Which architecture have you found most useful for your use case? Share your experiences in the comments below</em></p>",
      "summary": "  \n    \n    \n  What is an LLM?  \n  \n  \nAn LLM (Large Language Model) is fundamentally a probabilistic model that predicts distributions over vocabulary tokens. At its core, an LLM understands a fixed set of words called a vocabulary and assigns probabilities to each word appearing in a given context.   \n  \nThe \"Large\" in LLM refers to the number of parameters the model contains. These models can have billions or even trillions of parameters, allowing them to capture complex patterns in language.",
      "publishedAt": "2025-12-02T11:15:31.000Z",
      "author": "Ryan Giggs",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 9.89709243503536,
      "ingestedAt": "2025-12-02T14:44:03.190Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099a1293",
      "title": "Can AI Deception Be Detected? Experts Weigh In - KnowTechie",
      "url": "https://news.google.com/rss/articles/CBMiU0FVX3lxTFBicU5KR1VsU1VwckZQRkdkdzFabUljRU5WWU1mOWc2NGdLZlB0dFN6OGJnRVloVUZZM1ZGZGFoSlhpb3NIcmJGdjVBdGVidGhkUDc4?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMiU0FVX3lxTFBicU5KR1VsU1VwckZQRkdkdzFabUljRU5WWU1mOWc2NGdLZlB0dFN6OGJnRVloVUZZM1ZGZGFoSlhpb3NIcmJGdjVBdGVidGhkUDc4?oc=5\">Can AI Deception Be Detected? Experts Weigh In</a>  KnowTechie",
      "summary": "Can AI Deception Be Detected? Experts Weigh In  KnowTechie",
      "publishedAt": "2025-12-02T11:34:28.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4859599818911837,
      "ingestedAt": "2025-12-02T14:44:03.190Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099a1298",
      "title": "Lyft Taps AWS and Anthropic to Launch Agentic AI - eWeek",
      "url": "https://news.google.com/rss/articles/CBMiVkFVX3lxTE1UWlhIMnIyY25wR1NSWnQ1TlhQWlRoYUpDbVVXY0pzUjFSVnF2anBaMDM5cGhlU2c5bkRLX3NqaDRjT2ZGa1JGSUVtSVlYTTZGam03RFJn?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMiVkFVX3lxTE1UWlhIMnIyY25wR1NSWnQ1TlhQWlRoYUpDbVVXY0pzUjFSVnF2anBaMDM5cGhlU2c5bkRLX3NqaDRjT2ZGa1JGSUVtSVlYTTZGam03RFJn?oc=5\">Lyft Taps AWS and Anthropic to Launch Agentic AI</a>  eWeek",
      "summary": "Lyft Taps AWS and Anthropic to Launch Agentic AI  eWeek",
      "publishedAt": "2025-12-02T11:18:20.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 9.40355154443285,
      "ingestedAt": "2025-12-02T14:44:03.190Z",
      "tags": [
        "agents",
        "Coding Agent Product Updates",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b099a129c",
      "title": "Nvidia, Microsoft invest $15 billion in AI startup Anthropic - Yahoo Finance",
      "url": "https://news.google.com/rss/articles/CBMihwFBVV95cUxQcnhFUjR1Y1pqY3BlUnJBbEZoaUpZeXRQYWR1emJkTDZTSUtYdEUzY2o3RUwzWGpjX1ZGMUVBVVJxSU93dVJadTF5dHgxZ3o3bTNQRHNNSkpjOXVKVTdaTm05X09iRjFiMXI2UmJJa3ZpRkNpcW84UWU0Um5UVHA0anh3Rk83bTA?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMihwFBVV95cUxQcnhFUjR1Y1pqY3BlUnJBbEZoaUpZeXRQYWR1emJkTDZTSUtYdEUzY2o3RUwzWGpjX1ZGMUVBVVJxSU93dVJadTF5dHgxZ3o3bTNQRHNNSkpjOXVKVTdaTm05X09iRjFiMXI2UmJJa3ZpRkNpcW84UWU0Um5UVHA0anh3Rk83bTA?oc=5\">Nvidia, Microsoft invest $15 billion in AI startup Anthropic</a>  Yahoo Finance",
      "summary": "Nvidia, Microsoft invest $15 billion in AI startup Anthropic  Yahoo Finance",
      "publishedAt": "2025-11-18T08:00:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.6226085608963907,
      "ingestedAt": "2025-12-02T14:44:03.190Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b0998fc84",
      "title": "Addressing the adding situation",
      "url": "https://xania.org/202512/02-adding-integers",
      "content": "<p>Article URL: <a href=\"https://xania.org/202512/02-adding-integers\">https://xania.org/202512/02-adding-integers</a></p> \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46120181\">https://news.ycombinator.com/item?id=46120181</a></p> \n<p>Points: 108</p> \n<p># Comments: 25</p>",
      "summary": "Article URL: https://xania.org/202512/02-adding-integers \nComments URL: https://news.ycombinator.com/item?id=46120181 \nPoints: 108 \n# Comments: 25",
      "publishedAt": "2025-12-02T11:30:29.000Z",
      "author": "messe",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.485666406037842,
      "ingestedAt": "2025-12-02T14:44:03.190Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b0995cbb3",
      "title": "DeepSeek returns 🐋, Apple AI chief quits 📱, Neuralink upgrades 🧠&nbsp;",
      "url": "https://www.inoreader.com/article/3a9c6e76b29e344c",
      "content": "<div class=\"email_is_html\"><div><div>       <div style=\"display: none; max-height: 0px; overflow: hidden\">DeepSeek has unveiled two new models. The startup claims DeepSeek-V3.2 matches the performance of OpenAI's GPT-5 across multiple reasoning benchmarks ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ </div>  <div style=\"display: none; max-height: 0px; overflow: hidden\"> <br /> </div>  <table align=\"center\"><tbody><tr><td valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"600\"><tbody><tr><td>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td>  <table width=\"100%\"><tbody><tr><td>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" style=\"margin-top: 0px\" width=\"100%\"><tbody><tr><td style=\"padding: 0px\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div style=\"text-align: center\"> <span style=\"margin-right: 0px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fsignup%3Futm_source=tldr/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/fVFO_X-D9z87Ms7mJWypBuQv4HOeZJTdSRbGRxlkybQ=434\" rel=\"noreferrer\" target=\"_blank\"><span>Sign Up</span></a> <span>|</span><span style=\"margin-right: 2px; margin-left: 2px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=advertisetopnav/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/jvcpbEaPejWLFSYYmYKKZ_XzVxyfBPmfWO8Rku2OJUk=434\" rel=\"noreferrer\" target=\"_blank\"><span>Advertise</span></a></span><span>|</span><span style=\"margin-left: 2px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa.tldrnewsletter.com%2Fweb-version%3Fep=1%26lc=7ab8e3ee-9621-11f0-8ad5-431c1b7ff1fe%26p=8d94c040-cf48-11f0-bf81-3dbde69b9a7d%26pt=campaign%26t=1764675476%26s=7311ffecc7722202e14a4e2ce193e186fa1b5d4fd45760c126f073b0f3ae86a6/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/XZhn-qnvnGBOnDxZNSYgTNALyLihsojzFjrosuLCQiA=434\" target=\"_blank\" rel=\"noreferrer\"><span>View Online</span></a></span> <br /> </span></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"text-align: center\"><span style=\"--darkreader-inline-color: #3db3ff; color: rgb(51, 175, 255) !important; font-size: 30px\">T</span><span style=\"font-size: 30px\"><span style=\"color: rgb(232, 192, 96) !important; --darkreader-inline-color: #e8c163; font-size: 30px\">L</span><span style=\"color: rgb(101, 195, 173) !important; --darkreader-inline-color: #6ec7b2; font-size: 30px\">D</span></span><span style=\"--darkreader-inline-color: #dd6e6e; color: rgb(220, 107, 107) !important; font-size: 30px\">R</span> <br /> </td></tr></tbody></table>  <br />  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"center\" height=\"20\" style=\"vertical-align: middle !important\" valign=\"middle\" width=\"100%\"><strong style=\"vertical-align: middle !important; height: 100%\">Together With </strong> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qawolf.com%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=ACQ_All_Demo_Conversions__NewsletterAudience_-_Newsletter_GoodbyeLowTestCoverage_20251202-None_Experiment-FALSE%26utm_term=headline-GoodbyeLowTestCoverageAndSlowQACycles%26utm_content=GoodbyeLowTestCoverage_ScheduleADemoToLearnMore_None_Headline%253AGoodbyeLowTestCoverageAndSlowQACycles____Newsletter-PrimaryPlacement_20251202_v1_/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/PH1c8hht16IWCfaloj9fh3eVZ16XUqvMEconKTQL4uA=434\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://images.tldr.tech/qawolf-3.png\" valign=\"middle\" style=\"vertical-align: middle !important; height: 100%\" alt=\"QA Wolf\" /></a></td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\"> <div style=\"text-align: center\">  <h1><strong>TLDR <span>2025-12-02</span></strong></h1> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qawolf.com%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=ACQ_All_Demo_Conversions__NewsletterAudience_-_Newsletter_GoodbyeLowTestCoverage_20251202-None_Experiment-FALSE%26utm_term=headline-GoodbyeLowTestCoverageAndSlowQACycles%26utm_content=GoodbyeLowTestCoverage_ScheduleADemoToLearnMore_None_Headline%253AGoodbyeLowTestCoverageAndSlowQACycles____Newsletter-PrimaryPlacement_20251202_v1_/2/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/Rzbd4OMrIeVhNZ0KTRLVMUUifPhqNI9TfCNcJC9LGyg=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Goodbye, low test coverage and slow QA cycles (Sponsor)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Bugs sneak out when less than 80% of user flows are tested before shipping. However, getting that kind of coverage (and staying there) is hard and pricey for any team.<p></p><p><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qawolf.com%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=ACQ_All_Demo_Conversions__NewsletterAudience_-_Newsletter_GoodbyeLowTestCoverage_20251202-None_Experiment-FALSE%26utm_term=body-QAWolf%26utm_content=GoodbyeLowTestCoverage_ScheduleADemoToLearnMore_None_Headline%253AGoodbyeLowTestCoverageAndSlowQACycles____Newsletter-PrimaryPlacement_20251202_v1_/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/R0h7DqvlJyYHeELeCitLa13fg9wJgcaTvXucdL5ErZo=434\" rel=\"noreferrer\" target=\"_blank\"><span>QA Wolf's</span></a> AI-native solution provides high-volume, high-speed test coverage for web and mobile apps, reducing your organization's QA cycle to minutes. </p>  <p>They can get you:</p>  <ul> <li><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qawolf.com%2Fhow-it-works%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=ACQ_All_Demo_Conversions__NewsletterAudience_-_Newsletter_GoodbyeLowTestCoverage_20251105-None_Experiment-FALSE%26utm_term=body-80PercentAutomatedE2ETestCoverageInWeeks%26utm_content=GoodbyeLowTestCoverage_ScheduleADemoToLearnMore_None_Headline%253AGoodbyeLowTestCoverageAndSlowQACycles____Newsletter-PrimaryPlacement_20251105_v1_/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/URfCxsYpp_bxuL5YCKPc8YcIy3-nNlkzTHMv1Fitd9U=434\" rel=\"noreferrer\" target=\"_blank\"><span>80% automated E2E test coverage in weeks</span></a>—not years</li> <li><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qawolf.com%2Fhow-it-works%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=ACQ_All_Demo_Conversions__NewsletterAudience_-_Newsletter_GoodbyeLowTestCoverage_20251105-None_Experiment-FALSE%26utm_term=body-UnlimitedParallelTestRuns%26utm_content=GoodbyeLowTestCoverage_ScheduleADemoToLearnMore_None_Headline%253AGoodbyeLowTestCoverageAndSlowQACycles____Newsletter-PrimaryPlacement_20251105_v1_/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/Nte-8YadKQvkyeAUfGjm8jS97ZF5ugZ5Xa2gqR8gUJs=434\" rel=\"noreferrer\" target=\"_blank\"><span>Unlimited parallel test runs</span></a></li> <li>24-hour maintenance and on-demand test creation</li> <li>Zero flakes, guaranteed</li> </ul>  <p>The benefit? No more manual E2E testing. No more slow QA cycles. No more bugs reaching production.</p>  <p>With QA Wolf, <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qawolf.com%2Fcase-studies%2Fdrata%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=ACQ_All_Demo_Conversions__NewsletterAudience_-_Newsletter_GoodbyeLowTestCoverage_20251202-None_Experiment-FALSE%26utm_term=body-DratasTeamOfEngineers%26utm_content=GoodbyeLowTestCoverage_ScheduleADemoToLearnMore_None_Headline%253AGoodbyeLowTestCoverageAndSlowQACycles____Newsletter-PrimaryPlacement_20251202_v1_/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/fMMtVe8A4QTRTH9Ctsirgx6lnuwA5BD6KJ_XWbKuNfY=434\" rel=\"noreferrer\" target=\"_blank\"><span>Drata's team of engineers</span></a> achieved 4x more test cases and <strong>86% faster QA cycles</strong>.</p>  <p><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qawolf.com%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=ACQ_All_Demo_Conversions__NewsletterAudience_-_Newsletter_GoodbyeLowTestCoverage_20251202-None_Experiment-FALSE%26utm_term=cta-ScheduleADemoToLearnMore%26utm_content=GoodbyeLowTestCoverage_ScheduleADemoToLearnMore_None_Headline%253AGoodbyeLowTestCoverageAndSlowQACycles____Newsletter-PrimaryPlacement_20251202_v1_/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/TgRO10YqDb_MCnkuswqvpODmg-X_kKZFE0SH7m3JsXA=434\" rel=\"noreferrer\" target=\"_blank\"><span>Schedule a demo to learn more</span></a>   </p> </span></span></div> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr> <tr><td>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 0px\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">📱</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Big Tech &amp; Startups</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FGGFLcz/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/IU0wHrwlcczix1uYchQINWJwOedz11cXG5Y7tlN-bSU=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Apple to Revamp AI Team After Announcing Top Executive's Departure (2 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Apple's senior vice president in charge of AI strategy has stepped down. His duties will be split among other senior vice presidents responsible for software engineering, services, and operations. Amar Subramanya, who was involved with developing Google's Gemini chatbot, will join Apple as vice president of AI. The reorganization signals a new era for Apple's AI work, which has so far failed to keep up with the industry.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fl5ysu5/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/sj9BTn7Z9uFtktQq6zSifjQ_nq6mkng7b02tCO-7EJw=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>DeepSeek Debuts New AI Models to Rival Google and OpenAI (3 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     DeepSeek has unveiled two new models. The startup claims DeepSeek-V3.2 matches the performance of OpenAI's GPT-5 across multiple reasoning benchmarks. DeepSeek-V3.2-Speciale is designed for mathematical computations and matches the performance of Google's Gemini-3 Pro. It also performed at gold medal levels on the International Math Olympiad.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>   <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🚀</span></div> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Science &amp; Futuristic Technology</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2F9to5google.com%2F2025%2F12%2F01%2Fsamsung-galaxy-z-trifold-specs-launch-us-confirmed%2F%3Futm_source=tldrnewsletter/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/qGdyP1Bz-zsR0A7I-NyA9kZF6D-C1f6QLZ2BRhCr0C4=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Samsung reveals Galaxy Z TriFold, confirms US launch in early 2026 (2 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     The Samsung Galaxy Z TriFold is set to launch in the US early next year. Its inner display measures 10 inches across. The phone's specs are overall pretty similar to the Galaxy Z Fold 7, though the battery is quite a bit bigger at 5,600 mAh. The device will come only in 'Crafted Black'. It only measures 3.9mm at its thinnest unfolded point. When folded up, the device measures 12.9mm thick, which is markedly thinner than the Galaxy Z Fold 5, which was only released two years ago.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.teslarati.com%2Fneuralink-first-patient-could-receive-an-upgrade-elon-musk%2F%3Futm_source=tldrnewsletter/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/wRJ_JETDu_UfgUF-RQmbMiaWxNxs4vLOmD5yNGNGVpM=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Neuralink's first patient could receive an upgrade (2 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Neuralink is considering an upgrade for its first human patient. Since receiving the implant, the patient has been able to play games, attend classes, and work. He is currently studying neuroscience and maintaining good grades, something that would be impossible without Neuralink. An upgrade would allow him to perform even more sophisticated tasks using only his mind.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>   <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">💻</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Programming, Design &amp; Data Science</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.datadoghq.com%2Fresources%2Fstate-of-cloud-security-2025%2F%3Futm_source=tldrnewsletter%26utm_medium=newsletter%26utm_campaign=dg-security-ww-cloud-sec-report-tldr/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/V1XYe9jS0R5zJkWyhQNuSjBC8zapcK36YLwKEUquR5k=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Datadog research: 59% of IAM users have access keys older than one year (Sponsor)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Long-lived credentials remain the most common cause of publicly documented cloud breaches — yet organizations are struggling to eliminate them. Datadog's <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.datadoghq.com%2Fresources%2Fstate-of-cloud-security-2025%2F%3Futm_source=tldrnewsletter%26utm_medium=newsletter%26utm_campaign=dg-security-ww-cloud-sec-report-tldr/2/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/FNYW5KTAOSQJm8SBi8zexjSb6z1S0nHDuXqRer6tDcU=434\" rel=\"noreferrer\" target=\"_blank\"><span>State of Cloud Security 2025</span></a> report analyzes security posture data from thousands of organizations using AWS, Azure, and Google Cloud. Get the data on credential risks, IMDSv2 adoption, overprivileged workloads, and more. <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.datadoghq.com%2Fresources%2Fstate-of-cloud-security-2025%2F%3Futm_source=tldrnewsletter%26utm_medium=newsletter%26utm_campaign=dg-security-ww-cloud-sec-report-tldr/3/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/42bMp5iLgVVPZaucePo8tZ97pSP41wyymd-VKH_hy0Y=434\" rel=\"noreferrer\" target=\"_blank\"><span>Read the full report</span></a> </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.iankduncan.com%2Fengineering%2F2025-12-01-punycode%3Futm_source=tldrnewsletter/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/NI1QjGRh_w5Z-QpvR8qJEhzCwhtt7ejvT3-ooQbf2uo=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Punycode: My New Favorite Algorithm (45 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Punycode is a special encoding used to convert Unicode characters to ASCII. It's deceptively simple on the surface, but there's real sophistication in how it manages the problems inherent in encoding arbitrary Unicode within DNS' ASCII-only infrastructure. The algorithm uses adaptive bias adjustment, variable-length encoding, and delta compression to achieve extremely dense results. This article looks at how to encode arbitrary Unicode within DNS's ASCII-only infrastructure. It provides a step-through visualizer at the end that lets readers watch the algorithm encode domains from various scripts.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.ploeh.dk%2F2025%2F12%2F01%2Ftreat-test-code-like-production-code%2F%3Futm_source=tldrnewsletter/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/TFq0fv4Kgb2MBMlg992saKfSLz0CWmsQp6rKi0SsKvc=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Treat test code like production code (4 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     The same coding standards applied to production code should also be applied to test code. Test code needs to be readable, well-factored, and reviewed, just like production code. Good code makes it easier for people working on the code to read, understand, and update. Applying coding standards and design principles to test code makes maintainability easier.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>    <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🎁</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><strong><h1>Miscellaneous</h1></strong></div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbMJ01w/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/0q680w1pKhE5siQlZAmZvd4IDRpNRY37-nSfnF4dVSE=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>College Students Flock to a New Major: AI (4 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Interest in understanding, using, and learning how to build AI technologies is soaring. Schools are racing to meet rising student and industry demand. Dozens of universities and colleges in the US have announced new AI departments, majors, minors, courses, interdisciplinary concentrations, and other programs. Students hope that the AI branding will open up more job opportunities.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.engadget.com%2Fsocial-media%2Finstagram-mandates-total-return-to-office-for-employees-in-2026-212738225.html%3Fsrc=rss%26utm_source=tldrnewsletter/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/rzMQIMqwFH1YfsRe5XvRObTYnBWXyXpeQqyXKzjQC3o=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Instagram mandates total return to office for employees in 2026 (2 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Instagram employees will be required to return to the office beginning February 2. Employees will still be able to work remotely when needed, but they have been asked to use their best judgment on when to take advantage of that flexibility. Other planned changes intended to make Instagram more nimble and creative next year include the scaling back of recurring meetings and a faster process for unblocking and decision-making. While executives insist that in-person work has important benefits, employees often disagree.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>   <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">⚡</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Quick Links</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dash0.com%2Flp%2Fopentelemetry-for-dummies%3Futm_campaign=224315594-OpenTelemetry%2520for%2520Dummies%2520Ebook%26utm_source=tldr%26utm_medium=newsletter%26utm_term=OTel%2520ebook%2520quicklinkDec2/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/t0ndN2-85yylZI9Spcbfr71YMpFf4QnQczMoD39kAiU=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Free Ebook: OpenTelemetry for Dummies (Sponsor)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Mountains of telemetry but still can't find the root cause? This Dummies Guide covers OTel SDKs, Collectors, and instrumentation techniques that you can use in production. <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dash0.com%2Flp%2Fopentelemetry-for-dummies%3Futm_campaign=224315594-OpenTelemetry%2520for%2520Dummies%2520Ebook%26utm_source=tldr%26utm_medium=newsletter%26utm_term=OTel%2520ebook%2520quicklinkDec2/2/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/oLu3knFPM1baf6MWkesu4YfuSixiNSeTvod8wpo60Nw=434\" rel=\"noreferrer\" target=\"_blank\"><span>Get your free copy (Dash0 Special Edition)</span></a> </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fabout-aws%2Fwhats-new%2F2025%2F11%2Fpreview-aws-interconnect-multicloud%2F%3Futm_source=tldrnewsletter/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/vB6Mo6MFbJAxLYUNSzmuwd40Se-E3ZC1oKQbdlFKdaY=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>AWS announces preview of AWS Interconnect - multicloud (1 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     AWS Interconnect - multicloud provides simple, resilient, and high-speed private connections to other cloud service providers.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FsUZnnx/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/1A0XyGAPAWkDy-Iq8U9y1OLMxZIcT5wyk8Y6HXwR_4I=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>OpenAI Takes Stake in Thrive Holdings, a Buyer of Services Firms (3 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Thrive Holdings was created to buy up and consolidate providers like accounting firms.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FVRbYeG/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/R_hDsBWVGl9Twnl1r4HEALbWkyqYevwrmX4ICk1bdsg=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Shopify Breaks Down on Busy Cyber Monday (3 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Shopify experienced an outage on Cyber Monday that interrupted operations for some merchants.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftonisagrista.com%2Fblog%2F2025%2Fgoogle-unkills-jpegxl%2F%3Futm_source=tldrnewsletter/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/xtaEZGiP3yLmYXWPdNQTr_U5NAqkGcpg1WQleyluVD0=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Google unkills JPEG XL? (5 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     The Chromium team has reversed the Obsolete tag for JPEG XL as it has decided to support the format in Blink, the engine behind Chrome.                                 </span> </span> </div> </td></tr></tbody></table>   <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstratechery.com%2F2025%2Fgoogle-nvidia-and-openai%2F%3Futm_source=tldrnewsletter/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/bQ8U67zNTI29OXdzVodgFURav9EPLJ_u41_aravMRaQ=434\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Google, Nvidia, and OpenAI (20 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     The heroes of the AI story over the last three years have been OpenAI and Nvidia, but Google is now striking back.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>     <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"left\" style=\"word-break: break-word; vertical-align: top; padding: 5px 10px\">  <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Want to advertise in TLDR? 📰 </p> <div style=\"margin-top: 10px\"> If your company is interested in reaching an audience of tech executives, decision-makers and engineers, you may want to <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/8MzIF0UkIDwjEwvCbxAQvdXUhO0cLc_RKl1NT88Qgww=434\" target=\"_blank\" rel=\"noreferrer\"><strong><span>advertise with us</span></strong></a>. </div> <br />    <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Want to work at TLDR? 💼 </p> <div style=\"margin-top: 10px\"> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/xJg3W1Jg2RKYtRtRCZ-sMKY0663wySKfTn7VqIKqZcE=434\" rel=\"noreferrer\" style=\"color: #0000EE; text-decoration: underline\" target=\"_blank\"><strong>Apply here</strong></a> or send a friend's resume to <a href=\"mailto:jobs@tldr.tech\" style=\"color: #0000EE; text-decoration: underline\" onclick=\"return rcmail.command('compose','jobs@tldr.tech',this)\" rel=\"noreferrer\">jobs@tldr.tech</a> and get $1k if we hire them! </div> <br />  <div> If you have any comments or feedback, just respond to this email! <br /> <br /> Thanks for reading, <br /> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftwitter.com%2Ftldrdan/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/22JjGVe7eID2leui2MLqSTaSXgNhZp7-Q-xXLCVAx3Y=434\" target=\"_blank\" rel=\"noreferrer\"><span>Dan Ni</span></a> &amp; <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftwitter.com%2FSteveFlanders22/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/QP9N7jocLLTqVpzV0OHWmjRLWRtldiJ9ZGYrEbRRZXA=434\" target=\"_blank\" rel=\"noreferrer\"><span>Stephen Flanders</span></a> <br /> <br /> </div> <br /> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Ftech%2Fmanage%3Femail=tldrai90%2540ino.to/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/iKh8A1K-r4edCWNVAPtIatUW7UbfLMvBB9HEzqFrczA=434\" target=\"_blank\" rel=\"noreferrer\">Manage your subscriptions</a> to our other newsletters on tech, startups, and programming. Or if TLDR isn't for you, please <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa.tldrnewsletter.com%2Funsubscribe%3Fep=1%26l=cfa2d55a-b7be-11e8-a3c9-06b79b628af2%26lc=7ab8e3ee-9621-11f0-8ad5-431c1b7ff1fe%26p=8d94c040-cf48-11f0-bf81-3dbde69b9a7d%26pt=campaign%26pv=4%26spa=1764673299%26t=1764675476%26s=ee03dac6de36b3fb3d7b24d4eab5fc9787dab6d0ab139b959f885ce508558d12/1/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/SFO0Dwftswd7pPA9lsutyCYrhHb4H_mgylwXKmR8LN8=434\" target=\"_blank\" rel=\"noreferrer\">unsubscribe</a>. <br /> </div> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table>    <img src=\"http://tracking.tldrnewsletter.com/CI0/0100019adedada6f-36429dec-bb5c-4a3e-8cd6-28c5b4c7a12a-000000/BF2r9p6FOFv1K39aZ3xUNpcAOAwM6klaRZ0CJIbayLc=434\" style=\"display: none; width: 1px; height: 1px\" /> </div></div></div>",
      "summary": "       DeepSeek has unveiled two new models. The startup claims DeepSeek-V3.2 matches the performance of OpenAI's GPT-5 across multiple reasoning benchmarks ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌                   Sign Up |Advertise|View Online     TLDR      Together With       TLDR 2025-12-02                                                                                                                     Goodbye, low test cover",
      "publishedAt": "2025-12-02T11:38:00.000Z",
      "author": "TLDR ",
      "source": "rss",
      "feedName": "TLDR",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 11.39435670355688,
      "ingestedAt": "2025-12-02T14:44:03.191Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "testing"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09958082",
      "title": "Agentgateway Review: A Feature-Rich New AI Gateway",
      "url": "https://dev.to/spacewander/agentgateway-review-a-feature-rich-new-ai-gateway-53lm",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2yg0naa6ipyn3y71oa8b.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><h2>  \n    \n    \n  Introduction  \n</h2>  \n  \n<p><a href=\"https://github.com/agentgateway/agentgateway\">agentgateway</a> is a data plane developed by solo specifically for AI scenarios. The data plane is written in Rust and can be configured via xDS (a gRPC-based protocol) and YAML. Recently they decided to replace kgateway’s AI data plane from Envoy to agentgateway. I expect the enterprise version of Gloo will follow. Previously, most AI-related data-plane features were implemented in Envoy calling a Go sidecar via ext_proc, and I guess the real-world results were mediocre.</p>  \n  \n<p>This gateway supports four AI scenarios:</p>  \n  \n<ol>  \n<li>MCP  \n</li>  \n<li>A2A  \n</li>  \n<li>Proxying inference requests to LLM providers  \n</li>  \n<li>Load balancing for inference services</li>  \n</ol>  \n  \n<p>Below I explain each of these scenarios. Note I’m discussing the open-source agentgateway — some features may exist only in the enterprise edition and are outside the scope of this doc.</p>  \n  \n<h2>  \n    \n    \n  MCP  \n</h2>  \n  \n<p>agentgateway was originally started to address the difficulty of handling stateful MCP requests in existing Envoy data planes. So its MCP support is the most complete.</p>  \n  \n<p>By default, agentgateway treats MCP as a stateful protocol. It has a SessionManager struct responsible for session creation and maintenance (<a href=\"https://github.com/agentgateway/agentgateway/blob/1ca00e32d3f475539a20120f72c45c05aaf80377/crates/agentgateway/src/mcp/session.rs#L370\">code link</a>). But this SessionManager is a local in-process store, which means if you run multiple agentgateway instances there’s no guarantee a client will hit the same SessionManager each time. If you want sticky sessions toward upstreams, it’s actually simpler to consistent-hash on the MCP-Session-ID header so the same session ID routes to the same backend even if requests land on different agentgateway instances. Extending SessionManager to use a remote store is another solution, but it’s more expensive. To me, making MCP stateful by default is a mistake. I’m glad they plan to make MCP a <a href=\"https://github.com/modelcontextprotocol/modelcontextprotocol/issues/1442\">default stateless protocol</a>.</p>  \n  \n<p>When there is more than one backend, agentgateway enables MCP multiplexing. For example, with tools: when listing tools, agentgateway sends tools/list to every backend, then rewrites tool names to the format <code>${backend_name}_${tool_name}</code>. When a tool call comes in, agentgateway routes to the actual backend. For methods that can’t be multiplexed, it returns an invalid method error.</p>  \n  \n<p>Besides forwarding to MCP backends, agentgateway supports converting RESTful APIs to MCP tools using an OpenAPI spec. Impressively, it supports using an entire spec as a backend and includes a fair amount of schema-parsing code. agentgateway positions itself here as an MCP-to-RESTful-API forwarder; it does not itself manage the RESTful APIs described in the OpenAPI spec. Some details are still missing — for example, bodies only support application/json, HTTPS upstreams aren’t supported yet, structured output is not yet supported, etc. There are also finer points (e.g., handling of additionalProperties) I haven’t dug fully into.</p>  \n  \n<p>agentgateway implements OAuth-based MCP authentication. It exposes protected resource metadata at paths like <code>/.well-known/oauth-protected-resource/${resource}</code>. However, if one host contains multiple resources, should each resource’s route-match config explicitly include that resource’s well-known path? Otherwise you can’t guarantee the request will route to the well-known path handler. One nice thing: agentgateway adds CORS headers to metadata responses, so when an MCP client runs in a browser (e.g., the MCP inspector) you don’t need to add a separate CORS middleware.</p>  \n  \n<p>agentgateway fetches public keys from a JWKS path to verify tokens were issued by the corresponding authorization server. There are two JWKS sources:</p>  \n  \n<ol>  \n<li>The user supplies a URL or a file path.  \n</li>  \n<li>The JWKS URL is derived from the issuer URL and issuer type.</li>  \n</ol>  \n  \n<p>The code that gets public keys from JWKS appears to be called <a href=\"https://github.com/agentgateway/agentgateway/blob/1ca00e32d3f475539a20120f72c45c05aaf80377/crates/agentgateway/src/types/local.rs#L1076\">only when parsing configuration</a>. So the JWKS does not seem to be periodically refreshed.</p>  \n  \n<p>Authorization is also implemented via OAuth. It uses a list of CEL expressions as filters, matching on JWT fields and MCP attributes. Example:<br>  \n</p>  \n  \n<div>  \n<pre><code>mcpAuthorization:  \n  rules:  \n  # Allow anyone to call 'echo'  \n  - 'mcp.tool.name == \"echo\"'  \n  # Only the test-user can call 'add'  \n  - 'jwt.sub == \"test-user\" &amp;&amp; mcp.tool.name == \"add\"'  \n  # Any authenticated user with the claim `nested.key == value` can access 'printEnv'  \n  - 'mcp.tool.name == \"printEnv\" &amp;&amp; jwt.nested.key == \"value\"'  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>Note: in multiplexing scenarios, mcpAuthorization runs before the tool lists are merged, so the tool names here do not include the backend-name prefix.</p>  \n  \n<p>agentgateway provides surprisingly few MCP-related metrics — basically just an mcp_requests counter — so you can’t see details like which tools are taking the most time.</p>  \n  \n<h2>  \n    \n    \n  A2A  \n</h2>  \n  \n<p>For A2A protocol scenarios, agentgateway implements two main features:</p>  \n  \n<ol>  \n<li>Rewrites agent card URLs so they point to the gateway instead of the proxied backend.  \n</li>  \n<li>Parses A2A JSON requests and records the request method for observability.</li>  \n</ol>  \n  \n<h2>  \n    \n    \n  Proxying inference requests to LLM providers  \n</h2>  \n  \n<p>Like other AI gateways, agentgateway can proxy inference requests to LLM providers. This proxying is not just raw forwarding: it adds value such as token-based observability and rate-limiting.</p>  \n  \n<p>When proxying SSE traffic it collects token usage and TTFT metrics. For non-SSE streaming formats (e.g., Bedrock’s AWS event stream) it provides dedicated parsers.</p>  \n  \n<p>I’ll dive into rate limiting, prompt protection, and related features in a follow-up.</p>  \n  \n<p>Another common capability is to lift some LLM client features into the gateway to reduce integration work — for example, smoothing differences between providers and offering an OpenAI-compatible external API.</p>  \n  \n<p>agentgateway supports this to an extent. Its design is not a full generic \"X provider to Y provider\" converter; instead it implements conversions for specific routing types. Currently it supports two route types:</p>  \n  \n<ol>  \n<li>OpenAI’s /v1/chat/completions  \n</li>  \n<li>Anthropic’s /v1/messages</li>  \n</ol>  \n  \n<p>In practice both /v1/chat/completions and /v1/messages are chat-style routes: OpenAI’s /v1/chat/completions is functionally equivalent to Anthropic’s /v1/messages. They implemented both separately for quick business onboarding: many code agents only implement Anthropic’s /v1/messages, and special-casing that endpoint makes it easy to immediately accept such clients. Implementing a full Anthropic-to-any-provider converter would be a much larger effort.</p>  \n  \n<p>This area is currently roughly sufficient but incomplete. Putting aside support for embeddings, batching, etc., agentgateway does not fully support /v1/chat/completions yet — for example, <a href=\"https://platform.openai.com/docs/guides/structured-outputs\">structured output</a> is not supported at the moment.</p>  \n  \n<h2>  \n    \n    \n  Inference Extension Support  \n</h2>  \n  \n<p>When the gateway API inference extension (<a href=\"https://gateway-api-inference-extension.sigs.k8s.io/\">https://gateway-api-inference-extension.sigs.k8s.io/</a>) first appeared I was skeptical. Distributed inference is a systems engineering problem; it feels presumptuous for a single scheduler implementation to try to become the standard. But with Red Hat driving the LLMD project and treating the inference extension as part of an out-of-the-box experience, the inference extension may gain traction. Red Hat has invested heavily in AI projects (e.g., vLLM) and has the resources to advance this work.</p>  \n  \n<p>Supporting the inference extension is actually not hard. The gateway needs to forward inference requests to a scheduler (called EPP in the inference extension) via Envoy’s gRPC ext_proc protocol. The scheduler’s response includes an x-gateway-destination-endpoint header that contains the target upstream address. The gateway then forwards the inference request to that address. Practically speaking the gateway is only doing forwarding here; the core logic lives in the scheduler. I’ve wondered: if the entire request is sent to the scheduler, why not let the scheduler process the request directly instead of having the gateway forward it? Is the scheduler only capable of handling input tokens and not output tokens?</p>  \n  \n<p>What’s the value of a self-hosted inference system? I think it’s to, under data-security constraints, be reasonably cost-competitive with external LLM providers. Large-model inference benefits from scale economics greatly — a self-hosted system is unlikely to beat cloud providers purely on price. To be more cost-effective you need scheduling innovations (e.g., better load balancing, more flexible disaggregated serving). If inference-extension support just means forwarding requests to the official scheduler, then the gateway isn’t adding meaningful value in that part of the chain.</p>  \n  \n<h2>  \n    \n    \n  Summary  \n</h2>  \n  \n<p>In summary, agentgateway is impressive for a project that’s been developed for only about half a year. Its feature richness stands out. It shows a clear focus on AI scenarios, and its ambition to rebuild the data plane in Rust (to replace the prior Envoy + Go external process approach) demonstrates strong intent and potential to address AI-specific protocols and performance needs.</p>  \n  \n<p>However, the documentation is incomplete: some implemented features (e.g., Anthropic /v1/messages support) <a href=\"https://github.com/agentgateway/website/blob/02e25020b185ed34c66704d6274708a24ffe098d/content/docs/llm/about.md?plain=1#L30\">aren’t documented</a>, while some documented items don’t exist in the code (e.g., the MCP metric list_calls_total referenced in the docs: <a href=\"https://github.com/agentgateway/website/blob/02e25020b185ed34c66704d6274708a24ffe098d/content/docs/mcp/mcp-observability.md?plain=1#L18\">https://github.com/agentgateway/website/blob/02e25020b185ed34c66704d6274708a24ffe098d/content/docs/mcp/mcp-observability.md?plain=1#L18</a>). Overall these are typical, understandable issues for a rapidly iterating early-stage open-source project and do not substantially detract from the project’s promise.</p>",
      "summary": "  \n    \n    \n  Introduction  \n  \n  \nagentgateway is a data plane developed by solo specifically for AI scenarios. The data plane is written in Rust and can be configured via xDS (a gRPC-based protocol) and YAML. Recently they decided to replace kgateway’s AI data plane from Envoy to agentgateway. I expect the enterprise version of Gloo will follow. Previously, most AI-related data-plane features were implemented in Envoy calling a Go sidecar via ext_proc, and I guess the real-world results were ",
      "publishedAt": "2025-12-02T11:23:19.000Z",
      "author": "spacewander",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 15.84147583985893,
      "ingestedAt": "2025-12-02T14:44:03.191Z",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "observability",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09958083",
      "title": "Cloud Cost Optimization for AI & Data-Intensive Systems: Save While Scaling",
      "url": "https://dev.to/oleksii_samoilenko/cloud-cost-optimization-for-ai-data-intensive-systems-save-while-scaling-ofb",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ffo1ru3g8ba45cufvmm9g.webp\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>Modern AI systems, LLM-powered applications, and data-intensive platforms generate enormous value — but they also generate enormous cloud bills. As organizations scale their machine learning pipelines, vector databases, real-time analytics, and GPU-heavy inference workloads, cloud costs can quickly spiral out of control. The result is familiar: impressive AI results paired with a CFO asking why the monthly cloud invoice suddenly doubled.</p>  \n  \n<p>This is where cloud cost optimization becomes essential. Companies that strategically design, architect, and operate AI workloads in the cloud can reduce costs by 30–70% without sacrificing performance. Effective cloud optimization isn’t just about cutting expenses — it’s about enabling sustainable scaling, predictable operations, and better resource management across the entire AI lifecycle.</p>  \n  \n<p>In this article, we break down the causes of high AI cloud spend, the most effective cloud cost optimization strategies, and actionable approaches to achieving meaningful cloud infrastructure cost optimization while still supporting rapid AI growth.</p>  \n  \n<p>Why AI &amp; Data Workloads Become Expensive</p>  \n  \n<p>AI workloads are fundamentally different from traditional applications. They require:</p>  \n  \n<ul>  \n<li>GPU-intensive compute for training and inference</li>  \n<li>High-performance storage for large datasets</li>  \n<li>Massive data movement across networks</li>  \n<li>Always-on services for real-time applications</li>  \n<li>Distributed infrastructure for scalability</li>  \n</ul>  \n  \n<p>Because of these factors, poor cloud planning can lead to unnecessary overspending. The biggest cost drivers include:</p>  \n  \n<p><strong>GPU Overprovisioning</strong></p>  \n  \n<p>Data teams often spin up the largest GPU instances available (e.g., A100 or H100) even when workloads don’t require that power.</p>  \n  \n<p><strong>Idle Compute Resources</strong></p>  \n  \n<p>Training jobs, MLOps pipelines, and inference services often run 24/7 — even when not in use.</p>  \n  \n<p><strong>Inefficient Storage</strong></p>  \n  \n<p>Storing large datasets in high-cost storage tiers or duplicating data across environments dramatically increases bills.</p>  \n  \n<p><strong>Lack of Autoscaling</strong></p>  \n  \n<p>Without autoscaling policies, systems remain over-allocated during low-traffic periods.</p>  \n  \n<p><strong>Poor Observability &amp; Cost Governance</strong></p>  \n  \n<p>Teams don’t have enough visibility over cost centers, resulting in runaway cloud bills.</p>  \n  \n<p>Key Cloud Cost Optimization Strategies for AI Teams</p>  \n  \n<p>To ensure sustainable scaling, organizations must adopt a combination of engineering practices, architectural choices, and ongoing operational monitoring. Below are the most effective cloud cost optimization strategies for AI-driven systems.</p>  \n  \n<p><strong>Choose the Right Hardware for the Job</strong></p>  \n  \n<p>AI workloads often rely heavily on GPUs — but the “biggest GPU available” is not always the optimal choice.</p>  \n  \n<p>Strategies that work:</p>  \n  \n<ul>  \n<li>Use smaller GPUs (e.g., T4, L4) for inference instead of A100/H100.</li>  \n<li>Utilize spot GPU instances for training jobs with checkpoints.</li>  \n<li>Consider ARM-based processors (e.g., AWS Graviton) for preprocessing and ETL tasks.</li>  \n<li>Mix GPU and CPU-based inference where latency allows.  \nBy right-sizing compute to workload requirements, companies often achieve 30–50% savings immediately.</li>  \n</ul>  \n  \n<p><strong>Implement Autoscaling and Right-Sizing Policies</strong></p>  \n  \n<p>AI systems frequently experience unpredictable traffic spikes. Autoscaling ensures that compute resources expand during peak usage and contract during low-demand periods.</p>  \n  \n<p>Best practices:</p>  \n  \n<ul>  \n<li>Use horizontal pod autoscaler (HPA) on Kubernetes.</li>  \n<li>Set up scale-to-zero for non-essential services.</li>  \n<li>Leverage serverless options for vector search, embeddings, or scheduled jobs.</li>  \n<li>Continuously track workloads with usage-based alerts to recommend right-sizing.</li>  \n</ul>  \n  \n<blockquote>  \n<p>Autoscaling alone can cut 20–40% of unnecessary spend.</p>  \n</blockquote>  \n  \n<p><strong>Optimize Cloud Storage for Data Pipelines</strong></p>  \n  \n<p>Storing AI datasets, embeddings, check-pointed training models, and log files can quickly get out of control.</p>  \n  \n<p>Effective optimization approaches:</p>  \n  \n<ul>  \n<li>Move historical datasets to cheaper storage tiers (e.g., S3 Glacier, Azure Archive).</li>  \n<li>Use columnar formats like Parquet to reduce storage size.</li>  \n<li>Deduplicate datasets with data versioning tools like DVC or LakeFS.</li>  \n<li>Archive ML logs and checkpoints automatically after validations.</li>  \n</ul>  \n  \n<blockquote>  \n<p>A well-designed data lifecycle plan is a key pillar of cloud infrastructure cost optimization, reducing storage costs by up to 80%.</p>  \n</blockquote>  \n  \n<p><strong>Use Efficient Vector Databases and Search Architectures</strong></p>  \n  \n<p>Vector search systems (Pinecone, Weaviate, Qdrant, Milvus) are essential for RAG, LLM retrieval, and semantic search, but they can be cost-heavy.</p>  \n  \n<p>Tips to control costs:</p>  \n  \n<ul>  \n<li>Use hybrid indexing to reduce vector storage.</li>  \n<li>Offload cold embeddings to object storage.</li>  \n<li>Use sharding and partial scale-out instead of overprovisioning large clusters.</li>  \n<li>Consider open-source solutions hosted on your own Kubernetes cluster.</li>  \n</ul>  \n  \n<p>Choosing the right database topology can reduce costs by 30–60%.</p>  \n  \n<p><strong>Compress, Quantize, and Optimize Models</strong></p>  \n  \n<p>Model compression dramatically reduces inference costs by allowing smaller or cheaper compute instances to serve requests.</p>  \n  \n<p>Popular optimization methods:</p>  \n  \n<ul>  \n<li>Quantization (FP16, INT8, INT4)</li>  \n<li>Pruning and distillation</li>  \n<li>Token-level caching for LLMs</li>  \n<li>Serving with optimized runtimes like ONNX Runtime or TensorRT</li>  \n</ul>  \n  \n<p>For many companies, model optimization means cutting inference costs in half with minimal accuracy loss.</p>  \n  \n<p>Use Spot Instances for Training</p>  \n  \n<p>Training LLMs, CV models, and deep neural networks is expensive, but spot GPU instances can slash cost if jobs are checkpointed.</p>  \n  \n<p>For example:</p>  \n  \n<ul>  \n<li>AWS EC2 Spot</li>  \n<li>GCP Preemptible Instances</li>  \n<li>Azure Spot VMs</li>  \n</ul>  \n  \n<p>Spot training can reduce costs by 70–90%, especially for long-running batch tasks.</p>  \n  \n<p><strong>Improve Observability and Cost Governance</strong></p>  \n  \n<p>Without proper monitoring, cost leaks remain invisible.</p>  \n  \n<p>Tools your team should use:</p>  \n  \n<ul>  \n<li>AWS Cost Explorer / Azure Cost Management</li>  \n<li>Kubecost for Kubernetes</li>  \n<li>DataDog or Grafana for resource usage</li>  \n<li>MLflow or Weights &amp; Biases to track training costs</li>  \n</ul>  \n  \n<p>For full cloud cost optimization, every team — AI, engineering, product — must see and own their usage patterns.</p>  \n  \n<p><strong>Adopt a Zero-Waste Cloud Philosophy</strong></p>  \n  \n<p>These advanced methods ensure minimal waste across the infrastructure:</p>  \n  \n<ul>  \n<li>Delete unused snapshots, volumes, clusters, and load balancers.</li>  \n<li>Shut down dev environments at night/weekends.</li>  \n<li>Separate dev/stage/prod with strict quotas.</li>  \n<li>Automate resource cleanup with cron jobs or Lambdas.</li>  \n</ul>  \n  \n<p>Teams that adopt zero-waste practices save up to 20% monthly with no engineering effort.</p>  \n  \n<p>Optimization Strategies for AI Training vs. Inference</p>  \n  \n<p>AI workloads fall into two categories — training and inference — and both require different optimization tactics.</p>  \n  \n<p>Training Optimization</p>  \n  \n<p>Training is GPU-heavy, long-running, and typically done in batches.</p>  \n  \n<p>Best strategies:</p>  \n  \n<ul>  \n<li>Use spot GPUs</li>  \n<li>Enable gradient checkpointing</li>  \n<li>Select smaller batch sizes</li>  \n<li>Choose cheaper regions</li>  \n<li>Perform distributed training when needed</li>  \n<li>Use autoscaling clusters like SageMaker or Vertex AI</li>  \n</ul>  \n  \n<p>Inference Optimization</p>  \n  \n<p>Inference must be fast, scalable, and cost-efficient.</p>  \n  \n<p>Best strategies:</p>  \n  \n<ul>  \n<li>Use small or quantized models</li>  \n<li>Deploy on smaller GPUs (T4/L4 or CPU for light tasks)</li>  \n<li>Use token streaming and caching</li>  \n<li>Autoscale aggressively</li>  \n<li>Use serverless inference (AWS Lambda + EFS, Vertex AI Serverless)</li>  \n</ul>  \n  \n<p>Building a Cloud Cost Optimization Culture</p>  \n  \n<p>Technology alone can’t solve the cost challenge — teams must adopt the right mindset.</p>  \n  \n<p>This includes:</p>  \n  \n<ul>  \n<li>Engineering estimating cloud impact before development</li>  \n<li>Architecture teams reviewing infra decisions</li>  \n<li>Product managers understanding budget implications</li>  \n<li>Finance collaborating with tech leaders</li>  \n<li>Automated alerts when cost thresholds are reached</li>  \n</ul>  \n  \n<p>Companies that follow this culture see long-term success with cloud infrastructure cost optimization.</p>  \n  \n<p>Scale AI Smartly, Not Expensively</p>  \n  \n<p>AI-driven systems and data-intensive workloads are inherently resource-hungry, but they don’t have to be financially unsustainable. By combining engineering best practices, architectural decisions, automation, and operational discipline, organizations can dramatically reduce cloud spend without compromising performance.</p>  \n  \n<p>The key to effective cloud cost optimization is simple:<br>  \nPay only for what brings value — eliminate everything else.</p>  \n  \n<p>With the right set of cloud cost optimization strategies, you can scale LLMs, vector databases, analytics pipelines, and real-time AI systems efficiently, confidently, and cost-effectively.</p>",
      "summary": "Modern AI systems, LLM-powered applications, and data-intensive platforms generate enormous value — but they also generate enormous cloud bills. As organizations scale their machine learning pipelines, vector databases, real-time analytics, and GPU-heavy inference workloads, cloud costs can quickly spiral out of control. The result is familiar: impressive AI results paired with a CFO asking why the monthly cloud invoice suddenly doubled.  \n  \nThis is where cloud cost optimization becomes essenti",
      "publishedAt": "2025-12-02T11:22:29.000Z",
      "author": "Oleksii Samoilenko",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 10.890564458456767,
      "ingestedAt": "2025-12-02T14:44:03.191Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability",
        "governance",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09958084",
      "title": "Transactional and Marketing Emails for AsanTyping.com",
      "url": "https://dev.to/danishyarkh/transactional-and-marketing-emails-for-asantypingcom-112i",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F64ya9zqgkribs3n5ctup.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>Over the past few weeks, I’ve been exploring different email service providers for transactional emails for AsanTyping.com. I tested MailerLite, Mailgun, Mailchimp, and even AWS SES.</p>  \n  \n<p>What I found:</p>  \n  \n<p>Most platforms are either too expensive, especially when it comes to marketing emails</p>  \n  \n<p>AWS SES is definitely the cheapest for transactional emails, but once you add marketing features, the pricing climbs quickly</p>  \n  \n<p>For my current needs, I needed something simple, reliable, and budget-friendly</p>  \n  \n<p>So for now, I’ve decided to use Resend’s free plan for all transactional emails — and honestly, it’s been a great fit.</p>  \n  \n<p>I’ve just finished integrating it, and now:<br>  \n✅ Email verification<br>  \n✅ Forgot password emails<br>  \n✅ Welcome emails<br>  \n…are all live and fully functional!</p>  \n  \n<p>It feels good to check this off the list. If you’re building something early-stage and want a lightweight solution for transactional emails, Resend might be worth a look.</p>",
      "summary": "Over the past few weeks, I’ve been exploring different email service providers for transactional emails for AsanTyping.com. I tested MailerLite, Mailgun, Mailchimp, and even AWS SES.  \n  \nWhat I found:  \n  \nMost platforms are either too expensive, especially when it comes to marketing emails  \n  \nAWS SES is definitely the cheapest for transactional emails, but once you add marketing features, the pricing climbs quickly  \n  \nFor my current needs, I needed something simple, reliable, and budget-fr",
      "publishedAt": "2025-12-02T11:20:03.000Z",
      "author": "Khalid Danishyar",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "pricing_business",
      "score": 9.404352303066785,
      "ingestedAt": "2025-12-02T14:44:03.191Z",
      "tags": [
        "code_review",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09958085",
      "title": "Creating Autonomous AI Agents – A Practical Guide for Businesses",
      "url": "https://dev.to/harshada_75eaf5c6bf7a194a/creating-autonomous-ai-agents-a-practical-guide-for-businesses-384k",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fq08uh01hg9ie79x95wx0.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>Artificial intelligence is quickly evolving from simple prompt-based systems into autonomous agents capable of reasoning, planning, and acting independently. Organizations across industries are now exploring the process of <a href=\"https://resurs.ai/\">creating autonomous AI agents</a><br>  \n to streamline operations, reduce manual work, and unlock intelligent automation at scale.</p>  \n  \n<p>Autonomous agents can execute tasks, interact with software tools, analyze data, and refine their performance—all with minimal human oversight. This makes them a powerful upgrade from traditional chatbots, static RPA, or single-step AI workflows.</p>  \n  \n<p>What Are Autonomous AI Agents?</p>  \n  \n<p>Autonomous AI agents are software entities designed to perform goal-driven tasks independently. They rely on a mix of large language models, memory systems, reasoning engines, and tool execution frameworks to complete workflows.</p>  \n  \n<p>These agents can:</p>  \n  \n<p>Break tasks into actionable steps</p>  \n  \n<p>Execute commands across systems</p>  \n  \n<p>Adapt to changing requirements</p>  \n  \n<p>Validate their results</p>  \n  \n<p>Improve performance over time</p>  \n  \n<p>Unlike simple automation scripts, autonomous agents can think, not just execute.</p>  \n  \n<p>Why Businesses Are Building Autonomous AI Agents</p>  \n  \n<p>Companies are deploying agentic systems to solve real operational challenges, including:</p>  \n  \n<p>High workflows dependency on human decision-making</p>  \n  \n<p>Time-consuming manual tasks</p>  \n  \n<p>Complex multi-step workflows</p>  \n  \n<p>Legacy automation limitations</p>  \n  \n<p>Scalability and personnel constraints</p>  \n  \n<p>This new generation of automation delivers:</p>  \n  \n<p>Benefit Impact<br>  \nEfficiency gains    Faster execution of processes<br>  \nAccuracy improvement    Fewer errors and quality checks<br>  \nCost savings    Reduced labor and operational overhead<br>  \n24/7 automation Full-time digital workforce<br>  \nAdaptability    Continuous learning and refinement</p>  \n  \n<p>The adoption curve is accelerating across finance, IT ops, HR automation, legal, cybersecurity, and logistics.</p>  \n  \n<p>Core System Requirements for Building Autonomous AI Agents</p>  \n  \n<p>To successfully start creating autonomous AI agents, organizations need:</p>  \n  \n<ol>  \n<li>A Reasoning LLM Core</li>  \n</ol>  \n  \n<p>The language model performs planning, problem-solving, and decision-making.</p>  \n  \n<ol>  \n<li>Tool Execution Environment</li>  \n</ol>  \n  \n<p>Agents require access to APIs, workflow automation platforms, or agentic AI workflow tools.</p>  \n  \n<ol>  \n<li>Memory Framework</li>  \n</ol>  \n  \n<p>Short-term and long-term memory support context, personalization, and iterative learning.</p>  \n  \n<ol>  \n<li>Monitoring &amp; Validation Layer</li>  \n</ol>  \n  \n<p>Ensures output accuracy, compliance, and safety guardrails.</p>  \n  \n<ol>  \n<li>Agentic AI Orchestration Layer</li>  \n</ol>  \n  \n<p>This enables multi-agent collaboration, task delegation, and lifecycle management.</p>  \n  \n<p>Well-architected orchestration is essential for enterprise adoption.</p>  \n  \n<p>Steps to Creating Autonomous AI Agents</p>  \n  \n<p>To simplify implementation, here’s a proven framework used by leading AI innovators:</p>  \n  \n<p>Step 1 — Define Use Case and Expected Output</p>  \n  \n<p>Start with measurable, repeatable workflows like data extraction, reporting, or request handling.</p>  \n  \n<p>Step 2 — Design Agent Capabilities</p>  \n  \n<p>Define whether the agent will retrieve information, automate tasks, evaluate output, or make decisions.</p>  \n  \n<p>Step 3 — Set Up Tools and Integrations</p>  \n  \n<p>Connect required systems such as CRM, ERP, cloud tools, messaging platforms, or internal applications.</p>  \n  \n<p>Step 4 — Add Memory and Feedback Loops</p>  \n  \n<p>Enable learning over time to improve performance and avoid repeating mistakes.</p>  \n  \n<p>Step 5 — Test, Observe, and Optimize</p>  \n  \n<p>Deploy in controlled environments before full-scale enterprise rollout.</p>  \n  \n<p>This structured approach ensures the agent is reliable, safe, and aligned with business goals.</p>  \n  \n<p>Real-World Use Cases for Autonomous Agents</p>  \n  \n<p>Companies are now using autonomous agents to:</p>  \n  \n<p>Process customer support and escalate complex cases</p>  \n  \n<p>Detect cyber threats and trigger automated responses</p>  \n  \n<p>Generate financial reports and reconcile data</p>  \n  \n<p>Run marketing campaigns and CRM workflows</p>  \n  \n<p>Manage IT operations and automated troubleshooting</p>  \n  \n<p>As maturity increases, these agents evolve into fully autonomous digital employees.</p>  \n  \n<p>Future of Autonomous Agent Systems</p>  \n  \n<p>With advances in reasoning models, memory, and orchestration, we will soon see:</p>  \n  \n<p>Autonomous teams of specialized AI agents</p>  \n  \n<p>Industry-specific prebuilt agent templates</p>  \n  \n<p>Policy-driven enterprise intelligence layers</p>  \n  \n<p>Self-healing and self-maintaining AI systems</p>  \n  \n<p>This represents a transformational shift in digital workforce infrastructure.</p>  \n  \n<p>Conclusion</p>  \n  \n<p>Organizations exploring creating autonomous AI agents<br>  \n are positioning themselves ahead of the next wave of intelligent automation. By combining reasoning, workflow execution, and structured orchestration, enterprises can create scalable AI systems capable of delivering 10x productivity and operational resilience.</p>  \n  \n<p>FAQs</p>  \n  \n<ol>  \n<li><p>How difficult is it to build autonomous agents?<br>  \nWith the right frameworks and tools, businesses can deploy their first agent within weeks—not months.</p></li>  \n<li><p>Do autonomous agents replace employees?<br>  \nThey augment teams by handling routine and repetitive tasks, allowing humans to focus on strategic work.</p></li>  \n<li><p>What skills are needed to build agentic systems?<br>  \nEngineering expertise helps, but many modern platforms support low-code and no-code deployment.</p></li>  \n<li><p>How do autonomous agents learn?<br>  \nThrough memory, feedback loops, result monitoring, and iterative refinement.</p></li>  \n<li><p>Can multiple agents work together?<br>  \nYes, with proper agentic AI orchestration, agents can collaborate and distribute complex tasks.</p></li>  \n</ol>",
      "summary": "Artificial intelligence is quickly evolving from simple prompt-based systems into autonomous agents capable of reasoning, planning, and acting independently. Organizations across industries are now exploring the process of creating autonomous AI agents  \n to streamline operations, reduce manual work, and unlock intelligent automation at scale.  \n  \nAutonomous agents can execute tasks, interact with software tools, analyze data, and refine their performance—all with minimal human oversight. This ",
      "publishedAt": "2025-12-02T11:18:33.000Z",
      "author": "harshada",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 15.342801611348195,
      "ingestedAt": "2025-12-02T14:44:03.191Z",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "observability",
        "governance",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09958087",
      "title": "&#128712; Mastering CSS Tooltips: A Quick & Practical Guide",
      "url": "https://dev.to/arsalanmeee/mastering-css-tooltips-a-quick-practical-guide-378e",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjpp17f8i7tt8l0smz4xn.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p><a href=\"https://makemychance.com/css-tooltip/\">Published on Makemychance.com</a><br>  \nTooltips are small UI elements that display helpful text when users hover or tap on an element. They improve UX without adding clutter — and with just a few lines of CSS, you can create clean, modern tooltips for any website.</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  🔍 What Is a Tooltip?  \n</h2>  \n  \n<p>A tooltip is a small popup text that appears on <strong>hover</strong>, <strong>focus</strong>, or <strong>tap</strong>.<br>  \nGreat for icons, buttons, forms, and feature explanations.</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  🧩 The Easiest Tooltip (HTML Only)  \n</h2>  \n  \n  \n  \n<div>  \n<pre><code><span>&lt;button</span> <span>title=</span><span>\"Click to submit\"</span><span>&gt;</span>Submit<span>&lt;/button&gt;</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>✔ Super quick<br>  \n✘ Can't customize the style</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  🎨 Custom CSS Tooltip (Modern UI)  \n</h2>  \n  \n<h3>  \n    \n    \n  HTML  \n</h3>  \n  \n  \n  \n<div>  \n<pre><code><span>&lt;div</span> <span>class=</span><span>\"tooltip\"</span><span>&gt;</span>  \n  Hover me  \n  <span>&lt;span</span> <span>class=</span><span>\"tooltip-text\"</span><span>&gt;</span>This is a tooltip<span>&lt;/span&gt;</span>  \n<span>&lt;/div&gt;</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<h3>  \n    \n    \n  CSS  \n</h3>  \n  \n  \n  \n<div>  \n<pre><code><span>.tooltip</span> <span>{</span>  \n  <span>position</span><span>:</span> <span>relative</span><span>;</span>  \n  <span>cursor</span><span>:</span> <span>pointer</span><span>;</span>  \n<span>}</span>  \n<span>.tooltip</span> <span>.tooltip-text</span> <span>{</span>  \n  <span>visibility</span><span>:</span> <span>hidden</span><span>;</span>  \n  <span>background</span><span>:</span> <span>#333</span><span>;</span>  \n  <span>color</span><span>:</span> <span>#fff</span><span>;</span>  \n  <span>padding</span><span>:</span> <span>6px</span> <span>10px</span><span>;</span>  \n  <span>border-radius</span><span>:</span> <span>4px</span><span>;</span>  \n  <span>position</span><span>:</span> <span>absolute</span><span>;</span>  \n  <span>bottom</span><span>:</span> <span>130%</span><span>;</span>  \n  <span>left</span><span>:</span> <span>50%</span><span>;</span>  \n  <span>transform</span><span>:</span> <span>translateX</span><span>(</span><span>-50%</span><span>);</span>  \n  <span>opacity</span><span>:</span> <span>0</span><span>;</span>  \n  <span>transition</span><span>:</span> <span>0.3s</span> <span>ease</span><span>;</span>  \n  <span>white-space</span><span>:</span> <span>nowrap</span><span>;</span>  \n<span>}</span>  \n<span>.tooltip</span><span>:hover</span> <span>.tooltip-text</span> <span>{</span>  \n  <span>visibility</span><span>:</span> <span>visible</span><span>;</span>  \n  <span>opacity</span><span>:</span> <span>1</span><span>;</span>  \n<span>}</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n  \n  \n  \n<h2>  \n    \n    \n  📌 Tooltip Positions  \n</h2>  \n  \n<ul>  \n<li>  \n<strong>Top</strong> (default)</li>  \n<li><strong>Bottom</strong></li>  \n<li><strong>Left</strong></li>  \n<li><strong>Right</strong></li>  \n</ul>  \n  \n<p>Just adjust <code>top</code>, <code>left</code>, <code>bottom</code>, or <code>transform</code> values.</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  🛈 Tooltip with Arrow  \n</h2>  \n  \n  \n  \n<div>  \n<pre><code><span>.tooltip</span> <span>.tooltip-text</span><span>::after</span> <span>{</span>  \n  <span>content</span><span>:</span> <span>\"\"</span><span>;</span>  \n  <span>position</span><span>:</span> <span>absolute</span><span>;</span>  \n  <span>top</span><span>:</span> <span>100%</span><span>;</span>  \n  <span>left</span><span>:</span> <span>50%</span><span>;</span>  \n  <span>border</span><span>:</span> <span>5px</span> <span>solid</span> <span>transparent</span><span>;</span>  \n  <span>border-top-color</span><span>:</span> <span>#333</span><span>;</span>  \n  <span>transform</span><span>:</span> <span>translateX</span><span>(</span><span>-50%</span><span>);</span>  \n<span>}</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n  \n  \n  \n<h2>  \n    \n    \n  📱 Tooltip on Mobile?  \n</h2>  \n  \n<p>Since there's no hover on mobile:<br>  \n✔ Show tooltip on <strong>click</strong><br>  \n✔ Use an <strong>info icon</strong> <code>(i)</code><br>  \n✔ Add a <strong>small JS toggle</strong> if needed</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  🛑 Common Mistakes  \n</h2>  \n  \n<p>❌ Too much text<br>  \n❌ Tooltip covering the element<br>  \n❌ Bad contrast<br>  \n❌ No mobile alternative</p>  \n  \n<p>Keep it simple and readable.</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  🔗 Helpful References  \n</h2>  \n  \n<ul>  \n<li>MDN Tooltip Basics  \n<a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/title\">https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/title</a>  \n</li>  \n<li>CSS-Tricks Tooltip Guide  \n<a href=\"https://css-tricks.com/css-tooltips/\">https://css-tricks.com/css-tooltips/</a>  \n</li>  \n</ul>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  🚀 Final Note  \n</h2>  \n  \n<p>Tooltips are tiny but powerful UX boosters. With simple CSS, you can create clean, smooth, and accessible tooltips that fit perfectly into any modern UI.</p>",
      "summary": "Published on Makemychance.com  \nTooltips are small UI elements that display helpful text when users hover or tap on an element. They improve UX without adding clutter — and with just a few lines of CSS, you can create clean, modern tooltips for any website.  \n  \n  \n  \n  \n  \n    \n    \n  🔍 What Is a Tooltip?  \n  \n  \nA tooltip is a small popup text that appears on hover, focus, or tap.  \nGreat for icons, buttons, forms, and feature explanations.  \n  \n  \n  \n  \n  \n    \n    \n  🧩 The Easiest Tooltip ",
      "publishedAt": "2025-12-02T11:17:37.000Z",
      "author": "Arsalan Mlaik",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "thought_leadership",
      "score": 6.928686393646564,
      "ingestedAt": "2025-12-02T14:44:03.192Z",
      "tags": [
        "code_review",
        "documentation",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b0995808b",
      "title": "YAMLpp is dynamic,self-generating YAML",
      "url": "https://dev.to/fralau/yamlpp-is-dynamicself-generating-yaml-2cb8",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fhztvaan01dltr0g6d7m6.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p><strong><a href=\"https://yaml.org/\">YAML</a></strong> is a great file format, used for writing configuration files; and more and more, as a foundation for Domain Specific Languages (DSLs), such as script files for Github workflows, Docker or Kubernetes.</p>  \n  \n<h2>  \n    \n    \n  How to make YAML dynamic?  \n</h2>  \n  \n<p>We have all, however, suffered from a frustration: <strong>YAML is fundamentally a static file format</strong>. How can we make it dynamic? </p>  \n  \n<p>It starts with the fact that some string, like <code>http://dev.company.local</code> appears several times across the files.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>server</span><span>:</span>  \n  <span>url</span><span>:</span> <span>http://dev.company.local</span>  \n  <span>...</span>  \n  \n<span>documentation</span><span>:</span>  \n  <span>url</span><span>:</span> <span>http://dev.company.local/docs</span>  \n  <span>...</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>And the day that address changes, what do we do?</p>  \n  \n<ol>  \n<li> Do a <strong>search/replace</strong> in the config files? Nah... <em>this is brittle and things will get worse before they get better</em>. And in any case, it doesn't solve many cases where a \"small\" change of parameter (from <code>dev</code> to <code>test</code> and from <code>test</code> to <code>live</code>) leads to significant changes in the YAML tree.</li>  \n<li>All right, then solve it with a little bit of <strong>templating</strong>, using Jinja (Python) or the Go native engine. Solved! ...Really? Everything is fine for a while, and then <em>the workflow crashes because some corner case broke the YAML syntax</em>. And we start fiddling-fiddling with spaces and escaping and newlines... and after 5 or 10 abort-debug-rerun cycles, the delivery is running late.</li>  \n<li>OK, what we can do now? Use a <strong>preprocessing language</strong> guaranteed to produce valid syntax, like <a href=\"https://jsonnet.org/\">Jsonnet</a> for json? That wil work, for sure. However, you have now a <em>friction</em> problem: it's new file format that requires implementing its own toolchain (Jsonnet is not json, so editor add-ons, parsers, interpreters, libraries, etc.) and you will need to drive the team through the learning curve of a language with its own syntax and semantic. Will you implement a serious project... or will you have a coffee break at that point, and keep doing things the same old way?</li>  \n</ol>  \n  \n<h2>  \n    \n    \n  Enter YAMLpp  \n</h2>  \n  \n<p>Faced with that <strong>problem</strong>, I decided that it needed a <strong>straightforward solution</strong>, which required basically only <em>one</em> utility (a pre-processor) to convert files into static YAML, and would change nothing else anywhere.</p>  \n  \n<p>Enter <strong><a href=\"https://yamlpp.readthedocs.io/en/latest/\">YAMLpp</a></strong> (<em>YAML Pre-Processor</em>). It is a language that makes YAML dynamic, but <em>it is itself YAML</em>. So you can use your current editor which does your color highlighting, syntax checking, and so on.</p>  \n  \n<p>Let's start with a Hello World example:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>.context</span><span>:</span>  \n  <span>name</span><span>:</span> <span>\"</span><span>World\"</span>  \n  \n<span>message</span><span>:</span> <span>\"</span><span>Hello</span><span> </span><span>{{</span><span> </span><span>name</span><span> </span><span>}}!\"</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>I don't think there is a lot to explain here. <code>.context</code> is called a <strong>construct</strong>, because it 'constructs' (builds) a YAML sub-tree or does something useful; in this case it defines a variable <code>name</code> and leaves no trace.</p>  \n  \n<p>All constructs start with a <code>.</code> (a dot). In YAMLpp, <code>.context</code> is known as a <strong>keyword</strong>.</p>  \n  \n<p>Then <code>message</code> contains a string that will be interpolated using the <code>name</code> variable.</p>  \n  \n<p>Here is the result:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>message</span><span>:</span> <span>\"</span><span>Hello</span><span> </span><span>World\"</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>As you see, the node <code>.context</code> disappeared. That's a rule for all YAMLpp constructs: they disappear and never make it to the target file.</p>  \n  \n<h2>  \n    \n    \n  YAMLpp is a metaprogramming tool  \n</h2>  \n  \n<p>It goes much further than that. </p>  \n  \n<p>It is a <strong>macro language</strong> written in YAML, the same language as the target language YAML.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>.context</span><span>:</span>  \n  <span>.platform</span><span>:</span> <span>dev</span>  \n  \n<span>server</span><span>:</span>  \n  <span>.if</span><span>:</span>  \n    <span>.if</span><span>:</span> <span>\"</span><span>(platform,</span><span> </span><span>{{</span><span> </span><span>prod</span><span> </span><span>}})\"</span>  \n    <span>.then</span><span>:</span>  \n      <span>url</span><span>:</span> <span>prod.machine.local</span>  \n      <span>...</span>  \n    <span>.else</span><span>:</span>  \n      <span>...</span>  \n      <span>url</span><span>:</span> <span>dev.machine.local</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>The result is thus:<br>  \n</p>  \n  \n<div>  \n<pre><code><span>server</span><span>:</span>  \n   <span>url</span><span>:</span> <span>dev.machine.local</span>  \n   <span>...</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>The <code>.if</code> construct is <strong>expanded</strong>: <code>.if</code>, <code>.then</code> and <code>.else</code> keywords will disappear, and only the intended final YAML nodes will remain.</p>  \n  \n<p><strong>For anyone who had anything to do with LISP or its offspring languages, you know that a language that generates itself through macros, is a huge deal, because it opens the doors to <em>meta-programming</em>.</strong> In essence, are able to create your own <strong>Domain-Specific Language</strong> (DSL), for configuration or scripting.</p>  \n  \n<p>For the others, what you write in <a href=\"https://yamlpp.readthedocs.io/en/latest/\">YAMLpp</a> is still YAML, but allows you to manipulate your target YAML tree in any way you please. You are now able to express <strong>high-level concepts</strong> in your config file or script in a way that is meaningful to <em>you</em> and your team (expressive, shorter, non-repetitive), which will be <strong>expanded</strong> into a form (YAML) that the machine will be able to test/interpret correctly and that everyone else will understand -- because its part of the standard specification. <br>  \nSo now, YAML can be either dynamic (when it contains YAMLpp keywords) or static (without them). But regardless, it is still valid YAML.</p>  \n<h2>  \n    \n    \n  Where do I go from there?  \n</h2>  \n  \n<p>A working prototype was developed in Python and it already has its test suite (with pytest). </p>  \n  \n<p>To install it:<br>  \n</p>  \n  \n<div>  \n<pre><code>pip <span>install </span>yamlppx  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>(Note the added 'x').</p>  \n  \n<p>You just need one command:<br>  \n</p>  \n  \n<div>  \n<pre><code>yamlpp input.yaml <span>-o</span> output.yaml  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>For more information see the <a href=\"https://github.com/fralau/yamlpp\">Github repo</a> and the <a href=\"https://yamlpp.readthedocs.io/\">documentation on ReadTheDocs</a>.</p>  \n  \n<p>You can also look at a <a href=\"https://yamlpp.readthedocs.io/en/latest/guide/\">webpage containing two realistic use cases, one for Kubernetes and one for Docker</a>.</p>",
      "summary": "YAML is a great file format, used for writing configuration files; and more and more, as a foundation for Domain Specific Languages (DSLs), such as script files for Github workflows, Docker or Kubernetes.  \n  \n  \n    \n    \n  How to make YAML dynamic?  \n  \n  \nWe have all, however, suffered from a frustration: YAML is fundamentally a static file format. How can we make it dynamic?   \n  \nIt starts with the fact that some string, like http://dev.company.local appears several times across the files. ",
      "publishedAt": "2025-12-02T11:15:36.000Z",
      "author": "Laurent Franceschetti",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "thought_leadership",
      "score": 6.927993330549165,
      "ingestedAt": "2025-12-02T14:44:03.192Z",
      "tags": [
        "code_review",
        "documentation",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b0995808f",
      "title": "Zero-Click SEO in 2026: When Winning Means Nobody Visits Your Site",
      "url": "https://dev.to/synergistdigitalmedia/zero-click-seo-in-2026-when-winning-means-nobody-visits-your-site-33df",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fres.cloudinary.com%2Fdstn7hw7h%2Fimage%2Fupload%2Fv1764673784%2Frhfp7pbr5ryfmurvkyxh.png\" alt=\"https%3A%2F%2Fres.cloudinary.com%2Fdstn7\"></p><p>Let me paint you a picture: You finally crack the code. Your content ranks #1 for a high-value keyword. You're celebrating with overpriced coffee. Then you check analytics and... nothing. Zero clicks.</p>  \n  \n<p>Welcome to zero-click search, where winning means Google answered the question so well that nobody needs to visit your website. <em>Chef's kiss</em> for user experience. Absolute nightmare for your traffic metrics.</p>  \n  \n<p>But here's the thing—this isn't going away. By late 2025, we're seeing AI Overviews (Google's fancy term for AI-generated answer boxes) on roughly 15-20% of queries. Featured snippets? They've been eating our lunch since 2017. And if you think this trend reverses in 2026, I have a metaverse timeshare to sell you.</p>  \n  \n<p>The question isn't whether to optimize for zero-click results. It's how to do it without tanking your entire content strategy.</p>  \n  \n<h2>  \n    \n    \n  Why Zero-Click Actually Matters (Even Though It Hurts)  \n</h2>  \n  \n<p>I know what you're thinking. \"Why would I optimize for something that doesn't send traffic?\" Fair question. Here's why you don't have a choice.</p>  \n  \n<p>First, visibility still counts. When your brand shows up in an AI Overview or featured snippet, you're getting prime real estate. Position zero. The penthouse suite of search results. Even if users don't click through immediately, they're seeing your brand as the authority.</p>  \n  \n<p>Second—and this surprised me when I dug into the data—featured snippets actually increase branded search. Users see you as the answer source, then search your brand name directly later. It's indirect traffic, but it's real. One client saw a 34% increase in branded searches three months after consistently winning featured snippets in their niche.</p>  \n  \n<p>Third, you're playing defense. If you don't optimize for these positions, your competitors will. And once they own that snippet real estate, clawing it back is brutal.</p>  \n  \n<h2>  \n    \n    \n  How AI Overviews Actually Work (The Technical Stuff)  \n</h2>  \n  \n<p>Google's AI Overviews pull from multiple sources to generate answers. They're not just grabbing your meta description and calling it a day. The system is:</p>  \n  \n<ul>  \n<li>Analyzing top-ranking pages for the query</li>  \n<li>Extracting relevant information across multiple sources</li>  \n<li>Synthesizing an answer using large language models</li>  \n<li>Citing sources (sometimes) with small attribution links</li>  \n<li>Prioritizing content that's structured, clear, and authoritative</li>  \n</ul>  \n  \n<p>The algorithm favors content that's already ranking well organically. You can't hack your way into an AI Overview from page 47. But if you're on page 1, especially positions 1-5, you're in the game.</p>  \n  \n<p>One critical detail: AI Overviews love recency. Fresh content gets weighted heavily for trending topics or news-related queries. That blog post from 2019? Not making the cut unless you've updated it recently.</p>  \n  \n<h2>  \n    \n    \n  The Featured Snippet Playbook (What Still Works)  \n</h2>  \n  \n<p>Featured snippets are the OG zero-click result. We've had years to figure these out, and the fundamentals haven't changed much:</p>  \n  \n<p><strong>Structure matters more than you think.</strong> Use clear H2s and H3s that match question formats. \"What is X?\" \"How to Y?\" \"Why does Z happen?\" Google loves content that mirrors how people actually search.</p>  \n  \n<p><strong>Answer the question immediately.</strong> Don't bury your answer in paragraph three after telling the entire history of the internet. First 50-60 words should directly address the query. Then you can add context and depth.</p>  \n  \n<p><strong>Lists and tables perform exceptionally well.</strong> If your content can be formatted as a numbered list, bullet points, or a comparison table, do it. These formats are snippet candy. I've seen mediocre content win snippets purely because it was structured as a clean table while better content was buried in paragraphs.</p>  \n  \n<p><strong>The 40-60 word sweet spot is real.</strong> Featured snippet answers typically fall in this range. Too short and you're not providing value. Too long and Google truncates or skips you.</p>  \n  \n<p>Here's what changed in 2025: Google started pulling snippets from video content more aggressively. If you have a YouTube video that answers the query clearly in the first 30 seconds, with good captions, you're now competing for snippet real estate. This opened up a whole new optimization vector.</p>  \n  \n<h2>  \n    \n    \n  Optimizing for AI Overview Citations  \n</h2>  \n  \n<p>This is where it gets interesting. Because AI Overviews synthesize from multiple sources, you're not trying to \"win\" the entire overview. You're trying to get cited as one of the authoritative sources.</p>  \n  \n<p>The winning strategy I've seen work:</p>  \n  \n<p><strong>Create genuinely comprehensive content.</strong> AI models favor sources that cover a topic thoroughly with supporting evidence. Thin content doesn't cut it. You need depth, examples, data points. The kind of content that takes actual expertise to create.</p>  \n  \n<p><strong>Use clear attribution for data and claims.</strong> When you cite statistics or research, be specific. \"According to a 2025 study...\" is better than \"Research shows...\" AI models pick up on this specificity and are more likely to cite you as a reliable source.</p>  \n  \n<p><strong>Answer related questions within your content.</strong> Don't just answer one query. Address the logical follow-up questions. If someone searches \"how to optimize for featured snippets,\" they probably also want to know what featured snippets are, why they matter, and how long it takes to see results. Cover the cluster.</p>  \n  \n<p><strong>Update frequently.</strong> I mentioned this earlier, but it's critical. Set calendar reminders to refresh your top-performing content every 3-6 months. Add new examples, update statistics, incorporate recent developments. AI Overviews have a strong recency bias.</p>  \n  \n<p>One client in the B2B SaaS space started getting cited in AI Overviews after we restructured their content to include more original research and data points. They weren't just rehashing the same advice everyone else was giving—they were adding proprietary insights from their customer data. That originality made them cite-worthy.</p>  \n  \n<h2>  \n    \n    \n  The Schema Markup Advantage  \n</h2>  \n  \n<p>Schema markup is like leaving breadcrumbs for Google's algorithms. It helps search engines understand exactly what your content is about and how it's structured.</p>  \n  \n<p>For zero-click optimization, focus on:</p>  \n  \n<p><strong>FAQ schema</strong> for question-and-answer content. This directly feeds into how AI Overviews structure information. Plus, it can trigger those expandable FAQ sections in search results.</p>  \n  \n<p><strong>HowTo schema</strong> for instructional content. Step-by-step guides with proper schema markup have a much higher chance of appearing in AI-generated answers.</p>  \n  \n<p><strong>Article schema</strong> with proper headline, author, and date published markup. This helps establish content freshness and authority.</p>  \n  \n<p>The implementation isn't rocket science. Tools like Schema.org have templates. Most modern CMS platforms have plugins that handle it. The hard part is actually doing it consistently across your content library.</p>  \n  \n<p>One caveat: Schema markup alone won't save poorly written content. It's an amplifier, not a substitute for quality. Think of it as making sure Google can properly read your already-good content.</p>  \n  \n<h2>  \n    \n    \n  When Zero-Click Becomes a Traffic Problem  \n</h2>  \n  \n<p>Let's address the elephant in the room. What do you do when you're winning featured snippets and AI Overview citations, but your traffic is tanking?</p>  \n  \n<p>First, measure what actually matters. If you're a lead generation business, track conversions, not just traffic. Sometimes fewer, more qualified visitors convert better than high traffic volumes. I've seen cases where traffic dropped 20% but leads increased 15% because the remaining traffic was more targeted.</p>  \n  \n<p>Second, optimize for the click-through on zero-click results. Yes, that sounds contradictory. But even in AI Overviews, there are citation links. Make sure:</p>  \n  \n<ul>  \n<li>Your brand name is clearly visible in citations</li>  \n<li>The preview text (if shown) is compelling</li>  \n<li>Your meta description still works hard even if it's not the primary display</li>  \n</ul>  \n  \n<p>Third, diversify your content strategy. Create content that <em>can't</em> be answered in a snippet. In-depth analyses, original research, interactive tools, comprehensive guides that require scrolling. The kind of content where the snippet is just a teaser.</p>  \n  \n<p>One e-commerce client dealt with this by creating \"quick answer\" content optimized for snippets, then linking to detailed product guides and comparison tools that required site visits. The snippets built authority and brand awareness. The deeper content drove conversions.</p>  \n  \n<h2>  \n    \n    \n  The 2026 Playbook: Practical Next Steps  \n</h2>  \n  \n<p>Here's what's actually working as we head into 2026:</p>  \n  \n<p><strong>Audit your current snippet performance.</strong> Tools like SEMrush and Ahrefs show you which keywords trigger featured snippets and whether you're winning them. Start there. Low-hanging fruit is content where you rank positions 2-5 for queries with existing snippets.</p>  \n  \n<p><strong>Create a \"snippet content\" category.</strong> Not every piece of content needs to chase zero-click optimization. But identify 20-30 high-value queries in your niche where winning the snippet or AI Overview citation would build authority. Build content specifically for these.</p>  \n  \n<p><strong>Implement a refresh cadence.</strong> Set up a system to update your top-performing content quarterly. Add new data, fresh examples, recent developments. This keeps you competitive for AI Overviews that favor recency.</p>  \n  \n<p><strong>Test video content for snippet opportunities.</strong> Create short, well-captioned videos that directly answer common questions in your niche. Upload to YouTube with optimized titles and descriptions. This is an underutilized vector for 2026.</p>  \n  \n<p><strong>Track indirect metrics.</strong> Set up custom reports in Google Analytics to track branded search increases, conversion rate changes, and time-to-conversion shifts. Zero-click optimization affects these metrics even when traffic stays flat.</p>  \n  \n<p><strong>Build content clusters, not individual posts.</strong> Create comprehensive topic clusters where your pillar content can get cited in AI Overviews, and your supporting content captures the long-tail traffic that still generates clicks. This is essentially what we covered in our <a href=\"https://example.com/ai-content-marketing-guide\">AI in Content Marketing: 2025 Strategy Guide</a>—the principles apply directly to zero-click optimization.</p>  \n  \n<h2>  \n    \n    \n  The Uncomfortable Truth About 2026  \n</h2>  \n  \n<p>Here's what I've learned after a year of obsessing over this: Zero-click search isn't the enemy. It's just a different game.</p>  \n  \n<p>The old model was simple. Rank high, get clicks, convert visitors. Linear and measurable.</p>  \n  \n<p>The new model is messier. Build authority through zero-click visibility, earn brand recognition, capture traffic through multiple touchpoints, convert over longer timelines. It's harder to attribute. It requires more patience. The metrics don't look as pretty in your monthly reports.</p>  \n  \n<p>But the businesses winning in 2026 aren't the ones fighting this trend. They're the ones who figured out how to use AI Overviews and featured snippets as top-of-funnel brand building, then created content strategies that capture demand at every other stage.</p>  \n  \n<p>You can't optimize your way back to 2015 when every search sent traffic. That world is gone. The question is whether you'll adapt your strategy or keep chasing metrics that matter less every quarter.</p>  \n  \n<p>Start with one high-value query in your niche. Optimize that content for zero-click results using the frameworks above. Measure what happens to branded search and conversions over 90 days. Then scale what works.</p>  \n  \n<p>The traffic might not come directly from that featured snippet. But if you're doing it right, it'll come.</p>",
      "summary": "Let me paint you a picture: You finally crack the code. Your content ranks #1 for a high-value keyword. You're celebrating with overpriced coffee. Then you check analytics and... nothing. Zero clicks.  \n  \nWelcome to zero-click search, where winning means Google answered the question so well that nobody needs to visit your website. Chef's kiss for user experience. Absolute nightmare for your traffic metrics.  \n  \nBut here's the thing—this isn't going away. By late 2025, we're seeing AI Overviews",
      "publishedAt": "2025-12-02T11:09:56.000Z",
      "author": "Drew Madore",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 11.873222145576548,
      "ingestedAt": "2025-12-02T14:44:03.192Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability",
        "governance",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09958093",
      "title": "EKS Standard vs. EKS Auto Mode: The Evolutionary Leap in Kubernetes Operations",
      "url": "https://dev.to/mechcloud_academy/eks-standard-vs-eks-auto-mode-the-evolutionary-leap-in-kubernetes-operations-287g",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F3q5eilympx9819derull.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>For years, Amazon Elastic Kubernetes Service (EKS) has been the gold standard for running containerized workloads on AWS. But let’s be honest: while EKS managed the Control Plane beautifully, the <strong>Data Plane</strong> (the worker nodes) remained a significant operational burden.</p>  \n  \n<p>As Platform Engineers and SREs, we’ve spent countless hours tuning Managed Node Groups (MNGs), debugging CNI plugin versions, wrestling with IAM Roles for Service Accounts (IRSA), and fine-tuning Karpenter to get our bin-packing logic just right.</p>  \n  \n<p>With the release of <strong>EKS Auto Mode</strong>, AWS has fundamentally shifted the Shared Responsibility Model.</p>  \n  \n<p>This isn't just a minor feature update; it is a fork in the road for how we architect clusters. This guide will dissect the architectural differences between EKS Standard and Auto Mode, analyze the \"under-the-hood\" mechanics, and help you decide which path to take.</p>  \n  \n<h2>  \n    \n    \n  The \"Standard\" Way: Maximum Control, Maximum Toil  \n</h2>  \n  \n<p>In what we now call <strong>EKS Standard</strong>, the division of labor is clear but uneven. AWS ensures the API server is up, but the moment a packet leaves the control plane, it’s your problem.</p>  \n  \n<h3>  \n    \n    \n  The Standard Architecture  \n</h3>  \n  \n<p>In a standard cluster, you are the architect of the infrastructure layer:</p>  \n  \n<ol>  \n<li> <strong>Compute:</strong> You define Auto Scaling Groups (ASGs) or Managed Node Groups. You select the instance families (<code>m5.large</code>, <code>c6g.xlarge</code>). You decide on Spot vs. On-Demand ratios.</li>  \n<li> <strong>Scaling:</strong> You install the Cluster Autoscaler or, more likely, <strong>Karpenter</strong>. You manage the provisioner CRDs to ensure nodes spin up when pods go pending.</li>  \n<li> <strong>Operations:</strong> You are responsible for the \"Add-on Lifecycle.\" When you upgrade Kubernetes from 1.29 to 1.30, you must manually ensure the VPC CNI, CoreDNS, and Kube-proxy are compatible.</li>  \n<li> <strong>Storage &amp; Networking:</strong> You manually install the EBS CSI driver and the AWS Load Balancer Controller (LBC) via Helm.</li>  \n</ol>  \n  \n<p><strong>The Pain Point:</strong> The \"Undifferentiated Heavy Lifting.\"<br>  \nEvery hour you spend fixing a conflict between the VPC CNI and a new node kernel is an hour you aren't spending on application reliability. Standard mode is powerful, but it requires a dedicated Platform Team to maintain the plumbing.</p>  \n<h2>  \n    \n    \n  Enter EKS Auto Mode: The \"Serverless\" Node Experience  \n</h2>  \n  \n<p>EKS Auto Mode is AWS’s answer to the operational overhead of Kubernetes. It is distinct from Fargate (which had severe limitations regarding DaemonSets and caching) because it still runs on EC2 instances—<strong>you just don't manage them.</strong></p>  \n  \n<p>When you enable Auto Mode, EKS takes ownership of the <strong>Compute, Storage, and Networking</strong> lifecycle within the cluster.</p>  \n<h3>  \n    \n    \n  1. Compute: The \"Invisible\" Karpenter  \n</h3>  \n  \n<p>In Auto Mode, the concept of a \"Node Group\" essentially vanishes. You don't create ASGs. You don't pick instance types.</p>  \n  \n<p>Instead, EKS uses <strong>Automated Node Pools</strong>.</p>  \n  \n<ul>  \n<li>  <strong>How it works:</strong> EKS analyzes pending pods. If a pod requests 4 vCPUs and 16GB RAM, EKS automatically provisions an EC2 instance that fits that workload and joins it to the cluster.</li>  \n<li>  <strong>Under the hood:</strong> It behaves remarkably like Karpenter is built directly into the Control Plane. It handles bin-packing, consolidation, and spot instance interruption handling automatically.</li>  \n<li>  <strong>Maintenance:</strong> AWS handles the OS patching. When a node needs a security update, EKS seamlessly drains the node and replaces it, adhering to your Pod Disruption Budgets (PDBs).</li>  \n</ul>  \n<h3>  \n    \n    \n  2. Networking: Native Load Balancing  \n</h3>  \n  \n<p>In Standard mode, exposing a service via an Application Load Balancer (ALB) meant installing the AWS Load Balancer Controller, setting up IAM roles, and managing CRDs.</p>  \n  \n<p>In Auto Mode, this is native.</p>  \n  \n<ul>  \n<li>  <strong>The Change:</strong> When you create a Service of <code>type: LoadBalancer</code>, EKS talks directly to the AWS networking APIs to provision a Network Load Balancer (NLB).</li>  \n<li>  <strong>Ingress:</strong> Similarly, creating an Ingress resource automatically triggers ALB creation without requiring a third-party controller running in your cluster.</li>  \n</ul>  \n<h3>  \n    \n    \n  3. Storage: Built-in CSI  \n</h3>  \n  \n<p>Stateful workloads in Standard mode often break during upgrades because the EBS CSI driver version falls behind the cluster version. In Auto Mode, the <strong>EBS CSI functionality is embedded</strong>. You simply request a Persistent Volume Claim (PVC), and the storage appears.</p>  \n<h2>  \n    \n    \n  Security: The Paradigm Shift  \n</h2>  \n  \n<p>This is perhaps the most controversial change for old-school Ops teams: <strong>EKS Auto Mode locks down the nodes.</strong></p>  \n<h3>  \n    \n    \n  No SSH, No SSM  \n</h3>  \n  \n<p>In Auto Mode, you cannot SSH into the worker nodes. You cannot use AWS Systems Manager (SSM) Session Manager to jump into a node and run <code>htop</code>.</p>  \n  \n<ul>  \n<li>  <strong>Why?</strong> The nodes are treated as ephemeral resources managed by AWS.</li>  \n<li>  <strong>The Benefit:</strong> This enforces an immutable infrastructure pattern. If a node is \"acting weird,\" you don't fix it; you delete the pod, and EKS replaces the node.</li>  \n</ul>  \n<h3>  \n    \n    \n  EKS Pod Identity  \n</h3>  \n  \n<p>Auto Mode moves away from the complex IRSA (IAM Roles for Service Accounts) OIDC setup. It defaults to <strong>EKS Pod Identity</strong>.<br>  \nThis creates a local agent on the nodes that intercepts AWS API calls from your pods and exchanges a token for temporary AWS credentials. It is significantly easier to set up in Terraform/CloudFormation than the OIDC provider method.</p>  \n<h2>  \n    \n    \n  Comparison Matrix: Standard vs. Auto  \n</h2>  \n  \n<div><table>  \n<thead>  \n<tr>  \n<th>Feature</th>  \n<th>EKS Standard</th>  \n<th>EKS Auto Mode</th>  \n</tr>  \n</thead>  \n<tbody>  \n<tr>  \n<td><strong>Node Management</strong></td>  \n<td>  \n<strong>Manual</strong> (Node Groups / Karpenter)</td>  \n<td>  \n<strong>Automatic</strong> (Managed Node Pools)</td>  \n</tr>  \n<tr>  \n<td><strong>OS Patching</strong></td>  \n<td>You trigger rollouts / AMI updates</td>  \n<td>Fully Automated by AWS</td>  \n</tr>  \n<tr>  \n<td><strong>Instance Selection</strong></td>  \n<td>You define classes (e.g., <code>t3</code>, <code>m5</code>)</td>  \n<td>EKS selects based on Pod Spec</td>  \n</tr>  \n<tr>  \n<td><strong>Load Balancing</strong></td>  \n<td>Install AWS LBC Helm Chart</td>  \n<td>Native / Built-in</td>  \n</tr>  \n<tr>  \n<td><strong>EBS Storage</strong></td>  \n<td>Install EBS CSI Driver</td>  \n<td>Native / Built-in</td>  \n</tr>  \n<tr>  \n<td><strong>Node Access</strong></td>  \n<td>SSH / SSM enabled</td>  \n<td><strong>Strictly Prohibited</strong></td>  \n</tr>  \n<tr>  \n<td><strong>Custom User Data</strong></td>  \n<td>Allowed (Custom Scripts)</td>  \n<td>Not Supported</td>  \n</tr>  \n<tr>  \n<td><strong>Cost</strong></td>  \n<td>EC2 + Control Plane ($0.10/hr)</td>  \n<td>EC2 + Control Plane + Flat fee</td>  \n</tr>  \n<tr>  \n<td><strong>Supported OS</strong></td>  \n<td>AL2, AL2023, Bottlerocket, Windows, Ubuntu</td>  \n<td>EKS Auto-optimized OS (AL2023 based)</td>  \n</tr>  \n</tbody>  \n</table></div>  \n<h2>  \n    \n    \n  Infrastructure as Code: The Difference  \n</h2>  \n  \n<p>The reduction in Terraform code required for Auto Mode is staggering.</p>  \n  \n<p><strong>The \"Standard\" Way (Simplified):</strong><br>  \nYou need to define the cluster, the node groups, the IAM roles for nodes, and the Helm releases for necessary controllers.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>module</span> <span>\"eks\"</span> <span>{</span>  \n  <span>source</span>  <span>=</span> <span>\"terraform-aws-modules/eks/aws\"</span>  \n  \n  <span># You define the hardware</span>  \n  <span>eks_managed_node_groups</span> <span>=</span> <span>{</span>  \n    <span>app_nodes</span> <span>=</span> <span>{</span>  \n      <span>instance_types</span> <span>=</span> <span>[</span><span>\"m5.large\"</span><span>]</span>  \n      <span>min_size</span>     <span>=</span> <span>2</span>  \n      <span>max_size</span>     <span>=</span> <span>10</span>  \n    <span>}</span>  \n  <span>}</span>  \n<span>}</span>  \n  \n<span># Then you must maintain this separately</span>  \n<span>resource</span> <span>\"helm_release\"</span> <span>\"aws_load_balancer_controller\"</span> <span>{</span>  \n  <span>name</span>       <span>=</span> <span>\"aws-load-balancer-controller\"</span>  \n  <span>repository</span> <span>=</span> <span>\"https://aws.github.io/eks-charts\"</span>  \n  <span>chart</span>      <span>=</span> <span>\"aws-load-balancer-controller\"</span>  \n  <span># ... extensive configuration ...</span>  \n<span>}</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p><strong>The \"Auto\" Way:</strong><br>  \nYou simply enable the capability flags.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>resource</span> <span>\"aws_eks_cluster\"</span> <span>\"auto\"</span> <span>{</span>  \n  <span>name</span> <span>=</span> <span>\"production-auto\"</span>  \n  \n  <span># The \"Easy Button\"</span>  \n  <span>compute_config</span> <span>{</span>  \n    <span>enabled</span>       <span>=</span> <span>true</span>   \n    <span>node_pools</span>    <span>=</span> <span>[</span><span>\"general-purpose\"</span><span>,</span> <span>\"system\"</span><span>]</span>   \n    <span>node_role_arn</span> <span>=</span> <span>aws_iam_role</span><span>.</span><span>auto_node_role</span><span>.</span><span>arn</span>  \n  <span>}</span>  \n  \n  <span># Native Networking</span>  \n  <span>kubernetes_network_config</span> <span>{</span>  \n    <span>elastic_load_balancing</span> <span>{</span>  \n      <span>enabled</span> <span>=</span> <span>true</span>   \n    <span>}</span>  \n  <span>}</span>  \n  \n  <span># Native Storage</span>  \n  <span>storage_config</span> <span>{</span>  \n    <span>block_storage</span> <span>{</span>  \n      <span>enabled</span> <span>=</span> <span>true</span>   \n    <span>}</span>  \n  <span>}</span>  \n<span>}</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p><em>No Node Groups to define. No Helm charts to manage for basic infrastructure.</em></p>  \n  \n<h2>  \n    \n    \n  When should you use which?  \n</h2>  \n  \n<h3>  \n    \n    \n  Case for EKS Auto Mode  \n</h3>  \n  \n<ol>  \n<li> <strong>Platform Efficiency:</strong> If your team spends more time upgrading clusters than building internal developer platforms (IDPs), switch to Auto. It drastically reduces \"Day 2\" operations.</li>  \n<li> <strong>Dynamic Workloads:</strong> If you run AI/ML training jobs, CI/CD runners, or batch processing, Auto Mode's ability to seamlessly scale from 0 to 100 nodes (and back) without configuring Karpenter is a huge win.</li>  \n<li> <strong>Greenfield Projects:</strong> Start here. Don't build technical debt (custom node groups) unless you prove you need them.</li>  \n</ol>  \n  \n<h3>  \n    \n    \n  Case for EKS Standard  \n</h3>  \n  \n<ol>  \n<li> <strong>Custom Kernel Requirements:</strong> If you need to load proprietary kernel modules, modify <code>sysctl</code> parameters that require root node access, or use a custom hardened AMI (like CIS benchmarks that deviate from AWS standards), you need Standard.</li>  \n<li> <strong>Legacy \"Pet\" Applications:</strong> If you have apps that require specific host-level configurations or mounting local instance store NVMe drives in a specific way that the CSI driver doesn't support yet.</li>  \n<li> <strong>Strict Compliance:</strong> If your compliance framework requires you to have SSH access to nodes for forensic analysis (though this is arguably an anti-pattern in cloud-native), Auto Mode's locked-down nature might be a blocker.</li>  \n</ol>  \n  \n<h2>  \n    \n    \n  Conclusion  \n</h2>  \n  \n<p>EKS Auto Mode is not just a wrapper; it is the maturation of Kubernetes on AWS. It acknowledges that for 90% of users, <strong>the node is just a utility</strong>.</p>  \n  \n<p>By abstracting the Data Plane, AWS allows Platform Engineers to move up the stack. Instead of being \"Server Mechanics\" fixing broken drivers and patching OS kernels, we can finally become \"Platform Architects,\" focusing on reliability, observability, and developer experience.</p>  \n  \n<p>If you are starting a new cluster today, <strong>start with Auto Mode</strong>. If you are on Standard, look at your backlog of maintenance tasks—if it's full of upgrades and patching, it might be time to plan your migration.</p>  \n  \n<p><em>Have you tried EKS Auto Mode yet? Did the lack of SSH access break your workflow? Let’s discuss in the comments below!</em></p>",
      "summary": "For years, Amazon Elastic Kubernetes Service (EKS) has been the gold standard for running containerized workloads on AWS. But let’s be honest: while EKS managed the Control Plane beautifully, the Data Plane (the worker nodes) remained a significant operational burden.  \n  \nAs Platform Engineers and SREs, we’ve spent countless hours tuning Managed Node Groups (MNGs), debugging CNI plugin versions, wrestling with IAM Roles for Service Accounts (IRSA), and fine-tuning Karpenter to get our bin-packi",
      "publishedAt": "2025-12-02T11:09:34.000Z",
      "author": "Akash",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 13.851840554499036,
      "ingestedAt": "2025-12-02T14:44:03.193Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "observability",
        "governance",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09958096",
      "title": "Why No Usable Data Found QR Code Scan Errors Occur 2026",
      "url": "https://dev.to/eira-wexford/why-no-usable-data-found-qr-code-scan-errors-occur-2026-30bn",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft66cw9itgr8szll3m9q8.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>By 2026, successfully scanning a QR code will be the easy part. The real challenge is the frustrating \"No Usable Data Found\" error that appears even when the scan seems to work.</p>  \n  \n<p>This error signals a shift from simple scanning problems to complex data-layer failures. This guide breaks down the advanced reasons these errors happen now and what you can do about them.</p>  \n  \n<h2>  \n    \n    \n  Understanding the \"No Usable Data Found\" QR Code Error  \n</h2>  \n  \n<p>Getting this error doesn't mean your phone's camera is broken. It means your device read the code but couldn't understand or access the information it pointed to. The bridge between the physical code and the digital experience is down.</p>  \n  \n<h3>  \n    \n    \n  What \"No Usable Data Found\" Actually Means  \n</h3>  \n  \n<p>This message indicates a data retrieval failure. Your scanner app successfully decoded the pattern but found the resulting information to be empty, corrupted, or inaccessible. It's like following a map to a location only to find an empty lot where a building should be.</p>  \n  \n<h3>  \n    \n    \n  A Quick Look at How QR Codes Work  \n</h3>  \n  \n<p>A QR code is a visual storage device. It encodes information like a web address, text, or contact details into a black-and-white pixelated square. When you scan it, your device's app translates that pattern back into its original data and attempts to act on it, like opening a website.</p>  \n  \n<h2>  \n    \n    \n  Common Causes for QR Code Scan Failures in 2026  \n</h2>  \n  \n<p>While old-school issues still exist, the reasons for scan failures in 2026 are far more advanced. They often involve the complex data and experiences linked to the code itself.</p>  \n  \n<h3>  \n    \n    \n  Poor QR Code Print Quality or Physical Damage  \n</h3>  \n  \n<p>A classic problem that persists. Scratches, poor contrast, or blurry printing can make a code unreadable. If the code is printed on a glossy surface with heavy glare, scanners can also struggle to decode the pattern correctly.</p>  \n  \n<h3>  \n    \n    \n  Outdated or Incompatible QR Code Scanner Applications  \n</h3>  \n  \n<p>Many modern QR codes link to advanced experiences like augmented reality (AR) or blockchain assets. If your scanner app hasn't been updated, it may not support these new data types, resulting in an error even if the code itself is fine.</p>  \n  \n<h3>  \n    \n    \n  Incorrect QR Code Generation and Data Encoding  \n</h3>  \n  \n<p>This is a creator-side error. If the data was entered incorrectly when the QR code was made, it will point to a nonexistent link or contain garbled text. The code is generated perfectly, but the information inside is flawed from the start.</p>  \n  \n<h3>  \n    \n    \n  Issues with the Scanning Device Camera  \n</h3>  \n  \n<p>A dirty camera lens is a common culprit. Smudges or dirt can obstruct the camera's view, preventing it from clearly seeing the entire QR code pattern. Low-light conditions can also make it difficult for the camera to focus properly.</p>  \n  \n<h3>  \n    \n    \n  Environmental Interference During Scanning  \n</h3>  \n  \n<p>Scanning a code from too far away or at a sharp angle can cause failures. Insufficient lighting, shadows, or even a shaky hand can prevent the app from capturing a stable image of the code to analyze.</p>  \n  \n<h3>  \n    \n    \n  Data Incompatibility or Corrupted Information  \n</h3>  \n  \n<p>This is a major issue in 2026. A QR code might link to a 3D model for an AR experience that your phone doesn't support. It could also point to a digital asset on a blockchain, but the data is inaccessible because the network is congested or the data format is wrong.</p>  \n  \n<h3>  \n    \n    \n  Server-Side Problems with Dynamic QR Codes  \n</h3>  \n  \n<p>Dynamic QR codes point to a URL that then redirects to the final content. This allows the destination to be updated. But if the server managing this redirect is down or misconfigured, your scan will lead nowhere. Ensuring a stable backend is essential, a task often handled through expert <a href=\"https://indiit.com/mobile-app-development-texas/\">mobile app development texas</a> to build reliable systems.</p>  \n  \n<h3>  \n    \n    \n  Application or Platform Specific Restrictions  \n</h3>  \n  \n<p>Data privacy rules are stricter than ever. A QR code might be blocked from showing data because you haven't given consent per GDPR or CCPA rules. Similarly, authenticator apps like Microsoft Authenticator can fail if there are security restrictions or time synchronization issues with your account.</p>  \n  \n<h2>  \n    \n    \n  Practical Solutions to Resolve \"No Usable Data Found\" Errors  \n</h2>  \n  \n<p>Fixing these new-age QR code errors involves looking beyond the physical scan. Here are practical steps for both users and creators to solve the problem.</p>  \n  \n<h3>  \n    \n    \n  Essential Troubleshooting Steps for Users  \n</h3>  \n  \n<p>Start with the basics. Clean your phone's camera lens with a soft cloth. Check that you have a stable internet connection, as many QR codes require it. Try restarting the scanner app or your phone to clear any temporary glitches.</p>  \n  \n<h3>  \n    \n    \n  Optimizing Your QR Code Scanning Environment  \n</h3>  \n  \n<p>Ensure you are in a well-lit area without harsh shadows or glare. Position your phone directly in front of the code, not at an angle. Move closer or farther away until the camera focuses clearly and the entire code is visible in the frame.</p>  \n  \n<h3>  \n    \n    \n  Updating or Choosing a Better QR Code Scanner App  \n</h3>  \n  \n<p>If you suspect the issue is data incompatibility, check for updates to your scanner app. Consider using your phone's native camera app, as it's often updated with the operating system to support new technologies like AR.</p>  \n  \n<h3>  \n    \n    \n  Verifying and Regenerating the QR Code  \n</h3>  \n  \n<p>If you created the QR code, double-check the source data for typos or errors. Use a different device to scan and test the code yourself. If the data is incorrect, you will need to generate a new QR code with the corrected information.</p>  \n  \n<h3>  \n    \n    \n  Addressing Device Camera Malfunctions  \n</h3>  \n  \n<p>Open your regular camera app and see if it focuses correctly on other objects. If your camera is blurry or unable to focus, you may have a hardware issue that requires professional repair.</p>  \n  \n<h3>  \n    \n    \n  Checking Data Availability for Dynamic QR Codes  \n</h3>  \n  \n<p>If a QR code for a service or event isn't working, check the company's official website or social media for any announced outages. The problem may be with their server, not your device.</p>  \n  \n<h3>  \n    \n    \n  Contacting Support for Specific Application Services  \n</h3>  \n  \n<p>For issues with secure apps like authenticators or banking apps, the problem is likely account-specific. Contact the application's support team directly, as they can check for server-side issues or security flags on your account.</p>  \n  \n<h2>  \n    \n    \n  Preventing Future \"No Usable Data Found\" QR Code Issues  \n</h2>  \n  \n<p>A few smart habits can help you avoid these errors, whether you are creating QR codes or just scanning them.</p>  \n  \n<h3>  \n    \n    \n  Best Practices for Generating High-Quality QR Codes  \n</h3>  \n  \n<ul>  \n<li>  <strong>Use Dynamic Codes:</strong> Always choose dynamic QR codes over static ones. This lets you fix typos or update the destination link without having to reprint the code.</li>  \n<li>  <strong>Ensure High Resolution:</strong> Export your QR code as a vector file (like SVG or EPS) to ensure it stays sharp at any size.</li>  \n<li>  <strong>Add a Quiet Zone:</strong> Leave a clear, empty margin around the QR code. This \"quiet zone\" helps scanners distinguish the code from its surroundings.</li>  \n</ul>  \n  \n<h3>  \n    \n    \n  Smart Scanning Habits for Consistent Results  \n</h3>  \n  \n<ul>  \n<li>  <strong>Use the Native Camera:</strong> Your phone's built-in camera app is usually the most reliable and up-to-date scanner.</li>  \n<li>  <strong>Be Aware of Context:</strong> If a QR code promises an AR experience, be prepared for it to require more processing power and a strong internet connection.</li>  \n<li>  <strong>Don't Scan Suspicious Codes:</strong> Be cautious about scanning random QR codes from untrusted sources, as they can lead to phishing sites.</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  Frequently Asked Questions About QR Code Scan Errors  \n</h2>  \n  \n<h3>  \n    \n    \n  Why Is My iPhone Not Scanning QR Codes  \n</h3>  \n  \n<p>First, check your iPhone's settings. Go to Settings &gt; Camera and ensure \"Scan QR Codes\" is enabled. If it is, the issue could be a dirty lens, poor lighting, or an outdated iOS version that doesn't support the QR code's linked data type.</p>  \n  \n<h3>  \n    \n    \n  What Causes \"No Usable Data Found\" on Microsoft Authenticator  \n</h3>  \n  \n<p>This error on an authenticator app is almost always a server-side or account-sync issue. It can be caused by the time on your device being out of sync with the server, a temporary service outage, or an invalid activation code from the service you're trying to add.</p>  \n  \n<h3>  \n    \n    \n  Can a QR Code Expire or Become Invalid  \n</h3>  \n  \n<p>A static QR code, which has the data embedded directly, never expires. However, a dynamic QR code points to a web link that can be changed or deleted. If that link is broken or the service is discontinued, the QR code will effectively \"expire\" and lead to an error.</p>  \n  \n<h3>  \n    \n    \n  How Can I Test a QR Code Before Deployment  \n</h3>  \n  \n<p>Always test your QR code before printing it. Scan it with multiple devices (both iPhone and Android) and different scanner apps. Ensure it leads to the correct destination and that all content loads properly on a mobile device.</p>  \n  \n<h2>  \n    \n    \n  Conclusion  \n</h2>  \n  \n<p>The \"No Usable Data Found\" error in 2026 marks a shift in how we interact with QR codes. The problem is no longer just about a successful scan but about the reliability of the complex digital experiences they unlock, from personalized content to AR and blockchain data.</p>  \n  \n<p>Understanding these new failure points is key. For users, it means checking app compatibility and internet connections. For creators, it demands a focus on robust backends and testing across multiple platforms.</p>  \n  \n<p>Start by using your phone's native camera for scans and ensuring your apps are always up to date. This prepares your device to handle the advanced, data-rich world that QR codes now connect us to.</p>",
      "summary": "By 2026, successfully scanning a QR code will be the easy part. The real challenge is the frustrating \"No Usable Data Found\" error that appears even when the scan seems to work.  \n  \nThis error signals a shift from simple scanning problems to complex data-layer failures. This guide breaks down the advanced reasons these errors happen now and what you can do about them.  \n  \n  \n    \n    \n  Understanding the \"No Usable Data Found\" QR Code Error  \n  \n  \nGetting this error doesn't mean your phone's ",
      "publishedAt": "2025-12-02T11:08:39.000Z",
      "author": "Eira Wexford",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 10.388408048618428,
      "ingestedAt": "2025-12-02T14:44:03.193Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "testing",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09958098",
      "title": "🔥 AI News Hub — A Complete AI/Tech News Aggregator SaaS You Fully Own",
      "url": "https://dev.to/dhren2019/ai-news-hub-a-complete-aitech-news-aggregator-saas-you-fully-own-2067",
      "content": "<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fe63qxfyxh3j8aysf5j58.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fe63qxfyxh3j8aysf5j58.png\" alt=\"\" width=\"800\" height=\"444\"></a></p> \n \n<p>I built AI News Hub as a complete platform that automatically collects, organizes, and publishes the latest content from the AI world, programming, machine learning, dev tools, and tech tutorials. Every 2 hours, the system scrapes trusted sources, cleans the data, generates SEO-optimized posts, and updates a fully featured dashboard. It also sends push notifications to users whenever new content is available.</p> \n \n<p>The whole project is designed to be a plug-and-launch SaaS: it includes authentication, subscriptions, blog system, PRO mode (ads removed), backend API, scraper, SEO, and everything needed to run a polished production website.</p> \n \n<p>⚙️ Scraper · Backend · Dashboard · Push Notifications · Authentication · SEO · Blog · Friendly URLs<br> \nReact 18 · FastAPI Python · TailwindCSS · shadcn/ui · MongoDB Atlas · OneSignal · Clerk Auth</p> \n \n<p>🧩 FEATURES INCLUDED</p> \n \n<p>🎨 Frontend (React + Tailwind + shadcn)<br> \nWhat I built on the front:</p> \n \n<p>SEO-ready homepage</p> \n \n<p>/hub dashboard with all scraped news</p> \n \n<p>/subscription page for plans</p> \n \n<p>/profile for user details</p> \n \n<p>/post/:slug for individual articles</p> \n \n<p>/blog with a complete technical blogging system</p> \n \n<p>SEO: dynamic titles, meta descriptions, OpenGraph, JSON-LD, sitemap, robots, and clean URLs like:<br> \n/post/openai-new-model-2025</p> \n \n<p>🧠 Backend (FastAPI + Python)<br> \nThe backend exposes clean endpoints:<br> \n/api/articles<br> \n/api/post/{slug}<br> \n/api/dashboard<br> \n/api/notifications/send</p> \n \n<p>I included Pydantic models, error handling, and optional Clerk token validation.</p> \n \n<p>🤖 Automated Scraper<br> \nThis part is fully automated:</p> \n \n<p>Runs every 2 hours</p> \n \n<p>Normalizes and deduplicates content</p> \n \n<p>Inserts everything into MongoDB</p> \n \n<p>Triggers push notifications when new posts appear</p> \n \n<p>📡 Push Notifications<br> \nBuilt-in OneSignal integration:</p> \n \n<p>Automatic registration</p> \n \n<p>Service worker included</p> \n \n<p>Works for new article alerts</p> \n \n<p>💸 Monetization (Monthly Subscriptions)</p> \n \n<p>I added subscription billing using Clerk + Clerk Billing.</p> \n \n<p>🔐 PRO Mode:</p> \n \n<p>Paying users don’t see ads</p> \n \n<p>Free users see ads</p> \n \n<p>Automatic monthly billing</p> \n \n<p>You can set any price you want.</p> \n \n<p>📝 SEO Package</p> \n \n<p>Auto-generated titles</p> \n \n<p>Optimized meta descriptions</p> \n \n<p>Clean SEO-friendly slugs</p> \n \n<p>Article schema</p> \n \n<p>Dynamic sitemap + robots.txt</p> \n \n<p>🎯 Perfect For</p> \n \n<p>Developers wanting a ready SaaS</p> \n \n<p>Makers shipping a fast MVP</p> \n \n<p>Freelancers reselling SaaS to clients</p> \n \n<p>Students learning real-world architecture</p> \n \n<p>🧾 Summary of What I Built</p> \n \n<p>Full frontend</p> \n \n<p>Backend API</p> \n \n<p>Automated scraper</p> \n \n<p>Blog system</p> \n \n<p>Push notifications</p> \n \n<p>OAuth + Auth</p> \n \n<p>Subscriptions + PRO mode</p> \n \n<p>SEO + deployment-ready</p> \n \n<p><a href=\"https://ainewshub2025.netlify.app/\">FULL PROJECT HERE</a> and if you are interested you can purchase <a href=\"https://buy.polar.sh/polar_cl_lqtGvMKK6k7MSW521gbPP7U1U1ypSqINbywGp0vAIH8\">here</a>!</p> \n \n<p>It’s a complete, fully connected SaaS—ready to run or sell.</p>",
      "summary": " \n \nI built AI News Hub as a complete platform that automatically collects, organizes, and publishes the latest content from the AI world, programming, machine learning, dev tools, and tech tutorials. Every 2 hours, the system scrapes trusted sources, cleans the data, generates SEO-optimized posts, and updates a fully featured dashboard. It also sends push notifications to users whenever new content is available. \n \nThe whole project is designed to be a plug-and-launch SaaS: it includes authenti",
      "publishedAt": "2025-12-02T11:07:29.000Z",
      "author": "Dhren",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 10.387806884992044,
      "ingestedAt": "2025-12-02T14:44:03.193Z",
      "tags": [
        "code_review",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b0995809a",
      "title": "How to Protect Model Context Protocol (MCP) Servers with OpenAM and OpenIG",
      "url": "https://dev.to/maximthomas/how-to-protect-model-context-protocol-mcp-servers-with-openam-and-openig-h83",
      "content": "<h2> \n   \n   \n  Introduction \n</h2> \n \n<p>Large language model (LLM) agents can perform various tasks, from writing code or texts to booking airline tickets. Agents consist of a client that interacts with the user and a server that performs the required tasks. The interaction between the client, server, and LLM occurs via the Model Context Protocol <a href=\"https://modelcontextprotocol.io/docs/getting-started/intro\">MCP</a>.</p> \n \n<p>MCP servers often have access to sensitive information, such as an internal source code repository or a customer database. Of course, not all users should have access to this data, even through an agent. To protect against unauthorized access, the Model Context Protocol specification describes the possibility of authorization based on OAuth 2.1: <a href=\"https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization\">https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization</a>.</p> \n \n<p>In this article, we will deploy a simple MCP server developed based on <a href=\"https://spring.io/projects/spring-ai\">Spring AI</a> and close it with the <a href=\"https://github.com/OpenIdentityPlatform/OpenIG\">OpenIG</a> authorization gateway. The <a href=\"https://github.com/OpenIdentityPlatform/OpenAM\">OpenAM</a> authentication service will be responsible for authentication. </p> \n \n<p>We will use VS Code with the Copilot extension as the MCP client.</p> \n \n<h2> \n   \n   \n  Project description \n</h2> \n \n<p>The source code for the OpenAM, OpenIG, and MCP server configuration is available at: <a href=\"https://github.com/OpenIdentityPlatform/openam-openig-mcp-example\">https://github.com/OpenIdentityPlatform/openam-openig-mcp-example</a></p> \n \n<p>The project consists of three services described in the <code>docker-compose.yml</code> file.<br> \n</p> \n \n<div> \n<pre><code><span>services</span><span>:</span> \n  <span>openig</span><span>:</span> \n    <span>build</span><span>:</span> \n      <span>context</span><span>:</span> <span>./openig-docker</span> \n      <span>dockerfile</span><span>:</span> <span>Dockerfile</span> \n    <span>container_name</span><span>:</span> <span>openig</span> \n    <span>volumes</span><span>:</span> \n      <span>-</span> <span>./openig-config:/usr/local/openig-config:ro</span> \n    <span>ports</span><span>:</span>   \n      <span>-</span> <span>\"</span><span>8081:8080\"</span> \n    <span>environment</span><span>:</span> \n      <span>CATALINA_OPTS</span><span>:</span> <span>-Dopenig.base=/usr/local/openig-config -Dopenam=http://openam.example.org:8080/openam</span> \n    <span>networks</span><span>:</span> \n      <span>openam_network</span><span>:</span> \n        <span>aliases</span><span>:</span> \n          <span>-</span> <span>openig.example.org</span> \n \n  <span>openam</span><span>:</span> \n    <span>build</span><span>:</span> \n      <span>context</span><span>:</span> <span>./openam-docker</span> \n      <span>dockerfile</span><span>:</span> <span>Dockerfile</span> \n    <span>container_name</span><span>:</span> <span>openam</span> \n    <span>restart</span><span>:</span> <span>always</span> \n    <span>hostname</span><span>:</span> <span>openam.example.org</span> \n    <span>ports</span><span>:</span> \n      <span>-</span> <span>\"</span><span>8080:8080\"</span> \n    <span>volumes</span><span>:</span> \n      <span>-</span> <span>openam-data:/usr/openam/config</span> \n    <span>networks</span><span>:</span> \n      <span>openam_network</span><span>:</span> \n        <span>aliases</span><span>:</span> \n          <span>-</span> <span>openam.example.org</span> \n \n  <span>time-mcp-server</span><span>:</span> \n    <span>build</span><span>:</span> \n      <span>context</span><span>:</span> <span>./timeserver</span> \n      <span>dockerfile</span><span>:</span> <span>Dockerfile</span> \n    <span>container_name</span><span>:</span> <span>time-mcp-server</span> \n    <span>ports</span><span>:</span> \n      <span>-</span> <span>\"</span><span>8082:8080\"</span> \n    <span>networks</span><span>:</span> \n      <span>openam_network</span><span>:</span> \n        <span>aliases</span><span>:</span> \n          <span>-</span> <span>timeserver.example.org</span> \n<span>networks</span><span>:</span> \n  <span>openam_network</span><span>:</span> \n    <span>driver</span><span>:</span> <span>bridge</span> \n \n<span>volumes</span><span>:</span> \n  <span>openam-data</span><span>:</span> \n</code></pre> \n \n</div> \n \n \n \n<h2> \n   \n   \n  Preparing for launch \n</h2> \n \n<p>For example, the host name for OpenAM will be <code>openam.example.org</code>, and for OpenIG it will be <code>openig.example.org</code>. Open the <code>hosts</code> file and add the host names and IP addresses to it, for example<br> \n</p> \n \n<div> \n<pre><code>127.0.0.1 openam.example.org openig.example.org \n</code></pre> \n \n</div> \n \n \n \n<p>On Windows systems, the hosts file is located in the <code>C:\\Windows/System32/drivers/etc/hosts</code> directory, and on Linux or Mac OS in <code>/etc/hosts</code>.</p> \n \n<h2> \n   \n   \n  MCP Server \n</h2> \n \n<p>The MCP server has a method for returning the current time in ISO 8601 format.<br> \n</p> \n \n<div> \n<pre><code><span>@Service</span> \n<span>public</span> <span>class</span> <span>TimeService</span> <span>{</span> \n \n    <span>@Tool</span><span>(</span><span>name</span> <span>=</span> <span>\"current_time_service\"</span><span>,</span> <span>description</span> <span>=</span> <span>\"Returns current time in ISO 8601 format\"</span><span>)</span> \n    <span>public</span> <span>String</span> <span>getTime</span><span>()</span> <span>{</span> \n        <span>return</span>  <span>Instant</span><span>.</span><span>now</span><span>().</span><span>toString</span><span>();</span> \n    <span>}</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p>For more details on creating an MCP server, please refer to the <a href=\"https://docs.spring.io/spring-ai/reference/api/mcp/mcp-server-boot-starter-docs.html\">documentation</a> or in the Spring AI <a href=\"https://spring.io/blog/2025/09/16/spring-ai-mcp-intro-blog\">blog</a>.</p> \n \n<p>Start the OpenAM, OpenIG, and MCP server Docker containers with the command:<br> \n</p> \n \n<div> \n<pre><code>docker compose up <span>--build</span> \n</code></pre> \n \n</div> \n \n \n \n<p>Check the availability of the running MCP server with the command:<br> \n</p> \n \n<div> \n<pre><code>curl <span>-X</span> POST  <span>--location</span>  <span>\"http://localhost:8082/mcp\"</span> <span>\\</span> \n    <span>-H</span> <span>\"Content-Type: application/json\"</span> <span>\\</span> \n    <span>-H</span> <span>\"Accept: application/json, text/event-stream\"</span> <span>\\</span> \n    <span>-d</span> <span>'{ \n  \"jsonrpc\": \"2.0\", \n  \"id\": 0, \n  \"method\": \"initialize\", \n  \"params\": { \n    \"protocolVersion\": \"2025-06-18\", \n    \"capabilities\": {} \n  } \n}'</span> \n \n<span>{</span> \n  <span>\"id\"</span>: 0, \n  <span>\"jsonrpc\"</span>: <span>\"2.0\"</span>, \n  <span>\"result\"</span>: <span>{</span> \n    <span>\"capabilities\"</span>: <span>{</span> \n      <span>\"completions\"</span>: <span>{}</span>, \n      <span>\"prompts\"</span>: <span>{</span> \n        <span>\"listChanged\"</span>: <span>false</span> \n      <span>}</span>, \n      <span>\"resources\"</span>: <span>{</span> \n        <span>\"listChanged\"</span>: <span>false</span>, \n        <span>\"subscribe\"</span>: <span>false</span> \n      <span>}</span>, \n      <span>\"tools\"</span>: <span>{</span> \n        <span>\"listChanged\"</span>: <span>false</span> \n      <span>}</span> \n    <span>}</span>, \n    <span>\"protocolVersion\"</span>: <span>\"2025-03-26\"</span>, \n    <span>\"serverInfo\"</span>: <span>{</span> \n      <span>\"name\"</span>: <span>\"time-server-mcp\"</span>, \n      <span>\"version\"</span>: <span>\"0.0.1\"</span> \n    <span>}</span> \n  <span>}</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p>Let's check the availability of tools in the MCP server:<br> \n</p> \n \n<div> \n<pre><code>curl <span>-X</span> POST  <span>--location</span>  <span>\"http://localhost:8082/mcp\"</span> <span>\\</span> \n    <span>-H</span> <span>\"Content-Type: application/json\"</span> <span>\\</span> \n    <span>-H</span> <span>\"Accept: application/json, text/event-stream\"</span> <span>\\</span> \n    <span>-d</span> <span>'{ \n  \"jsonrpc\": \"2.0\", \n  \"id\": 1, \n  \"method\": \"tools/list\", \n  \"params\": {} \n}'</span>  \n \n<span>{</span> \n  <span>\"jsonrpc\"</span>: <span>\"2.0\"</span>, \n  <span>\"id\"</span>: 1, \n  <span>\"result\"</span>: <span>{</span> \n    <span>\"tools\"</span>: <span>[</span> \n      <span>{</span> \n        <span>\"name\"</span>: <span>\"current_time_service\"</span>, \n        <span>\"description\"</span>: <span>\"Returns current time in ISO 8601 format\"</span>, \n        <span>\"inputSchema\"</span>: <span>{</span> \n          <span>\"type\"</span>: <span>\"object\"</span>, \n          <span>\"properties\"</span>: <span>{}</span>, \n          <span>\"required\"</span>: <span>[]</span>, \n          <span>\"additionalProperties\"</span>: <span>false</span> \n        <span>}</span> \n      <span>}</span> \n    <span>]</span> \n  <span>}</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<h2> \n   \n   \n  Configuring OpenAM \n</h2> \n \n<p>OpenAM will be responsible for user authentication, issuing OAuth 2 <code>access_token</code> tokens, and validating them.</p> \n \n<p>If you have not yet configured OpenAM, perform a quick setup by running the command:<br> \n</p> \n \n<div> \n<pre><code>docker <span>exec</span> <span>-w</span> <span>'/usr/openam/ssoconfiguratortools'</span> openam bash <span>-c</span> <span>\\</span> \n<span>'echo \"ACCEPT_LICENSES=true \nSERVER_URL=http://openam.example.org:8080 \nDEPLOYMENT_URI=/$OPENAM_PATH \nBASE_DIR=$OPENAM_DATA_DIR \nlocale=en_US \nPLATFORM_LOCALE=en_US \nAM_ENC_KEY= \nADMIN_PWD=passw0rd \nAMLDAPUSERPASSWD=p@passw0rd \nCOOKIE_DOMAIN=example.org \nACCEPT_LICENSES=true \nDATA_STORE=embedded \nDIRECTORY_SSL=SIMPLE \nDIRECTORY_SERVER=openam.example.org \nDIRECTORY_PORT=50389 \nDIRECTORY_ADMIN_PORT=4444 \nDIRECTORY_JMX_PORT=1689 \nROOT_SUFFIX=dc=openam,dc=example,dc=org \nDS_DIRMGRDN=cn=Directory Manager \nDS_DIRMGRPASSWD=passw0rd\" &gt; conf.file &amp;&amp; java -jar openam-configurator-tool*.jar --file conf.file'</span> \n</code></pre> \n \n</div> \n \n \n \n<h3> \n   \n   \n  Configuring OAuth 2 in OpenAM \n</h3> \n \n<p>Open the OpenAM console at <a href=\"http://openam.example.org:8080/openam/console\">http://openam.example.org:8080/openam/console</a>. Enter the administrator login and password in the <code>User Name</code> and <code>Password</code> fields. In this case, they will be <code>amadmin</code> and <code>passw0rd</code>, respectively. </p> \n \n<p>Select <code>Top Level Realm</code> from the Realm list.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbg2088rk643g42jlw4ud.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbg2088rk643g42jlw4ud.png\" alt=\"OpenAM Realms List\" width=\"800\" height=\"433\"></a></p> \n \n<p>Next, <code>Configure OAuth Provider</code>.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fy0vepni7wpwcw983n71m.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fy0vepni7wpwcw983n71m.png\" alt=\"OpenAM Configure OAuth Provider\" width=\"800\" height=\"538\"></a></p> \n \n<p>Then select <code>Configure OAuth 2.0</code>.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg746yp3369nzj8ve8l97.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg746yp3369nzj8ve8l97.png\" alt=\"OpenAM Configure OAuth 2.0\" width=\"800\" height=\"424\"></a></p> \n \n<p>In the form that opens, you can leave the default settings unchanged. Click <code>Create</code>.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Famjie9iib7as12cejjjz.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Famjie9iib7as12cejjjz.png\" alt=\"OpenAM Configure OAuth 2.0 Step 2\" width=\"800\" height=\"525\"></a></p> \n \n<p>In the Realm settings, select Services from the menu on the left and open the OAuth2 Provider settings.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fdtxwnk9a0av5yiz4eaod.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fdtxwnk9a0av5yiz4eaod.png\" alt=\"OpenAM Realm Services\" width=\"800\" height=\"356\"></a></p> \n \n<p>Add the value <code>profile</code> to the <code>Scopes</code> and <code>Default Clients Scopes</code> settings. This scope will allow you to obtain basic information about the user. Enable the <code>Issue Refresh Tokens</code> and <code>Issue Refresh Tokens on Refreshing Access Tokens</code> options. Also, allow dynamic client registration by enabling the <code>Allow Open Dynamic Client Registration</code> option. This will allow the MCP client (VS Code) to automatically register with OpenAM without requiring any additional action from the user.</p> \n \n<p>For more information on configuring OpenAM, please refer to the <a href=\"https://doc.openidentityplatform.org/openam/\">documentation</a>.</p> \n \n<h2> \n   \n   \n  Configuring OpenIG \n</h2> \n \n<p>OpenIG will be responsible for authorizing requests. It will check the validity of <code>access_token</code> issued by OpenAM and proxy requests to OpenAM and the MCP server.</p> \n \n<p>Now let's check the OpenIG route configuration for proxying requests.</p> \n \n<h3> \n   \n   \n  Proxying requests to the MCP server. \n</h3> \n \n<p>The route will receive the <code>access_token</code> issued by OpenAM, passed in the <code>Authorization</code> header. If the <code>access_token</code> is valid, it will pass the request to the MCP server and return a response. If the <code>access_token</code> is invalid, OpenIG will return HTTP status 401.</p> \n \n<p><code>openig-config/config/routes/10-mcp.json</code><br> \n</p> \n \n<div> \n<pre><code><span>{</span><span> \n   </span><span>\"name\"</span><span>:</span><span> </span><span>\"${matches(request.uri.path, '^/mcp')}\"</span><span>,</span><span> \n   </span><span>\"condition\"</span><span>:</span><span> </span><span>\"${matches(request.uri.path, '^/mcp')}\"</span><span>,</span><span> \n   </span><span>\"monitor\"</span><span>:</span><span> </span><span>true</span><span>,</span><span> \n   </span><span>\"timer\"</span><span>:</span><span> </span><span>true</span><span>,</span><span> \n   </span><span>\"handler\"</span><span>:</span><span> </span><span>{</span><span> \n      </span><span>\"type\"</span><span>:</span><span> </span><span>\"Chain\"</span><span>,</span><span> \n      </span><span>\"config\"</span><span>:</span><span> </span><span>{</span><span> \n         </span><span>\"filters\"</span><span>:</span><span> </span><span>[</span><span> \n            </span><span>{</span><span> \n               </span><span>\"type\"</span><span>:</span><span> </span><span>\"OAuth2ResourceServerFilter\"</span><span>,</span><span> \n               </span><span>\"config\"</span><span>:</span><span> </span><span>{</span><span> \n                  </span><span>\"requireHttps\"</span><span>:</span><span> </span><span>false</span><span>,</span><span> \n                  </span><span>\"providerHandler\"</span><span>:</span><span> </span><span>\"ClientHandler\"</span><span>,</span><span>  \n                  </span><span>\"scopes\"</span><span>:</span><span> </span><span>[</span><span> \n                     </span><span>\"profile\"</span><span> \n                  </span><span>],</span><span> \n                  </span><span>\"tokenInfoEndpoint\"</span><span>:</span><span> </span><span>\"${system['openam'].concat('/oauth2/tokeninfo')}\"</span><span>  \n               </span><span>}</span><span> \n            </span><span>},</span><span> \n            </span><span>{</span><span> \n               </span><span>\"type\"</span><span>:</span><span> </span><span>\"ConditionEnforcementFilter\"</span><span>,</span><span> \n               </span><span>\"config\"</span><span>:</span><span> </span><span>{</span><span> \n                  </span><span>\"condition\"</span><span>:</span><span> </span><span>\"${not empty contexts['oauth2']}\"</span><span>,</span><span> \n                  </span><span>\"failureHandler\"</span><span>:</span><span> </span><span>\"RequireAuth\"</span><span> \n               </span><span>}</span><span> \n            </span><span>}</span><span> \n         </span><span>],</span><span> \n         </span><span>\"handler\"</span><span>:</span><span> </span><span>\"EndpointHandler\"</span><span> \n      </span><span>}</span><span> \n   </span><span>},</span><span> \n   </span><span>\"heap\"</span><span>:</span><span> </span><span>[</span><span> \n      </span><span>{</span><span> \n         </span><span>\"name\"</span><span>:</span><span> </span><span>\"RequireAuth\"</span><span>,</span><span> \n         </span><span>\"type\"</span><span>:</span><span> </span><span>\"StaticResponseHandler\"</span><span>,</span><span> \n         </span><span>\"config\"</span><span>:</span><span> </span><span>{</span><span> \n            </span><span>\"status\"</span><span>:</span><span> </span><span>401</span><span>,</span><span> \n            </span><span>\"headers\"</span><span>:</span><span> </span><span>{</span><span> \n               </span><span>\"WWW-Authenticate\"</span><span>:</span><span> </span><span>[</span><span> \n                  </span><span>\"Bearer realm=</span><span>\\\"</span><span>OpenIG</span><span>\\\"</span><span>\"</span><span> \n               </span><span>]</span><span> \n            </span><span>},</span><span> \n            </span><span>\"entity\"</span><span>:</span><span> </span><span>\"Authentication required\"</span><span> \n         </span><span>}</span><span> \n      </span><span>},</span><span> \n      </span><span>{</span><span> \n         </span><span>\"name\"</span><span>:</span><span> </span><span>\"EndpointHandler\"</span><span>,</span><span> \n         </span><span>\"type\"</span><span>:</span><span> </span><span>\"DispatchHandler\"</span><span>,</span><span> \n         </span><span>\"config\"</span><span>:</span><span> </span><span>{</span><span> \n            </span><span>\"bindings\"</span><span>:</span><span> </span><span>[</span><span> \n               </span><span>{</span><span> \n                  </span><span>\"handler\"</span><span>:</span><span> </span><span>\"ClientHandler\"</span><span>,</span><span> \n                  </span><span>\"baseURI\"</span><span>:</span><span> </span><span>\"http://time-mcp-server:8080/mcp\"</span><span> \n               </span><span>}</span><span> \n            </span><span>]</span><span> \n         </span><span>}</span><span> \n      </span><span>}</span><span> \n   </span><span>]</span><span> \n</span><span>}</span><span> \n</span></code></pre> \n \n</div> \n \n \n \n<p>The route consists of two filters. The first filter, <code>OAuth2ResourceServerFilter</code>, validates the <code>access_token</code> and, if successful, writes the data received from the <code>access_token</code> to the request context. The second filter, <code>ConditionEnforcementFilter</code>, checks the context and, if successful, forwards the request to the MCP server. Otherwise, it returns HTTP status 401.</p> \n \n<p>Let's make an unauthorized request to the MCP server and verify that OpenIG requires authorization.<br> \n</p> \n \n<div> \n<pre><code>curl <span>-v</span> http://openig.example.org:8081/mcp \n<span>*</span>   Trying 127.0.0.1:8081... \n<span>*</span> Connected to openig.example.org <span>(</span>127.0.0.1<span>)</span> port 8081 <span>(</span><span>#0)</span> \n<span>&gt;</span> GET /mcp HTTP/1.1 \n<span>&gt;</span> Host: openig.example.org:8081 \n<span>&gt;</span> User-Agent: curl/8.1.2 \n<span>&gt;</span> Accept: <span>*</span>/<span>*</span> \n<span>&gt;</span>  \n&lt; HTTP/1.1 401  \n&lt; WWW-Authenticate: Bearer <span>realm</span><span>=</span><span>\"OpenIG\"</span> \n&lt; Content-Length: 0 \n&lt; Date: Mon, 22 Sep 2025 08:00:48 GMT \n \n</code></pre> \n \n</div> \n \n \n \n<p>Proxying to <code>.well-known</code> endpoints</p> \n \n<p>According to the MCP specification, the client obtains data about the authorization server from endpoints located at the URL <code>&lt;MCP server host&gt;/.well-known/*</code>. The endpoints are located on OpenAM at the URL <code>&lt;OpenAM host&gt;/openam/.well-known</code>. The route for forwarding HTTP requests to MCP on OpenAM is as follows:</p> \n \n<p><code>openig-config/config/routes/20-well-known.json</code><br> \n</p> \n \n<div> \n<pre><code><span>{</span><span> \n  </span><span>\"name\"</span><span>:</span><span> </span><span>\"${matches(request.uri.path, '^/.well-known/.*}\"</span><span>,</span><span> \n  </span><span>\"condition\"</span><span>:</span><span> </span><span>\"${matches(request.uri.path, '^/.well-known/.*')}\"</span><span>,</span><span> \n  </span><span>\"monitor\"</span><span>:</span><span> </span><span>true</span><span>,</span><span> \n  </span><span>\"timer\"</span><span>:</span><span> </span><span>true</span><span>,</span><span> \n  </span><span>\"handler\"</span><span>:</span><span> </span><span>{</span><span> \n    </span><span>\"type\"</span><span>:</span><span> </span><span>\"Chain\"</span><span>,</span><span> \n    </span><span>\"config\"</span><span>:</span><span> </span><span>{</span><span> \n      </span><span>\"filters\"</span><span>:</span><span> </span><span>[</span><span> \n        </span><span>{</span><span> \n          </span><span>\"type\"</span><span>:</span><span> </span><span>\"HeaderFilter\"</span><span>,</span><span> \n          </span><span>\"config\"</span><span>:</span><span> </span><span>{</span><span> \n            </span><span>\"messageType\"</span><span>:</span><span> </span><span>\"REQUEST\"</span><span>,</span><span> \n            </span><span>\"add\"</span><span>:</span><span> </span><span>{</span><span> \n              </span><span>\"Host\"</span><span>:</span><span> </span><span>[</span><span> \n                </span><span>\"${matchingGroups(system['openam'],</span><span>\\\"</span><span>(http|https):</span><span>\\/\\/</span><span>(.[^</span><span>\\/</span><span>]*)</span><span>\\\"</span><span>)[2]}\"</span><span> \n              </span><span>]</span><span> \n            </span><span>},</span><span> \n            </span><span>\"remove\"</span><span>:</span><span> </span><span>[</span><span> \n              </span><span>\"Host\"</span><span>,</span><span> \n              </span><span>\"Origin\"</span><span> \n            </span><span>]</span><span> \n          </span><span>}</span><span> \n        </span><span>}</span><span> \n      </span><span>],</span><span> \n      </span><span>\"handler\"</span><span>:</span><span> </span><span>\"EndpointHandler\"</span><span> \n    </span><span>}</span><span> \n  </span><span>},</span><span> \n  </span><span>\"heap\"</span><span>:</span><span> </span><span>[</span><span> \n    </span><span>{</span><span> \n      </span><span>\"name\"</span><span>:</span><span> </span><span>\"EndpointHandler\"</span><span>,</span><span> \n      </span><span>\"type\"</span><span>:</span><span> </span><span>\"DispatchHandler\"</span><span>,</span><span> \n      </span><span>\"config\"</span><span>:</span><span> </span><span>{</span><span> \n        </span><span>\"bindings\"</span><span>:</span><span> </span><span>[</span><span> \n          </span><span>{</span><span> \n            </span><span>\"expression\"</span><span>:</span><span> </span><span>\"${matches(request.uri.path, '^/.well-known/openid-configuration$')}\"</span><span>,</span><span> \n            </span><span>\"handler\"</span><span>:</span><span> </span><span>\"ClientHandler\"</span><span>,</span><span> \n            </span><span>\"baseURI\"</span><span>:</span><span> </span><span>\"${system['openam'].concat('/oauth2/.well-known/openid-configuration')}\"</span><span> \n          </span><span>}</span><span> \n        </span><span>]</span><span> \n      </span><span>}</span><span> \n    </span><span>}</span><span> \n  </span><span>]</span><span> \n</span><span>}</span><span> \n</span></code></pre> \n \n</div> \n \n \n \n<p>The <code>HeaderFilter</code> filter adds the OpenAM Host HTTP header specified in the <code>openam</code> system parameter in the <code>docker-compose.yaml</code> file, and the <code>EndpointHandler</code> handler forwards the request to the <code>/openam/.well-known/openid-configuration</code> endpoint deployed in the <code>openam</code> Docker container.</p> \n \n<p>Let's check the endpoint operation:<br> \n</p> \n \n<div> \n<pre><code> curl <span>-v</span> http://openig.example.org:8081/.well-known/openid-configuration \n \n <span>{</span> \n   <span>\"acr_values_supported\"</span> : <span>[]</span>, \n   <span>\"authorization_endpoint\"</span> : <span>\"http://openam.example.org:8080/openam/oauth2/authorize\"</span>, \n   <span>\"check_session_iframe\"</span> : <span>\"http://openam.example.org:8080/openam/oauth2/connect/checkSession\"</span>, \n   <span>\"claims_parameter_supported\"</span> : <span>false</span>, \n   <span>\"claims_supported\"</span> : <span>[]</span>, \n   <span>\"device_authorization_endpoint\"</span> : <span>\"http://openam.example.org:8080/openam/oauth2/device/code\"</span>, \n   <span>\"end_session_endpoint\"</span> : <span>\"http://openam.example.org:8080/openam/oauth2/connect/endSession\"</span>, \n   <span>\"id_token_encryption_alg_values_supported\"</span> : <span>[</span> \n      <span>\"RSA-OAEP\"</span>, \n      <span>\"RSA-OAEP-256\"</span>, \n      <span>\"A128KW\"</span>, \n      <span>\"RSA1_5\"</span>, \n      <span>\"A256KW\"</span>, \n      <span>\"dir\"</span>, \n      <span>\"A192KW\"</span> \n   <span>]</span>, \n   <span>\"id_token_encryption_enc_values_supported\"</span> : <span>[</span> \n      <span>\"A256GCM\"</span>, \n      <span>\"A192GCM\"</span>, \n      <span>\"A128GCM\"</span>, \n      <span>\"A128CBC-HS256\"</span>, \n      <span>\"A192CBC-HS384\"</span>, \n      <span>\"A256CBC-HS512\"</span> \n   <span>]</span>, \n   <span>\"id_token_signing_alg_values_supported\"</span> : <span>[</span> \n      <span>\"ES384\"</span>, \n      <span>\"HS256\"</span>, \n      <span>\"HS512\"</span>, \n      <span>\"ES256\"</span>, \n      <span>\"RS256\"</span>, \n      <span>\"HS384\"</span>, \n      <span>\"ES512\"</span> \n   <span>]</span>, \n   <span>\"issuer\"</span> : <span>\"http://openam.example.org:8080/openam/oauth2\"</span>, \n   <span>\"jwks_uri\"</span> : <span>\"http://openam.example.org:8080/openam/oauth2/connect/jwk_uri\"</span>, \n   <span>\"registration_endpoint\"</span> : <span>\"http://openam.example.org:8080/openam/oauth2/connect/register\"</span>, \n   <span>\"response_types_supported\"</span> : <span>[</span> \n      <span>\"code\"</span>, \n      <span>\"code token\"</span>, \n      <span>\"token\"</span> \n   <span>]</span>, \n   <span>\"scopes_supported\"</span> : <span>[]</span>, \n   <span>\"subject_types_supported\"</span> : <span>[</span> \n      <span>\"public\"</span> \n   <span>]</span>, \n   <span>\"token_endpoint\"</span> : <span>\"http://openam.example.org:8080/openam/oauth2/access_token\"</span>, \n   <span>\"token_endpoint_auth_methods_supported\"</span> : <span>[</span> \n      <span>\"client_secret_post\"</span>, \n      <span>\"private_key_jwt\"</span>, \n      <span>\"none\"</span>, \n      <span>\"client_secret_basic\"</span> \n   <span>]</span>, \n   <span>\"userinfo_endpoint\"</span> : <span>\"http://openam.example.org:8080/openam/oauth2/userinfo\"</span>, \n   <span>\"version\"</span> : <span>\"3.0\"</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n \n \n<p>For more details on configuring OpenIG, please refer to the documentation.</p> \n \n<p>Configuring VS Code to work with the MCP server</p> \n \n<p>You must have the extensions for working with Copilot, GitHub Copilot, and GitHub Copilot Chat installed and configured. Instructions on how to do this are available at: <a href=\"https://code.visualstudio.com/docs/copilot/setup\">https://code.visualstudio.com/docs/copilot/setup</a>.</p> \n \n<p>Add the MCP server to VS Code.</p> \n \n<p>For example, to add MCP to your workspace, create a file named <code>mcp.json</code> in the <code>.vscode</code> directory of your workspace:</p> \n \n<p><code>mcp.json</code>:<br> \n</p> \n \n<div> \n<pre><code><span>{</span><span> \n  </span><span>\"servers\"</span><span>:</span><span> </span><span>{</span><span> \n    </span><span>\"time-mcp-server\"</span><span>:</span><span> </span><span>{</span><span> \n      </span><span>\"type\"</span><span>:</span><span> </span><span>\"http\"</span><span>,</span><span> \n      </span><span>\"url\"</span><span>:</span><span> </span><span>\"http://openig.example.org:8081/mcp\"</span><span> \n    </span><span>}</span><span> \n  </span><span>}</span><span> \n</span><span>}</span><span> \n</span></code></pre> \n \n</div> \n \n \n \n<p>Other ways to add an MCP server are described at: <a href=\"https://code.visualstudio.com/docs/copilot/customization/mcp-servers#_add-an-mcp-server\">https://code.visualstudio.com/docs/copilot/customization/mcp-servers#_add-an-mcp-server</a></p> \n \n<p>In the VS Code extensions list, click on the settings for the added MCP server and select <code>Start Server</code> from the menu that appears.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft4ift973bsahq0a77jwa.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft4ift973bsahq0a77jwa.png\" alt=\"VS Code Start MCP Server\" width=\"800\" height=\"404\"></a></p> \n \n<p>Allow the MCP server to authenticate on the OpenIG host</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwx4g8q6u925u8tbyuaga.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwx4g8q6u925u8tbyuaga.png\" alt=\"VS Code MCP Authenticate\" width=\"788\" height=\"518\"></a></p> \n \n<p>A browser window with authentication will open. </p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fai8d21r5wiidn4vhrr66.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fai8d21r5wiidn4vhrr66.png\" alt=\"OpenAM Login\" width=\"800\" height=\"572\"></a></p> \n \n<p>Enter the login and password for the test user: <code>demo</code>  and <code>changeit</code>, respectively</p> \n \n<p>Confirm access to data for the Visual Studio Code application. If you want to disable the data access confirmation dialog, enable <code>Allow clients to skip consent</code> in the OAuth2 Provider settings in OpenAM. </p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fp331k8ti4jsh57rrlm0a.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fp331k8ti4jsh57rrlm0a.png\" alt=\"OpenAM OAuth2 Consent\" width=\"800\" height=\"535\"></a></p> \n \n<p>After confirming, you will be redirected back to VS Code.</p> \n \n<p>Open the chat with GitHub Copilot. To do this, select <strong>Show and Run Commands</strong> from the command menu:</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2ye5o9bzgr5pxfo10ltb.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2ye5o9bzgr5pxfo10ltb.png\" alt=\"OpenAM Run Commands\" width=\"800\" height=\"267\"></a></p> \n \n<p>Then select <strong>Chat: New Chat</strong>:</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fpikum1km0ec98wqb787z.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fpikum1km0ec98wqb787z.png\" alt=\"OpenAM New Chat\" width=\"800\" height=\"209\"></a></p> \n \n<p>In the chat window that opens, enter the question: <code>What is the current time?</code> . Copilot will respond that it does not have access to the current time:</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwv9hwmcw3suz3qzap08i.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwv9hwmcw3suz3qzap08i.png\" alt=\"OpenAM Time Request\" width=\"800\" height=\"643\"></a></p> \n \n<p>Now switch the chat to Agent mode at the bottom and ask the question again:</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fs2oy54n8j2e75r9uxi86.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fs2oy54n8j2e75r9uxi86.png\" alt=\"Copilot Set the Agent Mode\" width=\"800\" height=\"202\"></a></p> \n \n<p>Allow access to the function for obtaining the current time in the MCP server:</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgidwbwyssb4z0bm42762.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgidwbwyssb4z0bm42762.png\" alt=\"Copilot Allow MCP Call\" width=\"800\" height=\"286\"></a></p> \n \n<p>Copilot will receive information about the current time from the MCP server and return the correct response.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fm2elpzc3f3osaxb97khl.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fm2elpzc3f3osaxb97khl.png\" alt=\"Copilot Successfol response\" width=\"800\" height=\"235\"></a></p> \n \n<h2> \n   \n   \n  Conclusion \n</h2> \n \n<p>In this article, we demonstrated the practical integration of OpenAM and OpenIG to provide secure access to the MCP server based on OAuth 2.1. OpenAM acts as a reliable authentication and authorization center, issuing and validating tokens, while OpenIG filters requests, blocking unauthorized access and proxying traffic to protected resources. This approach minimizes the risk of sensitive data leaks - from internal repositories to customer databases.</p> \n \n<p>Download the source code from GitHub, test the configuration, and integrate it into your projects. For in-depth study, refer to the official documentation: <a href=\"https://doc.openidentityplatform.org/openam/\">OpenAM</a> and <a href=\"https://doc.openidentityplatform.org/openig/\">OpenIG</a>.</p>",
      "summary": " \n   \n   \n  Introduction \n \n \nLarge language model (LLM) agents can perform various tasks, from writing code or texts to booking airline tickets. Agents consist of a client that interacts with the user and a server that performs the required tasks. The interaction between the client, server, and LLM occurs via the Model Context Protocol MCP. \n \nMCP servers often have access to sensitive information, such as an internal source code repository or a customer database. Of course, not all users shoul",
      "publishedAt": "2025-12-02T11:02:35.000Z",
      "author": "Maxim Thomas",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 15.330654938404429,
      "ingestedAt": "2025-12-02T14:44:03.193Z",
      "tags": [
        "code_review",
        "documentation",
        "agents",
        "ide",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09936399",
      "title": "Every terrible thing the Trump administration did in November 2025",
      "url": "https://www.inoreader.com/article/3a9c6e76b2989c66",
      "content": "<div class=\"email_is_html\"><div><div style=\"font-kerning: auto; --image-offset-margin: -120px\"><img src=\"https://eotrx.substackcdn.com/open?token=eyJtIjoiPDIwMjUxMjAyMTEyMjI5LjMuODBmY2Y3MzcwOTRjYmYwNUBtZzIuc3Vic3RhY2suY29tPiIsInUiOjE2MTk4NTQ4MCwiciI6ImJ5dGVieXRlZ284OEBpbm8udG8iLCJkIjoibWcyLnN1YnN0YWNrLmNvbSIsInAiOjE4MDQ1NzIxMCwidCI6Im5ld3NsZXR0ZXIiLCJhIjoiZXZlcnlvbmUiLCJzIjoxNzI1OTU1LCJjIjoicG9zdCIsImYiOnRydWUsInBvc2l0aW9uIjoidG9wIiwiaWF0IjoxNzY0Njc0NTU5LCJleHAiOjE3NjcyNjY1NTksImlzcyI6InB1Yi0wIiwic3ViIjoiZW8ifQ.E3fi95kC3pIA1hZAMggdHMCm6B43lNRiWQtWJNj2OgU\" width=\"1\" height=\"1\" border=\"0\" style=\"height: 1px !important; width: 1px !important; border-width: 0 !important; margin-top: 0 !important; margin-bottom: 0 !important; margin-right: 0 !important; margin-left: 0 !important; padding-top: 0 !important; padding-bottom: 0 !important; padding-right: 0 !important; padding-left: 0 !important\" /><div style=\"display: none; font-size: 1px; color: #333333; line-height: 1px; max-height: 0px; max-width: 0px; opacity: 0; overflow: hidden\">I made a list.</div><div style=\"display: none; font-size: 1px; color: #333333; line-height: 1px; max-height: 0px; max-width: 0px; opacity: 0; overflow: hidden\">͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­</div><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td></td><td width=\"550\"></td><td></td></tr><tr><td></td><td width=\"550\" align=\"left\"><div style=\"font-size: 16px; line-height: 26px; max-width: 550px; width: 100%; margin: 0 auto; overflow-wrap: break-word\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"right\" style=\"height: 20px\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><span style=\"font-family: SF Pro Text, -apple-system, system-ui, BlinkMacSystemFont, Inter, Segoe UI, Roboto, Helvetica, Arial, sans-serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol; font-size: 13px; color: unset; list-style: none; text-decoration: unset; margin: 0\"><div style=\"list-style: none; color: unset; text-align: right; font-size: 12px; line-height: 16px; text-decoration: unset; margin: 0\"><span style=\"list-style: none; color: unset; text-decoration: unset; margin: 0\">Forwarded this email? <a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9rZWVwdHJhY2suc3Vic3RhY2suY29tL3N1YnNjcmliZT91dG1fc291cmNlPWVtYWlsJnV0bV9jYW1wYWlnbj1lbWFpbC1zdWJzY3JpYmUmcj0yb2Z3c28mbmV4dD1odHRwcyUzQSUyRiUyRmtlZXB0cmFjay5zdWJzdGFjay5jb20lMkZwJTJGZXZlcnktdGVycmlibGUtdGhpbmctdGhlLXRydW1wLWFkbWluaXN0cmF0aW9uLWE3NSIsInAiOjE4MDQ1NzIxMCwicyI6MTcyNTk1NSwiZiI6dHJ1ZSwidSI6MTYxOTg1NDgwLCJpYXQiOjE3NjQ2NzQ1NTksImV4cCI6MjA4MDI1MDU1OSwiaXNzIjoicHViLTAiLCJzdWIiOiJsaW5rLXJlZGlyZWN0In0.kLRirtr0W-IE4Ny4Q00EGTZJ4Ko6arfEGiM6Uir09eg?\" style=\"list-style: none; color: unset; text-decoration: unset; margin: 0; -webkit-text-decoration-line: underline; text-decoration-line: underline\" target=\"_blank\" rel=\"noreferrer\">Subscribe here</a> for more</span></div></span></td></tr></tbody></table></td></tr></tbody></table><div dir=\"auto\" style=\"--image-offset-margin: -120px; padding: 32px 0 0 0; font-size: 16px; line-height: 26px\"><div style=\"font-size: 16px; line-height: 26px\"><h1 dir=\"auto\" style=\"direction: auto; text-align: start; unicode-bidi: isolate; color: rgb(54,55,55); font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 0; line-height: 36px; font-size: 32px\"><a href=\"https://substack.com/app-link/post?publication_id=1725955&amp;post_id=180457210&amp;utm_source=post-email-title&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=2ofwso&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE4MDQ1NzIxMCwiaWF0IjoxNzY0Njc0NTU5LCJleHAiOjE3NjcyNjY1NTksImlzcyI6InB1Yi0xNzI1OTU1Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.vgjK3EA3RKiMU4cbeXf9vssaeU2sF4nwfFIOxESj6U8\" style=\"color: rgb(54,55,55); text-decoration: none\" target=\"_blank\" rel=\"noreferrer\">Every terrible thing the Trump administration did in November 2025</a></h1><h3 dir=\"auto\" style=\"direction: auto; text-align: start; unicode-bidi: isolate; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: normal; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 4px 0 0; color: #777777; line-height: 24px; font-size: 18px; margin-top: 12px\">I made a list.</h3><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"margin: 1em 0; height: 20px; align-items: center\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><div style=\"list-style: none; font-size: 11px; line-height: 20px; text-decoration: unset; color: rgb(54,55,55); margin: 0; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px\"><a style=\"list-style: none; color: rgb(54,55,55); margin: 0; font-size: 11px; line-height: 20px; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px; text-decoration: none\" href=\"https://substack.com/@keeptrack\" target=\"_blank\" rel=\"noreferrer\">KeepTrack</a></div></td></tr></tbody></table></td></tr><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><div style=\"list-style: none; font-size: 11px; line-height: 20px; text-decoration: unset; color: rgb(119,119,119); margin: 0; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px\">Dec 2</div></td></tr></tbody></table></td></tr></tbody></table></td><td align=\"right\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><a href=\"https://substack.com/@keeptrack\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!A2NR!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce3ddf18-3843-4d36-8e70-f1d277be2784_609x609.png\" style=\"box-sizing: border-box; border-radius: 500000px; max-width: 550px; border: none; vertical-align: middle; width: 40px; height: 40px; min-width: 40px; min-height: 40px; object-fit: cover; margin: 0px; display: inline\" width=\"40\" height=\"40\" /></a></td></tr></tbody></table></td></tr></tbody></table><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"border-top: 1px solid rgb(0,0,0,.1); border-bottom: 1px solid rgb(0,0,0,.1); min-width: 100%\"><tbody><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr><tr><td><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=1725955&amp;post_id=180457210&amp;utm_source=substack&amp;isFreemail=true&amp;submitLike=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE4MDQ1NzIxMCwicmVhY3Rpb24iOiLinaQiLCJpYXQiOjE3NjQ2NzQ1NTksImV4cCI6MTc2NzI2NjU1OSwiaXNzIjoicHViLTE3MjU5NTUiLCJzdWIiOiJyZWFjdGlvbiJ9.wpG2d0NiQjP7YYDMeX6Lv3S6VPcrjogRfg1Kghqlu6o&amp;utm_medium=email&amp;utm_campaign=email-reaction&amp;r=2ofwso\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!PeVs!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideHeart%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=1725955&amp;post_id=180457210&amp;utm_source=substack&amp;utm_medium=email&amp;isFreemail=true&amp;comments=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE4MDQ1NzIxMCwiaWF0IjoxNzY0Njc0NTU5LCJleHAiOjE3NjcyNjY1NTksImlzcyI6InB1Yi0xNzI1OTU1Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.vgjK3EA3RKiMU4cbeXf9vssaeU2sF4nwfFIOxESj6U8&amp;r=2ofwso&amp;utm_campaign=email-half-magic-comments&amp;action=post-comment&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!x1tS!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideComments%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=1725955&amp;post_id=180457210&amp;utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;utm_campaign=email-share&amp;action=share&amp;triggerShare=true&amp;isFreemail=true&amp;r=2ofwso&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE4MDQ1NzIxMCwiaWF0IjoxNzY0Njc0NTU5LCJleHAiOjE3NjcyNjY1NTksImlzcyI6InB1Yi0xNzI1OTU1Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.vgjK3EA3RKiMU4cbeXf9vssaeU2sF4nwfFIOxESj6U8\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!_L14!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideShare2%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9vcGVuLnN1YnN0YWNrLmNvbS9wdWIva2VlcHRyYWNrL3AvZXZlcnktdGVycmlibGUtdGhpbmctdGhlLXRydW1wLWFkbWluaXN0cmF0aW9uLWE3NT91dG1fc291cmNlPXN1YnN0YWNrJnV0bV9tZWRpdW09ZW1haWwmdXRtX2NhbXBhaWduPWVtYWlsLXJlc3RhY2stY29tbWVudCZhY3Rpb249cmVzdGFjay1jb21tZW50JnI9Mm9md3NvJnRva2VuPWV5SjFjMlZ5WDJsa0lqb3hOakU1T0RVME9EQXNJbkJ2YzNSZmFXUWlPakU0TURRMU56SXhNQ3dpYVdGMElqb3hOelkwTmpjME5UVTVMQ0psZUhBaU9qRTNOamN5TmpZMU5Ua3NJbWx6Y3lJNkluQjFZaTB4TnpJMU9UVTFJaXdpYzNWaUlqb2ljRzl6ZEMxeVpXRmpkR2x2YmlKOS52Z2pLM0VBM1JLaU1VNGNiZVhmOXZzc2FlVTJzRjRud2ZGSU94RVNqNlU4IiwicCI6MTgwNDU3MjEwLCJzIjoxNzI1OTU1LCJmIjp0cnVlLCJ1IjoxNjE5ODU0ODAsImlhdCI6MTc2NDY3NDU1OSwiZXhwIjoyMDgwMjUwNTU5LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.tSw6-Rw5PIUKARv05A98Xf-ZdzVpE4vXZcXgp7G0TIc?&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!5EGt!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FNoteForwardIcon%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td></tr></tbody></table></td><td align=\"right\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://open.substack.com/pub/keeptrack/p/every-terrible-thing-the-trump-administration-a75?utm_source=email&amp;redirect=app-store&amp;utm_campaign=email-read-in-app\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><div style=\"font-size: 16px; line-height: 26px; display: inline-block; vertical-align: middle; max-width: 0; min-height: 18px\"></div><span style=\"vertical-align: middle; margin-right: 4px\">READ IN APP</span><img src=\"https://substackcdn.com/image/fetch/$s_!ET-_!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideArrowUpRight%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"min-width: 18px; min-height: 18px; border: none; vertical-align: middle; margin-right: 0; margin-left: 0; max-width: 18px\" /></a></td></tr></tbody></table></td></tr></tbody></table></td></tr></tbody></table></td></tr><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr></tbody></table></div></div><div dir=\"auto\" style=\"--image-offset-margin: -120px; padding: 32px 0 0 0; font-size: 16px; line-height: 26px\"><div dir=\"auto\" style=\"text-align: initial; font-size: 16px; line-height: 26px; width: 100%; word-break: break-word; margin-bottom: 16px\"><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto; margin-top: 0\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1456\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/968cb4a5-281c-42c5-afac-d7af766f4fa5?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0; margin-top: 0; margin-bottom: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"304.4642857142857\" src=\"https://substackcdn.com/image/fetch/$s_!Yw-4!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecedf20a-e033-4ea3-9699-409fe3f1090e_1457x807.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table></div><h1 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 2em\"><strong>PARDON PAY-TO-PLAY</strong></h1><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Last month, President Trump issued over half a dozen pardons to white collar criminals and political allies, some of whom paid lobbyists hundreds of thousands of dollars to get their case before the president. Those who had their crimes pardoned or sentences commuted include:</strong></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>Former Tennessee House speaker </span><a href=\"https://substack.com/redirect/031f5a33-1fac-422e-b636-4ded1caae2b3?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Glen Casada</a><span> and his chief of staff, Cade Cothren, both Republicans who were sentenced to over two years in prison for fraud, money laundering, and conspiracy charges tied to a scheme involving constituent mailer services.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><a href=\"https://substack.com/redirect/e5a2e308-a601-442b-b6ae-f6670b6060f9?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Robert Harshbarger Jr.</a><span>, the husband of Tennessee Republican Rep. Diana Harshbarger, who pleaded guilty in 2013 to health care fraud and distributing a misbranded drug.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><a href=\"https://substack.com/redirect/c1c9dd90-01ed-494a-8228-3b0c9a9c221b?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Michael McMahon</a><span>, a New York cop convicted of acting as an unregistered agent of a foreign government (China), interstate stalking, and conspiracy related to a scheme to harass a Chinese expatriate.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><a href=\"https://substack.com/redirect/fe63e292-e225-4d1b-8353-c4629672cc68?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Darryl Strawberry</a><span>, a former baseball player who pleaded guilty to felony tax evasion in 1995.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><a href=\"https://substack.com/redirect/da81f0cd-e13e-4de7-b518-5f45de0acc44?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Joe Lewis</a><span>, a British billionaire who pleaded guilty to federal insider trading last year and was given three years probation.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><a href=\"https://substack.com/redirect/786ce6d6-cae5-4144-9785-18973a5968ac?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Joseph Schwartz</a><span>, who was sentenced to three years in prison for his role in a $38 million employment tax fraud scheme involving nursing homes. Schwartz </span><a href=\"https://substack.com/redirect/eb6fdf92-a996-4c12-9aa0-8269567b11b7?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">paid</a><span> nearly a million dollars to lobby for a pardon:</span></p><blockquote style=\"border-left: 4px solid #EA82FF; margin: 20px 0; padding: 0\"><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); margin-left: 20px; line-height: 26px; font-size: 16px\">In April, Alina Habba, the U.S. attorney for New Jersey, extolled her office’s role in the sentencing of a former nursing home magnate to three years in prison for defrauding the government of $38 million. The man, Joseph Schwartz, was alleged to have overseen a “collapsed nursing home empire” and “willfully” failed to pay employment taxes, Habba’s announcement said.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); margin-left: 20px; line-height: 26px; font-size: 16px\">Around that time, Schwartz paid $960,000 to two lobbyists “seeking a federal pardon,” according to their lobbying filing…Liz Oyer, a former U.S. pardon attorney who was fired by Trump in March, said the involvement of the lobbyists — and the huge payment — heightens concern that there is “a special tier of justice for people who can afford to pay.”</p></blockquote><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><a href=\"https://substack.com/redirect/89a91d13-ff2f-4f6c-849c-2ed6faba6ae0?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">David Gentile</a><span>, a private equity CEO who was convicted of a $1.6 billion fraud scheme in which he used investor funds to subsidize his lavish lifestyle. Gentile had served less than two weeks of a seven-year sentence when Trump issued a commutation over Thanksgiving weekend.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>Trump also </span><a href=\"https://substack.com/redirect/636f3e9c-afa8-45fe-b931-d0d0eb4ef110?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">vowed to pardon</a><span> former Honduran president Juan Orlando Hernández, who was sentenced to 45 years in prison in 2024 for importing over 400 tons of cocaine into the U.S. The DOJ </span><a href=\"https://substack.com/redirect/92cc36fe-71ed-4a8a-9715-458db4396465?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">said</a><span> at the time that “Hernández abused his power to support one of the largest and most violent drug trafficking conspiracies in the world, in part by working with Sinaloa Cartel leader Joaquin “El Chapo” Guzmán.</span></p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>There has been rampant speculation about why Trump would pardon a prolific drug trafficker while simultaneously murdering people at sea under the pretense of stopping the international flow of illegal drugs. It could be that Trump completely bought into the “political prosecution” narrative spun by people like </span><a href=\"https://substack.com/redirect/5cdd4b15-0ea8-46d5-a92e-ab68e8505ead?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Roger Stone</a><span>, or it could be that techno-oligarchs like Thiel and Andreessen are advocating for the man who greenlit their corporate enclave “</span><a href=\"https://substack.com/redirect/e6ebf332-8bae-4938-a7a0-d2c618b5559d?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Próspera</a><span>” in Honduras. Alternatively, perhaps Trump simply admires big time criminals like Vladimir Putin and Silk Road founder Ross Ulbricht.</span></p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Meanwhile, Trump issued additional pardons for people connected to the Jan. 6 insurrection - including one for a completely unrelated offense, sending a message to his supporters that committing crime on Trump’s behalf will result in immunity from prosecution of all crimes generally.</strong></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><a href=\"https://substack.com/redirect/fcdbee79-e384-439a-9a94-e1ff72c3cdfe?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Suzanne Kaye</a><span>, a Florida woman who may have taken part in the Jan. 6 insurrection and posted videos on social media threatening to shoot FBI agents for questioning her about her participation.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><a href=\"https://substack.com/redirect/4e366514-278e-4dc1-8a6d-4401ccf59829?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Dan Wilson</a><span>, a January 6 insurrectionist who was previously pardoned for taking part in the attack on the Capitol, was issued a second pardon for a separate conviction for illegally possessing firearms.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><a href=\"https://substack.com/redirect/0ea58583-7af0-4af7-a28a-ef52b15ee1cb?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Dozens of fake electors</a><span> who took part in trying to overturn the 2020 presidential election results, including prominent architects of the insurrection like Rudy Giuliani, Mark Meadows, John Eastman, Sidney Powell, and Jenna Ellis.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Simultaneously, Trump is championing corrupt payouts of taxpayer money for people who assisted him in trying to overturn the results of the 2020 election, compensating his co-conspirators for their crimes.</strong></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>A </span><a href=\"https://substack.com/redirect/e9f42799-541f-4de7-ad22-0aa1f7c9e38a?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">clause</a><span> contained in the funding bill to reopen the government gives senators the ability to sue the Department of Justice for half a million dollars if their phone records were subpoenaed as part of former special counsel Jack Smith’s investigation into Jan. 6. South Carolina Sen. Lindsey Graham (R) </span><a href=\"https://substack.com/redirect/6f9b6d93-528b-40f8-8001-313eff153913?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">blocked</a><span> a measure to repeal the provision, saying that he intends to sue the federal government.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>The Department of Justice is </span><a href=\"https://substack.com/redirect/1555863f-2ac0-4dc4-967a-e3df679e0e63?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">reportedly</a><span> in the midst of negotiating a settlement with Michael Flynn, who claims that he was unjustly prosecuted by former special counsel Robert Mueller despite pleading guilty to “willfully and knowingly” making false statements to the FBI in 2017. According to Bloomberg, Flynn is seeking $50 million.</span></p><blockquote style=\"border-left: 4px solid #EA82FF; margin: 20px 0; padding: 0\"><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); margin-left: 20px; line-height: 26px; font-size: 16px\">“Rogue FBI actors orchestrated a politically motivated hoax to attempt to shatter his life, all while staging a soft coup against President Trump, draining millions in lost opportunities and legal fees from Flynn while the government lavished payouts on those very bad-faith saboteurs,” [Flynn’s lawyer] wrote.</p></blockquote><div style=\"font-size: 16px; line-height: 26px\"><hr style=\"margin: 32px 0; padding: 0; height: 1px; background: #e6e6e6; border: none\" /></div><h1 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 2em\"><strong>ICE SURVEILLANCE</strong></h1><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 12:</strong><span> “Dem governors unwittingly share DMV data with ICE, lawmakers warn,” </span><a href=\"https://substack.com/redirect/ba7d8f88-9210-4e27-9c17-5ecccd76cd3d?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">FedScoop</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 12:</strong><span> “ICE Plans to Spend $180 Million on Bounty Hunters to Stalk Immigrants: Newly released documents provide more details about ICE’s plan to use bounty hunters and private investigators to find the location of undocumented immigrants,” </span><a href=\"https://substack.com/redirect/a58f98ed-cda8-42fc-851e-f82d550d842b?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">404 Media</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 12:</strong><span> “DHS Kept Chicago Police Records for Months in Violation of Domestic Espionage Rules: The Department of Homeland Security collected data on Chicago residents accused of gang ties to test if police files could feed an FBI watchlist,” </span><a href=\"https://substack.com/redirect/9abed218-2305-4baf-8af3-f6ee0d1d3b27?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Wired</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 13:</strong><span> “Google Has Chosen a Side in Trump’s Mass Deportation Effort: Google is hosting a CBP app that uses facial recognition to identify immigrants, while simultaneously removing apps that report the location of ICE officials because Google sees ICE as a vulnerable group.” </span><a href=\"https://substack.com/redirect/eba9b31b-09b3-4db4-af2f-49766711e827?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">404 Media</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 18:</strong><span> “IRS Accessed Massive Database of Americans Flights Without a Warrant,” </span><a href=\"https://substack.com/redirect/3c38c5c2-01c7-4ad0-88d6-c58d83207ede?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">404 Media</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 18:</strong><span> “Contractor Recruiting People on LinkedIn to Physically Track Immigrants for ICE, Will Pay $300,” </span><a href=\"https://substack.com/redirect/129c8d6c-dee1-4753-8ab2-83d296724e8f?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">404 Media</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 20:</strong><span> “Border Patrol is monitoring US drivers and detaining those with ‘suspicious’ travel patterns,” </span><a href=\"https://substack.com/redirect/f5eb8a52-2c28-4c25-9f10-39f4c22450b2?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">AP</a></p><blockquote style=\"border-left: 4px solid #EA82FF; margin: 20px 0; padding: 0\"><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); margin-left: 20px; line-height: 26px; font-size: 16px\">The predictive intelligence program has resulted in people being stopped, searched and in some cases arrested. A network of cameras scans and records vehicle license plate information, and an algorithm flags vehicles deemed suspicious based on where they came from, where they were going and which route they took. Federal agents in turn may then flag local law enforcement.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); margin-left: 20px; line-height: 26px; font-size: 16px\">Suddenly, drivers find themselves pulled over — often for reasons cited such as speeding, failure to signal, the wrong window tint or even a dangling air freshener blocking the view. They are then aggressively questioned and searched, with no inkling that the roads they drove put them on law enforcement’s radar.</p></blockquote><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 21:</strong><span> “The FBI spied on a Signal group chat of immigration activists, records reveal,” </span><a href=\"https://substack.com/redirect/9294a109-49ba-4f9c-901a-96a02f2b0b2d?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Guardian</a></p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“NYPD Confirms Involvement in FBI Probe Targeting Volunteer Observers in Immigration Court,” </span><a href=\"https://substack.com/redirect/21a12878-d22b-4280-aefb-3af6007aa5eb?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">The City</a></p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 25:</strong><span> “ICE Offers Up to $280 Million to Immigrant-Tracking ‘Bounty Hunter’ Firms,” </span><a href=\"https://substack.com/redirect/9604fb9f-9125-44bd-8853-5ae502d8ff0f?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Wired</a></p><h1 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 2em\"><strong>IMMIGRATION POLICY</strong></h1><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"556\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/799e0eff-2f77-4235-8b69-d53a11f8deb8?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"433.65384615384613\" src=\"https://substackcdn.com/image/fetch/$s_!l0tb!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0db650e8-0c31-4b5e-a827-ae6e17d8f79e_1760x1388.jpeg\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 5:</strong><span> “Unannounced Child ‘Wellness Checks’ by Armed Federal Agents Raise Concerns: Critics say the visits are a ruse to arrest immigrants,” </span><a href=\"https://substack.com/redirect/4d90e686-550c-4d8c-9192-cb3c27515a10?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">PRS Memphis</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 6:</strong><span> “The DOJ has been firing judges with immigrant defense backgrounds,” </span><a href=\"https://substack.com/redirect/ac36179b-3f9e-4860-b32e-5614c66ec65c?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NPR</a></p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><a href=\"https://substack.com/redirect/7c59cd9d-3d2d-4b39-98ff-48f31200cc67?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">AP</a><span>: “Over the past nine months, the Trump administration has fired almost 90 immigration judges seen by Trump’s allies as too lenient…It is unclear who ordered the firings or how they were selected. But those removed granted asylum at markedly higher rates than their peers — in about half of all cases since August 2023 compared with 34% nationwide, according to Mobile Pathways. Among those purged were all 10 that appeared on the DHS Bureaucrat Watch List, a website created last year with funding from The Heritage Foundation, whose Project 2025 was a blueprint for the Trump administration’s policies and personnel decisions.”</span></p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 7:</strong><span> “DHS ditched software that archived officials’ electronic messages: Those officials are now asked to screenshot their messages and upload them to a shared drive,” </span><a href=\"https://substack.com/redirect/dfdbae4f-9b9b-4b73-bb35-84227a834085?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">FedScoop</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 7:</strong><span> “ICE Is Pitching A Call Center That Would Track Immigrant Kids,” </span><a href=\"https://substack.com/redirect/1a9ec34b-045e-49ce-8a88-a1eeacf7df0d?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">HuffPost</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 7:</strong><span> “Feds Tell Faith Leaders ‘No More Prayer’ Outside Broadview Facility,” </span><a href=\"https://substack.com/redirect/6975e828-e0da-4fd8-9007-9263c85cbfda?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Book Club Chicago</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 7:</strong><span> “Immigrants with health conditions may be denied visas under new Trump administration guidance,” </span><a href=\"https://substack.com/redirect/d52ae93f-683a-4705-864c-0a444371e28f?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">ABC</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 10:</strong><span> “Senate Democrat questions Trump administration’s $7.5M payment to Equatorial Guinea,” </span><a href=\"https://substack.com/redirect/5dcc05ef-edb3-48b3-87e6-737fb13cedd9?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">AP</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 16:</strong><span> “Homeland Security Missions Falter Amid Focus on Deportations: Under President Trump, an agency intended to keep Americans safe has diverted resources from combating child abuse, trafficking and terrorism,” </span><a href=\"https://substack.com/redirect/96fe07e9-3ff1-46ed-9cd0-9633beed8dbf?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NYT</a></p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Drug Arrests and Gun Seizures Fell as Homeland Security Pursued Immigration: Internal documents reveal the impact on crime fighting as the Trump administration diverts special agents to its mass deportation agenda,” </span><a href=\"https://substack.com/redirect/ec3a303d-267f-4e6a-bf88-96135e9ec91c?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NYT</a></p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 17:</strong><span> “Trump administration sues California over law banning masked federal agents,” </span><a href=\"https://substack.com/redirect/dba9f34d-e02a-4ad9-968e-69a3622fced2?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">AP</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 18:</strong><span> “Two Weeks of Surveillance Footage From ICE Detention Center ‘Irretrievably Destroyed’,” </span><a href=\"https://substack.com/redirect/4a7ddbff-cb21-4fad-906c-423c047d61a2?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">404 Media</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 19:</strong><span> “Appeals court blocks judge’s order restricting use of force during federal immigration crackdown in Chicago,” </span><a href=\"https://substack.com/redirect/5f86a149-d22b-47ca-8edf-f95284dab1d8?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">CNN</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 20:</strong><span> “Document Threatens Immigrant Children With ‘Prolonged’ Detention,” </span><a href=\"https://substack.com/redirect/dba14ccf-3062-4fff-bd41-42f0b99d233c?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">HuffPost</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 21:</strong><span> “Border Agents Target Mississippi, Louisiana in ‘Swamp Sweep,’ Aiming to Arrest Thousands of Immigrants,” </span><a href=\"https://substack.com/redirect/b121a865-9cf3-4158-8030-2cdbb859a305?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Mississippi Free Press</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 24:</strong><span> “ICE Targets Traffic Courts Across Tennessee,” </span><a href=\"https://substack.com/redirect/9861d02c-0836-4af8-9b0e-53f51044cd21?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Memphis Flyer</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 25:</strong><span> “DOJ acknowledges Kristi Noem made decision to continue deportation flights to El Salvador despite judge’s order,” </span><a href=\"https://substack.com/redirect/524ebaf5-083f-42c2-a436-71458f763aeb?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">CNN</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 26:</strong><span> “US stops processing Afghan immigration requests after attack near White House,” </span><a href=\"https://substack.com/redirect/1663f82d-4d6c-4fa9-85ee-43dc02e18d45?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Reuters</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Other stories:</strong></p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px\"><span>“Border patrol agent who shot Chicago woman boasted about it in text messages [IL],” </span><a href=\"https://substack.com/redirect/17bd56d0-59b8-46af-84bb-773da8bbe373?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NBC</a></p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>UPDATE: “ICE agent who shot woman in Chicago drove Border Patrol vehicle back to Maine,” </span><a href=\"https://substack.com/redirect/e9e4245a-e091-435f-8930-44e860fd59df?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">WGME</a><span>; “The Feds Suddenly Want to Drop Their Charges Against a Woman Shot by a Border Patrol Agent: The move avoids the release of more messages from a Border Patrol agent whose previous texts showed he had bragged about shooting her,” </span><a href=\"https://substack.com/redirect/295e5f94-0a00-4f14-92c5-07038200e7e3?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Mother Jones</a><span>; “Judge dismisses charges against Chicago woman shot by Border Patrol,” </span><a href=\"https://substack.com/redirect/39dd5aa1-2253-4b0b-a3ba-db854320b1ed?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NBC</a></p></li></ul></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Child care worker detained by ICE inside a Chicago day care as children watched [IL],” </span><a href=\"https://substack.com/redirect/6c16d83e-6b42-4d9c-9b5a-4375d2e3e7b5?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">19th News</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“‘They destroyed us’: East Chicago woman with schizophrenia who opted for deportation now missing in Mexico [IL],” </span><a href=\"https://substack.com/redirect/e82e5418-414f-4129-af34-1ea49b1668f4?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Chicago Tribune</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Dad says he and his toddler were pepper sprayed by federal immigration agents [IL],” </span><a href=\"https://substack.com/redirect/19542bf9-88c7-4be8-9353-210407f79749?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NBC</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Cops, protesters injured, 21 arrested after scuffles erupt during Broadview ICE facility protest [IL],” </span><a href=\"https://substack.com/redirect/caf41593-7fb3-4433-b71b-0ef676a4a451?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Chicago Sun Times</a><span>; “At least 7 faith leaders arrested at Broadview ICE facility protest,” </span><a href=\"https://substack.com/redirect/a5d27630-21b2-4576-8158-d1cd6846246c?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Nat’l Catholic Reporter</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Sanford grandfather, born in refugee camp, nabbed by ICE after 70 years in U.S. [FL],” </span><a href=\"https://substack.com/redirect/5c073ae6-8967-4d9c-a131-3cc94daa5b23?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Orlando Sentinel</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Family speaks out after death of man deported by ICE in vegetative state [TX],” </span><a href=\"https://substack.com/redirect/5bad0fed-e961-46a3-9b96-577af1c0f268?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Guardian</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Austin school district in disarray after ICE detains multiple employees [TX],” </span><a href=\"https://substack.com/redirect/52734ee9-1523-4e65-ad12-966328475f9e?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Chron</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“ICE video shows officers planned to ‘smash’ into Charlotte man filming Border Patrol [NC],” </span><a href=\"https://substack.com/redirect/662c8e77-9646-4b2d-b0ea-64ab523196db?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Charlotte Observer</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“She Was Deported in Error. Her Child Was Left Behind [NY],” </span><a href=\"https://substack.com/redirect/b321219e-a7b5-4fed-9d8a-21d311632702?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NYT</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Queens Family Demands Answers After Chinese Immigrant Found Bound in ICE Jail [NY],” </span><a href=\"https://substack.com/redirect/c5330815-acc4-42f2-b6e0-dbfe45c05ffb?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Documented NY</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“BU student brags on social media about calling ICE on Allston car wash workers [MA],” </span><a href=\"https://substack.com/redirect/6e6f998e-a1c7-4661-a5e8-120e665351b1?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">WBUR</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“19-year-old college student deported despite judge’s order blocking her removal [MA],” </span><a href=\"https://substack.com/redirect/b6f22d3b-be7a-494e-a8c7-d989aea46a01?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">ABC</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“How a RI judge intervened after a teen intern was wrongfully detained by ICE,” </span><a href=\"https://substack.com/redirect/edc435cd-fc7d-4fa1-81c4-2e2febe40f68?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Providence Journal</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Attorney for Ontario [CA] man shot by ICE agent disputes claim that his client tried to assault officers,” </span><a href=\"https://substack.com/redirect/c06f2100-44b2-458f-8073-1a43881f4d93?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">ABC</a><span>; “Man shot in back by ICE pleads not guilty to assault charges,” </span><a href=\"https://substack.com/redirect/4324db34-7339-4e5c-a870-fecf7bc9eff8?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">LA Times</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Armed US immigration agents drive off with toddler after arrest of father [CA],” </span><a href=\"https://substack.com/redirect/15814222-2eae-4bde-bfa6-5f8ffc165e3c?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Guardian</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“An L.A. man was detained in an immigration raid. No one knows where he is [CA],” </span><a href=\"https://substack.com/redirect/3aa68a88-7cf0-4d9f-8176-bd6d928a7b7f?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">LA Times</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“British mum detained by ICE agents in California as six-month old baby and stunned husband watch on [CA],” </span><a href=\"https://substack.com/redirect/77bd31ec-f2a3-45fb-9f6a-459deab22680?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">LBC</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Indigenous actor Elaine Miles says ICE called her tribal ID ‘fake’ [WA],” </span><a href=\"https://substack.com/redirect/7b9a487b-d4f3-4557-a30d-0a4d9ba66462?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Seattle Times</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Issaquah on edge after ICE arrest outside preschool [WA],” </span><a href=\"https://substack.com/redirect/7d97eca0-ef5c-4bab-b18a-2a30893fe2ec?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">KUOW</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“High school senior, a U.S. citizen, detained by ICE in Oregon,” </span><a href=\"https://substack.com/redirect/4df025ea-f535-444c-94f4-562d88ae2842?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Oregon Live</a></p></li></ul><div style=\"font-size: 16px; line-height: 26px\"><hr style=\"margin: 32px 0; padding: 0; height: 1px; background: #e6e6e6; border: none\" /></div><h1 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 2em\"><strong>DEPARTMENT OF JUSTICE</strong></h1><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 1:</strong><span> “FBI Ousts Leader as Patel Fumes Over Attention to Agency Jet Use,” </span><a href=\"https://substack.com/redirect/76015ece-f79c-4d7c-8f71-3fb8311ee54f?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Bloomberg</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 4:</strong><span> “FBI fires, unfires, then refires agents linked to Jack Smith probe,” </span><a href=\"https://substack.com/redirect/33920f96-ab2f-4af0-89d3-fb1a321f0816?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NBC</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 6:</strong><span> “Justice Dept. Is Said to Be Investigating D.C. Mayor Over Foreign Trip,” </span><a href=\"https://substack.com/redirect/005a951d-ba28-4b21-8b4d-835101f347bd?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NYT</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 7:</strong><span> “Feds move to subpoena former CIA director and others who investigated Russian interference in Trump’s 2016 campaign,” </span><a href=\"https://substack.com/redirect/c969ae9d-001b-4954-8522-4aae103e900f?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">CNN</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 10:</strong><span> “F.B.I. Director Is Said to Have Made a Pledge to Head of MI5, Then Broken It: The episode has contributed to concerns among intelligence allies that Kash Patel, brash and partisan, is also unpredictable and even unreliable,” </span><a href=\"https://substack.com/redirect/ace54396-8dfb-4e45-b095-96dd8a263616?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NYT</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 14:</strong><span> “Michael Flynn, DOJ in Settlement Talks Over $50 Million Claim,” </span><a href=\"https://substack.com/redirect/4f5da5a0-6004-48cf-b35e-7b63d896c00a?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Bloomberg</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 14:</strong><span> “Justice Department quietly replaced ‘identical’ Trump signatures on recent pardons,” </span><a href=\"https://substack.com/redirect/152203cb-4f06-454a-9720-502b3be337db?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">AP</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 16:</strong><span> “The Unraveling of the Justice Department: Sixty attorneys describe a year of chaos and suspicion,” </span><a href=\"https://substack.com/redirect/4ccb5b0f-64be-43d8-a930-cd47642f5327?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NYT</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 17:</strong><span> “Whistleblower accuses Ed Martin of ‘concealing and destroying’ records related to DOJ’s weaponization group,” </span><a href=\"https://substack.com/redirect/1e5818c8-770b-4062-bbb2-675329e59cb7?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">The Hill</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 19:</strong><span> “Veteran FBI employee sues bureau after being fired over displaying a pride flag,” </span><a href=\"https://substack.com/redirect/930e8f34-ce8f-49f0-a5ce-5b35b05f2350?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">CNN</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 19:</strong><span> “Full grand jury didn’t see final Comey indictment, prosecutors admit,” </span><a href=\"https://substack.com/redirect/47bb7635-360d-448a-98f4-f7a092f0da05?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Guardian</a></p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Judge dismisses cases against James Comey and Letitia James after finding prosecutor was unlawfully appointed,” </span><a href=\"https://substack.com/redirect/056f8fb3-a959-4e4c-8205-ea4d506800ba?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NBC</a></p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 20:</strong><span> “DOJ, FBI probing top Trump administration officials over investigations of president’s adversaries,” </span><a href=\"https://substack.com/redirect/5bde64df-2adb-41de-a074-7e6422c2f86a?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">ABC</a></p><blockquote style=\"border-left: 4px solid #EA82FF; margin: 20px 0; padding: 0\"><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); margin-left: 20px; line-height: 26px; font-size: 16px\">Sources said the DOJ and FBI are scrutinizing whether U.S. Pardon Attorney Ed Martin and Federal Housing Finance Agency Director Bill Pulte enlisted individuals outside the Department of Justice to probe allegations of mortgage fraud amid ongoing investigations of Sen. Adam Schiff and New York Attorney General Letitia James.</p></blockquote><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>Related: “Trump ousts watchdog of US housing regulator involved in mortgage probes of his foes,” </span><a href=\"https://substack.com/redirect/297976a9-e94b-4808-bafb-daf699506d41?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Reuters</a></p></li></ul><div style=\"font-size: 16px; line-height: 26px\"><hr style=\"margin: 32px 0; padding: 0; height: 1px; background: #e6e6e6; border: none\" /></div><h1 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 2em\"><strong>DEPARTMENT OF DEFENSE</strong></h1><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><em><span>The U.S. military conducted </span><a href=\"https://substack.com/redirect/f52f9dd9-dba4-4ba5-b12e-988dac41ca0b?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">seven</a><span> boat strikes in the Caribbean and eastern Pacific in November, killing 21 people and bringing the total death toll to over 80 since September.</span></em></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 1:</strong><span> “Trump administration tells Congress war law doesn’t apply to cartel strikes,” </span><a href=\"https://substack.com/redirect/821bfcda-355e-4772-8935-7e9780610c95?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">WaPo</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 2:</strong><span> “Hegseth bars military officials from discussing drug boat strikes with Congress without prior approval,” </span><a href=\"https://substack.com/redirect/05068e04-0f56-4822-a2e2-3a9ec70071c8?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">CNN</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 7:</strong><span> “Trump has accused boat crews of being narco-terrorists. The truth, AP found, is more nuanced,” </span><a href=\"https://substack.com/redirect/52233a39-e7d6-4c80-8cf7-e43add4b7a10?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">AP</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 11:</strong><span> “UK suspends some intelligence sharing with US over boat strike concerns in major break,” </span><a href=\"https://substack.com/redirect/6c9a06a0-1b47-4727-91b8-a38d4b356a10?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">CNN</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 11:</strong><span> “‘She was the best man for the job’: Hegseth’s policies are pushing qualified women out of the military,” </span><a href=\"https://substack.com/redirect/8f2f7e82-cde3-41cb-87ad-17a8baf4049b?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">CNN</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 12:</strong><span> “U.S. troops not liable in boat strikes, classified Justice Dept. memo says,” </span><a href=\"https://substack.com/redirect/08be780c-e06a-4b72-9c09-18d2e2238cfc?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">WaPo</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 13:</strong><span> “Trump briefed on updated military options in Venezuela,” </span><a href=\"https://substack.com/redirect/c3b23648-acbc-46a6-8077-034860c9cabd?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">ABC</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 13:</strong><span> “Display about Black soldiers in World War II removed from US military cemetery,” </span><a href=\"https://substack.com/redirect/d6b3084c-1744-4e54-9c99-43698023527c?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">CNN</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 14:</strong><span> “Secret U.S. Memo Authorizing Drug-Boat Strikes Cites Chemical Weapon Threat,” </span><a href=\"https://substack.com/redirect/bae04744-6d2c-4b8a-a970-879141aa5c7f?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">WSJ</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 18:</strong><span> “Trump Said to Authorize C.I.A. Plans for Covert Action in Venezuela,” </span><a href=\"https://substack.com/redirect/cb412713-077e-4081-baf0-853aada57ce3?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NYT</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 19:</strong><span> “Democratic lawmakers urge troops to disobey illegal orders,” </span><a href=\"https://substack.com/redirect/7b9ea8ad-e008-4826-98de-37d41bf8efd1?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">CNN</a></p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Trump accuses Democrats of ‘seditious behavior, punishable by death,’ for urging military to ignore illegal orders,” </span><a href=\"https://substack.com/redirect/1a327f84-e82e-40cb-a03d-b61f2d628853?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NBC</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Pete Hegseth orders US navy to investigate Mark Kelly’s comments,” </span><a href=\"https://substack.com/redirect/9734b4e4-5386-4491-a4af-0587f6ad08c6?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Guardian</a></p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 25:</strong><span> “U.S. ready to cut support to Scouts, accusing them of attacking ‘boy-friendly spaces’,” </span><a href=\"https://substack.com/redirect/0b4fbc57-397a-455a-b55b-43274fef6bfa?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NPR</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 28:</strong><span> “Hegseth order on first Caribbean boat strike, officials say: Kill them all,” </span><a href=\"https://substack.com/redirect/00c0fcec-18c3-4819-a401-2a124293a05e?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">WaPo</a></p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“White House says admiral ordered follow-on strike on alleged drug boat, insists attack was lawful,” </span><a href=\"https://substack.com/redirect/a9dbffae-74b3-49b2-ac5f-32a5003ee222?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Reuters</a></p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 29:</strong><span> “Trump says Venezuelan airspace should be considered closed,” </span><a href=\"https://substack.com/redirect/046b6710-73da-4f9a-9fec-2f87ccddd37e?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Reuters</a></p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Venezuela calls Trump’s call to close airspace a ‘colonialist threat’,” </span><a href=\"https://substack.com/redirect/23c31d73-5fcb-41b6-9e74-8be38038389d?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NPR</a></p></li></ul><div style=\"font-size: 16px; line-height: 26px\"><hr style=\"margin: 32px 0; padding: 0; height: 1px; background: #e6e6e6; border: none\" /></div><h1 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 2em\"><strong>CULTURAL CONTROL</strong></h1><h3 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.375em\"><strong>Media</strong></h3><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 3</strong><span> “CBS News heavily edits Trump 60 Minutes interview, cutting boast network ‘paid me a lotta money’,” </span><a href=\"https://substack.com/redirect/7530a80a-ef91-4858-8c50-7220874b9b46?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Guardian</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 9:</strong><span> “Top BBC bosses resign after criticism of the broadcaster’s editing of a Trump speech,” </span><a href=\"https://substack.com/redirect/edbf4373-0098-4a5a-9ec5-8813b75c6fb9?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">AP</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 12:</strong><span> “Lawmakers say Paramount Skydance ‘stonewalling’ probe into Trump merger approval,” </span><a href=\"https://substack.com/redirect/292752a3-e816-4924-8350-214358c0f772?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Reuters</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 15:</strong><span> “Trump says he will take legal action against BBC over Panorama edit,” </span><a href=\"https://substack.com/redirect/30a54fc0-97a3-46f2-b916-5e92f248d124?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">BBC</a></p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“BBC apologizes for edit of Trump speech but says it won’t provide legal compensation,” </span><a href=\"https://substack.com/redirect/b15b2a2b-47fc-4b77-beef-d6aae80124da?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NPR</a></p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 18:</strong><span> “Paramount Skydance Denies That Its Warner Bros. Discovery Bid Involves Arab Sovereign Wealth Funds,” </span><a href=\"https://substack.com/redirect/b731d789-b006-4998-8a09-f4c71d43a7fd?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Variety</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 20:</strong><span> “Larry Ellison discussed axing CNN hosts with White House in takeover bid talks,” </span><a href=\"https://substack.com/redirect/6a43e6e4-6225-4d0a-9b12-9ffcc72b9c71?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Guardian</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 25:</strong><span> “BBC Accused Of Censorship After Removing Claim That Trump Is “Most Openly Corrupt President In History” From Prestigious Radio Show,” </span><a href=\"https://substack.com/redirect/a65c754e-1374-4d95-b108-b13a9490cc0f?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Deadline</a></p><h3 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.375em\"><strong>Universities</strong></h3><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 7:</strong><span> “Cornell reaches $60 million deal with Trump administration to restore funding,” </span><a href=\"https://substack.com/redirect/3707ccfb-bfb6-4d2d-aa63-4edb03b5050a?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NBC</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 7:</strong><span> “[GOP-led House Judiciary Committee] Accuses GMU President of Lying About DEI Efforts,” </span><a href=\"https://substack.com/redirect/36203003-d61d-419f-b80b-9ea9ae8258ae?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Inside Higher Ed</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 28:</strong><span> “Northwestern University and Trump administration reach $75 million agreement over frozen research funds,” </span><a href=\"https://substack.com/redirect/25b5bbcb-4c8b-42ce-8142-c086d51c7f69?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">ABC</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 30:</strong><span> “Fewer international students are enrolling at U.S. colleges, which could cost the country $1 billion, reports find,” </span><a href=\"https://substack.com/redirect/0d54b216-3613-424a-be2d-edd5f9a3ac84?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">CNBC</a></p><div style=\"font-size: 16px; line-height: 26px\"><hr style=\"margin: 32px 0; padding: 0; height: 1px; background: #e6e6e6; border: none\" /></div><h1 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 2em\"><strong>ENVIRONMENT</strong></h1><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 11:</strong><span> “Trump plan would open California to offshore oil drilling,” </span><a href=\"https://substack.com/redirect/8934e4e9-f192-4fca-a13c-e6b60ceafbc1?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">WaPo</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 19:</strong><span> “Trump moves to strip protections for endangered and threatened species,” </span><a href=\"https://substack.com/redirect/5a1d1a4a-88be-42be-bc6d-aba10e429154?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">CNN</a></p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Dismantling the Endangered Species Act will hurt a lot more than just wildlife,” </span><a href=\"https://substack.com/redirect/a669b0f6-4c8c-42f1-a31a-e54459cf5ed6?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Grist</a></p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 25:</strong><span> “EPA to abandon air pollution rule that would prevent thousands of U.S. deaths,” </span><a href=\"https://substack.com/redirect/b740261f-b389-437e-9f64-89aaeec7e3be?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">WaPo</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 28:</strong><span> “Trump order to keep Michigan power plant open costs taxpayers $113m,” </span><a href=\"https://substack.com/redirect/e3f86092-604b-46e5-8985-e96d94fa887d?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Guardian</a></p><div style=\"font-size: 16px; line-height: 26px\"><hr style=\"margin: 32px 0; padding: 0; height: 1px; background: #e6e6e6; border: none\" /></div><h1 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 2em\"><strong>CORRUPTION, BRIBERY, AND GENERAL LAWLESSNESS</strong></h1><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 5:</strong><span> “Tech Billionaire Marc Andreessen Bet Big on Trump. It’s Paying Off for Silicon Valley,” </span><a href=\"https://substack.com/redirect/9ff7bc50-a87d-4158-b8df-2c477e50a91b?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">ProPublica</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 7:</strong><span> “Trump Wine Hits Government Shelves,” </span><a href=\"https://substack.com/redirect/5bee94e3-f0a9-4a82-be37-acf595415a74?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Forbes</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 8:</strong><span> “Trump wants Commanders’ new D.C. stadium named for him,” </span><a href=\"https://substack.com/redirect/fa0bd707-4260-426c-904e-557452ade5fb?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">ESPN</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 14:</strong><span> “Preservationists sue Trump over plans to paint Eisenhower building,” </span><a href=\"https://substack.com/redirect/7b3bc4df-85cc-4408-9273-bb01a309388c?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">WaPo</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 19:</strong><span> “Majority of corporate Trump ballroom donors represented by 3 lobbying firms, watchdog says,” </span><a href=\"https://substack.com/redirect/52625efa-b007-4ffa-b69a-f5f2f0ee13db?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">CBS</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 19:</strong><span> “White House floats executive order to rein in state AI laws,” </span><a href=\"https://substack.com/redirect/c469f269-2e10-48c1-82c4-599b3e3bd28b?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Axios</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 21:</strong><span> “Senate Democrats are investigating the Kennedy Center for ‘cronyism, corruption’,” </span><a href=\"https://substack.com/redirect/26a252f9-ac1d-4b4f-a82f-ee2bc47cbc32?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NPR</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 26:</strong><span> “Trump wants a bigger White House ballroom. His architect disagrees.” </span><a href=\"https://substack.com/redirect/78eb1f59-b3ee-4df5-8427-0e415b506d49?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">WaPo</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 26:</strong><span> “Howard Lutnick’s Former Wall Street Firm Is Having Its Best Year Ever,” </span><a href=\"https://substack.com/redirect/17d0a165-98fd-478f-bbb4-a491c6b92106?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">WSJ</a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Nov. 27:</strong><span> “Swiss lawmakers seek probe into whether gifts to Trump by business leaders breached law,” </span><a href=\"https://substack.com/redirect/596d9524-ad34-409a-baab-84e0da4b93f4?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Reuters</a></p><div style=\"font-size: 16px; line-height: 26px\"><hr style=\"margin: 32px 0; padding: 0; height: 1px; background: #e6e6e6; border: none\" /></div><h1 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 2em\"><strong>MISCELLANEOUS</strong></h1><ul style=\"margin-top: 0; padding: 0; margin-bottom: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“US consumer watchdog says it is legally blocked from accessing funds,” </span><a href=\"https://substack.com/redirect/83aa05f4-24f2-4371-86d6-311b671dbd9b?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Reuters</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“FDA’s top drug regulator resigns after federal officials probe ‘serious concerns’ about his conduct,” </span><a href=\"https://substack.com/redirect/8ae1f195-438d-41a3-8e82-e77c48d92763?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">AP</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Trump admin axed 383 active clinical trials, dumping over 74K participants,” </span><a href=\"https://substack.com/redirect/d82f48a1-4772-4c59-88d3-c6bb85c65cdd?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">ArsTechnica</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px\"><span>“CDC website changed to include false claims that link autism and vaccines,” </span><a href=\"https://substack.com/redirect/8dbcb830-376f-4153-8374-bcbc8f2761fa?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">CNN</a></p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Kennedy Says He Told C.D.C. to Change Website’s Language on Autism and Vaccines,” </span><a href=\"https://substack.com/redirect/2c0980d4-83b8-4a34-b3fe-ff888c1377ce?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NYT</a></p></li></ul></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Louisiana official who called Covid-19 vaccines ‘dangerous’ given key CDC post,” </span><a href=\"https://substack.com/redirect/0f85a428-79e5-4eb9-8daa-7cd993bc063a?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Guardian</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“FDA official plans to change vaccine approval process, claiming that Covid-19 shots caused child deaths,” </span><a href=\"https://substack.com/redirect/8384e6bb-afd9-4198-9403-09723af60698?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">CNN</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“FEMA chief steps down as Trump administration prepared to oust him,” </span><a href=\"https://substack.com/redirect/6070b3e7-813d-4c72-b58c-43d4391c05f6?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">CNN</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“K-12 moving to Labor as Trump administration accelerates bid to dismantle Education Department,” </span><a href=\"https://substack.com/redirect/da112b64-c3b9-4b55-83e4-c8aa15e9b96d?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Chalkbeat</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“The Trump administration plans major cuts to long-term housing for homelessness,” </span><a href=\"https://substack.com/redirect/0b789dd3-752c-4681-b8b2-9da47098daaa?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">OPB</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“International tourists face national park price hikes of up to $170,” </span><a href=\"https://substack.com/redirect/644c7174-a808-4698-84d1-7816f03aa5cb?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Axios</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Steve Witkoff coached a Putin aide on how Russian leader should pitch Trump on Ukraine peace plan, report reveals,” </span><a href=\"https://substack.com/redirect/7918eb43-c941-4af7-89a2-d31a13227b9d?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">PBS</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“US senators say Rubio told them Trump’s Ukraine peace plan is Russia’s ‘wish list’,” </span><a href=\"https://substack.com/redirect/97fdc269-f577-4252-ab6b-0b2021a9e52e?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">AP</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><span>“Trump officially requests pardon for Netanyahu, Israeli president’s office says,” </span><a href=\"https://substack.com/redirect/41d50227-b761-46d3-905f-94690e1400fc?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NBC</a></p></li></ul></div></div><div style=\"margin: 32px 0 0; width: 100%; box-sizing: border-box; border-top: 1px solid #e6e6e6; font-size: 16px; line-height: 26px\"></div><div style=\"--image-offset-margin: -120px; font-size: 16px; line-height: 26px\"><div style=\"margin: 32px 0; font-size: 16px; line-height: 26px\"><div dir=\"auto\" style=\"text-align: initial; font-size: 16px; line-height: 26px; width: 100%; word-break: break-word; margin-bottom: 16px\"><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px; margin-top: 0\"><em>You’re currently a free subscriber to KeepTrack. To support this newsletter, consider upgrading your subscription.</em></p><div style=\"margin: 0 0 1em; direction: ltr; font-size: 16px; line-height: 26px; margin-bottom: 0\"><div style=\"text-decoration: unset; list-style: none; font-size: 16px; line-height: 26px; text-align: center; cursor: pointer; border-radius: 4px\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9rZWVwdHJhY2suc3Vic3RhY2suY29tL3N1YnNjcmliZT91dG1fc291cmNlPXBvc3QmdXRtX2NhbXBhaWduPWVtYWlsLWNoZWNrb3V0Jm5leHQ9aHR0cHMlM0ElMkYlMkZrZWVwdHJhY2suc3Vic3RhY2suY29tJTJGcCUyRmV2ZXJ5LXRlcnJpYmxlLXRoaW5nLXRoZS10cnVtcC1hZG1pbmlzdHJhdGlvbi1hNzUmcj0yb2Z3c28mdG9rZW49ZXlKMWMyVnlYMmxrSWpveE5qRTVPRFUwT0RBc0ltbGhkQ0k2TVRjMk5EWTNORFUxT1N3aVpYaHdJam94TnpZM01qWTJOVFU1TENKcGMzTWlPaUp3ZFdJdE1UY3lOVGsxTlNJc0luTjFZaUk2SW1Ob1pXTnJiM1YwSW4wLkxlYkdxMEtwaUd3Tmp4MmhhUUFGUjBSX3A0aWcwcWlXajhHTkJCZVZrX00iLCJwIjoxODA0NTcyMTAsInMiOjE3MjU5NTUsImYiOnRydWUsInUiOjE2MTk4NTQ4MCwiaWF0IjoxNzY0Njc0NTU5LCJleHAiOjIwODAyNTA1NTksImlzcyI6InB1Yi0wIiwic3ViIjoibGluay1yZWRpcmVjdCJ9.QJmcjhRkCaIgF-TVJ36CRpFXiklLfNgr-TEUSfkwaEI?&amp;utm_medium=email&amp;utm_source=subscribe-widget&amp;utm_content=180457210\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; box-sizing: border-box; cursor: pointer; border: none; border-radius: 8px; font-size: 14px; line-height: 20px; font-weight: 600; text-align: center; opacity: 1; outline: none; white-space: nowrap; text-decoration: none !important; background-color: #EA82FF; color: #ffffff !important; margin: 0 auto; padding: 12px 20px; height: auto\" target=\"_blank\" rel=\"noreferrer\"><span style=\"color: #ffffff; text-decoration: none\">Upgrade to paid</span></a></div></div></div></div></div><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"border-top: 1px solid rgb(0,0,0,.1); border-bottom: 1px solid rgb(0,0,0,.1); min-width: 100%\"><tbody><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr><tr><td><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"margin: 0 auto\"><tbody><tr><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=1725955&amp;post_id=180457210&amp;utm_source=substack&amp;isFreemail=true&amp;submitLike=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE4MDQ1NzIxMCwicmVhY3Rpb24iOiLinaQiLCJpYXQiOjE3NjQ2NzQ1NTksImV4cCI6MTc2NzI2NjU1OSwiaXNzIjoicHViLTE3MjU5NTUiLCJzdWIiOiJyZWFjdGlvbiJ9.wpG2d0NiQjP7YYDMeX6Lv3S6VPcrjogRfg1Kghqlu6o&amp;utm_medium=email&amp;utm_campaign=email-reaction&amp;r=2ofwso\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!PeVs!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideHeart%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"margin-right: 8px; min-width: 18px; min-height: 18px; border: none; vertical-align: middle; max-width: 18px\" /><span style=\"vertical-align: middle\">Like</span></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=1725955&amp;post_id=180457210&amp;utm_source=substack&amp;utm_medium=email&amp;isFreemail=true&amp;comments=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE4MDQ1NzIxMCwiaWF0IjoxNzY0Njc0NTU5LCJleHAiOjE3NjcyNjY1NTksImlzcyI6InB1Yi0xNzI1OTU1Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.vgjK3EA3RKiMU4cbeXf9vssaeU2sF4nwfFIOxESj6U8&amp;r=2ofwso&amp;utm_campaign=email-half-magic-comments&amp;action=post-comment&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!x1tS!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideComments%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"margin-right: 8px; min-width: 18px; min-height: 18px; border: none; vertical-align: middle; max-width: 18px\" /><span style=\"vertical-align: middle\">Comment</span></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9vcGVuLnN1YnN0YWNrLmNvbS9wdWIva2VlcHRyYWNrL3AvZXZlcnktdGVycmlibGUtdGhpbmctdGhlLXRydW1wLWFkbWluaXN0cmF0aW9uLWE3NT91dG1fc291cmNlPXN1YnN0YWNrJnV0bV9tZWRpdW09ZW1haWwmdXRtX2NhbXBhaWduPWVtYWlsLXJlc3RhY2stY29tbWVudCZhY3Rpb249cmVzdGFjay1jb21tZW50JnI9Mm9md3NvJnRva2VuPWV5SjFjMlZ5WDJsa0lqb3hOakU1T0RVME9EQXNJbkJ2YzNSZmFXUWlPakU0TURRMU56SXhNQ3dpYVdGMElqb3hOelkwTmpjME5UVTVMQ0psZUhBaU9qRTNOamN5TmpZMU5Ua3NJbWx6Y3lJNkluQjFZaTB4TnpJMU9UVTFJaXdpYzNWaUlqb2ljRzl6ZEMxeVpXRmpkR2x2YmlKOS52Z2pLM0VBM1JLaU1VNGNiZVhmOXZzc2FlVTJzRjRud2ZGSU94RVNqNlU4IiwicCI6MTgwNDU3MjEwLCJzIjoxNzI1OTU1LCJmIjp0cnVlLCJ1IjoxNjE5ODU0ODAsImlhdCI6MTc2NDY3NDU1OSwiZXhwIjoyMDgwMjUwNTU5LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.tSw6-Rw5PIUKARv05A98Xf-ZdzVpE4vXZcXgp7G0TIc?&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!5EGt!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FNoteForwardIcon%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"margin-right: 8px; min-width: 18px; min-height: 18px; border: none; vertical-align: middle; max-width: 18px\" /><span style=\"vertical-align: middle\">Restack</span></a></td></tr></tbody></table></td></tr></tbody></table></td><td align=\"right\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr></tr></tbody></table></td></tr></tbody></table></td></tr><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr></tbody></table><div style=\"color: rgb(119,119,119); text-align: center; font-size: 16px; line-height: 26px; padding: 24px0\"><div style=\"font-size: 16px; line-height: 26px; padding-bottom: 24px\"><p style=\"list-style: none; font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; padding-bottom: 0; font-size: 12px; line-height: 16px; margin: 0; color: rgb(119,119,119); text-decoration: unset\">© 2025 <span>Keep Track</span><br />548 Market Street PMB 72296, San Francisco, CA 94104 <br /><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9rZWVwdHJhY2suc3Vic3RhY2suY29tL2FjdGlvbi9kaXNhYmxlX2VtYWlsP3Rva2VuPWV5SjFjMlZ5WDJsa0lqb3hOakU1T0RVME9EQXNJbkJ2YzNSZmFXUWlPakU0TURRMU56SXhNQ3dpYVdGMElqb3hOelkwTmpjME5UVTVMQ0psZUhBaU9qRTNPVFl5TVRBMU5Ua3NJbWx6Y3lJNkluQjFZaTB4TnpJMU9UVTFJaXdpYzNWaUlqb2laR2x6WVdKc1pWOWxiV0ZwYkNKOS5Zbkoxa0JUMHVLaDg0V192QVRCUUdMTlByX2hGZ3IxMmEta3BZSUpjdE5JIiwicCI6MTgwNDU3MjEwLCJzIjoxNzI1OTU1LCJmIjp0cnVlLCJ1IjoxNjE5ODU0ODAsImlhdCI6MTc2NDY3NDU1OSwiZXhwIjoyMDgwMjUwNTU5LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.CbQ4CpuYEJltdld_TOKN3Sof-pSXmDkf-UV7tbZsBYE?\" style=\"text-decoration: underline; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><span style=\"color: rgb(119,119,119); text-decoration: underline\">Unsubscribe</span></a></p></div><p style=\"padding: 0 24px; font-size: 12px; line-height: 20px; margin: 0; color: rgb(119,119,119); font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; padding-bottom: 0; margin-top: 0\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9zdWJzdGFjay5jb20vc2lnbnVwP3V0bV9zb3VyY2U9c3Vic3RhY2smdXRtX21lZGl1bT1lbWFpbCZ1dG1fY29udGVudD1mb290ZXImdXRtX2NhbXBhaWduPWF1dG9maWxsZWQtZm9vdGVyJmZyZWVTaWdudXBFbWFpbD1ieXRlYnl0ZWdvODhAaW5vLnRvJnI9Mm9md3NvIiwicCI6MTgwNDU3MjEwLCJzIjoxNzI1OTU1LCJmIjp0cnVlLCJ1IjoxNjE5ODU0ODAsImlhdCI6MTc2NDY3NDU1OSwiZXhwIjoyMDgwMjUwNTU5LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.IvFwif6a7Vb0Lkbiyxa4ydvTEOMBR545c_405QDKO7g?\" style=\"color: rgb(119,119,119); text-decoration: none; display: inline-block; margin: 0 4px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!LkrL!,w_270,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpublish-button%402x.png\" width=\"135\" alt=\"Start writing\" height=\"40\" style=\"max-width: 550px; border: none !important; vertical-align: middle\" /></a></p></div></div></td><td></td></tr></tbody></table><img src=\"https://eotrx.substackcdn.com/open?token=eyJtIjoiPDIwMjUxMjAyMTEyMjI5LjMuODBmY2Y3MzcwOTRjYmYwNUBtZzIuc3Vic3RhY2suY29tPiIsInUiOjE2MTk4NTQ4MCwiciI6ImJ5dGVieXRlZ284OEBpbm8udG8iLCJkIjoibWcyLnN1YnN0YWNrLmNvbSIsInAiOjE4MDQ1NzIxMCwidCI6Im5ld3NsZXR0ZXIiLCJhIjoiZXZlcnlvbmUiLCJzIjoxNzI1OTU1LCJjIjoicG9zdCIsImYiOnRydWUsInBvc2l0aW9uIjoiYm90dG9tIiwiaWF0IjoxNzY0Njc0NTU5LCJleHAiOjE3NjcyNjY1NTksImlzcyI6InB1Yi0wIiwic3ViIjoiZW8ifQ.piIi_eK3UpOiiQa1ipYIcklbBrfx01pL6YX-H2GZu8w\" width=\"1\" height=\"1\" border=\"0\" style=\"height: 1px !important; width: 1px !important; border-width: 0 !important; margin-top: 0 !important; margin-bottom: 0 !important; margin-right: 0 !important; margin-left: 0 !important; padding-top: 0 !important; padding-bottom: 0 !important; padding-right: 0 !important; padding-left: 0 !important\" /><img width=\"1\" height=\"1\" src=\"https://email.mg2.substack.com/o/eJxMkE2KxCAQRk_TLoPaGnXhWYKWlYx0okHLHnL7oX8Ws6jNK3g8PgiEW22XP2snlrxKwmrL0Aszq9korR3DI-R92bBgC4RpCfTvq6xgP36WVoEwPN2TEcpCdBpWY23SWqUAjmUvudRCcimElNJN98nyFVZzN9wpiCvXN8WPTU59xE4BHhPUg-W-rA3fAZ7aQPbKXMJIGQugxye2q5YvzskLy5U2UvAPoetEX_C370iEjZ0jLlCPY5RM14IlxB3TVzziniFQruUtMlI7rVnz8SJ83VatvSmeS52osj5iqkfIxT8QT2oBHow-I46O7a2YhbNaWc6eXv4FAAD__wWVdmA\" /></div></div></div>",
      "summary": "I made a list.͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏  ",
      "publishedAt": "2025-12-02T11:22:45.000Z",
      "author": "KeepTrack ",
      "source": "rss",
      "feedName": "Byte Byte Go",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 14.850966107019133,
      "ingestedAt": "2025-12-02T14:44:03.195Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "observability",
        "governance",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b0991af76",
      "title": "Code Gen strikes back ⚔️, stakeholders and the product model 🤝, micromanagement 🔍",
      "url": "https://www.inoreader.com/article/3a9c6e76b29a5089",
      "content": "<div class=\"email_is_html\"><div><div>    <div style=\"display: none; max-height: 0px; overflow: hidden\">Most AI narratives break down under closer data: code-gen tools are rebounding, job trends are mixed, and early valuations don't predict much ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ </div>  <div style=\"display: none; max-height: 0px; overflow: hidden\"> <br /> </div>  <table align=\"center\"><tbody><tr><td valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"600\"><tbody><tr><td>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td>  <table width=\"100%\"><tbody><tr><td>    <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" style=\"margin-top: 0px\" width=\"100%\"><tbody><tr><td style=\"padding: 0px\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div style=\"text-align: center\"> <span style=\"margin-right: 0px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fproduct%3Futm_source=tldrproduct/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/NsJdvJX12dm4jBWz63CDIpa5ZZNmnuz862Fg14qjNys=433\" rel=\"noreferrer\" target=\"_blank\"><span>Sign Up</span></a> |<span style=\"margin-right: 2px; margin-left: 2px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrproduct%26utm_medium=newsletter%26utm_campaign=advertisetopnav/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/ro_uJXhw3Z8Z4vzc9LGD85tS-2me9RxPie2KJIAbSC0=433\" rel=\"noreferrer\" target=\"_blank\"><span>Advertise</span></a></span>|<span style=\"margin-left: 2px\"><a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa.tldrnewsletter.com%2Fweb-version%3Fep=1%26lc=8cb8b57e-9621-11f0-9764-a3daa84d9496%26p=96bb946a-cf2e-11f0-85e6-3dbde69b9a7d%26pt=campaign%26t=1764673911%26s=97c77d3da06bfdaaffb85815f809134c2ac3f94eb1cd427d8af331e266fabfbc/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/bONxTfQDsQKmaqm9zuTfEPm5stLlopge9vnzA-eks6U=433\" target=\"_blank\" rel=\"noreferrer\"><span>View Online</span></a></span> <br /> </span></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"text-align: center\"><span style=\"--darkreader-inline-color: #3db3ff; color: rgb(51, 175, 255) !important; font-size: 30px\">T</span><span style=\"font-size: 30px\"><span style=\"color: rgb(232, 192, 96) !important; --darkreader-inline-color: #e8c163; font-size: 30px\">L</span><span style=\"color: rgb(101, 195, 173) !important; --darkreader-inline-color: #6ec7b2; font-size: 30px\">D</span></span><span style=\"--darkreader-inline-color: #dd6e6e; color: rgb(220, 107, 107) !important; font-size: 30px\">R</span> <br /> </td></tr></tbody></table>  <br />  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"center\" height=\"20\" style=\"vertical-align: middle !important\" valign=\"middle\" width=\"100%\"><strong style=\"vertical-align: middle !important; height: 100%\">Together With </strong> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetronome.com%2Fwebinars%2Fmonetization-operating-model-webinar-series%3Futm_campaign=monetization-wp%26utm_medium=newsletter%26utm_source=tldr-product%26utm_content=/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/VX1QsgANt-L21tmhddLAoTs2EMqbCPN4HISGeMNCDxY=433\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://images.tldr.tech/metronome2.png\" valign=\"middle\" style=\"vertical-align: middle !important; height: 100%\" alt=\"Metronome\" /></a></td></tr></tbody></table>   <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\"> <div style=\"text-align: center\">  <h1><strong>TLDR Product Management <span>2025-12-02</span></strong></h1> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetronome.com%2Fwebinars%2Fmonetization-operating-model-webinar-series%3Futm_campaign=monetization-wp%26utm_medium=newsletter%26utm_source=tldr-product%26utm_content=/2/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/2f3gjBvoD7bVGzAc_B0UneD0uDQ1X5h84FNrAA7xKKM=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>The playbook for AI monetization is still WIP. Lovable's Head of Growth shares what's working (Sponsor)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     💺 Seat-based models break down when AI does the work.<p></p><p>💸 Usage-based pricing leaves money on the table.</p><p>🤔 Outcomes-based pricing? Most teams aren't sure where to start.</p><p>No one has AI pricing completely figured out. But <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetronome.com%2Fwebinars%2Fmonetization-operating-model-webinar-series%3Futm_campaign=monetization-wp%26utm_medium=newsletter%26utm_source=tldr-product%26utm_content=/3/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/lsyYdwPeuIBf0X7d-Uah9w630FfyAst2RWm4TFgLGLc=433\" rel=\"noreferrer\" target=\"_blank\"><span>Lovable learned a few lessons</span></a> on their road to $200MN ARR...</p>  <p>👉 Lovable's Head of Growth, Elena Verna, will be sharing some these lessons in <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetronome.com%2Fwebinars%2Fmonetization-operating-model-webinar-series%3Futm_campaign=monetization-wp%26utm_medium=newsletter%26utm_source=tldr-product%26utm_content=/4/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/I6gXi_vHsLWnzlEE0wU3ixRIf4m_TzkafU9V0g332Gc=433\" rel=\"noreferrer\" target=\"_blank\"><span>next's week free Metronome webinar.</span></a></p>  <p>Learn how to navigate the tension between self-serve and enterprise monetization, and how to start thinking about outcome-based value. <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetronome.com%2Fwebinars%2Fmonetization-operating-model-webinar-series%3Futm_campaign=monetization-wp%26utm_medium=newsletter%26utm_source=tldr-product%26utm_content=/5/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/QnRCQS-CLugv6jS3hqbuV31U98JMOnFlXPYT-lwOE6s=433\" rel=\"noreferrer\" target=\"_blank\"><span>Register for the webinar →</span></a>   </p> </span></span></div> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr> <tr><td>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 0px\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">📱</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>News &amp; Trends</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.a16z.news%2Fp%2Fcharts-of-the-week-narrative-violation%3Futm_source=tldrproduct/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/zVTYgI9isvvqqUle9QKCkgqISZyRKyOdKjP3ggZF3wA=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Charts of the Week: Code Gen Strikes Back (7 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Most AI narratives break down under closer data: code-gen tools are rebounding, job trends are mixed, early valuations don't predict much, and data centers are scaling at extraordinary speed. The real lesson is not to overreact to noisy data or tidy storylines.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstratechery.com%2F2025%2Fgoogle-nvidia-and-openai%2F%3Futm_source=tldrproduct/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/7vD5J9b1jIYlueBA3f2RhzfSw7yor4aekjTq4tE5bJc=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Google, Nvidia, and OpenAI (14 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Google's TPU push threatens Nvidia's dominance, and Google's consumer reach challenges OpenAI, though ChatGPT's large user base still gives it a strong moat. OpenAI must adopt advertising to protect that position and compete with Google's scale.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🚀</span></div> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Opinions &amp; Tutorials</strong></h1> </div> </div> </td></tr></tbody></table>   <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.svpg.com%2Fstakeholders-and-the-product-model%3Futm_source=tldrproduct/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/92s4r7UWn8YhAgPH1XQYCrRmLtWom7cGH4AjI5MjaFc=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Stakeholders and the Product Model (6 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Stakeholders support product teams by sharing context, framing clear problems, and enabling access to customers and data. This lets teams quickly test and deliver solutions that drive real business results while also managing essential operational tasks.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenn.substack.com%2Fp%2F9-9-6-0%3Futm_source=tldrproduct/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/xW7TpRoQYKroHX-PvT_pznid20Z1g62rkCMkX7SAlcQ=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>9-9-6-0 (10 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     The AI boom may be a bubble fueled by extreme valuations and harsh work culture, where many workers risk years of effort for equity that could end up worthless. The real takeaway is to value people now, because when the frenzy ends, the memories we make with each other matter more than the outcomes.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🧑‍💻</span></div> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Resources &amp; Tools</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmiro.com%2Fresources%2Fai-prototyping-landscape-guide%2F%3Futm_campaign=glb-26q4-nsp-wp-c3_o2-prototypes_product_guide%26utm_source=tldr%26utm_medium=paidmedia%26utm_content=sponsorship%26src=-tldr_glb/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/njE1Z7NSOvedIOIM8-sCvTCQTO2WPdo9D4zH9tsDgmo=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Before you pick an AI prototyping tool, answer one question (Sponsor)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Most teams ask &quot;Which tool is best?&quot; The better question: &quot;Which approach gets us from idea to collaboration fastest?&quot; <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmiro.com%2Fresources%2Fai-prototyping-landscape-guide%2F%3Futm_campaign=glb-26q4-nsp-wp-c3_o2-prototypes_product_guide%26utm_source=tldr%26utm_medium=paidmedia%26utm_content=sponsorship%26src=-tldr_glb/2/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/FQ5iO_17jfgGGsfVFJHeclip5LQ1KwBR0X91SiBOp8o=433\" rel=\"noreferrer\" target=\"_blank\"><span>Miro's AI prototyping landscape guide</span></a> provides a requirement question set, decision framework, and evaluation scorecard so you can pick the tools that actually match your needs. <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmiro.com%2Fresources%2Fai-prototyping-landscape-guide%2F%3Futm_campaign=glb-26q4-nsp-wp-c3_o2-prototypes_product_guide%26utm_source=tldr%26utm_medium=paidmedia%26utm_content=sponsorship%26src=-tldr_glb/3/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/EpnhhPvXGpmQ3xSy7HJ3xgPwBQ54pZlgFE-iZlqewo8=433\" rel=\"noreferrer\" target=\"_blank\"><span>Get the guide</span></a> </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.productmarketfit.tech%2Fp%2Fhow-deepsky-got-acquired-by-airtable%3Futm_source=tldrproduct/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/74EGK6ID5XE8V8_dMXwtCOleobgoIvBewCxkKngB7Y0=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>How Deepsky got acquired by Airtable just 4 months out of stealth (12 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Airtable acquired DeepSky because it could turn unclear problems into structured, trustworthy decisions, something most AI tools skip. Its reasoning system and polished outputs paired naturally with Airtable's workflow tools, completing the path from problem to action.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lennysnewsletter.com%2Fp%2Fthis-week-on-how-i-ai-pms-who-use%3Futm_source=tldrproduct/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/EpqE4E4WOpm6g3Z0n28Zf4VX_f-iRrZ4QIHx3y1gOGA=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>PMs who use AI will replace those who don't (40 minute video)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Marily Nika's workflow chains AI tools together to turn ideas into full product visions in minutes, using structured debates and rapid &quot;tool hopping&quot; to find better answers. Her message is simple: PMs who master AI will far outperform those who do not.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">🎁</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><strong><h1>Miscellaneous</h1></strong></div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fshreyasdoshi.substack.com%2Fp%2Funderstanding-micromanagement%3Fr=3k9fu%26utm_source=tldrproduct/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/JD_5O2ZJHryfQdPHxikF37JDEd1aIdOa76DLWz2H2FI=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Understanding Micromanagement (4 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Micromanagement can be useful when a project is complex or demands strong taste, but it is harmful when driven by insecurity. The key is to communicate clearly so your involvement feels intentional and supportive, not controlling.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mostlymetrics.com%2Fp%2Fthe-awkward-exit-150m-to-300m-acquisitions%3Futm_source=tldrproduct/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/LexVOB7Uj0YJeXbcJOVoftumuDfdwTCM5-NgUPFHbL8=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>The Awkward Exit: $150M to $300M Acquisitions (10 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Mid-range exits often feel great for founders but disappointing for later investors, creating tension when a company gets acquisition offers in the $150–$300M range. The best defense is smart fundraising, early secondary liquidity, and clear alignment with investors from the start.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>    <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\"><span style=\"font-size: 36px\">⚡</span></div></div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding-top: 0px; padding-bottom: 0px\"> <div> <div style=\"text-align: center\">  <h1><strong>Quick Links</strong></h1> </div> </div> </td></tr></tbody></table>  <table style=\"table-layout: fixed; width: 100%\" width=\"100%\"><tbody><tr><td style=\"padding: 0; border-collapse: collapse; border-spacing: 0; margin: 0\" valign=\"top\">  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flearn.lokalise.com%2FeBook-design-stage-localization.html%3Futm_source=tldr%26utm_medium=content_syndication%26utm_content=design-stage-ebook%26utm_campaign_id=link-1/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/s6xIgj00GkGYB-LZM7W3VjdxGSBxKnsBW9OPqb6jXgc=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>AI-powered design-stage localization guide by Lokalise (Sponsor)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Discover the essentials to accelerate design-stage localization with AI, from automated quality scoring that flags design issues early to intelligent routing that delivers 90% publish-ready translations. <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flearn.lokalise.com%2FeBook-design-stage-localization.html%3Futm_source=tldr%26utm_medium=content_syndication%26utm_content=design-stage-ebook%26utm_campaign_id=link-1/2/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/4lRQiQsrSsS1izurC1nKBpJhmoiGgX3t5aygswxtNa8=433\" rel=\"noreferrer\" target=\"_blank\"><span>Get the guide (PDF)</span></a> </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.news.aakashg.com%2Fp%2Fnano-banana-pro-prompts%3Futm_source=tldrproduct/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/HzwE_yq3GWqiwYRGWT_awO_gTwZwhTJFGdg5N0Q4PEY=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>Steal My Top 5 Nano Banana Pro Prompts (7 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     Knowing how to use AI tools is now a real career advantage.                                 </span> </span> </div> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <span>                                 <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1994900315292979426.html%3Futm_source=tldrproduct/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/tnVx5HYsAI0XGRJu0WaRjHgD4cTEYoSiS4vNDar4xq4=433\" target=\"_blank\" rel=\"noreferrer\">                                     <span>                                         <strong>AI Agents Will Unlock the Work Companies Could Never Afford (2 minute read)</strong>                                     </span> </a> <br /> <br /> <span style=\"font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, Verdana, sans-serif\">                                     AI agents make high-value but historically unaffordable work suddenly scalable.                                 </span> </span> </div> </td></tr></tbody></table>  </td></tr></tbody></table>     <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"left\" style=\"word-break: break-word; vertical-align: top; padding: 5px 10px\">  <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Love TLDR? Tell your friends and get rewards! </p> </td></tr> <tr><td style=\"padding: 0px 10px 15px\"> <div> Share your referral link below with friends to get free TLDR swag! </div> </td></tr> <tr><td align=\"left\" style=\"padding: 10px\"> <div> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frefer.tldr.tech%2Fc6d64c9f%2F9/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/XbaNrkASVJmwAX_mzvr0OhIlfvp8M-uomMPxZnD-yL8=433\" style=\"color: #464ba4; text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">https://refer.tldr.tech/c6d64c9f/9</a> </div> </td></tr> <tr></tr> <tr><td align=\"left\" style=\"padding: 5px 10px\"> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhub.sparklp.co%2Fsub_56d1326febb3%2F9/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/7d56YNxwh6MK7zm0tw2GeRIU64xlXHjFPH0nckMmTPw=433\" style=\"font-size: 16px; line-height: 1.6; padding: 10px 0; display: inline-block; text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\"><span style=\"mso-text-raise: 13pt; text-decoration: underline\">Track your referrals here.</span></a> </td></tr></tbody></table>     <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td align=\"left\" style=\"word-break: break-word; vertical-align: top; padding: 5px 10px\">  <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Want to advertise in TLDR? 📰 </p> <div style=\"margin-top: 10px\"> If your company is interested in reaching an audience of product management professionals and decision makers, you may want to <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrproduct%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/8kPFOKd3BEh42he5cOdBWXdIoBfEr1mb63CSv_hDZmI=433\" target=\"_blank\" rel=\"noreferrer\"><strong><span>advertise with us</span></strong></a>. </div> <br />    <p style=\"padding: 0; margin: 0; font-size: 22px; color: #000000; line-height: 1.6; font-weight: bold\"> Want to work at TLDR? 💼 </p> <div style=\"margin-top: 10px\"> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/5YB-qPOEnB7-K392DHojDiDHDQhBcsxThX6ZjtlS4pU=433\" rel=\"noreferrer\" style=\"color: #0000EE; text-decoration: underline\" target=\"_blank\"><strong>Apply here</strong></a> or send a friend's resume to <a href=\"mailto:jobs@tldr.tech\" style=\"color: #0000EE; text-decoration: underline\" onclick=\"return rcmail.command('compose','jobs@tldr.tech',this)\" rel=\"noreferrer\">jobs@tldr.tech</a> and get $1k if we hire them! </div> <br />  <div> If you have any comments or feedback, just respond to this email! <br /> <br /> Thanks for reading, <br /> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fin%2Fellenle%2F/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/h5xeRqYQQo14MwV8ZlPqWsx0u3D2JzfLDvqYvvEzH8Q=433\" target=\"_blank\" rel=\"noreferrer\"><span>Ellen Le</span></a> <br /> <br /> </div> <br /> </td></tr></tbody></table>  <table align=\"center\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\"><tbody><tr><td style=\"padding: 15px 15px\"> <div> <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fproduct%2Fmanage%3Femail=tldrai90%2540ino.to/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/b-epUnd5KV015LdPESXp_LO_j_-cxg98RG62GK5OWcw=433\" target=\"_blank\" rel=\"noreferrer\">Manage your subscriptions</a> to our other newsletters on tech, startups, and programming. Or if TLDR Product Management isn't for you, please <a href=\"https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fa.tldrnewsletter.com%2Funsubscribe%3Fep=1%26l=c8e26b12-3e94-11ed-9a32-0241b9615763%26lc=8cb8b57e-9621-11f0-9764-a3daa84d9496%26p=96bb946a-cf2e-11f0-85e6-3dbde69b9a7d%26pt=campaign%26pv=4%26spa=1764673304%26t=1764673911%26s=432d33230978939f4dc6620d86449d6d6af9e134388bb0c9ef1f29796eac1e5b/1/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/TGNmoYTHMAOdgwM4OupMO2MbGGxGZT3FOPwhj9CDzn4=433\" target=\"_blank\" rel=\"noreferrer\">unsubscribe</a>. <br /> </div> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table> </td></tr></tbody></table>   <img src=\"http://tracking.tldrnewsletter.com/CI0/0100019adec2fb79-0ed57be7-7579-41d0-b29e-83cd534db591-000000/W6qYOS_49wAX0A3qWjNjvUbnf23IwG8ZpraEvhscBnI=433\" style=\"display: none; width: 1px; height: 1px\" /> </div></div></div>",
      "summary": "    Most AI narratives break down under closer data: code-gen tools are rebounding, job trends are mixed, and early valuations don't predict much ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌  ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌                     Sign Up |Advertise|View Online     TLDR      Together With        TLDR Product Management 2025-12-02                                                                                                                     The playbook",
      "publishedAt": "2025-12-02T11:11:54.000Z",
      "author": "TLDR Product ",
      "source": "rss",
      "feedName": "TLDR",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 10.390082884996692,
      "ingestedAt": "2025-12-02T14:44:03.195Z",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "observability",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09912358",
      "title": "How To Lead | Ben Horowitz on My First Million",
      "url": "https://a16z.simplecast.com/episodes/how-to-lead-ben-horowitz-on-my-first-million-hZ9jj4__",
      "content": "<p>A16Z co-founder Ben Horowitz joins Shaan Puri and Sam Parr on My First Million to talk about how to be a great leader.</p><p> </p><p>Resources:</p><p>Follow Ben on X: https://x.com/bhorowitz</p><p>Follow Shaan on X: https://x.com/ShaanVP</p><p>Follow Sam on X: https://x.com/thesamparr</p><p> </p><p>Stay Updated:</p><p>If you enjoyed this episode, be sure to like, subscribe, and share with your friends!</p><p>Find a16z on X: <a href=\"https://x.com/a16z\">https://x.com/a16z</a></p><p>Find a16z on LinkedIn: https://www.linkedin.com/company/a16z</p><p>Listen to the a16z Podcast on Spotify: https://open.spotify.com/show/5bC65RDvs3oxnLyqqvkUYX</p><p>Listen to the a16z Podcast on Apple Podcasts: https://podcasts.apple.com/us/podcast/a16z-podcast/id842818711</p><p>Follow our host: https://x.com/eriktorenberg</p><p>Please note that the content here is for informational purposes only; should NOT be taken as legal, business, tax, or investment advice or be used to evaluate any investment or security; and is not directed at any investors or potential investors in any a16z fund. a16z and its affiliates may maintain investments in the companies discussed. For more details please see http://a16z.com/disclosures.</p> \n<p></p><p><strong>Stay Updated:</strong></p><p>Find a16z on <a href=\"https://x.com/a16z\">X</a></p><p>Find a16z on<a href=\"https://www.linkedin.com/company/a16z\"> LinkedIn</a></p><p>Listen to the a16z Podcast on <a href=\"https://open.spotify.com/show/5bC65RDvs3oxnLyqqvkUYX?si=3E8B3qT9TyiwAHJ7JnaKbg\">Spotify</a></p><p>Listen to the a16z Podcast on <a href=\"https://podcasts.apple.com/us/podcast/a16z-podcast/id842818711\">Apple Podcasts</a></p><p>Follow our host: https://twitter.com/eriktorenberg</p><p> </p><p>Please note that the content here is for informational purposes only; should NOT be taken as legal, business, tax, or investment advice or be used to evaluate any investment or security; and is not directed at any investors or potential investors in any a16z fund. a16z and its affiliates may maintain investments in the companies discussed. For more details please see a16z.com/disclosures.</p><br> <p>Hosted by Simplecast, an AdsWizz company. See <a href=\"https://pcm.adswizz.com\">pcm.adswizz.com</a> for information about our collection and use of personal data for advertising.</p>",
      "summary": "A16Z co-founder Ben Horowitz joins Shaan Puri and Sam Parr on My First Million to talk about how to be a great leader. Resources:Follow Ben on X: https://x.com/bhorowitzFollow Shaan on X: https://x.com/ShaanVPFollow Sam on X: https://x.com/thesamparr Stay Updated:If you enjoyed this episode, be sure to like, subscribe, and share with your friends!Find a16z on X: https://x.com/a16zFind a16z on LinkedIn: https://www.linkedin.com/company/a16zListen to the a16z Podcast on Spotify: https://open.spoti",
      "publishedAt": "2025-12-02T11:00:00.000Z",
      "author": "content+a16zpodcast@a16z.com (Shaan Puri, Sam Parr, Ben Horowitz)",
      "source": "rss",
      "feedName": "a16z Podcast",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 3.461317219900485,
      "ingestedAt": "2025-12-02T14:44:03.195Z",
      "tags": [
        "Tech Podcasts"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b0990d910",
      "title": "Code Smell 315 - Cloudflare Feature Explosion",
      "url": "https://dev.to/mcsee/code-smell-315-cloudflare-feature-explosion-28p9",
      "content": "<p><em>When bad configuration kills all internet proxies</em></p> \n \n<blockquote> \n<p>TL;DR: Overly large auto-generated config can crash your system.</p> \n</blockquote> \n \n<h1> \n   \n   \n  Problems 😔 \n</h1> \n \n<ul> \n<li>Config overload</li> \n<li> \n<a href=\"https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb\">Hardcoded</a> limit</li> \n<li>Lack of validations</li> \n<li>Crash on overflow</li> \n<li>Fragile <a href=\"https://dev.to/mcsee/coupling-the-one-and-only-software-design-problem-2pd7\">coupling</a> \n</li> \n<li>Cascading Failures</li> \n<li><a href=\"https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3\">Hidden Assumptions</a></li> \n<li>Silent duplication</li> \n<li>Unexpected Crashes</li> \n<li>Thread panics in critical paths</li> \n<li>Treating internal data as trusted input</li> \n<li>Poor observability</li> \n<li>Single point of failure in internet infrastructure</li> \n</ul> \n \n<h1> \n   \n   \n  Solutions 😃 \n</h1> \n \n<ol> \n<li>Validate inputs early</li> \n<li>Enforce soft limits</li> \n<li>Fail-fast on parse</li> \n<li>Monitor config diffs</li> \n<li>Version config safely</li> \n<li>Use backpressure mechanisms</li> \n<li>Degrade functionality gracefully</li> \n<li>Log and continue</li> \n<li>Improve degradation metrics</li> \n<li>Implement proper Result/Option handling with fallbacks</li> \n<li>Treat all configuration as untrusted input</li> \n</ol> \n \n<h1> \n   \n   \n  Refactorings ⚙️ \n</h1> \n \n \n<div> \n  <a href=\"https://dev.to/mcsee\"> \n    </a><div><a href=\"https://dev.to/mcsee\"> \n      <img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg\" alt=\"mcsee\"> \n    </a></div><a href=\"https://dev.to/mcsee\"> \n  </a> \n  <a href=\"https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc\"> \n    </a><div><a href=\"https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc\"> \n      </a><h2><a href=\"https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc\">Refactoring 004 - Remove Unhandled Exceptions</a></h2><a href=\"https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc\"> \n      </a><h3><a href=\"https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc\">Maxi Contieri ・ Feb 10 '22</a></h3><a href=\"https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc\"> \n      </a><div><a href=\"https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc\"> \n        <span>#programming</span> \n        <span>#exceptions</span> \n        <span>#oop</span> \n        <span>#cleancod</span> \n      </a></div><a href=\"https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc\"> \n    </a></div><a href=\"https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc\"> \n  </a> \n</div> \n \n \n \n<div> \n  <a href=\"https://dev.to/mcsee\"> \n    </a><div><a href=\"https://dev.to/mcsee\"> \n      <img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg\" alt=\"mcsee\"> \n    </a></div><a href=\"https://dev.to/mcsee\"> \n  </a> \n  <a href=\"https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51\"> \n    </a><div><a href=\"https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51\"> \n      </a><h2><a href=\"https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51\">Refactoring 024 - Replace Global Variables with Dependency Injection</a></h2><a href=\"https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51\"> \n      </a><h3><a href=\"https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51\">Maxi Contieri ・ Mar 9</a></h3><a href=\"https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51\"> \n      </a><div><a href=\"https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51\"> \n        <span>#webdev</span> \n        <span>#programming</span> \n        <span>#javascript</span> \n        <span>#beginners</span> \n      </a></div><a href=\"https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51\"> \n    </a></div><a href=\"https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51\"> \n  </a> \n</div> \n \n \n \n<div> \n  <a href=\"https://dev.to/mcsee\"> \n    </a><div><a href=\"https://dev.to/mcsee\"> \n      <img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg\" alt=\"mcsee\"> \n    </a></div><a href=\"https://dev.to/mcsee\"> \n  </a> \n  <a href=\"https://dev.to/mcsee/refactoring-035-separate-exception-types-179j\"> \n    </a><div><a href=\"https://dev.to/mcsee/refactoring-035-separate-exception-types-179j\"> \n      </a><h2><a href=\"https://dev.to/mcsee/refactoring-035-separate-exception-types-179j\">Refactoring 035 - Separate Exception Types</a></h2><a href=\"https://dev.to/mcsee/refactoring-035-separate-exception-types-179j\"> \n      </a><h3><a href=\"https://dev.to/mcsee/refactoring-035-separate-exception-types-179j\">Maxi Contieri ・ Oct 14</a></h3><a href=\"https://dev.to/mcsee/refactoring-035-separate-exception-types-179j\"> \n      </a><div><a href=\"https://dev.to/mcsee/refactoring-035-separate-exception-types-179j\"> \n        <span>#webdev</span> \n        <span>#programming</span> \n        <span>#ai</span> \n        <span>#beginners</span> \n      </a></div><a href=\"https://dev.to/mcsee/refactoring-035-separate-exception-types-179j\"> \n    </a></div><a href=\"https://dev.to/mcsee/refactoring-035-separate-exception-types-179j\"> \n  </a> \n</div> \n \n \n<h1> \n   \n   \n  Context 💬 \n</h1> \n \n<p>In the early hours of November 18, 2025, Cloudflare’s global network began failing to deliver core HTTP traffic, generating a <a href=\"https://mgx.dev/blog/cloudflare1119\">flood of 5xx errors</a> to end users.</p> \n \n<p>This was not caused by an external attack or security problem.</p> \n \n<p>The outage stemmed from an internal \"latent defect\" triggered by a <a href=\"https://techcrunch.com/2025/11/18/cloudflare-blames-massive-internet-outage-on-latent-bug/\">routine configuration change</a></p> \n \n<p>The failure fluctuated over time, until a fix was <a href=\"https://techcrunch.com/2025/11/18/cloudflare-blames-massive-internet-outage-on-latent-bug/\">fully deployed</a>.</p> \n \n<p>The root cause lay in a software bug in Cloudflare’s Bot Management module and its downstream proxy logic.</p> \n \n<p>The Technical Chain of Events</p> \n \n<ol> \n<li><p><strong>Database Change (11:05 UTC)</strong>: A ClickHouse permissions update made previously implicit table access explicit, allowing users to see metadata from both the <code>default</code> and <code>r0</code> databases.</p></li> \n<li><p><strong>SQL Query Assumption</strong>: A Bot Management query lacked a database name filter:<br> \n</p></li> \n</ol> \n \n<div> \n<pre><code>   <span>SELECT</span> <span>name</span><span>,</span> <span>type</span> <span>FROM</span> <span>system</span><span>.</span><span>columns</span> \n   <span>WHERE</span> <span>table</span> <span>=</span> <span>'http_requests_features'</span> \n   <span>ORDER</span> <span>BY</span> <span>name</span><span>;</span> \n</code></pre> \n \n</div> \n \n \n<p>This query began returning duplicate rows—once for <code>default</code> database, once for <code>r0</code> database.</p> \n \n<ol> \n<li><p><strong>Feature File Explosion</strong>: The machine learning feature file doubled from ~60 features to over 200 features with duplicate entries.</p></li> \n<li><p><strong>Hard Limit Exceeded</strong>: The Bot Management module had a hard-coded limit of 200 features (for memory pre-allocation), which was now exceeded.</p></li> \n<li><p><strong>The Fatal .unwrap()</strong>: The Rust code called <code>.unwrap()</code> on a Result that was now returning an error, causing the thread to panic with \"called Result::unwrap() on an Err value\". <em>see code below</em></p></li> \n<li><p><strong>Global Cascade</strong>: This panic propagated across all 330+ data centers globally, bringing down core CDN services, Workers KV, Cloudflare Access, Turnstile, and the dashboard.</p></li> \n</ol> \n \n<p>The estimated financial impact across affected businesses ranges from $180-360 million.</p> \n<h1> \n   \n   \n  Sample Code 📖 \n</h1> \n<h2> \n   \n   \n  Wrong ❌ \n</h2> \n \n \n \n \n<div> \n<pre><code><span>let</span> <span>features</span><span>:</span> <span>Vec</span><span>&lt;</span><span>Feature</span><span>&gt;</span> <span>=</span> <span>load_features_from_db</span><span>();</span> \n<span>let</span> <span>max</span> <span>=</span> <span>200</span><span>;</span> \n<span>assert!</span><span>(</span><span>features</span><span>.len</span><span>()</span> <span>&lt;=</span> <span>max</span><span>);</span> \n<span># This magic number assumption </span> \n<span># is actually wrong                              </span> \n \n<span>for</span> <span>f</span> <span>in</span> <span>features</span> <span>{</span> \n    <span>proxy</span><span>.add_bot_feature</span><span>(</span><span>f</span><span>.unwrap</span><span>());</span> \n    <span># You also call unwrap() on every feature. </span> \n    <span># If the database returns an invalid entry </span> \n    <span># or a parsing error,</span> \n    <span># you trigger another panic. </span> \n    <span># You give your runtime no chance to recover. </span> \n    <span># You force a crash on a single bad element.</span> \n<span>}</span> \n \n<span># A quiet config expansion turns into</span> \n<span># a full service outage </span> \n<span># because you trust input that you should validate </span> \n<span># and you use failure primitives (assert!, unwrap()) </span> \n<span># that kills your program </span> \n<span># instead of guiding it to safety</span> \n</code></pre> \n \n</div> \n \n<h2> \n   \n   \n  Right 👉 \n</h2> \n \n \n \n \n<div> \n<pre><code><span>fn</span> <span>load_and_validate</span><span>(</span><span>max</span><span>:</span> <span>usize</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>Feature</span><span>&gt;</span><span>,</span> <span>String</span><span>&gt;</span> <span>{</span> \n    <span>let</span> <span>raw</span><span>:</span> <span>Vec</span><span>&lt;</span><span>Result</span><span>&lt;</span><span>Feature</span><span>,</span> <span>Error</span><span>&gt;&gt;</span> <span>=</span> <span>load_features_from_db</span><span>();</span> \n \n    <span>if</span> <span>raw</span><span>.len</span><span>()</span> <span>&gt;</span> <span>max</span> <span>{</span> \n        <span>return</span> <span>Err</span><span>(</span><span>format!</span><span>(</span> \n            <span>\"too many features: {} &gt; {}\"</span><span>,</span>  \n            <span>raw</span><span>.len</span><span>(),</span> <span>max</span> \n        <span>));</span> \n    <span>}</span> \n \n    <span>Ok</span><span>(</span><span>raw</span><span>.into_iter</span><span>()</span> \n        <span>.filter_map</span><span>(|</span><span>r</span><span>|</span> <span>r</span><span>.ok</span><span>())</span> \n        <span>.collect</span><span>())</span> \n<span>}</span> \n</code></pre> \n \n</div> \n \n<h1> \n   \n   \n  Detection 🔍 \n</h1> \n \n<p>You can detect this code smell by searching your codebase for specific keywords:</p> \n \n<ul> \n<li> \n<code>.unwrap()</code> - Any direct call to this method</li> \n<li> \n<code>.expect()</code> - Similarly dangerous</li> \n<li> \n<code>panic!()</code> - Explicit panics in non-test code</li> \n<li> \n<code>thread::panic_any()</code> - Panic without context</li> \n</ul> \n \n<p>When you find these patterns, ask yourself: \"What happens to my system when this Result contains an Err?\" If your honest answer is \"the thread crashes and the request fails,\" then you've found the smell.</p> \n \n<p>You can also use automated linters. Most Rust style guides recommend tools like <code>clippy</code>, which flags <code>unwrap()</code> usage in production code paths. </p> \n \n<p>When you configure <code>clippy</code> with the <code>#![deny(unwrap_in_result)]</code> attribute, you prevent new <code>unwrap()</code> calls from entering your codebase.</p> \n<h1> \n   \n   \n  Tags 🏷️ \n</h1> \n \n<ul> \n<li>Fail-Fast</li> \n</ul> \n<h1> \n   \n   \n  Level 🔋 \n</h1> \n \n<p>[x] Advanced</p> \n<h1> \n   \n   \n  Why the Bijection Is Important 🗺️ \n</h1> \n \n<p>Your internal config generator must map exactly what your code expects. </p> \n \n<p>A mismatched config (e.g., duplicated metadata) breaks the <a href=\"https://dev.to/mcsee/the-one-and-only-software-design-principle-3086\">bijection</a> between what your config represents and what your proxy code handles. </p> \n \n<p>When you assume \"this file will always have ≤200 entries\", you break that <a href=\"https://dev.to/mcsee/what-is-wrong-with-software-5pa\">mapping</a>.</p> \n \n<p>Reality sends 400 entries → your model explodes → the real world wins, your service loses.</p> \n \n<p>That mismatch causes subtle failures that cascade, especially when you ignore validation or size constraints.</p> \n \n<p>Ensuring a clean mapping between the config source and code input helps prevent crashes and unpredictable behavior.</p> \n<h1> \n   \n   \n  AI Generation 🤖 \n</h1> \n \n<p>AI generators often prioritize \"correct\" logic over \"resilient\" logic. </p> \n \n<p>If you ask an AI to \"ensure the list is never larger than 200 items,\" it might generate an assertion or a panic because that is the most direct way to satisfy the requirement, introducing this smell.</p> \n \n<p>The irony: Memory-safe languages like Rust prevent undefined behavior and memory corruption, but they can't prevent logic errors, poor error handling, or architectural assumptions. <strong>Memory safety ≠ System safety.</strong></p> \n<h1> \n   \n   \n  AI Detection 🧲 \n</h1> \n \n<p>AI can easily detect this if you instruct it to look for availability risks. </p> \n \n<p>You can use linters combined with AI to flag panic calls in production code.</p> \n \n<p>Human review on critical functions is more important than ever.</p> \n<h2> \n   \n   \n  Try Them! 🛠 \n</h2> \n \n<p><em>Remember: AI Assistants make lots of mistakes</em></p> \n \n<blockquote> \n<p>Suggested Prompt: remove all .unwrap() and .expect() calls. Return Result instead and validate the vector bounds explicitly</p> \n</blockquote> \n \n<div><table> \n<thead> \n<tr> \n<th>Without Proper Instructions</th> \n<th>With Specific Instructions</th> \n</tr> \n</thead> \n<tbody> \n<tr> \n<td><a href=\"https://chat.openai.com/?q=Correct+and+explain+this+code%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60\">ChatGPT</a></td> \n<td><a href=\"https://chat.openai.com/?q=remove+all+.unwrap%28%29+and+.expect%28%29+calls.+Return+Result+instead+and+validate+the+vector+bounds+explicitly%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60\">ChatGPT</a></td> \n</tr> \n<tr> \n<td><a href=\"https://claude.ai/new?q=Correct+and+explain+this+code%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60\">Claude</a></td> \n<td><a href=\"https://claude.ai/new?q=remove+all+.unwrap%28%29+and+.expect%28%29+calls.+Return+Result+instead+and+validate+the+vector+bounds+explicitly%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60\">Claude</a></td> \n</tr> \n<tr> \n<td><a href=\"https://www.perplexity.ai/?q=Correct+and+explain+this+code%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60\">Perplexity</a></td> \n<td><a href=\"https://www.perplexity.ai/?q=remove+all+.unwrap%28%29+and+.expect%28%29+calls.+Return+Result+instead+and+validate+the+vector+bounds+explicitly%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60\">Perplexity</a></td> \n</tr> \n<tr> \n<td><a href=\"https://www.bing.com/chat?showconv=1&amp;sendquery=1&amp;q=Correct+and+explain+this+code%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60\">Copilot</a></td> \n<td><a href=\"https://www.bing.com/chat?showconv=1&amp;sendquery=1&amp;q=remove+all+.unwrap%28%29+and+.expect%28%29+calls.+Return+Result+instead+and+validate+the+vector+bounds+explicitly%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60\">Copilot</a></td> \n</tr> \n<tr> \n<td><a href=\"https://you.com/search?q=Correct+and+explain+this+code%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60\">You</a></td> \n<td><a href=\"https://you.com/search?q=remove+all+.unwrap%28%29+and+.expect%28%29+calls.+Return+Result+instead+and+validate+the+vector+bounds+explicitly%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60\">You</a></td> \n</tr> \n<tr> \n<td><a href=\"https://gemini.google.com/\">Gemini</a></td> \n<td><a href=\"https://gemini.google.com/\">Gemini</a></td> \n</tr> \n<tr> \n<td><a href=\"https://chat.deepseek.com/\">DeepSeek</a></td> \n<td><a href=\"https://chat.deepseek.com/\">DeepSeek</a></td> \n</tr> \n<tr> \n<td><a href=\"https://www.meta.ai/chat\">Meta AI</a></td> \n<td><a href=\"https://www.meta.ai/\">Meta AI</a></td> \n</tr> \n<tr> \n<td><a href=\"https://grok.com/\">Grok</a></td> \n<td><a href=\"https://grok.com/\">Grok</a></td> \n</tr> \n<tr> \n<td><a href=\"https://chat.qwen.ai/\">Qwen</a></td> \n<td><a href=\"https://chat.qwen.ai/\">Qwen</a></td> \n</tr> \n</tbody> \n</table></div> \n<h1> \n   \n   \n  Conclusion 🏁 \n</h1> \n \n<p>Auto-generated config can hide duplication or grow unexpectedly.</p> \n \n<p>If your code assumes size limits or blindly trusts its input, you risk a catastrophic crash.</p> \n \n<p>Validating inputs is good; crashing because an input is slightly off is a disproportionate response that turns a minor defect into a global outage.</p> \n \n<p>Validate config, enforce limits, handle failures, and avoid assumptions.</p> \n \n<p>That’s how you keep your system stable and fault-tolerant.</p> \n<h1> \n   \n   \n  Relations 👩‍❤️‍💋‍👨 \n</h1> \n \n \n<div> \n  <a href=\"https://dev.to/mcsee\"> \n    </a><div><a href=\"https://dev.to/mcsee\"> \n      <img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg\" alt=\"mcsee\"> \n    </a></div><a href=\"https://dev.to/mcsee\"> \n  </a> \n  <a href=\"https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55\"> \n    </a><div><a href=\"https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55\"> \n      </a><h2><a href=\"https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55\">Code Smell 122 - Primitive Obsession</a></h2><a href=\"https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55\"> \n      </a><h3><a href=\"https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55\">Maxi Contieri ・ Mar 17 '22</a></h3><a href=\"https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55\"> \n      </a><div><a href=\"https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55\"> \n        <span>#oop</span> \n        <span>#webdev</span> \n        <span>#tutorial</span> \n        <span>#beginners</span> \n      </a></div><a href=\"https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55\"> \n    </a></div><a href=\"https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55\"> \n  </a> \n</div> \n \n \n \n \n<div> \n  <a href=\"https://dev.to/mcsee\"> \n    </a><div><a href=\"https://dev.to/mcsee\"> \n      <img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg\" alt=\"mcsee\"> \n    </a></div><a href=\"https://dev.to/mcsee\"> \n  </a> \n  <a href=\"https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb\"> \n    </a><div><a href=\"https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb\"> \n      </a><h2><a href=\"https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb\">Code Smell 02 - Constants and Magic Numbers</a></h2><a href=\"https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb\"> \n      </a><h3><a href=\"https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb\">Maxi Contieri ・ Oct 21 '20</a></h3><a href=\"https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb\"> \n      </a><div><a href=\"https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb\"> \n        <span>#beginners</span> \n        <span>#codenewbie</span> \n        <span>#100daysofcode</span> \n        <span>#codequality</span> \n      </a></div><a href=\"https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb\"> \n    </a></div><a href=\"https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb\"> \n  </a> \n</div> \n \n \n \n<div> \n  <a href=\"https://dev.to/mcsee\"> \n    </a><div><a href=\"https://dev.to/mcsee\"> \n      <img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg\" alt=\"mcsee\"> \n    </a></div><a href=\"https://dev.to/mcsee\"> \n  </a> \n  <a href=\"https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3\"> \n    </a><div><a href=\"https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3\"> \n      </a><h2><a href=\"https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3\">Code Smell 198 - Hidden Assumptions</a></h2><a href=\"https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3\"> \n      </a><h3><a href=\"https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3\">Maxi Contieri ・ Mar 2 '23</a></h3><a href=\"https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3\"> \n      </a><div><a href=\"https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3\"> \n        <span>#webdev</span> \n        <span>#beginners</span> \n        <span>#programming</span> \n        <span>#tutorial</span> \n      </a></div><a href=\"https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3\"> \n    </a></div><a href=\"https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3\"> \n  </a> \n</div> \n \n \n<h1> \n   \n   \n  More Information 📕 \n</h1> \n \n<p><a href=\"https://techcrunch.com/2025/11/18/cloudflare-blames-massive-internet-outage-on-latent-bug/\">Cloudflare Blog</a></p> \n \n<p><a href=\"https://www.cloudflarestatus.com/incidents/8gmgl950y3h\">Cloudflare Status</a></p> \n \n<p><a href=\"https://techcrunch.com/2025/11/18/cloudflare-blames-massive-internet-outage-on-latent-bug/\">TechCrunch Coverage</a></p> \n \n<p><a href=\"https://mgx.dev/blog/cloudflare1119\">MGX Deep Technical Analysis</a></p> \n \n<p><a href=\"https://hackaday.com/2025/11/20/how-one-uncaught-rust-exception-took-out-cloudflare/\">Hackaday: How One Uncaught Rust Exception Took Out Cloudflare</a></p> \n \n<p><a href=\"https://www.cnbc.com/2025/11/18/cloudflare-outage-briefly-takes-chatgpt-claude-services-offline.html\">CNBC: Financial Impact Analysis</a></p> \n \n<h1> \n   \n   \n  Disclaimer 📘 \n</h1> \n \n<p>Code Smells are my <a href=\"https://dev.to/mcsee/i-wrote-more-than-90-articles-on-2021-here-is-what-i-learned-1n3a\">opinion</a>.</p> \n \n \n \n \n<blockquote> \n<p>A good programmer is someone who always looks both ways before crossing a one-way street<br> \nDouglas Crockford</p> \n</blockquote> \n \n<p><em>Douglas Crockford</em></p> \n \n \n<div> \n  <a href=\"https://dev.to/mcsee\"> \n    </a><div><a href=\"https://dev.to/mcsee\"> \n      <img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg\" alt=\"mcsee\"> \n    </a></div><a href=\"https://dev.to/mcsee\"> \n  </a> \n  <a href=\"https://dev.to/mcsee/software-engineering-great-quotes-26ci\"> \n    </a><div><a href=\"https://dev.to/mcsee/software-engineering-great-quotes-26ci\"> \n      </a><h2><a href=\"https://dev.to/mcsee/software-engineering-great-quotes-26ci\">Software Engineering Great Quotes</a></h2><a href=\"https://dev.to/mcsee/software-engineering-great-quotes-26ci\"> \n      </a><h3><a href=\"https://dev.to/mcsee/software-engineering-great-quotes-26ci\">Maxi Contieri ・ Dec 28 '20</a></h3><a href=\"https://dev.to/mcsee/software-engineering-great-quotes-26ci\"> \n      </a><div><a href=\"https://dev.to/mcsee/software-engineering-great-quotes-26ci\"> \n        <span>#codenewbie</span> \n        <span>#programming</span> \n        <span>#quotes</span> \n        <span>#software</span> \n      </a></div><a href=\"https://dev.to/mcsee/software-engineering-great-quotes-26ci\"> \n    </a></div><a href=\"https://dev.to/mcsee/software-engineering-great-quotes-26ci\"> \n  </a> \n</div> \n \n \n \n \n \n<p>This article is part of the CodeSmell Series.</p> \n \n \n<div> \n  <a href=\"https://dev.to/mcsee\"> \n    </a><div><a href=\"https://dev.to/mcsee\"> \n      <img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg\" alt=\"mcsee\"> \n    </a></div><a href=\"https://dev.to/mcsee\"> \n  </a> \n  <a href=\"https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc\"> \n    </a><div><a href=\"https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc\"> \n      </a><h2><a href=\"https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc\">How to Find the Stinky parts of your Code</a></h2><a href=\"https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc\"> \n      </a><h3><a href=\"https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc\">Maxi Contieri ・ May 21 '21</a></h3><a href=\"https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc\"> \n      </a><div><a href=\"https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc\"> \n        <span>#codenewbie</span> \n        <span>#tutorial</span> \n        <span>#codequality</span> \n        <span>#beginners</span> \n      </a></div><a href=\"https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc\"> \n    </a></div><a href=\"https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc\"> \n  </a> \n</div>",
      "summary": "When bad configuration kills all internet proxies \n \n \nTL;DR: Overly large auto-generated config can crash your system. \n \n \n \n   \n   \n  Problems 😔 \n \n \n \nConfig overload \n \nHardcoded limit \nLack of validations \nCrash on overflow \nFragile coupling \n \nCascading Failures \nHidden Assumptions \nSilent duplication \nUnexpected Crashes \nThread panics in critical paths \nTreating internal data as trusted input \nPoor observability \nSingle point of failure in internet infrastructure \n \n \n \n   \n   \n  Soluti",
      "publishedAt": "2025-12-02T11:00:00.000Z",
      "author": "Maxi Contieri",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 10.87842553926526,
      "ingestedAt": "2025-12-02T14:44:03.196Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b0990d91c",
      "title": "Meet Bumblebee: Agentic AI Flagging Risky Merchants in Under 90 Seconds",
      "url": "https://dev.to/razorpaytech/meet-bumblebee-agentic-ai-flagging-risky-merchants-in-under-90-seconds-2nlf",
      "content": "<p><strong>contributors: <a href=\"https://dev.to/parin-k\">@parin-k</a>, <a href=\"https://dev.to/sumit12dec\">@sumit12dec</a></strong></p> \n \n<p>If you're familiar with a payments company, you know the drill. Risk agents manually review thousands of merchant websites every month, checking for red flags: sketchy privacy policies, misaligned pricing, questionable social media presence, suspicious domain registration patterns. </p> \n \n<p>At Razorpay, our risk operations team was conducting 10,000 to 12,000 manual website reviews monthly, each taking roughly four minutes of human attention. That's 700 to 800 human hours consumed every month, and the quality was inconsistent because different agents would interpret the same signals differently.</p> \n \n<p>The traditional approach to fraud detection involves throwing bodies at the problem or building rigid rule engines that break the moment fraudsters adapt their tactics. We needed something better, something that could scale with our transaction volume while actually getting smarter over time. </p> \n \n<p>That's why we built what we're calling <strong>Agentic Risk</strong>, a multi-agent AI system that automates merchant website evaluation from end to end while maintaining the nuanced judgment that used to require human expertise.</p> \n \n<p>Here's what makes this interesting: we didn't just replace humans with AI and call it done. We went through three distinct architectural iterations, each one teaching us hard lessons about what works and what doesn't when you're building AI agents for production fraud detection. </p> \n \n<p>The journey from our initial n8n prototype through an AI agent to our current multi-agent architecture reveals fundamental truths about building reliable AI systems at scale.</p> \n \n<p><iframe allowfullscreen=\"allowfullscreen\" width=\"710\" height=\"399\" src=\"https://www.inoreader.com/yt-embed/?v=819bPUF0of8\" referrerpolicy=\"strict-origin-when-cross-origin\" style=\"width:100%;aspect-ratio:16/9;height:auto;display:block;border:0;\"> \n</iframe> \n</p> \n \n<h2> \n   \n   \n  <strong>The Business Problem: When Manual Review Can't Keep Up</strong> \n</h2> \n \n<p>Let me paint the picture of what risk operations looked like before automation. When a new merchant signs up for Razorpay or when our fraud detection system flags an existing merchant, a case lands in our Risk Case Manager system. A human agent picks up that case and begins the investigation dance. </p> \n \n<p>This process takes four minutes when everything goes smoothly, but that's rarely the case. Websites are structured differently, policy pages are hidden in weird places, domain information services have different interfaces, and social media handles aren't always obvious. The worst part isn't the time; it's the inconsistency. One agent might flag a merchant for having a generic privacy policy while another agent considers the same policy acceptable. </p> \n \n<p>We were also paying thousands of dollars monthly for a third-party explicit content screening service, and it was generating about 50 alerts per month with less than 10% precision. Moreover, this service only caught one specific type of risk while ignoring dozens of other fraud indicators we cared about.</p> \n \n<p>The fundamental issue was that we had excellent observability tools, structured data systems, and experienced risk analysts, but the connective tissue between all these components was human labor. Scaling meant hiring more agents, which meant more inconsistency, higher cost, and no improvement in detection speed or accuracy.</p> \n \n<h2> \n   \n   \n  <strong>Phase 1: The n8n Prototype - When Visual Orchestration Hits Its Limits</strong> \n</h2> \n \n<p>We started with n8n, a visual workflow automation platform, to quickly prototype and validate our hypothesis. Within weeks, we had a working proof-of-concept integrating webhook ingestion, merchant metadata fetching, website content review via multimodal AI, domain lookups, GST enrichment, fraud metrics, and LLM-based risk analysis.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgqjfn2uho4pyhh2pn9we.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgqjfn2uho4pyhh2pn9we.png\" alt=\"bumblebee-n8n-workflow\" width=\"800\" height=\"47\"></a></p> \n \n<p>The prototype validated that automation was feasible and helped us identify the complete set of data points needed. However, n8n quickly revealed fundamental limitations: <strong>branch explosion</strong> (handling edge cases created unmaintainable 40-node workflows with duplicated logic), <strong>observability gaps</strong> (debugging failed nodes was painful with coarse logs), and <strong>platform instability</strong> (non-deterministic behavior in HTTP and merge operations). The n8n prototype taught us that production-grade risk automation would require a code-first approach with proper observability and the ability to use Python libraries directly.</p> \n \n<h2> \n   \n   \n  <strong>Phase 2: Python + ReAct Agent - Better Control, New Bottlenecks</strong> \n</h2> \n \n<p>We rebuilt as a Python web application with an API frontend and task workers. This immediately solved several Phase 1 problems: native Python libraries, structured logging with trace IDs, proper exception handling with retry logic, and complex NLP preprocessing capabilities.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F0g0sjs9puoe27ldedilz.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F0g0sjs9puoe27ldedilz.png\" alt=\"bumblebee-sequence-diagram\" width=\"800\" height=\"733\"></a></p> \n \n<p>The core was a single ReAct-style agent that iteratively reasoned about which tools to call, executed them, and incorporated results until producing a structured risk assessment. Phase 2 brought full observability, easy tool addition, and dynamic behavior that replaced brittle conditional logic.</p> \n \n<p>However, new bottlenecks emerged. <strong>Token bloat</strong> became critical as the agent accumulated 50KB+ of HTML content, domain data, and fraud metrics in its context window, regularly hitting token limits. <strong>Sequential execution</strong> meant tool invocations happened one after another even when they had no dependencies, scaling linearly with tool count. <strong>Temperature conflation</strong> forced a compromise setting that was suboptimal for both exploration (tool selection) and exploitation (final scoring). Phase 2 proved agentic orchestration was right, but single-agent architecture couldn't scale to thousands of concurrent evaluations.</p> \n \n<h2> \n   \n   \n  <strong>Phase 3: Multi-Agent Architecture - When Specialization Wins</strong> \n</h2> \n \n<p>The breakthrough came when we stopped treating fraud detection as a single AI task and started building a <strong>multi-agent collaboration system</strong>. Rather than one agent doing everything, we split responsibilities across specialized agents optimized for specific roles: Planner, Fetchers, and Analyzer.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F74bokg78giaw38ehj9ml.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F74bokg78giaw38ehj9ml.png\" alt=\"bumblebee-multiagent-arch\" width=\"800\" height=\"503\"></a></p> \n \n<p><strong>The Planner Agent</strong> receives the merchant case, examines available tools, checks system health and API quotas, and generates an execution plan. This isn't a rigid script; it's a structured specification of what information to gather, with priorities, timeouts, token budgets, and expected schemas. The Planner enforces business rules deterministically. Skip GST validation for non-Indian merchants. Deprioritize social media checks for B2B merchants where social presence matters less. This reduces unnecessary API calls and focuses resources on high-signal checks.</p> \n \n<p><strong>Data Fetcher Agents</strong> execute in parallel, each owning one data source or tool. Website scraping, WHOIS lookups, fraud database queries, social media metrics, pricing comparisons, policy verification. Here's the critical insight: fetchers don't just retrieve raw data. They perform <strong>local data pruning</strong> before returning results.</p> \n \n<p>The website content reviewer doesn't send back 50KB of HTML. It extracts only relevant sections: privacy policies, contact information, pricing tables, product descriptions. Using keyword matching or lightweight NLP models, it returns a compact JSON payload with structured snippets, confidence scores, and provenance links. This solves the token bloat problem. Instead of accumulating full raw outputs, the system maintains small, information-dense summaries.</p> \n \n<p>Each fetcher compresses its domain's data into a format optimized for downstream analysis. Fetchers also implement caching for data that doesn't change frequently. WHOIS information and domain reputation scores get cached with appropriate TTLs, reducing redundant external API calls and improving throughput during traffic spikes.</p> \n \n<p><strong>The Analyzer Agent</strong> consumes these structured payloads and produces the final risk assessment. It runs <strong>deterministic rules first</strong>: hard thresholds for fraud metrics, blacklist checks, compliance violations. These rules are fast, explainable, and don't require LLM inference.</p> \n \n<p>Only after deterministic rules does the Analyzer invoke the LLM for interpretive tasks: generating human-readable summaries, explaining why certain indicators triggered, identifying nuanced patterns that don't fit simple rules. Because fetchers already pruned and structured the data, the Analyzer's LLM calls work with minimal context, avoiding token limit issues entirely.</p> \n \n<p>Different agents use different temperature settings tuned for their roles. The Planner runs at medium temperature for flexible tool selection. The Analyzer uses very low temperature for deterministic risk scoring and higher temperature when generating business narratives where creative expression improves readability. This per-agent temperature control eliminates the compromises from Phase 2.</p> \n \n<p><strong>The execution model</strong> leverages Celery for orchestration. When a case arrives, the API enqueues a planning job. The Planner generates the execution plan and enqueues multiple fetcher jobs in parallel. As fetchers complete, their results stream into a shared state store. The Analyzer subscribes to fetcher completion events and begins processing as soon as enough data is available, not waiting for every fetcher if some are slow or failing.</p> \n \n<p>If a fetcher fails entirely (website unreachable, API rate-limiting), the Planner degrades gracefully. The Analyzer proceeds with available data and flags the missing information for manual review rather than blocking the entire evaluation. This resilience was impossible in Phase 2's sequential architecture.</p> \n \n<h2> \n   \n   \n  <strong>The Results: When Architecture Meets Reality</strong> \n</h2> \n \n<p>The shift to multi-agent architecture produced measurable improvements across every dimension. <strong>Token usage dropped 60%</strong> through fetcher-level pruning and elimination of full raw data in LLM context. <strong>End-to-end latency fell from 35 seconds to 8-12 seconds</strong> via parallel fetcher execution and focused LLM calls. <strong>Success rate rose from 88% to 99%+</strong>, measured as cases completing without token limits or LLM failures.</p> \n \n<p><strong>Cost per evaluation decreased</strong> despite adding sophisticated analysis. Smaller context windows meant cheaper LLM calls. Caching at the fetcher level reduced external API expenses. The system now handles thousands of concurrent evaluations without bottlenecking, scaling horizontally by adding task workers rather than vertically with bigger servers.</p> \n \n<p>The most important improvement is <strong>maintainability and extensibility</strong>. Adding a new risk signal requires writing a new fetcher agent with its pruning logic and output schema. The Planner automatically incorporates new tools once registered. The Analyzer adapts to new data sources without modification. This composability enables continuous fraud detection improvement by adding signals incrementally rather than requiring architectural rewrites.</p> \n \n<p>The multi-agent approach provides <strong>observability impossible in earlier phases</strong>. Each agent logs trace IDs, tokens consumed, latency, confidence scores, and reasoning. When a case produces unexpected results, we replay the exact sequence of fetcher outputs, examine what the Analyzer saw, and understand why it reached that conclusion. This audit trail is critical for debugging, regulatory compliance, and explaining decisions to merchants who dispute risk assessments.</p> \n \n<h2> \n   \n   \n  <strong>What We Learned: Principles for Building Production AI Agents</strong> \n</h2> \n \n<p>Our journey from n8n through ReAct to multi-agent orchestration taught us several lessons that apply broadly to anyone building AI systems for production use cases.</p> \n \n<p><strong>Start simple, evolve deliberately</strong>. N8n was the right choice for Phase 1 even though we knew it wouldn't scale. Rapid prototyping and stakeholder validation matter more than architectural purity in early stages. What's critical is recognizing when you've outgrown your current approach and having the discipline to rebuild rather than patch over fundamental limitations.</p> \n \n<p><strong>Token budgets are real constraints</strong>. Many blog posts about AI agents gloss over token management, but in production systems with large, messy real-world data, token limits are where architectures break. Design explicitly for token efficiency: prune early, prune often, and never pass raw, unstructured data to LLMs when you can send structured summaries instead.</p> \n \n<p><strong>Specialization beats generalization at scale</strong>. A single agent trying to handle planning, data fetching, and analysis will hit walls that can't be solved with better prompts or bigger models. Splitting responsibilities across specialized agents with clear interfaces between them produces systems that are faster, more reliable, and easier to understand.</p> \n \n<p><strong>Temperature is not a hyperparameter you tune once</strong>. Different tasks need different temperature settings, and trying to find a compromise temperature for a single agent produces mediocre results everywhere. Per-agent temperature control is a fundamental architectural requirement, not an optimization detail.</p> \n \n<p><strong>Parallelism matters more than model size</strong>. Running multiple smaller, focused agents in parallel often outperforms running one large agent sequentially, both in terms of latency and cost. This runs counter to the instinct to throw the biggest model available at every problem.</p> \n \n<p><strong>Observability is not optional</strong>. Without structured logging, trace IDs, and the ability to replay decision sequences, debugging production AI systems is nearly impossible. Invest in observability infrastructure early, ideally before you have a production incident that requires it.</p> \n \n<h2> \n   \n   \n  <strong>The Path Forward: Continuous Improvement by Design</strong> \n</h2> \n \n<p>What makes the multi-agent architecture particularly powerful is that it's designed for continuous improvement. As we accumulate more cases, we can identify patterns where the Analyzer produces low-confidence results or where human reviewers frequently override AI decisions. These cases become training data for improving fetcher pruning heuristics, refining Planner rules, and tuning Analyzer prompts.</p> \n \n<p>We're exploring several extensions. Fine-tuning small, specialized models for specific fetchers rather than relying entirely on general-purpose LLMs. This could further reduce cost and latency while improving accuracy for domain-specific tasks like policy compliance checking. Implementing feedback loops where human overrides automatically update Planner rules or Analyzer thresholds, creating a self-improving system that gets smarter as risk operators correct its mistakes.</p> \n \n<p>Another direction is adding <strong>predictive agents</strong> that don't just evaluate merchant risk at onboarding but continuously monitor for behavioral changes that might indicate fraud. Imagine fetchers running periodically in the background, detecting when a merchant's website content changes significantly, when pricing diverges from competitors, or when social media presence suddenly evaporates. The same multi-agent architecture that handles point-in-time evaluation can drive continuous risk monitoring with minimal modification.</p> \n \n<h2> \n   \n   \n  <strong>Why This Matters Beyond Fraud Detection</strong> \n</h2> \n \n<p>I've been talking about merchant risk evaluation specifically, but the architectural patterns we discovered apply broadly to any domain where AI agents need to process large amounts of heterogeneous data, make complex decisions, and produce explainable results. Financial services, healthcare, supply chain management, cybersecurity, and legal research all have similar characteristics: multiple data sources with different formats and latencies, domain expertise encoded in rules and models, and requirements for auditability and compliance.</p> \n \n<p>The lesson isn't \"use multi-agent architecture for everything.\" The lesson is that as AI systems scale from demos to production, the architecture that got you to the first prototype often becomes the main thing preventing you from scaling further. Having the discipline to recognize when you've hit architectural limits, the willingness to rebuild from first principles, and the engineering rigor to measure improvements objectively separates successful production AI from expensive science projects.</p> \n \n<p>At Razorpay, we've taken fraud detection from a manual, inconsistent process consuming 800 agent hours monthly to an automated system that evaluates merchants in seconds with higher accuracy and comprehensive audit trails. We've reduced our per-review time by 75%, improved detection consistency, and freed up risk operators to focus on genuinely complex cases that require human judgment. And we've done it with an architecture that gets better over time rather than more fragile.</p> \n \n<p>If you're building AI agents for production use cases, the technology is ready. The LLMs are capable, the orchestration frameworks exist, and the integration tools work. The hard part is designing systems that handle real-world messiness, scale with your business, and maintain reliability when things inevitably break. That's where architecture matters, and that's what we learned the hard way through three iterations of building Agentic Risk.</p> \n \n<p><em>editor: <a href=\"https://dev.to/paaarth96\">@paaarth96</a></em> </p>",
      "summary": "contributors: @parin-k, @sumit12dec \n \nIf you're familiar with a payments company, you know the drill. Risk agents manually review thousands of merchant websites every month, checking for red flags: sketchy privacy policies, misaligned pricing, questionable social media presence, suspicious domain registration patterns.  \n \nAt Razorpay, our risk operations team was conducting 10,000 to 12,000 manual website reviews monthly, each taking roughly four minutes of human attention. That's 700 to 800 h",
      "publishedAt": "2025-12-02T10:59:50.000Z",
      "author": "Ankur",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 18.789852409609576,
      "ingestedAt": "2025-12-02T14:44:03.196Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "observability",
        "governance",
        "Tech Articles",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b0990d91e",
      "title": "Copy and Paste on Proxmox VM",
      "url": "https://dev.to/woovi/copy-and-paste-on-proxmox-vm-15il",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frncah6jqm7ku0jwkba62.jpg\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>Copy and paste does not work well on Proxmox KVM on the UI.<br>  \nYou can try to connect using a serial port to enable copy and paste over the UI.</p>  \n<h2>  \n    \n    \n  Using a Script  \n</h2>  \n  \n<p>We created a script to simulate keyboard typing and paste data over KVM when using the UI.</p>  \n  \n<p>Open your Chrome Dev Tools, and paste this.<br>  \n</p>  \n  \n<div>  \n<pre><code><span>function</span> <span>simulateKeyEvent</span><span>(</span><span>el</span><span>,</span> <span>eventType</span><span>,</span> <span>key</span><span>,</span> <span>options</span> <span>=</span> <span>{})</span> <span>{</span>  \n  <span>const</span> <span>evt</span> <span>=</span> <span>new</span> <span>KeyboardEvent</span><span>(</span><span>eventType</span><span>,</span> <span>{</span> <span>key</span><span>,</span> <span>...</span><span>options</span> <span>});</span>  \n  <span>el</span><span>.</span><span>dispatchEvent</span><span>(</span><span>evt</span><span>);</span>  \n<span>}</span>  \n  \n<span>const</span> <span>sendKey</span> <span>=</span> <span>(</span><span>char</span><span>)</span> <span>=&gt;</span> <span>{</span>  \n  <span>let</span> <span>capsLockOn</span> <span>=</span> <span>false</span><span>;</span>  \n  <span>const</span> <span>SHIFT_NEEDED</span> <span>=</span> <span>/</span><span>[</span><span>A-Z!@#$%^&amp;*()_+{}:\"&lt;&gt;?~|</span><span>]</span><span>/</span><span>;</span>  \n  \n  <span>const</span> <span>canvas</span> <span>=</span> <span>document</span><span>.</span><span>querySelector</span><span>(</span><span>\"</span><span>canvas</span><span>\"</span><span>);</span>  \n  \n  <span>canvas</span><span>.</span><span>focus</span><span>();</span>  \n  \n  <span>if </span><span>(</span><span>char</span> <span>===</span> <span>'</span><span>\\n</span><span>'</span><span>)</span> <span>{</span>  \n    <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>\"</span><span>keydown</span><span>\"</span><span>,</span> <span>\"</span><span>Enter</span><span>\"</span><span>);</span>  \n    <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>\"</span><span>keyup</span><span>\"</span><span>,</span> <span>\"</span><span>Enter</span><span>\"</span><span>);</span>  \n  <span>}</span> <span>else</span> <span>{</span>  \n    <span>const</span> <span>needsShift</span> <span>=</span> <span>SHIFT_NEEDED</span><span>.</span><span>test</span><span>(</span><span>char</span><span>);</span>  \n    <span>const</span> <span>isUpperCase</span> <span>=</span> <span>char</span> <span>&gt;=</span> <span>'</span><span>A</span><span>'</span> <span>&amp;&amp;</span> <span>char</span> <span>&lt;=</span> <span>'</span><span>Z</span><span>'</span><span>;</span>  \n  \n    <span>if </span><span>(</span><span>needsShift</span><span>)</span> <span>{</span>  \n      <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>\"</span><span>keydown</span><span>\"</span><span>,</span> <span>\"</span><span>Shift</span><span>\"</span><span>,</span> <span>{</span> <span>keyCode</span><span>:</span> <span>16</span> <span>});</span>  \n    <span>}</span>  \n  \n    <span>if </span><span>(</span><span>isUpperCase</span> <span>&amp;&amp;</span> <span>capsLockOn</span><span>)</span> <span>{</span>  \n      <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>\"</span><span>keydown</span><span>\"</span><span>,</span> <span>char</span><span>.</span><span>toLowerCase</span><span>());</span>  \n      <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>\"</span><span>keyup</span><span>\"</span><span>,</span> <span>char</span><span>.</span><span>toLowerCase</span><span>());</span>  \n    <span>}</span> <span>else</span> <span>{</span>  \n      <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>\"</span><span>keydown</span><span>\"</span><span>,</span> <span>char</span><span>);</span>  \n      <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>\"</span><span>keyup</span><span>\"</span><span>,</span> <span>char</span><span>);</span>  \n    <span>}</span>  \n  \n    <span>if </span><span>(</span><span>needsShift</span><span>)</span> <span>{</span>  \n      <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>\"</span><span>keyup</span><span>\"</span><span>,</span> <span>\"</span><span>Shift</span><span>\"</span><span>,</span> <span>{</span> <span>keyCode</span><span>:</span> <span>16</span> <span>});</span>  \n    <span>}</span>  \n  \n    <span>if </span><span>(</span><span>char</span> <span>===</span> <span>\"</span><span>CapsLock</span><span>\"</span><span>)</span> <span>{</span>  \n      <span>capsLockOn</span> <span>=</span> <span>!</span><span>capsLockOn</span><span>;</span>  \n      <span>console</span><span>.</span><span>log</span><span>(</span><span>\"</span><span>Caps Lock state changed:</span><span>\"</span><span>,</span> <span>capsLockOn</span><span>);</span>  \n    <span>}</span>  \n  <span>}</span>  \n<span>};</span>  \n  \n<span>function</span> <span>cp</span><span>(</span><span>text</span><span>)</span> <span>{</span>  \n  <span>let</span> <span>index</span> <span>=</span> <span>0</span><span>;</span>  \n  \n  <span>function</span> <span>typeChar</span><span>()</span> <span>{</span>  \n    <span>if </span><span>(</span><span>index</span> <span>&lt;</span> <span>text</span><span>.</span><span>length</span><span>)</span> <span>{</span>  \n      <span>sendKey</span><span>(</span><span>text</span><span>[</span><span>index</span><span>]);</span>  \n      <span>index</span><span>++</span><span>;</span>  \n      <span>setTimeout</span><span>(</span><span>typeChar</span><span>,</span> <span>100</span><span>);</span> <span>// Adjust delay as needed</span>  \n    <span>}</span>  \n  <span>}</span>  \n  \n  <span>typeChar</span><span>();</span>  \n<span>}</span>  \n</code></pre>  \n  \n</div>  \n  \n  \n  \n<p>To paste something, you first need to focus on the UI, and then type <code>cp('mydata')</code></p>  \n  \n<p>You can use this when you don't have access to the VM over SSH.</p>  \n  \n<h2>  \n    \n    \n  In Summary  \n</h2>  \n  \n<p>If you are creative enough, you can solve these limitations.</p>  \n  \n  \n  \n  \n<p>Woovi<br>  \n<a href=\"https://www.woovi.com\">Woovi</a> is a fintech platform revolutionizing how businesses and developers handle payments in Brazil. Built with a developer-first mindset, Woovi simplifies integration with instant payment methods like Pix, enabling companies to receive payments seamlessly and automate financial workflows.</p>  \n  \n<p>If you want to work with us, we are <a href=\"https://woovi.com/jobs/\">hiring</a>!</p>",
      "summary": "Copy and paste does not work well on Proxmox KVM on the UI.  \nYou can try to connect using a serial port to enable copy and paste over the UI.  \n  \n    \n    \n  Using a Script  \n  \n  \nWe created a script to simulate keyboard typing and paste data over KVM when using the UI.  \n  \nOpen your Chrome Dev Tools, and paste this.  \n  \n  \n  \nfunction simulateKeyEvent(el, eventType, key, options = {}) {  \n  const evt = new KeyboardEvent(eventType, { key, ...options });  \n  el.dispatchEvent(evt);  \n}  \n  \nc",
      "publishedAt": "2025-12-02T10:52:48.000Z",
      "author": "Sibelius Seraphini",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.931567862353548,
      "ingestedAt": "2025-12-02T14:44:03.196Z",
      "tags": [
        "code_review",
        "retrieval",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b0990d922",
      "title": "“EU AI Act: The Code is the Compliance — Why TAUGuard is Already the Architecture We Needed”",
      "url": "https://dev.to/michal_harcej/eu-ai-act-the-code-is-the-compliance-why-tauguard-is-already-the-architecture-we-needed-5e5m",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjwkfqu5njw4kgyo3qcqj.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>The Misunderstanding That Reveals the Future</p>  \n  \n<blockquote>  \n<p>“They told us: ‘The EU AI Act is coming — better prepare.’<br><br>  \nBut what if some of us didn’t need to prepare?<br><br>  \nWhat if we <em>built for that world</em> before the ink dried on the legislation?”</p>  \n</blockquote>  \n  \n<p>Some see the upcoming regulation as another compliance burden.<br><br>  \nWe built TAUGuard — not as a response, but as the foundation.  </p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  The Regulatory Landscape Isn’t a Barrier — It’s a Signal  \n</h2>  \n  \n<p>The advent of regulation such as the EU AI Act, alongside standards like ISO 42001 and frameworks such as NIST RMF, signals a tectonic shift:  </p>  \n  \n<ul>  \n<li>From <strong>retrospective compliance</strong> (reports, audits) → to <strong>real-time assurance</strong>.  \n</li>  \n<li>From <strong>static documentation</strong> → to <strong>dynamic, executable compliance</strong>.  \n</li>  \n<li>From <strong>paper‑trail governance</strong> → to <strong>code-anchored governance</strong> — fresh, live, unforgeable.  \n</li>  \n</ul>  \n  \n<p>In other words: laws and standards no longer ask whether you did it — they ask whether you can prove it, in real time.</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  TAUGuard — Sovereignty-Native, Not Afterthought  \n</h2>  \n  \n<p>TAUGuard is not “yet to be built.” It exists. It runs. It enforces.  </p>  \n  \n<p>What TAU delivers:</p>  \n  \n<ul>  \n<li>  \n<strong>Sub‑100 ms anomaly detection</strong> — nervous‑system‑level latency for AI infra.  \n</li>  \n<li>  \n<strong>Blockchain-anchored audit trail</strong> — immutable, transparent memory of intent &amp; action.  \n</li>  \n<li>  \n<strong>Live alignment protocols + runtime controls</strong> — ensuring actions stay within allowed boundaries, preventing Loss of Control (LoC).  \n</li>  \n<li>  \n<strong>Permission, provenance, accountability baked in</strong> — co‑authorship boundaries, identity assurance, verified origin.  \n</li>  \n</ul>  \n  \n<p>This isn’t a compliance tool layered on after deployment.<br><br>  \nIt’s a sovereign stack built from day one to meet — and exceed — the demands of this new regulatory realm.</p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  Why Many Still Don’t Get It (a note to the “show me the demo/pitch deck” crowd)  \n</h2>  \n  \n<p>Because TAU doesn’t stare at you from a UI.<br><br>  \nIt pulses in the background of infrastructure.  </p>  \n  \n<p>Much like how the early internet existed — not as flashy websites — but as protocols, routers, invisible trust networks.<br><br>  \nAsk yourself: did you invest in TCP/IP when it was just 1s and 0s running through cables?<br><br>  \nNo — yet everything built on top of it changed the world.  </p>  \n  \n<p>TAUGuard isn’t about dashboards or sales decks.<br><br>  \nIt’s about embedding trust, memory, and control into the bloodstream of AI workflows.  </p>  \n  \n  \n  \n  \n<h2>  \n    \n    \n  From Vision to Reality — A Call to the Guardians of the Next Web  \n</h2>  \n  \n<p>We didn’t wait for the AI Act to codify trust.<br><br>  \nWe coded it.  </p>  \n  \n<p>TAUGuard isn’t a “soon-to-launch promise.”<br><br>  \nIt is the memory of truth inside an internet that forgot how to prove anything.  </p>  \n  \n<p>If you believe sovereignty over your infrastructure isn’t optional — but inevitable —<br><br>  \nIf you believe that real control must be traceable, immutable, and live —  </p>  \n  \n<p>Then TAUGuard isn’t optional.<br><br>  \nIt’s essential.  </p>  \n  \n<p>Because the future of AI won’t be a battle of models.<br><br>  \nIt will be a battle of <strong>trust stacks</strong>.<br><br>  \nAnd TAU is already standing — ready for the arms‑race.</p>  \n  \n  \n  \n  \n<p><em>— TAUGuard Core Team</em><br><br>  \n<em>“We don’t adapt to the AI Act. We embody it.”</em></p>",
      "summary": "The Misunderstanding That Reveals the Future  \n  \n  \n“They told us: ‘The EU AI Act is coming — better prepare.’  \nBut what if some of us didn’t need to prepare?  \nWhat if we built for that world before the ink dried on the legislation?”  \n  \n  \nSome see the upcoming regulation as another compliance burden.  \nWe built TAUGuard — not as a response, but as the foundation.    \n  \n  \n  \n  \n  \n    \n    \n  The Regulatory Landscape Isn’t a Barrier — It’s a Signal  \n  \n  \nThe advent of regulation such as",
      "publishedAt": "2025-12-02T10:46:07.000Z",
      "author": "Michal Harcej",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 15.812271432561495,
      "ingestedAt": "2025-12-02T14:44:03.196Z",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "ide",
        "governance",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b0990d923",
      "title": "Designing with Web Components: Custom Elements & Shadow DOM in HTML",
      "url": "https://dev.to/djamware_tutorial_eba1a61/designing-with-web-components-custom-elements-shadow-dom-in-html-1j45",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fae7cxvwsbnuyrdgalpdn.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>I’ve published a new tutorial on Djamware.com that dives deep into building native, framework-agnostic Web Components using modern browser standards.</p>  \n  \n<p>In this guide, you’ll learn:</p>  \n  \n<ul>  \n<li>What Web Components are and why they matter</li>  \n<li>How Custom Elements and Shadow DOM work</li>  \n<li>How to build components using templates and slots</li>  \n<li>How to pass data with attributes and properties</li>  \n<li>Best practices for performance, accessibility, and component APIs</li>  \n<li>How to build a real  component with encapsulated styles</li>  \n</ul>  \n  \n<p>👉 Read the full tutorial:<br>  \n<a href=\"https://www.djamware.com/post/692e739ecfbd910fde2b32a1/designing-with-web-components-custom-elements-shadow-dom-in-html\">https://www.djamware.com/post/692e739ecfbd910fde2b32a1/designing-with-web-components-custom-elements-shadow-dom-in-html</a></p>  \n  \n<p>If you're building reusability into your UI or creating cross-framework components, this tutorial is for you.</p>  \n  \n<p>Let me know what components you’d like me to build next!</p>",
      "summary": "I’ve published a new tutorial on Djamware.com that dives deep into building native, framework-agnostic Web Components using modern browser standards.  \n  \nIn this guide, you’ll learn:  \n  \n  \nWhat Web Components are and why they matter  \nHow Custom Elements and Shadow DOM work  \nHow to build components using templates and slots  \nHow to pass data with attributes and properties  \nBest practices for performance, accessibility, and component APIs  \nHow to build a real  component with encapsulated s",
      "publishedAt": "2025-12-02T10:43:32.000Z",
      "author": "Djamware Tutorial",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "thought_leadership",
      "score": 4.940701673016598,
      "ingestedAt": "2025-12-02T14:44:03.196Z",
      "tags": [
        "code_review",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b0990d927",
      "title": "We’re Not Just “Online” Anymore — We’re Connected. Welcome to ConnectApp",
      "url": "https://dev.to/tobi_jesee_731fd28a3212b0/were-not-just-online-anymore-were-connected-welcome-to-connectapp-3em",
      "content": "<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgatpz1ajwdpe0gw1akks.jpg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgatpz1ajwdpe0gw1akks.jpg\" alt=\"\" width=\"612\" height=\"408\"></a><br> \nLet’s be real.</p> \n \n<p>We live in a world where you can meet someone in Tokyo, collaborate with someone in Lagos, and build with someone in New York — all before lunch. But even with all this tech, real connection still feels hard.</p> \n \n<p>That’s exactly why ConnectApp Inc. exists.</p> \n \n<p>We’re not here to be just another app.<br> \nWe’re here to change how Gen Z connects, creates, and grows in the digital world.</p> \n \n<p>🌍 Connection, But Make It Real</p> \n \n<p>Gen Z doesn’t want noise.<br> \nWe want meaningful connections, authentic communities, and platforms that actually do something for us.</p> \n \n<p>ConnectApp is built to:</p> \n \n<p>Link people with shared interests and goals</p> \n \n<p>Support creators, entrepreneurs, students, and innovators</p> \n \n<p>Turn conversations into collaborations</p> \n \n<p>Make networking feel natural, not forced</p> \n \n<p>No pressure. No fake energy. Just real people building real things.</p> \n \n<p>💡 Built for the Future (Not the Past)</p> \n \n<p>We’re a generation that moves fast, thinks global, and builds digitally. ConnectApp is designed with:</p> \n \n<p>A clean, modern experience</p> \n \n<p>Smart networking tools</p> \n \n<p>Spaces for projects, ideas, and opportunities</p> \n \n<p>Features that grow with your ambition</p> \n \n<p>Whether you’re:</p> \n \n<p>A startup founder</p> \n \n<p>A creative</p> \n \n<p>A student</p> \n \n<p>A freelancer</p> \n \n<p>Or just someone with big ideas</p> \n \n<p>There’s space for you here.</p> \n \n<p>🔗 More Than an App — It’s a Movement</p> \n \n<p>ConnectApp Inc. is about:</p> \n \n<p>Collaboration over competition</p> \n \n<p>Community over clout</p> \n \n<p>Impact over hype</p> \n \n<p>We believe the next big things won’t come from isolated individuals — they’ll come from connected minds.</p> \n \n<p>And that’s where you come in.</p> \n \n<p>✨ Join the New Wave of Digital Connection</p> \n \n<p>Gen Z is rewriting the rules of work, networking, and collaboration. ConnectApp is simply giving us the tools to do it better.</p> \n \n<p>This is your space to:</p> \n \n<p>Connect</p> \n \n<p>Create</p> \n \n<p>Grow</p> \n \n<p>Build the future together</p> \n \n<p>The future isn’t just digital. It’s connected.<br> \nAnd it starts with us.</p> \n \n<p>— ConnectApp.inc</p>",
      "summary": " \nLet’s be real. \n \nWe live in a world where you can meet someone in Tokyo, collaborate with someone in Lagos, and build with someone in New York — all before lunch. But even with all this tech, real connection still feels hard. \n \nThat’s exactly why ConnectApp Inc. exists. \n \nWe’re not here to be just another app. \nWe’re here to change how Gen Z connects, creates, and grows in the digital world. \n \n🌍 Connection, But Make It Real \n \nGen Z doesn’t want noise. \nWe want meaningful connections, aut",
      "publishedAt": "2025-12-02T10:41:03.000Z",
      "author": "tobi jesee",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.4341024152476125,
      "ingestedAt": "2025-12-02T14:44:03.197Z",
      "tags": [
        "code_review",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b0990d92d",
      "title": "Unleash Dynamic Content: Mastering the Elementor Flip Box Widget for Engaging WordPress Sites",
      "url": "https://dev.to/artarasaneh2025/unleash-dynamic-content-mastering-the-elementor-flip-box-widget-for-engaging-wordpress-sites-3akh",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fci0z41ue036ivddgbifa.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><h2>Unleash Dynamic Content: Mastering the Elementor Flip Box Widget for Engaging WordPress Sites</h2>  \n  \n<p>In today's competitive digital landscape, capturing and retaining user attention is paramount for any website owner. Merely presenting static information often falls short of creating a memorable user experience. This is where dynamic, interactive elements come into play, transforming passive browsing into an engaging journey. For WordPress users leveraging the power of Elementor, a popular page builder, there's a straightforward yet incredibly effective tool to achieve this: the Flip Box widget. Designed to add a layer of interactivity and visual flair, the Elementor Flip Box allows you to display information in a novel way that encourages exploration and makes your content truly pop.</p>  \n  \n<h3>What is the Elementor Flip Box Widget?</h3>  \n  \n<p>The Elementor Flip Box widget is an innovative design element that presents content on two distinct sides – a 'front' and a 'back' – within a single container. Initially, users see the front side, which typically contains a concise piece of information, an icon, or a brief heading. Upon a specific interaction, usually a hover action or a click, the box animates and 'flips' to reveal the hidden content on its back side. This secondary content often provides more detailed information, a call to action, or supplementary media.</p>  \n  \n<p>As a native feature within Elementor, it's easily accessible for any WordPress user, eliminating the need for complex custom coding to achieve sophisticated interactive effects. Leveraging the Flip Box allows you to condense information into visually appealing, bite-sized chunks, enhancing both aesthetics and functional efficiency. It exemplifies how Elementor empowers users to create professional, dynamic layouts with intuitive drag-and-drop functionality, significantly improving the user experience on any WordPress-powered platform.</p>  \n  \n<h3>Elevate Your Design: Key Benefits of Using Elementor Flip Boxes</h3>  \n  \n<p>Integrating the Flip Box widget into your Elementor-built WordPress site offers a multitude of advantages, significantly boosting your site's appeal and effectiveness:</p>  \n  \n<ul>  \n  <li>  \n<strong>Enhanced User Engagement:</strong> Its interactive nature naturally draws users in, prompting deeper interaction and content discovery, leading to longer session durations and memorable experiences.</li>  \n  <li>  \n<strong>Optimal Space Efficiency:</strong> Flip Boxes effectively present a summary on the front and detailed information on the back, saving valuable screen real estate while maintaining a clean, professional layout.</li>  \n  <li>  \n<strong>Increased Visual Appeal:</strong> Offering various flip directions (horizontal, vertical, 3D cube) and customizable animations, Flip Boxes add a modern, dynamic touch, making your site visually distinct and engaging.</li>  \n  <li>  \n<strong>Effective Call-to-Actions (CTAs):</strong> The back side is ideal for embedding compelling CTAs or contact information, guiding engaged users seamlessly towards their next step.</li>  \n  <li>  \n<strong>Versatile Application:</strong> Incredibly versatile, the Elementor Flip Box widget seamlessly adapts to showcase team bios, product features, services, or FAQs, fitting various content types and design needs across your WordPress website.</li>  \n</ul>  \n  \n<h3>Mastering the Flip Box: Setup &amp; Customization Tips for WordPress Users</h3>  \n  \n<p>Implementing and customizing an Elementor Flip Box on your WordPress site is a straightforward process, thanks to Elementor's intuitive interface. Here’s a basic guide to get you started and some tips for mastery:</p>  \n  \n<ol>  \n  <li>  \n<strong>Adding the Widget:</strong> Open your page or post in the Elementor editor. Search for \"Flip Box\" in the widgets panel and drag it onto your canvas.</li>  \n  <li>  \n<strong>Content Configuration:</strong> In the 'Content' tab, you'll find options for both the 'Front' and 'Back' sides. For the front, you can add an icon, title, description, and even a button. Repeat this process for the back side, often including a more detailed description or a primary call-to-action. You can also choose your desired 'Flip Effect' (e.g., Slide, Push, Zoom, 3D), 'Flip Direction', and whether it flips on 'Hover' or 'Click'.</li>  \n  <li>  \n<strong>Styling for Impact:</strong> The 'Style' tab is where you bring your Flip Box to life. Customize the background (color or image), padding, border, and typography for both the front and back. Adjust the icon and button styles to match your brand's aesthetics. Experiment with different animations and timings to find the perfect balance between subtlety and impact.</li>  \n  <li>  \n<strong>Responsiveness:</strong> Always check how your Flip Box appears on different devices (desktop, tablet, mobile) using Elementor's responsive mode. Adjust settings as needed to ensure optimal display and functionality across all screen sizes.</li>  \n</ol>  \n  \n<p>For a deeper dive into all the practical steps, advanced settings, and creative ideas for leveraging this powerful Elementor tool to its fullest potential on your WordPress site, consider exploring comprehensive guides available online. These resources often provide detailed walkthroughs and examples that can significantly enhance your design capabilities. For instance, to dive deeper into the practical steps and creative ideas for leveraging this powerful tool, explore comprehensive guides like this one on the <a href=\"https://artarasaneh.com/flip-box-widget-elementor\">Elementor Flip Box widget</a>.</p>  \n  \n<h3>Conclusion: Transform Your WordPress Site with Elementor Flip Boxes</h3>  \n  \n<p>The Elementor Flip Box widget is more than just a visually appealing element; it's a strategic tool for enhancing user engagement, optimizing content display, and adding a professional polish to any WordPress website. By effectively utilizing its dual-sided design and customizable animations, you can captivate your audience, convey information efficiently, and guide them seamlessly through your site. Whether showcasing services, introducing team members, or highlighting product features, the Flip Box offers a dynamic and interactive solution that elevates the overall user experience. Embrace this powerful Elementor feature to transform your static pages into interactive masterpieces, making your WordPress site truly stand out.</p>",
      "summary": "Unleash Dynamic Content: Mastering the Elementor Flip Box Widget for Engaging WordPress Sites  \n  \nIn today's competitive digital landscape, capturing and retaining user attention is paramount for any website owner. Merely presenting static information often falls short of creating a memorable user experience. This is where dynamic, interactive elements come into play, transforming passive browsing into an engaging journey. For WordPress users leveraging the power of Elementor, a popular page bu",
      "publishedAt": "2025-12-02T10:39:40.000Z",
      "author": "Artarasaneh",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 9.87950827690151,
      "ingestedAt": "2025-12-02T14:44:03.197Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b0990d931",
      "title": "Branch development with git",
      "url": "https://dev.to/aaronmccollum/branch-development-with-git-joc",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fv0v11xs8opjh7ga2hw9p.jpg\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>Branch development is a key aspect of software development. The idea here is you clone (i.e. copy) the code you’re going to work on onto your own computer, then you can make your necessary changes, then lastly save it back to the official code repository. In other words, you take the copied code and “branch” off with it to make changes. Then you “merge” your branch back into the main repository of code.</p>  \n  \n<p>In my old job, Pega handled this on it’s own. It had it’s own branching tools, so I could copy the low code settings into my own branch, make my changes, then check them back in to the official code. But outside of low-code tools like Pega, developers primarily use git to help manage their branch development.</p>  \n  \n<p>With 100Devs, I’ve been pushing the HTML and CSS projects into Github. In the past, I would not create branches. I would just clone the code down, make changes, then merge the code straight into the main branch without creating my own branches or opening a pull request. While that would work for small one-developer projects, that won’t work long-term in my learning.</p>  \n  \n<p>Below is the workflow I’ve been using for the past few weeks. I’m writing this mainly for myself — I forget the terminal commands sometimes and will need a reminder. But hopefully this can also help you, the reader, if you are just starting out and learning git.</p>  \n  \n<p><code>git checkout -b &lt;branchName&gt;</code> : This command does two things — it first creates a new branch with whatever value you provide (without the &lt; and &gt; symbols), and it automatically checks out your code into this new branch.</p>  \n  \n<p><code>git branch</code> : This will list out all the branches you have. The branch you are currently working in might have a * beside it’s name, be colored green, or be bolded in the terminal.</p>  \n  \n<p><code>git status</code> : This shows all the updated files that are not staged for the next commit. Usually a terminal will show these files in the color red.</p>  \n  \n<p><code>git add &lt;fileName&gt; or git add .</code> : The first command adds a specific file name to the staging area for the next commit. The second command adds everything that’s been changed. Be careful with the second command, as you may not want to add everything for the next commit quite yet.</p>  \n  \n<p><code>git commit -m “&lt;commit message&gt;”</code> : This commits the changes in git. This won’t send the code remotely to Github quite yet, but it does save the code into a new “version” of your codebase. I always add a message, and I am learning to follow <a href=\"https://www.conventionalcommits.org/en/v1.0.0/\">conventional commits</a>.</p>  \n  \n<p><code>git push origin &lt;branchname&gt;</code> : This pushes your code remotely into Github. If a branch does not exist yet, it will create a branch in Github with your updated code. Github will then give you the option to open a pull request, or open a draft pull request (which I didn’t know about until a few days ago, and I love that feature).</p>  \n  \n<p>That’s it! I wanted to document the common commands I use in git. I’ll be posting again soon with some more learnings from 100Devs, but I wanted to get this post out first as I know it will come in handy later on for more projects I’m building.</p>  \n  \n<p>What other git commands do you think I should know and put into practice?</p>",
      "summary": "Branch development is a key aspect of software development. The idea here is you clone (i.e. copy) the code you’re going to work on onto your own computer, then you can make your necessary changes, then lastly save it back to the official code repository. In other words, you take the copied code and “branch” off with it to make changes. Then you “merge” your branch back into the main repository of code.  \n  \nIn my old job, Pega handled this on it’s own. It had it’s own branching tools, so I coul",
      "publishedAt": "2025-12-02T10:39:26.000Z",
      "author": "Aaron McCollum",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 7.409545448551965,
      "ingestedAt": "2025-12-02T14:44:03.197Z",
      "tags": [
        "code_review",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b0990d93e",
      "title": "Data Analytics in Healthcare: Engineering the Future of Intelligent Medicine",
      "url": "https://dev.to/rank_alchemy_5ad282cec75d/data-analytics-in-healthcare-engineering-the-future-of-intelligent-medicine-lmg",
      "content": "<p><img src=\"https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fnl5hoj9p1m4wx1wxoivs.png\" alt=\"https%3A%2F%2Fdev-to-uploads.s3.amazonaw\"></p><p>If you work in development, data engineering, AI, or medtech, you’re witnessing one of the biggest transformations happening right now: the shift toward data-driven healthcare.</p>  \n  \n<p>Healthcare has always produced massive datasets, but until recently, most of that data stayed trapped in silos. Today, with modern analytics stacks, AI models, IoMT devices, and scalable infrastructure, engineering teams are helping unlock insights that directly influence patient outcomes.</p>  \n  \n<p>In this DEV-focused breakdown, we’ll explore how data analytics is reshaping healthcare, from predictive modeling to XR-powered training — and why developers play a critical role in this evolution.</p>  \n  \n<h2>  \n    \n    \n  Why Healthcare Is Becoming a Data-First Industry  \n</h2>  \n  \n<p><strong>Healthcare systems generate exabyte-scale data from:</strong></p>  \n  \n<ul>  \n<li>EHRs and hospital databases</li>  \n<li>Medical imaging (MRI, CT, Ultrasound)</li>  \n<li>Wearables and IoMT devices</li>  \n<li>Genomics and biomarkers</li>  \n<li>Pharmacy and insurance records</li>  \n<li>Telehealth and remote monitoring platforms</li>  \n</ul>  \n  \n<p><strong>From an engineering perspective, this data is:</strong></p>  \n  \n<ul>  \n<li>High-volume</li>  \n<li>Highly sensitive (HIPAA/PHI)</li>  \n<li>Multi-modal</li>  \n<li>Often unstructured</li>  \n<li>Time-critical</li>  \n</ul>  \n  \n<p>This makes healthcare one of the most complex — and rewarding — data environments.</p>  \n  \n<h2>  \n    \n    \n  1. Predictive Analytics: Preventing Problems Before They Happen  \n</h2>  \n  \n<p>Predictive models are now integrated directly into clinical workflows.</p>  \n  \n<p><strong>Common use cases:</strong></p>  \n  \n<ul>  \n<li>Early sepsis detection</li>  \n<li>Predicting cardiac risk</li>  \n<li>Hospital readmission forecasting</li>  \n<li>Chronic disease deterioration</li>  \n<li>ER demand prediction</li>  \n</ul>  \n  \n<p><strong>Tech enabling it:</strong></p>  \n  \n<ul>  \n<li>Time-series modeling</li>  \n<li>Gradient boosting</li>  \n<li>Deep learning architectures</li>  \n<li>Real-time event processing</li>  \n<li>FHIR-based APIs</li>  \n</ul>  \n  \n<p>Developers are now building pipelines where raw data becomes actionable intelligence at the bedside.</p>  \n  \n<h2>  \n    \n    \n  2. Data-Driven Personalization of Treatment  \n</h2>  \n  \n<p>Healthcare is moving from one-size-fits-all to individualized plans.</p>  \n  \n<h2>  \n    \n    \n  Inputs powering personalization:  \n</h2>  \n  \n<ul>  \n<li>Genomic datasets</li>  \n<li>Wearable health metrics</li>  \n<li>Lifestyle and behavioral data</li>  \n<li>Longitudinal clinical history</li>  \n</ul>  \n  \n<p><strong>Outputs include:</strong></p>  \n  \n<ul>  \n<li>Personalized drug regimens</li>  \n<li>Tailored cancer therapy pathways</li>  \n<li>Predictive rehabilitation protocols</li>  \n<li>Adaptive treatment recommendations</li>  \n</ul>  \n  \n<p><strong>Behind the scenes, this requires:</strong></p>  \n  \n<ul>  \n<li>Feature engineering across multi-modal data</li>  \n<li>ML orchestration (e.g., Airflow / Prefect)</li>  \n<li>Interoperable data models using HL7/FHIR standards</li>  \n</ul>  \n  \n<h2>  \n    \n    \n  3. Applying AI to Clinical Decision Support  \n</h2>  \n  \n<p>AI isn’t replacing doctors — but it is augmenting their ability to diagnose, triage, and plan treatment.</p>  \n  \n<p>Examples of AI in healthcare analytics:</p>  \n  \n<ul>  \n<li>NLP models extracting insights from physician notes</li>  \n<li>Vision AI detecting anomalies in radiology scans</li>  \n<li>LLM-powered medical assistants</li>  \n<li>Predictive triage for emergency scenarios</li>  \n</ul>  \n  \n<p>Typical technical stack might include:</p>  \n  \n<p>model_type: \"Hybrid CNN + Transformer\"<br>  \nframework: \"TensorFlow / PyTorch\"<br>  \ndeployment: \"Docker + Kubernetes\"<br>  \ndata_standard: \"FHIR R4\"<br>  \nsecurity: \"HIPAA, SOC 2, PHI Encryption\"</p>  \n  \n<p>If you want to see the broader use-case landscape, here’s the full overview:<br>  \n👉 [<a href=\"https://citrusbits.com/data-analytics-in-healthcare/\">https://citrusbits.com/data-analytics-in-healthcare/</a>]</p>  \n  \n<h2>  \n    \n    \n  4. XR + Data Analytics: The Future of Hands-On Medicine  \n</h2>  \n  \n<p>Extended Reality (XR) and Virtual Reality (VR) are gaining momentum in medtech, especially when combined with analytics.</p>  \n  \n<p>Developer-focused XR use cases:</p>  \n  \n<ul>  \n<li>Surgical rehearsal using patient-specific data</li>  \n<li>Digital twins for complex procedure planning</li>  \n<li>Data-driven VR rehabilitation platforms</li>  \n<li>Interactive anatomy learning powered by real datasets</li>  \n</ul>  \n  \n<p><strong>XR tech stack often involves:</strong></p>  \n  \n<ul>  \n<li>Unity or Unreal Engine</li>  \n<li>OpenXR framework</li>  \n<li>Custom API endpoints for clinical datasets</li>  \n<li>ML-integrated simulations</li>  \n</ul>  \n  \n<p>Healthcare training is becoming immersive, and coders are the ones building it.</p>  \n  \n<h2>  \n    \n    \n  The Medtech Engineering Convergence  \n</h2>  \n  \n<p>Whether you're a backend engineer, data scientist, ML researcher, or XR developer, the future of medtech relies on your work.</p>  \n  \n<p>Teams like the <a href=\"https://citrusbits.com/\">Medtech and AI Healthcare Innovation Company</a><br>  \n At CitrusBits are engineering:</p>  \n  \n<ul>  \n<li>Predictive analytics systems</li>  \n<li>Medical imaging AI pipelines</li>  \n<li>IoMT device ecosystems</li>  \n<li>XR surgical planning tools</li>  \n<li>Patient-centric mobile and web platforms</li>  \n<li>Intelligent healthcare dashboards</li>  \n</ul>  \n  \n<p>This intersection of data, AI, and immersive tech is where the next decade of healthcare innovation lives.</p>  \n  \n<h2>  \n    \n    \n  Final Thoughts  \n</h2>  \n  \n<p>Healthcare is no longer just a clinical field — it’s a data engineering challenge. The developers who can work with sensitive data, scalable architectures, AI models, interoperability standards, and immersive technologies will shape the next generation of patient care.</p>  \n  \n<p>If you’re building in this space, you’re not just coding apps,<br>  \nYou’re coding the future of medicine.</p>",
      "summary": "If you work in development, data engineering, AI, or medtech, you’re witnessing one of the biggest transformations happening right now: the shift toward data-driven healthcare.  \n  \nHealthcare has always produced massive datasets, but until recently, most of that data stayed trapped in silos. Today, with modern analytics stacks, AI models, IoMT devices, and scalable infrastructure, engineering teams are helping unlock insights that directly influence patient outcomes.  \n  \nIn this DEV-focused br",
      "publishedAt": "2025-12-02T10:39:15.000Z",
      "author": "Rank Alchemy",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "pricing_business",
      "score": 8.397408476023207,
      "ingestedAt": "2025-12-02T14:44:03.197Z",
      "tags": [
        "code_review",
        "ide",
        "observability",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b098f448d",
      "title": "How to Achieve Significant Build Performance Gains With TeamCity",
      "url": "https://blog.jetbrains.com/teamcity/2025/12/build-performance-gains/",
      "content": "<p><em>This article is brought to you by <a href=\"https://www.technewstoday.com/author/mduduzi/\">Mdu Sibisi</a>, draft.dev.</em></p> \n \n \n \n<p>Long <a href=\"https://software.land/build-time/\">build times</a> interfere with developer momentum, disrupt flow states, and negatively impact productivity. While seemingly minor gaps in work rate may appear negligible in the moment, they quickly accumulate into significant increases in release cycle times and infrastructure costs. (There’s a reason high-performance development teams <a href=\"https://cloud.google.com/blog/products/devops-sre/announcing-the-2024-dora-report\">run their CI/CD pipelines several times a day</a>).</p> \n \n \n \n<p>While it’s tempting to reach for the latest software or methodology that promises to maximize your engineering team’s output, carelessly swapping tools with a trial-and-error-like approach isn’t a good idea: <a href=\"https://www.researchgate.net/profile/Omowunmi-Adebayo-4/publication/388122113_Software_Adoption_in_Project_Management_and_Their_Impact_on_Project_Efficiency_and_Collaboration/links/678b3b1f82501639f5f64d6e/Software-Adoption-in-Project-Management-and-Their-Impact-on-Project-Efficiency-and-Collaboration.pdf\">Premature adoption</a> can often do more harm than good. It’s far more effective to adopt a strategy informed by data, where you first identify leaks and performance bottlenecks and remedy them accordingly.</p> \n \n \n \n<p>After all, you must ensure your team is using the most optimal tools <em>and</em> that they’re using those tools optimally.</p> \n \n \n \n<p>This guide offers a three-phase blueprint for achieving significant CI/CD pipeline performance boosts that will stick around even as you scale:</p> \n \n \n \n<ul> \n<li>Identify: Pinpoint pipeline flaws.</li> \n \n \n \n<li>Optimize: Apply effective solutions.</li> \n \n \n \n<li>Continuously improve: Apply long-term strategies to avert future faults.</li> \n</ul> \n \n \n \n<h2><strong>Phase 1: Assess performance and identify bottlenecks</strong></h2> \n \n \n \n<p>Before attempting any optimization, first assess the current performance of your pipeline and identify the causes of poor performance. This allows you to make informed improvements and prove their value.</p> \n \n \n \n<p><em>Time</em> is one of the most reliable and straightforward metrics to start with, particularly in the early stages of your pipeline audit:</p> \n \n \n \n<ul> \n<li>How long does full pipeline completion take (from commit to production)?</li> \n \n \n \n<li>What is the average duration of each pipeline stage (build, test, and deploy)?</li> \n \n \n \n<li>Which steps consistently take the longest?</li> \n</ul> \n \n \n \n<p>Also, it’s important to survey or speak to developers about their time perception of the pipeline, and observe their behavior. They may be batching work or avoiding commits due to long feedback cycles.</p> \n \n \n \n<p>They might even be avoiding experimentation based on unhealthy cultures centered around “waiting for the build”.</p> \n \n \n \n<p>Use the data gathered from these sources to set and adjust your benchmarks and <a href=\"https://www.ibm.com/think/topics/service-level-agreement\">service-level agreements</a>, such as build duration (time) and CPU cores or memory usage (resource utilization).</p> \n \n \n \n<p>You don’t necessarily have to start with granular benchmarks and monitoring criteria per stage or process. You can keep it as broad as measuring overall pipeline completion time for the sake of simplicity.</p> \n \n \n \n<p>While legacy platforms might hide away data in text logs, modern CI/CD platforms provide you with the visibility you need to assess performance out of the box. </p> \n \n \n \n<p>In <a href=\"https://www.jetbrains.com/teamcity/\">TeamCity</a>, the <em>Statistics</em> dashboard provides deeper insights into the performance of each build:</p> \n \n \n \n<img style=\"height:auto;\" src=\"https://blog.jetbrains.com/wp-content/uploads/2025/12/image-1_statistics-dashboard-in-teamcity.png\" alt=\"\"> \n \n \n \n<p>This view visualizes recent and historical build data using key metrics like build duration, success rate, test count, artifact size, and queue time. You can also create custom charts to track the metrics that matter the most to your business.</p> \n \n \n \n<p>For a more granular view, the <a href=\"https://www.jetbrains.com/help/teamcity/build-time-report.html\"><em>Build Time</em> report</a> shows the time taken by each stage of your pipeline so you can pinpoint the most significant bottlenecks:</p> \n \n \n \n<img style=\"height:auto;\" src=\"https://blog.jetbrains.com/wp-content/uploads/2025/12/image-2_build-time.png\" alt=\"\"> \n \n \n \n<p>By default, TeamCity attaches its <a href=\"https://www.jetbrains.com/help/teamcity/performance-monitor.html\">Performance Monitor</a> to each of your build configurations created from a URL. This provides you with a handy way of tracking resource utilization:</p> \n \n \n \n<img style=\"height:auto;\" src=\"https://blog.jetbrains.com/wp-content/uploads/2025/12/image-3_perfmon.png\" alt=\"\"> \n \n \n \n<p>In addition to the Performance Monitor (and a host of other <a href=\"https://www.jetbrains.com/help/teamcity/adding-build-features.html\">build features</a>), TeamCity’s notifier can trigger real-time alerts for any important build events (such as fails, successful completions, or hangs) via email or Slack.</p> \n \n \n \n<h2><strong>Phase 2: Optimize performance</strong></h2> \n \n \n \n<p>Once you know what’s not working, you want to identify the highest-impact changes your team can make to boost performance.</p> \n \n \n \n<p>Avoid initiatives that may break the system or result in widespread interruptions, like upgrading build tools without validating downstream dependencies or switching shell or OS environments. Small incremental changes that can be isolated are the least likely to disrupt your team’s productivity and the easiest to trace and roll back if something goes wrong.</p> \n \n \n \n<p>Some sure-fire techniques that are usually relatively simple to apply include the following:</p> \n \n \n \n<ul> \n<li><strong>Build caches:</strong> Cache external (package manager) dependencies and build artifacts.</li> \n \n \n \n<li><strong>Parallelization:</strong> Run tests in parallel across agents or containers, splitting test suites by file, type, or historical timing data. You can also build microservices or modules concurrently.</li> \n \n \n \n<li><strong>Modularization:</strong> Break monolithic pipelines into modular build configurations.</li> \n \n \n \n<li><strong>Selective and incremental building:</strong> Use VCS triggers and path filters to build only affected modules. Use build fingerprint or checksums to avoid rebuilding unchanged components.</li> \n</ul> \n \n \n \n<p>TeamCity’s built-in features can help your team achieve several quick wins.For example, the <a href=\"https://www.jetbrains.com/help/teamcity/build-cache.html\">build caches</a> can be enabled to intelligently reuse compiled artifacts and dependencies:</p> \n \n \n \n<img style=\"height:auto;\" src=\"https://blog.jetbrains.com/wp-content/uploads/2025/12/image-4_build-with-cache-without-cache.png\" alt=\"\"> \n \n \n \n<p>In the example above, using a build cache reduced the build time by <strong>73 percent</strong>. (Keep in mind that the time you’ll save depends on the complexity of your build and which files you choose to cache.)</p> \n \n \n \n<p><a href=\"https://www.jetbrains.com/help/teamcity/artifact-dependencies.html\">Artifact dependencies</a> let you reuse files and curb any redundant compilation or build steps, allowing for pipeline modularization and faster executions:</p> \n \n \n \n<img style=\"height:auto;\" src=\"https://blog.jetbrains.com/wp-content/uploads/2025/12/image-5_after-artifact-dependency-added.png\" alt=\"\"> \n \n \n \n<p>TeamCity’s architecture allows tests to run in parallel using separate <a href=\"https://www.jetbrains.com/help/teamcity/install-and-start-teamcity-agents.html\">build agents</a>. So instead of two (or more) builds running consecutively, they’ll run simultaneously, providing faster feedback to developers and thus shortening the release cycle:</p> \n \n \n \n<img style=\"height:auto;\" src=\"https://blog.jetbrains.com/wp-content/uploads/2025/12/image-6_no-changes.png\" alt=\"\"> \n \n \n \n<h2><strong>Phase 3: Continuously improve</strong></h2> \n \n \n \n<p>Performance is not a one-off fix; it must be continually monitored and sustained as your team and codebase scale.</p> \n \n \n \n<p>Keep the following three considerations in mind to ensure continuous improvement:</p> \n \n \n \n<ul> \n<li>Use elastic infrastructure that can scale up to meet peak demand and scale down to save costs.</li> \n \n \n \n<li>Design pipelines that only build what has changed to speed up builds.</li> \n \n \n \n<li>Create a culture of performance: Empower your team with the necessary visibility and tools to track trends, catch regressions, and implement improvements.</li> \n</ul> \n \n \n \n<p>TeamCity supports continuous improvement, regardless of your scale. Its <a href=\"https://www.jetbrains.com/help/teamcity/cloud/teamcity-integration-with-cloud-solutions.html#Cloud+Agents+and+Executors\">cloud profiles</a> let you manage cloud agents on AWS, Azure, or GCP so you can scale infrastructure up and down to meet demand.</p> \n \n \n \n<p>When it comes to orchestration, you can use features like <a href=\"https://www.jetbrains.com/help/teamcity/snapshot-dependencies.html\">snapshot dependencies</a> to create build chains that only build the components impacted by change, which is crucial for managing microservices architecture and speeding up builds. <a href=\"https://www.jetbrains.com/help/teamcity/build-configuration-template.html#Creating+build+configuration+template\">Build templates</a>, on the other hand, allow your team to define a configuration once and apply it across multiple projects, which simplifies updates and maintenance.</p> \n \n \n \n<p>Finally, TeamCity’s <a href=\"https://www.jetbrains.com/help/teamcity/tracking-user-actions.html\">audit</a> log and historical performance reports offer the transparency and accountability needed for a performance-oriented culture by letting you track changes and share successes with stakeholders.</p> \n \n \n \n<img style=\"height:auto;\" src=\"https://blog.jetbrains.com/wp-content/uploads/2025/12/image-7_teamcity-audit.png\" alt=\"\"> \n \n \n \n<p>You can even use TeamCity’s REST API to export data into external tools such as Grafana and Excel to present to stakeholders.</p> \n \n \n \n<h2><strong>Conclusion</strong></h2> \n \n \n \n<p>Implementing meaningful change is not always easy. However, by identifying and addressing performance bottlenecks and measuring progress, you and your team will be rewarded with visible improvements in product output and quality. Use this positive feedback to maintain your momentum.</p> \n \n \n \n<p>While addressing process bottlenecks is crucial to improving performance, the tools your team uses also shape the way they work. Legacy CI/CD tools that require extensive customization, external plugins, and specialized expertise often hinder performance because they keep teams stuck in ineffective workloads.</p> \n \n \n \n<p>If you’re secretly wondering whether your CI/CD setup is holding back your team, be sure to check out the following resources:</p> \n \n \n \n<ul> \n<li>Is your CI/CD tool helping or hindering performance?</li> \n \n \n \n<li>The migration decision framework: Quantify the risk of your legacy system to determine whether migration is worth consideration or not.</li> \n</ul>",
      "summary": "This article is brought to you by Mdu Sibisi, draft.dev. \n \n \n \nLong build times interfere with developer momentum, disrupt flow states, and negatively impact productivity. While seemingly minor gaps in work rate may appear negligible in the moment, they quickly accumulate into significant increases in release cycle times and infrastructure costs. (There’s a reason high-performance development teams run their CI/CD pipelines several times a day). \n \n \n \nWhile it’s tempting to reach for the lates",
      "publishedAt": "2025-12-02T10:34:32.000Z",
      "author": "Olga Bedrina",
      "source": "rss",
      "feedName": "JetBrains Company Blog",
      "sourceType": "engineering_blog",
      "contentType": "product_launch",
      "score": 17.28473771782797,
      "ingestedAt": "2025-12-02T14:44:03.197Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "observability",
        "governance",
        "Tech Company Blogs",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b098c705a",
      "title": "From 1.2GB to 54MB: My Docker Image Went on a Diet",
      "url": "https://dev.to/tusharsharma099/from-12gb-to-54mb-my-docker-image-went-on-a-diet-17nd",
      "content": "<p>When I first containerized a Node.js app, I felt pretty good about myself. I had a Dockerfile, I built it, and it worked.</p> \n \n<p>Then I checked the size.</p> \n \n<p><strong>1.2GB. For a single Node.js service.</strong></p> \n \n<p>That’s when reality hit me. My image wasn’t lean—it was obese. It slowed down builds, bloated my CI/CD pipeline, took forever to push to the registry, and ate storage like there was no tomorrow.</p> \n \n<p>So, I put my Docker image on a strict diet. After a few rounds of optimizations, it went from <strong>1.2GB → 250MB → 54MB</strong>.</p> \n \n<p>Here’s the story of how I cut the fat—and how you can too.</p> \n \n<p><strong>Step 1: The Heavyweight Start</strong><br> \nHere’s what my original Dockerfile looked like:</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgw7tx40tka895ys6afko.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgw7tx40tka895ys6afko.png\" alt=\"\" width=\"800\" height=\"297\"></a></p> \n \n<ul> \n<li><p>node:16 is Debian-based and heavy (~350MB).</p></li> \n<li><p>npm install installed <strong>everything</strong>-dev and production dependencies.</p></li> \n<li><p>No .dockerignore, so logs, git history, and node_modules sneaked into the image</p></li> \n</ul> \n \n<p>The result? A <strong>1.2GB monster</strong> that slowed everything down.</p> \n \n<p><strong>Step 2: Choosing a Leaner Base</strong><br> \nThe first fix was swapping node:16 for node:16-alpine.</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fob03v461nlhpqbdpwhl7.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fob03v461nlhpqbdpwhl7.png\" alt=\"\" width=\"800\" height=\"72\"></a></p> \n \n<p>That <strong>one-line change cut my image down to ~250MB.</strong></p> \n \n<p>Lesson: <strong>Your base image choice can make or break your build.</strong></p> \n \n<p>⚠️ Caveat: Alpine uses <strong>musl</strong> instead of glibc. If your app has native modules (sharp, bcrypt, canvas), you may need extra packages.</p> \n \n<p><strong>Step 3: Multi-Stage Builds</strong><br> \nMy app uses TypeScript, so I had build tools sitting inside the final image. Big mistake. They added hundreds of MBs I didn’t need in production.</p> \n \n<p>Enter <strong>multi-stage builds:</strong></p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9sjeqt981a5xvtlg4xng.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9sjeqt981a5xvtlg4xng.png\" alt=\"\" width=\"800\" height=\"500\"></a></p> \n \n<p>Now, the final image contains only:</p> \n \n<ul> \n<li>Compiled JavaScript (dist/)</li> \n<li>Production dependencies \nNo dev dependencies. No build cache. No clutter.</li> \n</ul> \n \n<p>This dropped my image to ~120MB.</p> \n \n<p><strong>Step 4: Prune and Ignore Junk</strong><br> \nAnother culprit: files that had no business being in production.</p> \n \n<p>I added a .dockerignore:</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvn0laqnk0h3geixd9409.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvn0laqnk0h3geixd9409.png\" alt=\"\" width=\"800\" height=\"238\"></a></p> \n \n<p>And I cleaned up caches in the Dockerfile:</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F4603x6yjw50v0bwydcwj.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F4603x6yjw50v0bwydcwj.png\" alt=\"\" width=\"800\" height=\"125\"></a><br> \nEnd result: no accidental junk, no wasted MBs.</p> \n \n<p><strong>Step 5: Minimize Layers</strong><br> \nAt first, I had a Dockerfile with multiple RUN statements:</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fevu9umlmp6ljnkkf0yyr.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fevu9umlmp6ljnkkf0yyr.png\" alt=\"\" width=\"800\" height=\"113\"></a><br> \nEach RUN adds a layer. I combined them into one:</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjgryccmdmgt0obybbzru.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjgryccmdmgt0obybbzru.png\" alt=\"\" width=\"800\" height=\"115\"></a><br> \nThis small tweak shaved off ~15MB. Not huge, but every MB counts when you’re pulling images in production.</p> \n \n<p><strong>Step 6: Measuring and Iterating</strong><br> \nThe key to trimming images is measuring:</p> \n \n<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftr1i6zove38uezyfalx6.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftr1i6zove38uezyfalx6.png\" alt=\"\" width=\"800\" height=\"92\"></a><br> \nWith docker history, I saw exactly which layer was eating space and optimized from there.</p> \n \n<p><strong>Final Weight Check</strong><br> \nOriginal: 1.2GB<br> \n<strong>After switching to Alpine :</strong> ~250MB<br> \n<strong>After multi-stage + pruning:</strong> 120MB<br> \n<strong>After .dockerignore + cleanup:</strong> 54MB 🎉<br> \nThat’s a <strong>~95% reduction</strong>. Pulls went from minutes to seconds, and CI/CD pipelines stopped crawling.</p> \n \n<p><strong>Lessons Learned</strong><br> \n<strong>Pick the right base image</strong> – Defaults are rarely optimal.<br> \n<strong>Multi-stage builds are gold</strong> – Keep dev tools out of production.<br> \n<strong>Use .dockerignore religiously</strong>– Don’t ship junk.<br> \n<strong>Prune aggressively</strong> – Caches, logs, temp files… delete them.<br> \n<strong>Measure constantly</strong>– Know what’s eating space before fixing it.</p> \n \n<p><strong>Conclusion</strong><br> \nCutting Docker image size isn’t just about bragging rights—it’s about faster deploys, lower registry costs, and fewer headaches.</p> \n \n<p>My Node.js image went on a diet and lost <strong>1.1GB</strong>, and I’ll never go back to lazy Dockerfiles again.</p> \n \n<p>If your containers are bloated, trust me: a few tweaks can make them featherweight.</p> \n \n<p>So… is your Docker image on a healthy diet?</p> \n \n<p><strong>📬 Contact</strong><br> \nIf you’d like to connect, collaborate, or discuss DevOps, feel free to reach out:</p> \n \n<p>GitHub: <a href=\"https://github.com/tusharsharma099\">https://github.com/tusharsharma099</a><br> \nLinkedIn: <a href=\"http://www.linkedin.com/in/tushar-sharma-77063225b\">www.linkedin.com/in/tushar-sharma-77063225b</a></p>",
      "summary": "When I first containerized a Node.js app, I felt pretty good about myself. I had a Dockerfile, I built it, and it worked. \n \nThen I checked the size. \n \n1.2GB. For a single Node.js service. \n \nThat’s when reality hit me. My image wasn’t lean—it was obese. It slowed down builds, bloated my CI/CD pipeline, took forever to push to the registry, and ate storage like there was no tomorrow. \n \nSo, I put my Docker image on a strict diet. After a few rounds of optimizations, it went from 1.2GB → 250MB →",
      "publishedAt": "2025-12-02T10:36:20.000Z",
      "author": "tusharsharma099",
      "source": "rss",
      "feedName": "The Practical Developer",
      "sourceType": "general",
      "contentType": "thought_leadership",
      "score": 6.420618681983938,
      "ingestedAt": "2025-12-02T14:44:03.197Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "076611c99092231b0a6a536641c2b7bb",
      "title": "Inside Mirakl’s Agent Commerce Vision",
      "url": "https://openai.com/index/mirakl",
      "content": "Mirakl is redefining commerce through AI agents and ChatGPT Enterprise—achieving faster documentation, smarter customer support, and building toward agent-native commerce with Mirakl Nexus.",
      "summary": "Mirakl is redefining commerce through AI agents and ChatGPT Enterprise—achieving faster documentation, smarter customer support, and building toward agent-native commerce with Mirakl Nexus.",
      "publishedAt": "2025-12-01T22:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "feature_update",
      "score": 16.98878395994319,
      "ingestedAt": "2025-12-02T17:25:37.135Z",
      "tags": [
        "code_review",
        "documentation",
        "agents",
        "ide"
      ]
    },
    {
      "id": "c37edffbc0bcec7054c216279ad0111c",
      "title": "Funding grants for new research into AI and mental health",
      "url": "https://openai.com/index/ai-mental-health-research-grants",
      "content": "OpenAI is awarding up to $2 million in grants for research at the intersection of AI and mental health. The program supports projects that study real-world risks, benefits, and applications to improve safety and well-being.",
      "summary": "OpenAI is awarding up to $2 million in grants for research at the intersection of AI and mental health. The program supports projects that study real-world risks, benefits, and applications to improve safety and well-being.",
      "publishedAt": "2025-12-01T12:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 5.496871790108833,
      "ingestedAt": "2025-12-02T17:25:37.136Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "f3c5b2b4c2d25e481cbd56bee8520263",
      "title": "OpenAI and NORAD team up to bring new magic to “NORAD Tracks Santa”",
      "url": "https://openai.com/index/norad-holiday-collaboration",
      "content": "OpenAI and NORAD are bringing new magic to “NORAD Tracks Santa” with three ChatGPT holiday tools that let families create festive elves, toy coloring pages, and custom Christmas stories.",
      "summary": "OpenAI and NORAD are bringing new magic to “NORAD Tracks Santa” with three ChatGPT holiday tools that let families create festive elves, toy coloring pages, and custom Christmas stories.",
      "publishedAt": "2025-12-01T06:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "score": 2.6997922931229703,
      "ingestedAt": "2025-12-02T17:25:37.136Z",
      "tags": []
    },
    {
      "id": "efc2132fd740754392307a254df17fb8",
      "title": "Accenture and OpenAI accelerate enterprise AI success",
      "url": "https://openai.com/index/accenture-partnership",
      "content": "Accenture and OpenAI are collaborating to help enterprises bring agentic AI capabilities into the core of their business and unlock new levels of growth.",
      "summary": "Accenture and OpenAI are collaborating to help enterprises bring agentic AI capabilities into the core of their business and unlock new levels of growth.",
      "publishedAt": "2025-12-01T05:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "score": 10.767076568589895,
      "ingestedAt": "2025-12-02T17:25:37.136Z",
      "tags": [
        "code_review",
        "agents"
      ]
    },
    {
      "id": "af929e0847f74f35541bbc73cba6e4ac",
      "title": "OpenAI takes an ownership stake in Thrive Holdings to accelerate enterprise AI adoption",
      "url": "https://openai.com/index/thrive-holdings",
      "content": "OpenAI takes an ownership stake in Thrive Holdings to accelerate enterprise AI adoption, embedding frontier research and engineering directly into accounting and IT services to boost speed, accuracy, and efficiency while creating a scalable model for industry-wide transformation.",
      "summary": "OpenAI takes an ownership stake in Thrive Holdings to accelerate enterprise AI adoption, embedding frontier research and engineering directly into accounting and IT services to boost speed, accuracy, and efficiency while creating a scalable model for industry-wide transformation.",
      "publishedAt": "2025-12-01T05:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "pricing_business",
      "score": 11.215704758947806,
      "ingestedAt": "2025-12-02T17:25:37.136Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ]
    },
    {
      "id": "8dd9bb0697e2908dd121f4607aa9269b",
      "title": "Mixpanel security incident: what OpenAI users need to know",
      "url": "https://openai.com/index/mixpanel-incident",
      "content": "OpenAI shares details about a Mixpanel security incident involving limited API analytics data. No API content, credentials, or payment details were exposed. Learn what happened and how we’re protecting users.",
      "summary": "OpenAI shares details about a Mixpanel security incident involving limited API analytics data. No API content, credentials, or payment details were exposed. Learn what happened and how we’re protecting users.",
      "publishedAt": "2025-11-26T19:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "security_incident",
      "score": 7.853951820490492,
      "ingestedAt": "2025-12-02T17:25:37.136Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "622b219d37ea512e1caa20624c858ed7",
      "title": "[Subscribers only] Dev Writers Retreat 2025: WRITING FOR HUMANS — 10 Fellowship spots left!",
      "url": "https://www.latent.space/p/dwr2025",
      "content": "A unique most-expenses-paid Writing Fellowship to take stock of 2025, work on your non-fiction writing skills, and meet fellow subscribers in sunny San Diego!",
      "summary": "A unique most-expenses-paid Writing Fellowship to take stock of 2025, work on your non-fiction writing skills, and meet fellow subscribers in sunny San Diego!",
      "publishedAt": "2025-11-28T03:21:27.000Z",
      "author": "Shawn swyx Wang",
      "source": "rss",
      "feedName": "Latent Space",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "score": 2.8826393691819234,
      "ingestedAt": "2025-12-02T17:25:37.492Z",
      "tags": []
    },
    {
      "id": "7e20c863771e02937748a52078cc73f6",
      "title": "Anthropic Is on Track to Turn a Profit Much Faster Than OpenAI (7 minute read)",
      "url": "https://www.wsj.com/tech/ai/openai-anthropic-profitability-e9f5bcd6?st=2AqtKX&reflink=desktopwebshare_permalink&mod=tldr&utm_source=tldrnewsletter",
      "content": "Anthropic expects to break even for the first time in 2028. OpenAI's operating losses for 2028 are forecasted to swell to about $74 billion - it doesn't expect to turn a profit until 2030. OpenAI is investing far more into its chips and data centers to turn OpenAI into a multitrillion-dollar tech giant. The strategy requires near-constant fundraising, which could backfire if markets cool on the technology or its near-term profitability.",
      "summary": "Anthropic expects to break even for the first time in 2028. OpenAI's operating losses for 2028 are forecasted to swell to about $74 billion - it doesn't expect to turn a profit until 2030. OpenAI is investing far more into its chips and data centers to turn OpenAI into a multitrillion-dollar tech giant. The strategy requires near-constant fundraising, which could backfire if markets cool on the technology or its near-term profitability.",
      "publishedAt": "2025-11-11T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 1.0592603233503717,
      "ingestedAt": "2025-12-02T17:25:38.391Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "eb949fe5364740115402a6d34b7af8a6",
      "title": "Altman And Masa Back a 27-Year-Old's Plan to Build a New Bell Labs Ultra (13 minute read)",
      "url": "https://www.corememory.com/p/exclusive-altman-and-masa-back-episteme-louis-andre?utm_source=tldrnewsletter",
      "content": "Louis Andre, a 27-year-old who grew up in Europe, recently revealed a new company backed by Sam Altman called Episteme. Episteme is an effort to attract the world's top scientists and have them work on a wide range of breakthrough products. It will allow scientists to do away with fundraising pressures and grant writing and spend most of their time on research. The company will also help scientists deal with intellectual property concerns, tax issues, hiring, and other day-to-day support functions to help them turn their ideas into blockbuster products.",
      "summary": "Louis Andre, a 27-year-old who grew up in Europe, recently revealed a new company backed by Sam Altman called Episteme. Episteme is an effort to attract the world's top scientists and have them work on a wide range of breakthrough products. It will allow scientists to do away with fundraising pressures and grant writing and spend most of their time on research. The company will also help scientists deal with intellectual property concerns, tax issues, hiring, and other day-to-day support functions to help them turn their ideas into blockbuster products.",
      "publishedAt": "2025-11-11T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 1.271112388020446,
      "ingestedAt": "2025-12-02T17:25:38.391Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "639f57b9dc98fb1968749b33de521f9e",
      "title": "git-rewrite-commits (GitHub Repo)",
      "url": "https://github.com/f/git-rewrite-commits?utm_source=tldrnewsletter",
      "content": "git-rewrite-commits is an AI-powered git commit message writer. It can automatically rewrite entire git commit histories with AI. The tool is perfect for cleaning up commit histories before open-sourcing projects. It can also aid in improving repository maintainability.",
      "summary": "git-rewrite-commits is an AI-powered git commit message writer. It can automatically rewrite entire git commit histories with AI. The tool is perfect for cleaning up commit histories before open-sourcing projects. It can also aid in improving repository maintainability.",
      "publishedAt": "2025-11-11T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 1.0592603233503717,
      "ingestedAt": "2025-12-02T17:25:38.391Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "1a3e1ac48c59039b055238f896cf6947",
      "title": "Meta Ray-Ban Display Review: First Generation Heads-Up Mobile Computing (42 minute read)",
      "url": "https://www.uploadvr.com/meta-ray-ban-display-review/?utm_source=tldrnewsletter",
      "content": "Smartphones have become humanity's second cognitive organ, but using the devices harshly disconnects users from the world around them. The only form factor that seems to have a chance to replace the smartphone is AR glasses. Meta Ray-Ban Display glasses are still highly dependent on mobile devices, and they only provide a small display visible to one eye, but they give users a glimpse into what the future may look like. This article provides a detailed review of the device along with the Meta Neural Band that pairs with it. The technology is still very much a first-generation product, but it shows promise.",
      "summary": "Smartphones have become humanity's second cognitive organ, but using the devices harshly disconnects users from the world around them. The only form factor that seems to have a chance to replace the smartphone is AR glasses. Meta Ray-Ban Display glasses are still highly dependent on mobile devices, and they only provide a small display visible to one eye, but they give users a glimpse into what the future may look like. This article provides a detailed review of the device along with the Meta Neural Band that pairs with it. The technology is still very much a first-generation product, but it shows promise.",
      "publishedAt": "2025-11-11T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 1.271112388020446,
      "ingestedAt": "2025-12-02T17:25:38.391Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "640cad1490b911d04896ff52bd8f829e",
      "title": "Riding in a Chinese Robotaxi Is Pretty Smooth—That's a Problem for Waymo (8 minute read)",
      "url": "https://www.wsj.com/business/autos/china-robotaxi-self-driving-waymo-254ce0a1?st=BCZFqE&reflink=desktopwebshare_permalink&mod=tldr&utm_source=tldrnewsletter",
      "content": "Baidu, Pony AI, and WeRide each have hundreds of robotaxis on the road in China operating commercial paid services without a human safety driver. Their technology and rider experience are generally similar to the driverless taxis operated by Waymo in the US. China's robotaxi fleet is projected to grow to tens of thousands of vehicles by the end of next year. The Chinese companies are testing autonomous vehicles in around half a dozen countries - Waymo is only testing its vehicles in one country outside of the US.",
      "summary": "Baidu, Pony AI, and WeRide each have hundreds of robotaxis on the road in China operating commercial paid services without a human safety driver. Their technology and rider experience are generally similar to the driverless taxis operated by Waymo in the US. China's robotaxi fleet is projected to grow to tens of thousands of vehicles by the end of next year. The Chinese companies are testing autonomous vehicles in around half a dozen countries - Waymo is only testing its vehicles in one country outside of the US.",
      "publishedAt": "2025-11-10T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 1.8738519345195301,
      "ingestedAt": "2025-12-02T17:25:38.392Z",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "testing"
      ]
    },
    {
      "id": "4ec7c2530ab48ba4cadc01cf34187ff3",
      "title": "Genetically Engineered Babies Are Banned. Tech Titans Are Trying to Make One Anyway (20 minute read)",
      "url": "https://www.wsj.com/tech/biotech/genetically-engineered-babies-tech-billionaires-6779efc8?st=PwkEK8&reflink=desktopwebshare_permalink&mod=tldr&utm_source=tldrnewsletter",
      "content": "Preventive, a startup backed by OpenAI CEO Sam Altman and his husband, as well as Coinbase CEO Brian Armstrong, is working toward creating genetically engineered human children. Editing gene embryos with the intention of creating babies is banned in the US and many countries, so the company has been searching for places to experiment where the practice is allowed. Preventive is part of a growing number of startups funded by powerful people in Silicon Valley that are pushing the boundaries of fertility and working to commercialize reproductive genetic technologies.",
      "summary": "Preventive, a startup backed by OpenAI CEO Sam Altman and his husband, as well as Coinbase CEO Brian Armstrong, is working toward creating genetically engineered human children. Editing gene embryos with the intention of creating babies is banned in the US and many countries, so the company has been searching for places to experiment where the practice is allowed. Preventive is part of a growing number of startups funded by powerful people in Silicon Valley that are pushing the boundaries of fertility and working to commercialize reproductive genetic technologies.",
      "publishedAt": "2025-11-10T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 0.9862378602734369,
      "ingestedAt": "2025-12-02T17:25:38.392Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "7ea37385504cdffb522fc77ef74df730",
      "title": "Valdi (GitHub Repo)",
      "url": "https://github.com/Snapchat/Valdi?utm_source=tldrnewsletter",
      "content": "Valdi is a cross-platform UI framework that compiles TypeScript directly to native views on iOS, Android, and macOS. It delivers native performance without sacrificing developer velocity with automatic view recycling, optimized component rendering, an optimized layout engine, and viewport-aware rendering. Valdi eliminates the traditional compile-test-debug cycle that slows native development with instant hot reload, full VSCode debugging, and a familiar syntax. It integrates easily into existing apps and can be scaled as needed.",
      "summary": "Valdi is a cross-platform UI framework that compiles TypeScript directly to native views on iOS, Android, and macOS. It delivers native performance without sacrificing developer velocity with automatic view recycling, optimized component rendering, an optimized layout engine, and viewport-aware rendering. Valdi eliminates the traditional compile-test-debug cycle that slows native development with instant hot reload, full VSCode debugging, and a familiar syntax. It integrates easily into existing apps and can be scaled as needed.",
      "publishedAt": "2025-11-10T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 0.5917427161640622,
      "ingestedAt": "2025-12-02T17:25:38.392Z",
      "tags": [
        "ide"
      ]
    },
    {
      "id": "96062b2305a6dad19b538b9dc257fa26",
      "title": "You Need To Become A Full Stack Person (20 minute read)",
      "url": "https://den.dev/blog/full-stack-person/?utm_source=tldrnewsletter",
      "content": "Work is much messier today than when roles were somewhat rigid and handoffs were clean. Loops are now tighter, and the fast path from idea to impact crosses multiple disciplines. It is now better to have cross-domain fluency along with product sense and engineering craft. AI doesn't lower the cost of choosing the right thing, shaping it well, and operating it in the wild. Models can accelerate execution, but developers still own intent, architecture, and accountability.",
      "summary": "Work is much messier today than when roles were somewhat rigid and handoffs were clean. Loops are now tighter, and the fast path from idea to impact crosses multiple disciplines. It is now better to have cross-domain fluency along with product sense and engineering craft. AI doesn't lower the cost of choosing the right thing, shaping it well, and operating it in the wild. Models can accelerate execution, but developers still own intent, architecture, and accountability.",
      "publishedAt": "2025-11-10T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 1.1834854323281243,
      "ingestedAt": "2025-12-02T17:25:38.392Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "8b47582e6cab2a0e4d4839448f5dc2eb",
      "title": "Inside Cursor (38 minute read)",
      "url": "https://joincolossus.com/article/inside-cursor/?utm_source=tldrnewsletter",
      "content": "There's a lot of mystique about Cursor. This article takes a look inside the company from the perspective of a team member who has been there for two months. The company's leaders are enthusiastic about establishing a new playbook for company building. The employees are focused on the mission - the company has made them wealthy, but there is zero chatter from employees about getting rich.",
      "summary": "There's a lot of mystique about Cursor. This article takes a look inside the company from the perspective of a team member who has been there for two months. The company's leaders are enthusiastic about establishing a new playbook for company building. The employees are focused on the mission - the company has made them wealthy, but there is zero chatter from employees about getting rich.",
      "publishedAt": "2025-11-10T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 0.5917427161640622,
      "ingestedAt": "2025-12-02T17:25:38.392Z",
      "tags": [
        "ide"
      ]
    },
    {
      "id": "2b625c6d6ff43bd6c75ac31365f8de18",
      "title": "External Secrets (GitHub Repo)",
      "url": "https://github.com/external-secrets/external-secrets?utm_source=tldrnewsletter",
      "content": "The External Secrets Operator is a Kubernetes operator that integrates external secret management systems, reads information from external APIs, and automatically injects the values into a Kubernetes Secret.",
      "summary": "The External Secrets Operator is a Kubernetes operator that integrates external secret management systems, reads information from external APIs, and automatically injects the values into a Kubernetes Secret.",
      "publishedAt": "2025-11-10T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 0.39449514410937475,
      "ingestedAt": "2025-12-02T17:25:38.392Z",
      "tags": []
    },
    {
      "id": "53831353c815c26716d4d86df2db6dba",
      "title": "UnisonDB (GitHub Repo)",
      "url": "https://github.com/ankur-anand/unisondb?utm_source=tldrnewsletter",
      "content": "UnisonDB is a database designed specifically for Edge AI and Edge Computing that enables near-instant fan-out replication across hundreds of nodes while preserving strong consistency and durability.",
      "summary": "UnisonDB is a database designed specifically for Edge AI and Edge Computing that enables near-instant fan-out replication across hundreds of nodes while preserving strong consistency and durability.",
      "publishedAt": "2025-11-10T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 0.9862378602734369,
      "ingestedAt": "2025-12-02T17:25:38.392Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "9952073cf888a10cd5d55fb14eaa4e84",
      "title": "You Should Write An Agent (13 minute read)",
      "url": "https://fly.io/blog/everyone-write-an-agent/?utm_source=tldrnewsletter",
      "content": "Some concepts are easy to grasp in the abstract, but others you really need to try before you understand them. Regardless of whether or not LLM agents are snake oil or not, it's worth learning how to make one - startups are raising millions building agents at a time when nobody really knows anything yet. It's easy to get started. This post walks readers through how to create an agent.",
      "summary": "Some concepts are easy to grasp in the abstract, but others you really need to try before you understand them. Regardless of whether or not LLM agents are snake oil or not, it's worth learning how to make one - startups are raising millions building agents at a time when nobody really knows anything yet. It's easy to get started. This post walks readers through how to create an agent.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "feature_update",
      "score": 1.4328181436315623,
      "ingestedAt": "2025-12-02T17:25:38.392Z",
      "tags": [
        "agents"
      ]
    },
    {
      "id": "550a71824c32d770d1218caf7d6239cd",
      "title": "Dead framework theory (17 minute read)",
      "url": "https://aifoc.us/dead-framework-theory/?utm_source=tldrnewsletter",
      "content": "As large language models become more dominant, new frameworks will find it hard to gain traction because the models won't have enough training data about them. The industry needs to innovate and build new frameworks, libraries, and platform features to push the web forward and create competition. Developers need to have clear strategies to get their work into the LLM training corpus, system prompts, and developer minds. If the industry continues its current focus on maintainability and developer experience, we'll end up in a world where the web is built by LLMs using only what is entrenched in the training data.",
      "summary": "As large language models become more dominant, new frameworks will find it hard to gain traction because the models won't have enough training data about them. The industry needs to innovate and build new frameworks, libraries, and platform features to push the web forward and create competition. Developers need to have clear strategies to get their work into the LLM training corpus, system prompts, and developer minds. If the industry continues its current focus on maintainability and developer experience, we'll end up in a world where the web is built by LLMs using only what is entrenched in the training data.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 0.7960100797953124,
      "ingestedAt": "2025-12-02T17:25:38.392Z",
      "tags": [
        "code_review"
      ]
    },
    {
      "id": "70804ac7f06fffcdbed0547550c7283d",
      "title": "Game design is simple, actually (25 minute read)",
      "url": "https://www.raphkoster.com/2025/11/03/game-design-is-simple-actually/?utm_source=tldrnewsletter",
      "content": "This post presents a deconstructive view on how games are designed. It presents twelve aspects of game design to help readers get better at making games of any type. Each of the topics is deep, and there are plenty of links for further reading. The fun of making games is always right outside the edge of what the designers know.",
      "summary": "This post presents a deconstructive view on how games are designed. It presents twelve aspects of game design to help readers get better at making games of any type. Each of the topics is deep, and there are plenty of links for further reading. The fun of making games is always right outside the edge of what the designers know.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 0.9552120957543748,
      "ingestedAt": "2025-12-02T17:25:38.392Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "253597e7e73c428eb248780c8d7f71ac",
      "title": "WhatsApp finally lets you message people on other apps (3 minute read)",
      "url": "https://www.androidpolice.com/whatsapp-support-for-third-party-apps/?utm_source=tldrnewsletter",
      "content": "The new feature will likely be locked to certain regions, so it probably won't be made available to those outside of Europe.",
      "summary": "The new feature will likely be locked to certain regions, so it probably won't be made available to those outside of Europe.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 0.9552120957543748,
      "ingestedAt": "2025-12-02T17:25:38.392Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "6c08aba98e87abc0eae035db3d29868c",
      "title": "How I stopped worrying and learned to love the easy fix (4 minute read)",
      "url": "https://tn1ck.com/blog/how-i-stopped-worrying-and-learned-to-love-the-easy-fix?utm_source=tldrnewsletter",
      "content": "An easy fix is a pragmatic step forward that delivers value immediately - it can always be refactored later.",
      "summary": "An easy fix is a pragmatic step forward that delivers value immediately - it can always be refactored later.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 1.0348131037339061,
      "ingestedAt": "2025-12-02T17:25:38.392Z",
      "tags": [
        "code_review",
        "retrieval"
      ]
    },
    {
      "id": "9a092f9bc29fcb8e20d1ffee5ea5ac26",
      "title": "Tesla to begin Cybercab production in April, Musk claims (4 minute read)",
      "url": "https://techcrunch.com/2025/11/06/tesla-to-begin-cybercab-production-in-april-musk-claims/?utm_source=tldrnewsletter",
      "content": "The Cybercab, which won't have pedals, a steering wheel, or side mirrors, will be starting production in April at Tesla's factory in Austin, Texas.",
      "summary": "The Cybercab, which won't have pedals, a steering wheel, or side mirrors, will be starting production in April at Tesla's factory in Austin, Texas.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 0.9552120957543748,
      "ingestedAt": "2025-12-02T17:25:38.392Z",
      "tags": [
        "code_review",
        "ide"
      ]
    },
    {
      "id": "158c1fe092815a223b6428ffd98dadf7",
      "title": "Microsoft Lays Out Ambitious AI Vision, Free From OpenAI (5 minute read)",
      "url": "https://www.wsj.com/tech/ai/microsoft-lays-out-ambitious-ai-vision-free-from-openai-297652ff?st=QXB65W&reflink=desktopwebshare_permalink&mod=tldr&utm_source=tldrnewsletter",
      "content": "Microsoft's new MAI Superintelligence Team, led by Mustafa Suleyman, will put human interests and guardrails first, creating systems that are aligned to human values by default.",
      "summary": "Microsoft's new MAI Superintelligence Team, led by Mustafa Suleyman, will put human interests and guardrails first, creating systems that are aligned to human values by default.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "score": 0.31840403165489406,
      "ingestedAt": "2025-12-02T17:25:38.393Z",
      "tags": []
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09f1388a",
      "title": "Delty (YC X25) Is Hiring",
      "url": "https://www.ycombinator.com/companies/delty/jobs/aPWMaiq-full-stack-software-engineer",
      "content": "<p><img src=\"https://bookface-images.s3.amazonaws.com/logos/9ed8722c9e1f6e49ec12fb5e3887f8f622e06921.png?1746945557\" alt=\"9ed8722c9e1f6e49ec12fb5e3887f8f622e06921\"></p><p>Article URL: <a href=\"https://www.ycombinator.com/companies/delty/jobs/aPWMaiq-full-stack-software-engineer\">https://www.ycombinator.com/companies/delty/jobs/aPWMaiq-full-stack-software-engineer</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46126822\">https://news.ycombinator.com/item?id=46126822</a></p>  \n<p>Points: 0</p>  \n<p># Comments: 0</p>",
      "summary": "Article URL: https://www.ycombinator.com/companies/delty/jobs/aPWMaiq-full-stack-software-engineer  \nComments URL: https://news.ycombinator.com/item?id=46126822  \nPoints: 0  \n# Comments: 0",
      "publishedAt": "2025-12-02T21:00:59.000Z",
      "author": "lalitkundu",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4965764253103184,
      "ingestedAt": "2025-12-02T21:47:02.926Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09f018d9",
      "title": "Anthropic Acquires Bun in First Acquisition as It Looks to Grow Claude Code - ADWEEK",
      "url": "https://news.google.com/rss/articles/CBMickFVX3lxTE01Y1Q4aEQtMFJ5RmVSOVZsUzBpbm9nd29lQnVWeWRVc01JWFR2dmNCMWtiN1MwbDJfRWduRmloZlM5blg5SmdPdFhma1FXeDdmUER4N3AwdkRoNUZZUkNrQTlMSGVWYVVHMU1kRHlDbWRPdw?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMickFVX3lxTE01Y1Q4aEQtMFJ5RmVSOVZsUzBpbm9nd29lQnVWeWRVc01JWFR2dmNCMWtiN1MwbDJfRWduRmloZlM5blg5SmdPdFhma1FXeDdmUER4N3AwdkRoNUZZUkNrQTlMSGVWYVVHMU1kRHlDbWRPdw?oc=5\">Anthropic Acquires Bun in First Acquisition as It Looks to Grow Claude Code</a>  ADWEEK",
      "summary": "Anthropic Acquires Bun in First Acquisition as It Looks to Grow Claude Code  ADWEEK",
      "publishedAt": "2025-12-02T20:19:02.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "funding_mna",
      "score": 3.4847528475067175,
      "ingestedAt": "2025-12-02T21:47:02.927Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09f018dc",
      "title": "Anthropic partners with Rwandan Government and ALX to bring AI education to hundreds of thousands of learners across Africa - Anthropic",
      "url": "https://news.google.com/rss/articles/CBMif0FVX3lxTE1LWENFREJMbGtWOG94WVZIYm92R3U2VjBWeVVzS2N6czh3bU9BMVhMQUdNenQxdUQ1RktVZzVzbUNLU0loMmFQTndnSWh5aXBzMzJCaFpJUm40MnRjM1JfOHR6aUxMem91VTJYWnAtYWFCNUtfRzNKQ0pBRm1WQzQ?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMif0FVX3lxTE1LWENFREJMbGtWOG94WVZIYm92R3U2VjBWeVVzS2N6czh3bU9BMVhMQUdNenQxdUQ1RktVZzVzbUNLU0loMmFQTndnSWh5aXBzMzJCaFpJUm40MnRjM1JfOHR6aUxMem91VTJYWnAtYWFCNUtfRzNKQ0pBRm1WQzQ?oc=5\">Anthropic partners with Rwandan Government and ALX to bring AI education to hundreds of thousands of learners across Africa</a>  Anthropic",
      "summary": "Anthropic partners with Rwandan Government and ALX to bring AI education to hundreds of thousands of learners across Africa  Anthropic",
      "publishedAt": "2025-11-17T08:00:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 0.4931274058920126,
      "ingestedAt": "2025-12-02T21:47:02.927Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ef3aa8",
      "title": "Comment on The ultimate guide on how to run an effective meeting by Mastering Team Management: Unlock Your Potential",
      "url": "https://www.atlassian.com/blog/loom/how-to-run-an-effective-meeting#comment-24921",
      "content": "<p><img src=\"https://www.atlassian.com/blog/wp-content/uploads/2025/07/loom-blog-hero.png\" alt=\"loom-blog-hero.png\"></p><p>[…] Effectiveness: Review Atlassian’s effective meeting guide to design check-ins that surface issues quickly and end with clear action […]</p>",
      "summary": "[…] Effectiveness: Review Atlassian’s effective meeting guide to design check-ins that surface issues quickly and end with clear action […]",
      "publishedAt": "2025-12-02T20:36:42.000Z",
      "author": "Mastering Team Management: Unlock Your Potential",
      "source": "rss",
      "feedName": "Comments for Atlassian Blog Work Life",
      "sourceType": "engineering_blog",
      "contentType": "thought_leadership",
      "score": 4.48432451247078,
      "ingestedAt": "2025-12-02T21:47:02.927Z",
      "tags": [
        "ide",
        "Tech Company Blogs"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ec639b",
      "title": "Paged Out",
      "url": "https://pagedout.institute/",
      "content": "<p><img src=\"https://pagedout.institute/static/img/issue_7_cover_small.png\" alt=\"issue_7_cover_small.png\"></p><p>Article URL: <a href=\"https://pagedout.institute\">https://pagedout.institute</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46126217\">https://news.ycombinator.com/item?id=46126217</a></p>  \n<p>Points: 26</p>  \n<p># Comments: 2</p>",
      "summary": "Article URL: https://pagedout.institute  \nComments URL: https://news.ycombinator.com/item?id=46126217  \nPoints: 26  \n# Comments: 2",
      "publishedAt": "2025-12-02T20:14:20.000Z",
      "author": "varjag",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4931173652264826,
      "ingestedAt": "2025-12-02T21:47:02.929Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ec63a2",
      "title": "Free static site generator for small restaurants and cafes",
      "url": "https://lite.localcafe.org/",
      "content": "<p><img src=\"https://lite.localcafe.org/images/pasta.jpg\" alt=\"pasta.jpg\"></p><p>Article URL: <a href=\"https://lite.localcafe.org/\">https://lite.localcafe.org/</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46126141\">https://news.ycombinator.com/item?id=46126141</a></p>  \n<p>Points: 14</p>  \n<p># Comments: 3</p>",
      "summary": "Article URL: https://lite.localcafe.org/  \nComments URL: https://news.ycombinator.com/item?id=46126141  \nPoints: 14  \n# Comments: 3",
      "publishedAt": "2025-12-02T20:08:55.000Z",
      "author": "fullstacking",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4927162425757312,
      "ingestedAt": "2025-12-02T21:47:02.929Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09eb6be0",
      "title": "Comment on Triangles at work: GPU rendering shapes and connectors in Confluence whiteboards by binance US-registrera",
      "url": "https://www.atlassian.com/blog/atlassian-engineering/gpu-rendering-shapes-and-connectors-in-confluence-whiteboards#comment-24920",
      "content": "<p><img src=\"https://www.atlassian.com/blog/wp-content/uploads/2024/08/render.jpg\" alt=\"render.jpg\"></p><p>Can you be more specific about the content of your article? After reading it, I still have some doubts. Hope you can help me. <a href=\"https://www.binance.com/hu/register?ref=IQY5TET4\">https://www.binance.com/hu/register?ref=IQY5TET4</a></p>",
      "summary": "Can you be more specific about the content of your article? After reading it, I still have some doubts. Hope you can help me. https://www.binance.com/hu/register?ref=IQY5TET4",
      "publishedAt": "2025-12-02T20:02:17.000Z",
      "author": "binance US-registrera",
      "source": "rss",
      "feedName": "Comments for Atlassian Blog Work Life",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "score": 3.979267115705867,
      "ingestedAt": "2025-12-02T21:47:02.929Z",
      "tags": [
        "Tech Company Blogs"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09ea6651",
      "title": "Turning Donated Dishes Into Culinary Careers",
      "url": "https://www.ebayinc.com/stories/news/turning-donated-dishes-into-culinary-careers/",
      "content": "<div style=\"margin-bottom:10px;\"><img src=\"https://static.ebayinc.com/static/assets/Uploads/Video/Thumbs/_resampled/FitWzIwMCwxMTNd/1142241008.jpg?fs=774b61b08030c0b3\" width=\"200\" height=\"113\" alt=\"Turning Donated Dishes Into Culinary Careers\"></div><div>Through eBay and the Kitchens for Good store in San Diego, Mary Scafidi transforms kitchenware into community impact.</div>",
      "summary": "Through eBay and the Kitchens for Good store in San Diego, Mary Scafidi transforms kitchenware into community impact.",
      "publishedAt": "2025-12-02T08:00:00.000Z",
      "author": "Nicole Grant Kriege, eBay News Team",
      "source": "rss",
      "feedName": "eBay Inc. News Feed",
      "sourceType": "general",
      "contentType": "general",
      "score": 2.3995146434721955,
      "ingestedAt": "2025-12-02T21:47:02.929Z",
      "tags": [
        "ide",
        "Tech Company Blogs"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e8854d",
      "title": "Microsoft Brings Anthropic's Claude Opus 4.5 to Foundry Preview - Visual Studio Magazine",
      "url": "https://news.google.com/rss/articles/CBMivAFBVV95cUxPUzlQY0JWVEFod0NJNllXMjB5Uzg0SWRKazlrb182dEFIMFBjSDN3TjBXcXd5MURRcW5QSklWVE1MNWtqVjZKN1ZqdnVKdzcxMHRzSVdRM0JGMzJZLWk2SV9wOUlLVFR6SHhZNi03TXpfS21nY2NQZnJ3UzZQb01UX1NOUWhVd1lucjFobGJSaWRSbmpJclJVRVRaNlpjTjNMa3gwXy1WZlhFZVhmSTRpT2pKZWlGRm9aX3VtWg?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMivAFBVV95cUxPUzlQY0JWVEFod0NJNllXMjB5Uzg0SWRKazlrb182dEFIMFBjSDN3TjBXcXd5MURRcW5QSklWVE1MNWtqVjZKN1ZqdnVKdzcxMHRzSVdRM0JGMzJZLWk2SV9wOUlLVFR6SHhZNi03TXpfS21nY2NQZnJ3UzZQb01UX1NOUWhVd1lucjFobGJSaWRSbmpJclJVRVRaNlpjTjNMa3gwXy1WZlhFZVhmSTRpT2pKZWlGRm9aX3VtWg?oc=5\">Microsoft Brings Anthropic's Claude Opus 4.5 to Foundry Preview</a>  Visual Studio Magazine",
      "summary": "Microsoft Brings Anthropic's Claude Opus 4.5 to Foundry Preview  Visual Studio Magazine",
      "publishedAt": "2025-12-02T19:47:05.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.473301494964331,
      "ingestedAt": "2025-12-02T21:47:02.932Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e8854f",
      "title": "Anthropic Confirms ‘Soul Document’ Used to Train Claude 4.5 Opus Character - WinBuzzer",
      "url": "https://news.google.com/rss/articles/CBMiswFBVV95cUxOZTc5WEFCVmFrOWVESzNlVVdlM2NFNHpTUFZRdjBjbzRRVW1VSk9ZU190aUU4UVp1S0J6eHg4Qkg3Ulp4NkFmQ05JUGZNQlRneHZ2VTBzeTJKTHNuOEVIVGdDZU5yU3ZCRC1QWlVPaVNKelBTeDczS0dBcnBaNkdzV091cUtDWnk1TzllOHYyak5aVXNmQWhYeUh4Vl9ERHhfYU4xVU1wME5XZmRBQVJkUGJTTQ?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMiswFBVV95cUxOZTc5WEFCVmFrOWVESzNlVVdlM2NFNHpTUFZRdjBjbzRRVW1VSk9ZU190aUU4UVp1S0J6eHg4Qkg3Ulp4NkFmQ05JUGZNQlRneHZ2VTBzeTJKTHNuOEVIVGdDZU5yU3ZCRC1QWlVPaVNKelBTeDczS0dBcnBaNkdzV091cUtDWnk1TzllOHYyak5aVXNmQWhYeUh4Vl9ERHhfYU4xVU1wME5XZmRBQVJkUGJTTQ?oc=5\">Anthropic Confirms ‘Soul Document’ Used to Train Claude 4.5 Opus Character</a>  WinBuzzer",
      "summary": "Anthropic Confirms ‘Soul Document’ Used to Train Claude 4.5 Opus Character  WinBuzzer",
      "publishedAt": "2025-12-02T19:32:59.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4900579801460245,
      "ingestedAt": "2025-12-02T21:47:02.932Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e88552",
      "title": "Is Claude really writing 90% of the code inside the company? Anthropic's own research suggests otherwise - Fortune",
      "url": "https://news.google.com/rss/articles/CBMi1wFBVV95cUxPQmp5ZFlVTXdSVmkybUdSdTE5NlNzVUFsUXVxOVpqR21SWVVPMW9CLW4xNmRnRGhOM1VGM1JNdy1BTkdGSWVnMmZkUFk5aXJsbUN2dHpsaUpOQmE4SmNBQ291ZU5QS081Qlk2TkZVX3hETFlzWklvYXFoOUZhdlFJZk5mWVplREFaQTNmaWI0NXBueFhvTHlLbWEwSXVMcEhudlRsb2pSMmtvTFFCUWw2SnE0NDk0ME5NZXVIWlJxdEpOalVsRFhZNWxzbVo3SUl0eVZnZEFmZw?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMi1wFBVV95cUxPQmp5ZFlVTXdSVmkybUdSdTE5NlNzVUFsUXVxOVpqR21SWVVPMW9CLW4xNmRnRGhOM1VGM1JNdy1BTkdGSWVnMmZkUFk5aXJsbUN2dHpsaUpOQmE4SmNBQ291ZU5QS081Qlk2TkZVX3hETFlzWklvYXFoOUZhdlFJZk5mWVplREFaQTNmaWI0NXBueFhvTHlLbWEwSXVMcEhudlRsb2pSMmtvTFFCUWw2SnE0NDk0ME5NZXVIWlJxdEpOalVsRFhZNWxzbVo3SUl0eVZnZEFmZw?oc=5\">Is Claude really writing 90% of the code inside the company? Anthropic's own research suggests otherwise</a>  Fortune",
      "summary": "Is Claude really writing 90% of the code inside the company? Anthropic's own research suggests otherwise  Fortune",
      "publishedAt": "2025-12-02T19:30:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 2.4830624891702855,
      "ingestedAt": "2025-12-02T21:47:02.932Z",
      "tags": [
        "ide",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e88554",
      "title": "How AI Is Transforming Work at Anthropic - Anthropic",
      "url": "https://news.google.com/rss/articles/CBMigAFBVV95cUxQZFBSaEZtOG1ieHpOdldrNVJEeGRCMEtJVmpNU1hOMHVNV1NldWJ1T3RuQlhTZXpkckk2akpUWFhqbGtabXRURFJiNDR4aG9YUjhUSzFadkZaYUJPNElQcDR4SkU2bHIyQkg3VDdmMWZ0eFM5ak52MThpcklEdVNfVg?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMigAFBVV95cUxQZFBSaEZtOG1ieHpOdldrNVJEeGRCMEtJVmpNU1hOMHVNV1NldWJ1T3RuQlhTZXpkckk2akpUWFhqbGtabXRURFJiNDR4aG9YUjhUSzFadkZaYUJPNElQcDR4SkU2bHIyQkg3VDdmMWZ0eFM5ak52MThpcklEdVNfVg?oc=5\">How AI Is Transforming Work at Anthropic</a>  Anthropic",
      "summary": "How AI Is Transforming Work at Anthropic  Anthropic",
      "publishedAt": "2025-12-02T19:22:29.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4892821103483307,
      "ingestedAt": "2025-12-02T21:47:02.932Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e88555",
      "title": "Anthropic Reveals Shocking AI Agents Risk For Crypto Security - BeInCrypto",
      "url": "https://news.google.com/rss/articles/CBMihgFBVV95cUxOMXJDcUR3SVJ2VWZNcll6cmhJaHowdU5IUGNkdDluQW1YcXRpcXFWX2x5MEh4bkdBVTQ0TDlvMU5KMEpybTBxT0FpSmZvajNRLWIyZlNDQUJvMmFFQzB1cnVwSFp5RkNjeW4tTTliVkdUSlR4RWE1Um5seU5VWDdLNUZHa1hRZw?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMihgFBVV95cUxOMXJDcUR3SVJ2VWZNcll6cmhJaHowdU5IUGNkdDluQW1YcXRpcXFWX2x5MEh4bkdBVTQ0TDlvMU5KMEpybTBxT0FpSmZvajNRLWIyZlNDQUJvMmFFQzB1cnVwSFp5RkNjeW4tTTliVkdUSlR4RWE1Um5seU5VWDdLNUZHa1hRZw?oc=5\">Anthropic Reveals Shocking AI Agents Risk For Crypto Security</a>  BeInCrypto",
      "summary": "Anthropic Reveals Shocking AI Agents Risk For Crypto Security  BeInCrypto",
      "publishedAt": "2025-12-02T19:07:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 10.416971231232269,
      "ingestedAt": "2025-12-02T21:47:02.932Z",
      "tags": [
        "agents",
        "Coding Agent Product Updates",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e8855b",
      "title": "New Claude for Nonprofits offers free training and reduced costs - Mashable",
      "url": "https://news.google.com/rss/articles/CBMiY0FVX3lxTE1FTzFOTDVqTDRfRG5wcUg3R1Jja3g4ZjJNaEdGSk5QUzJSNDVsYl80MGhiQ1F5Q1pDdm53V2hLUWxXemxVdDZGank3UEtad1owa1RNTmU3RDdHa2NVZXJjZ0hiTQ?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMiY0FVX3lxTE1FTzFOTDVqTDRfRG5wcUg3R1Jja3g4ZjJNaEdGSk5QUzJSNDVsYl80MGhiQ1F5Q1pDdm53V2hLUWxXemxVdDZGank3UEtad1owa1RNTmU3RDdHa2NVZXJjZ0hiTQ?oc=5\">New Claude for Nonprofits offers free training and reduced costs</a>  Mashable",
      "summary": "New Claude for Nonprofits offers free training and reduced costs  Mashable",
      "publishedAt": "2025-12-02T18:56:14.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.462032608598387,
      "ingestedAt": "2025-12-02T21:47:02.932Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e8855f",
      "title": "Microsoft, Nvidia to invest in Anthropic as Claude maker commits $30 billion to Azure - Yahoo Finance",
      "url": "https://news.google.com/rss/articles/CBMijAFBVV95cUxQQnR5U18zdTVwNDFwT3NmVXpqTHVfdnFtR1pKUk5odmRBRVJMLU53aXJmYVB5UE5CamFaQjR1YTY2bWJvVTdaLWIzeC1SdUVGejRyZTFTMUstTlhKdzN4SmxLcEQycUx4R2w5NmFWRnhUMFo1bWpUWG45STdzODJQcGY5SE5IWHJGNWJycA?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMijAFBVV95cUxQQnR5U18zdTVwNDFwT3NmVXpqTHVfdnFtR1pKUk5odmRBRVJMLU53aXJmYVB5UE5CamFaQjR1YTY2bWJvVTdaLWIzeC1SdUVGejRyZTFTMUstTlhKdzN4SmxLcEQycUx4R2w5NmFWRnhUMFo1bWpUWG45STdzODJQcGY5SE5IWHJGNWJycA?oc=5\">Microsoft, Nvidia to invest in Anthropic as Claude maker commits $30 billion to Azure</a>  Yahoo Finance",
      "summary": "Microsoft, Nvidia to invest in Anthropic as Claude maker commits $30 billion to Azure  Yahoo Finance",
      "publishedAt": "2025-11-18T08:00:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 0.529639262360357,
      "ingestedAt": "2025-12-02T21:47:02.932Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e31c5f",
      "title": "Claude 4.5 Opus' Soul Document",
      "url": "https://simonwillison.net/2025/Dec/2/claude-soul-document/",
      "content": "<p>Article URL: <a href=\"https://simonwillison.net/2025/Dec/2/claude-soul-document/\">https://simonwillison.net/2025/Dec/2/claude-soul-document/</a></p> \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46125184\">https://news.ycombinator.com/item?id=46125184</a></p> \n<p>Points: 132</p> \n<p># Comments: 67</p>",
      "summary": "Article URL: https://simonwillison.net/2025/Dec/2/claude-soul-document/ \nComments URL: https://news.ycombinator.com/item?id=46125184 \nPoints: 132 \n# Comments: 67",
      "publishedAt": "2025-12-02T19:05:54.000Z",
      "author": "the-needful",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4880575490269383,
      "ingestedAt": "2025-12-02T21:47:02.934Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e31c61",
      "title": "Cursed circuits: charge pump voltage halver",
      "url": "https://lcamtuf.substack.com/p/cursed-circuits-charge-pump-voltage",
      "content": "<p><img src=\"https://substackcdn.com/image/fetch/$s_!qOPC!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f75a889-0f10-4d1c-8b00-1a6267abfbe9_1892x762.png\" alt=\"https%3A%2F%2Fsubstack-post-media.s3.ama\"></p><p>Article URL: <a href=\"https://lcamtuf.substack.com/p/cursed-circuits-charge-pump-voltage\">https://lcamtuf.substack.com/p/cursed-circuits-charge-pump-voltage</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46124892\">https://news.ycombinator.com/item?id=46124892</a></p>  \n<p>Points: 30</p>  \n<p># Comments: 3</p>",
      "summary": "Article URL: https://lcamtuf.substack.com/p/cursed-circuits-charge-pump-voltage  \nComments URL: https://news.ycombinator.com/item?id=46124892  \nPoints: 30  \n# Comments: 3",
      "publishedAt": "2025-12-02T18:47:53.000Z",
      "author": "surprisetalk",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.460184870198334,
      "ingestedAt": "2025-12-02T21:47:02.934Z",
      "tags": [
        "code_review",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e3159a",
      "title": "25 reasons why email marketing is important in 2026",
      "url": "https://www.twilio.com/en-us/blog/insights/why-email-marketing-is-important",
      "content": "<p><img src=\"https://www.twilio.com/content/dam/twilio-com/global/en/blog/insights/card-images/generic-card-images/card-insights-fill-shapes-3.png\" alt=\"card-insights-fill-shapes-3.png\"></p>Email marketing delivers a 3,600% ROI and direct customer access. That's just the start, though. Here are 25 reasons why email marketing is important in 2026.",
      "summary": "Email marketing delivers a 3,600% ROI and direct customer access. That's just the start, though. Here are 25 reasons why email marketing is important in 2026.",
      "publishedAt": "2025-12-01T00:00:00.000Z",
      "author": "Jesse Sumrak",
      "source": "rss",
      "feedName": "Twilio Cloud Communications Blog",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "score": 3.490454702195489,
      "ingestedAt": "2025-12-02T21:47:02.934Z",
      "tags": [
        "Tech Company Blogs"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e3159c",
      "title": "Deploying AI at Scale Without Sacrificing Security",
      "url": "https://www.twilio.com/en-us/blog/insights/scaling-ai-securely",
      "content": "<p><img src=\"https://www.twilio.com/content/dam/twilio-com/global/en/blog/insights/card-images/card-insights-login-phone-verification-1.png\" alt=\"card-insights-login-phone-verification-1\"></p>Deploying AI securely isn’t a single decision. It’s a discipline. It’s a system that depends on three interconnected elements: people, process, and practice.",
      "summary": "Deploying AI securely isn’t a single decision. It’s a discipline. It’s a system that depends on three interconnected elements: people, process, and practice.",
      "publishedAt": "2025-12-01T00:00:00.000Z",
      "author": "Zachary Hanif",
      "source": "rss",
      "feedName": "Twilio Cloud Communications Blog",
      "sourceType": "engineering_blog",
      "contentType": "security_incident",
      "score": 10.471364106586467,
      "ingestedAt": "2025-12-02T21:47:02.934Z",
      "tags": [
        "code_review",
        "Tech Company Blogs"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e3159e",
      "title": "The Ins and Outs of DMARC Monitoring in 2026",
      "url": "https://www.twilio.com/en-us/blog/insights/dmarc-monitoring",
      "content": "<p><img src=\"https://www.twilio.com/content/dam/twilio-com/global/en/blog/insights/card-images/generic-card-images/card-insights-generic-logo-3.png\" alt=\"card-insights-generic-logo-3.png\"></p>Learn how DMARC monitoring helps you identify legitimate senders, block unauthorized email, and reach enforcement without disrupting your email program.",
      "summary": "Learn how DMARC monitoring helps you identify legitimate senders, block unauthorized email, and reach enforcement without disrupting your email program.",
      "publishedAt": "2025-11-28T00:00:00.000Z",
      "author": "Denis O'Sullivan",
      "source": "rss",
      "feedName": "Twilio Cloud Communications Blog",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "score": 6.338717854335878,
      "ingestedAt": "2025-12-02T21:47:02.934Z",
      "tags": [
        "code_review",
        "ide",
        "observability",
        "Tech Company Blogs"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e315a0",
      "title": "20 email newsletter examples that don't suck in 2026",
      "url": "https://www.twilio.com/en-us/blog/insights/12-must-open-email-newsletter-examples-to-learn-from",
      "content": "<p><img src=\"https://www.twilio.com/content/dam/twilio-com/global/en/blog/insights/card-images/generic-card-images/card-insights-fill-shapes-1.png\" alt=\"card-insights-fill-shapes-1.png\"></p>Stop sending boring newsletters. These 20 email examples prove that newsletters can be engaging, valuable, and worth opening every single time.",
      "summary": "Stop sending boring newsletters. These 20 email examples prove that newsletters can be engaging, valuable, and worth opening every single time.",
      "publishedAt": "2025-11-25T00:00:00.000Z",
      "author": "Jesse Sumrak",
      "source": "rss",
      "feedName": "Twilio Cloud Communications Blog",
      "sourceType": "engineering_blog",
      "contentType": "general",
      "score": 3.979182412717874,
      "ingestedAt": "2025-12-02T21:47:02.934Z",
      "tags": [
        "code_review",
        "Tech Company Blogs"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e315a3",
      "title": "How much does call center outsourcing cost in 2026?",
      "url": "https://www.twilio.com/en-us/blog/insights/cloud-comms/call-center-services-pricing",
      "content": "<p><img src=\"https://www.twilio.com/content/dam/twilio-com/global/en/blog/insights/card-images/card-insights-girl-writing-and-thinking-1.png\" alt=\"card-insights-girl-writing-and-thinking-\"></p>Call center outsourcing costs depend on whether you need inbound or outbound support, agent location, and call volume. Here's the pricing breakdown.",
      "summary": "Call center outsourcing costs depend on whether you need inbound or outbound support, agent location, and call volume. Here's the pricing breakdown.",
      "publishedAt": "2025-11-22T00:00:00.000Z",
      "author": "Jesse Sumrak",
      "source": "rss",
      "feedName": "Twilio Cloud Communications Blog",
      "sourceType": "engineering_blog",
      "contentType": "pricing_business",
      "score": 5.964527667198878,
      "ingestedAt": "2025-12-02T21:47:02.934Z",
      "tags": [
        "code_review",
        "agents",
        "Tech Company Blogs",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e21e99",
      "title": "Secret scanning updates — November 2025",
      "url": "https://github.blog/changelog/2025-12-02-secret-scanning-updates-november-2025",
      "content": "<p><img src=\"https://github.blog/wp-content/themes/github-2021-child/dist/img/social-v3-improvements.jpg\" alt=\"social-v3-improvements.jpg\"></p>  \n<p>GitHub secret scanning continually adds support for new secret types. The following updates were made during the month of November.</p>  \n<ul>  \n<li><strong>New provider patterns</strong>: Secret scanning added 24 new secret types from providers including Azure, Databricks, Microsoft, Paddle, PostHog, and more.</li>  \n<li><strong>Improved private key detection</strong>: New patterns for Elliptic Curve and generic PKCS#8 private keys, plus improved detection of escaped newlines.</li>  \n<li><strong>Extended metadata</strong>: Discord <code>discord_bot_token</code> now supports extended metadata checks.</li>  \n<li><strong>Validity checks:</strong> Validation for AWS Access Key IDs has been improved.</li>  \n<li><strong>Unlisted gists</strong>: Secrets in unlisted GitHub gists are now reported to secret scanning partners.</li>  \n</ul>  \n<h2><a href=\"https://github.blog/#new-patterns-added\">New patterns added<span></span></a></h2>  \n<p>We added the following new patterns this month. Secret scanning automatically detects any secrets matching these patterns in your repositories.</p>  \n<div><table>  \n<thead>  \n<tr>  \n<th>Provider</th>  \n<th>Secret type</th>  \n<th>Partner</th>  \n<th>User</th>  \n<th>Push protection</th>  \n</tr>  \n</thead>  \n<tbody>  \n<tr>  \n<td>Azure</td>  \n<td><code>azure_immersive_reader_key</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Azure</td>  \n<td><code>azure_logic_apps_url</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>crates.io</td>  \n<td><code>cratesio_api_token</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Databricks</td>  \n<td><code>databricks_account_session_token</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Databricks</td>  \n<td><code>databricks_federated_account_session_token</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Databricks</td>  \n<td><code>databricks_oauth_code</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Databricks</td>  \n<td><code>databricks_oauth_refresh_token</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Databricks</td>  \n<td><code>databricks_oauth_secret_token</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Databricks</td>  \n<td><code>databricks_oauth_single_use_refresh_token_child</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Databricks</td>  \n<td><code>databricks_oauth_single_use_refresh_token_parent</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Databricks</td>  \n<td><code>databricks_scoped_api_token</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Databricks</td>  \n<td><code>databricks_scoped_internal_token</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Databricks</td>  \n<td><code>databricks_token</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Databricks</td>  \n<td><code>databricks_workspace_session_token</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Microsoft</td>  \n<td><code>power_automate_webhook_sas</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>OneSignal</td>  \n<td><code>onesignal_rich_authentication_token</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Paddle</td>  \n<td><code>paddle_api_key</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Paddle</td>  \n<td><code>paddle_sandbox_api_key</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Pineapple Technologies Limited</td>  \n<td><code>pineapple_technologies_incident_api_key</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>PostHog</td>  \n<td><code>posthog_feature_flags_secure_api_key</code></td>  \n<td></td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>PostHog</td>  \n<td><code>posthog_personal_api_key</code></td>  \n<td></td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Rainforest Pay</td>  \n<td><code>rainforest_api_key</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Rainforest Pay</td>  \n<td><code>rainforest_sandbox_api_key</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n<tr>  \n<td>Raycast</td>  \n<td><code>raycast_access_token</code></td>  \n<td>✓</td>  \n<td>✓</td>  \n<td>✓ (configurable)</td>  \n</tr>  \n</tbody>  \n</table></div>  \n<h3><a href=\"https://github.blog/#private-key-patterns-added\">Private key patterns added<span></span></a></h3>  \n<p><a href=\"https://github.blog/changelog/2025-11-12-secret-scanning-improves-private-key-detection/\">As announced on November 12</a>, secret scanning now detects additional private key formats:</p>  \n<div><table>  \n<thead>  \n<tr>  \n<th>Provider</th>  \n<th>Secret type</th>  \n<th>Description</th>  \n</tr>  \n</thead>  \n<tbody>  \n<tr>  \n<td>Generic</td>  \n<td><code>ec_private_key</code></td>  \n<td>Elliptic Curve private keys</td>  \n</tr>  \n<tr>  \n<td>Generic</td>  \n<td><code>generic_private_key</code></td>  \n<td>Generic PKCS#8 private keys</td>  \n</tr>  \n</tbody>  \n</table></div>  \n<p>Like other generic patterns, both types can be configured for inclusion with push protection, but aren’t included by default.</p>  \n<h2><a href=\"https://github.blog/#detector-upgrades-and-improvements\">Detector upgrades and improvements<span></span></a></h2>  \n<p>The following private key patterns now also detect keys containing escaped newlines (<code>\\n</code>), a common format in configuration files and environment variables:</p>  \n<ul>  \n<li><code>ec_private_key</code></li>  \n<li><code>github_ssh_private_key</code></li>  \n<li><code>openssh_private_key</code></li>  \n<li><code>rsa_private_key</code></li>  \n</ul>  \n<p>Sentry token types were also renamed to match Sentry’s updated naming conventions:</p>  \n<div><table>  \n<thead>  \n<tr>  \n<th>Previous name</th>  \n<th>New name</th>  \n</tr>  \n</thead>  \n<tbody>  \n<tr>  \n<td><code>sentry_organization_token</code></td>  \n<td><code>sentry_org_auth_token</code></td>  \n</tr>  \n<tr>  \n<td><code>sentry_personal_token</code></td>  \n<td><code>sentry_user_auth_token</code></td>  \n</tr>  \n</tbody>  \n</table></div>  \n<p>The following secret type now supports extended metadata checks, providing additional context like owner information, creation dates, and organizational details.</p>  \n<div><table>  \n<thead>  \n<tr>  \n<th>Provider</th>  \n<th>Secret type</th>  \n</tr>  \n</thead>  \n<tbody>  \n<tr>  \n<td>Discord</td>  \n<td><code>discord_bot_token</code></td>  \n</tr>  \n</tbody>  \n</table></div>  \n<p>We’ve upgraded validity checks for the following type. With recent improvements to our validation of AWS Access Key IDs, most customers will see alerts that were previously labeled “unknown” switch to “valid” or “invalid”.</p>  \n<div><table>  \n<thead>  \n<tr>  \n<th>Provider</th>  \n<th>Pattern</th>  \n<th>Validity</th>  \n</tr>  \n</thead>  \n<tbody>  \n<tr>  \n<td>Amazon Web Services (AWS)</td>  \n<td><code>aws_access_key_id</code></td>  \n<td>✓</td>  \n</tr>  \n</tbody>  \n</table></div>  \n<h2><a href=\"https://github.blog/#partner-notification-updates\">Partner notification updates<span></span></a></h2>  \n<p><a href=\"https://github.blog/changelog/2025-11-25-secrets-in-unlisted-github-gists-are-now-reported-to-secret-scanning-partners/\">As announced on November 25</a>, secrets found in unlisted GitHub gists are now reported to secret scanning partners. Since unlisted gists are accessible to anyone with the URL, leaked secrets in gists should be treated like any other publicly exposed credential.</p>  \n<p>Learn more about <a href=\"https://docs.github.com/code-security/secret-scanning/introduction/about-secret-scanning\">secret scanning</a> and see the <a href=\"https://docs.github.com/code-security/secret-scanning/introduction/supported-secret-scanning-patterns\">full list of supported secrets</a> in our product documentation.</p>  \n  \n<p>The post <a href=\"https://github.blog/changelog/2025-12-02-secret-scanning-updates-november-2025\">Secret scanning updates — November 2025</a> appeared first on <a href=\"https://github.blog\">The GitHub Blog</a>.</p>",
      "summary": "  \nGitHub secret scanning continually adds support for new secret types. The following updates were made during the month of November.  \n  \nNew provider patterns: Secret scanning added 24 new secret types from providers including Azure, Databricks, Microsoft, Paddle, PostHog, and more.  \nImproved private key detection: New patterns for Elliptic Curve and generic PKCS#8 private keys, plus improved detection of escaped newlines.  \nExtended metadata: Discord discord_bot_token now supports extended ",
      "publishedAt": "2025-12-02T16:50:31.000Z",
      "author": "Allison",
      "source": "rss",
      "feedName": "Changelogs – The GitHub Blog",
      "sourceType": "engineering_blog",
      "contentType": "feature_update",
      "score": 11.824784494466495,
      "ingestedAt": "2025-12-02T21:47:02.934Z",
      "tags": [
        "code_review",
        "documentation",
        "ide",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e190cd",
      "title": "Amazon launches Trainium3",
      "url": "https://techcrunch.com/2025/12/02/amazon-releases-an-impressive-new-ai-chip-and-teases-a-nvidia-friendly-roadmap/",
      "content": "<p><img src=\"https://techcrunch.com/wp-content/uploads/2020/01/GettyImages-1136663877.jpg?resize=1200,800\" alt=\"GettyImages-1136663877.jpg?resize=1200,8\"></p><p>Article URL: <a href=\"https://techcrunch.com/2025/12/02/amazon-releases-an-impressive-new-ai-chip-and-teases-a-nvidia-friendly-roadmap/\">https://techcrunch.com/2025/12/02/amazon-releases-an-impressive-new-ai-chip-and-teases-a-nvidia-friendly-roadmap/</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46125155\">https://news.ycombinator.com/item?id=46125155</a></p>  \n<p>Points: 58</p>  \n<p># Comments: 21</p>",
      "summary": "Article URL: https://techcrunch.com/2025/12/02/amazon-releases-an-impressive-new-ai-chip-and-teases-a-nvidia-friendly-roadmap/  \nComments URL: https://news.ycombinator.com/item?id=46125155  \nPoints: 58  \n# Comments: 21",
      "publishedAt": "2025-12-02T19:04:31.000Z",
      "author": "thnaks",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 10.415688117845857,
      "ingestedAt": "2025-12-02T21:47:02.934Z",
      "tags": [
        "code_review",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e190d1",
      "title": "School Cell Phone Bans and Student Achievement (NBER Digest)",
      "url": "https://www.nber.org/digest/202512/school-cell-phone-bans-and-student-achievement",
      "content": "<p><img src=\"https://www.nber.org/sites/default/files/2022-06/NBER-FB-Share-Tile-1200.jpg\" alt=\"NBER-FB-Share-Tile-1200.jpg\"></p><p>Article URL: <a href=\"https://www.nber.org/digest/202512/school-cell-phone-bans-and-student-achievement\">https://www.nber.org/digest/202512/school-cell-phone-bans-and-student-achievement</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46124179\">https://news.ycombinator.com/item?id=46124179</a></p>  \n<p>Points: 37</p>  \n<p># Comments: 25</p>",
      "summary": "Article URL: https://www.nber.org/digest/202512/school-cell-phone-bans-and-student-achievement  \nComments URL: https://news.ycombinator.com/item?id=46124179  \nPoints: 37  \n# Comments: 25",
      "publishedAt": "2025-12-02T17:58:11.000Z",
      "author": "harias",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.483067606006562,
      "ingestedAt": "2025-12-02T21:47:02.934Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e01d8d",
      "title": "Anthropic acquires developer tool startup Bun to scale AI coding - Reuters",
      "url": "https://news.google.com/rss/articles/CBMivAFBVV95cUxQZHNQYlFBaUJ6dTZuSDNCWEVJYWI3UmVhUVp6SkhoNTZ0Z18tdEQ4LXZaRHRTa1RxcE5rUkV0eURCMGdSMll6MnQwZlBIVnZkdXlhblZ0aExMN09Ha2hLY1ZRYTFXYjhrS0I3OFgxLXByNXZuOUpReUx2V1VSdHhON3IzcFF6WUc2UG9TZmFtLUVSalU0eG1iaXpqdjJXZ0puVV9WQk5ZZG94Wkp3YTU2RGRNM2U5NUFsek81Xw?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMivAFBVV95cUxQZHNQYlFBaUJ6dTZuSDNCWEVJYWI3UmVhUVp6SkhoNTZ0Z18tdEQ4LXZaRHRTa1RxcE5rUkV0eURCMGdSMll6MnQwZlBIVnZkdXlhblZ0aExMN09Ha2hLY1ZRYTFXYjhrS0I3OFgxLXByNXZuOUpReUx2V1VSdHhON3IzcFF6WUc2UG9TZmFtLUVSalU0eG1iaXpqdjJXZ0puVV9WQk5ZZG94Wkp3YTU2RGRNM2U5NUFsek81Xw?oc=5\">Anthropic acquires developer tool startup Bun to scale AI coding</a>  Reuters",
      "summary": "Anthropic acquires developer tool startup Bun to scale AI coding  Reuters",
      "publishedAt": "2025-12-02T18:59:40.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.462792557902177,
      "ingestedAt": "2025-12-02T21:47:02.937Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e01d92",
      "title": "Anthropic acquires developer tool startup Bun to scale AI coding - Yahoo Finance UK",
      "url": "https://news.google.com/rss/articles/CBMilAFBVV95cUxOMGozb0JGRndBOFJCOE5zbzM3aW9QY2JiYks5ZEEwcmhXZEV6d05tMkFDcUNDRWVTM2tnMjdMQXBSdGkybEo2T2R5SW8wbUF3Q3BtdmVzenlJMW52Q0lmT19KUUp2T3o3YXg0bUdXUlNwT3NmR3BSUUd6cWNxMHZ4UW9oWVhOcFV1MTd1MXdkTmRJejQy?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMilAFBVV95cUxOMGozb0JGRndBOFJCOE5zbzM3aW9QY2JiYks5ZEEwcmhXZEV6d05tMkFDcUNDRWVTM2tnMjdMQXBSdGkybEo2T2R5SW8wbUF3Q3BtdmVzenlJMW52Q0lmT19KUUp2T3o3YXg0bUdXUlNwT3NmR3BSUUd6cWNxMHZ4UW9oWVhOcFV1MTd1MXdkTmRJejQy?oc=5\">Anthropic acquires developer tool startup Bun to scale AI coding</a>  Yahoo Finance UK",
      "summary": "Anthropic acquires developer tool startup Bun to scale AI coding  Yahoo Finance UK",
      "publishedAt": "2025-12-02T18:59:40.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4875975193007258,
      "ingestedAt": "2025-12-02T21:47:02.937Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e01d94",
      "title": "Anthropic acquires developer tool startup Bun to scale AI coding - MarketScreener",
      "url": "https://news.google.com/rss/articles/CBMiuAFBVV95cUxNUWhsTEtxaTJZZ09Cc09RQ2k3d0I1QlBacVo0dENLODRrOEQxTWRCLU9fN3F3NTA3N2wzZ3hlaklwaDE1RENEb0FscWYzZnV5MHgzLVlfSWI2UVhGbTBhZkUzcUpjLUxOdXYtUFAySm1CS0RwTUxRZVJydkNLX2c0dkxBcGVVU1hUdkxCZGUyOWQwNThMMm5jVVA0UEtseDI0cC05Tk83a01SR3JYeFZmd3daM1d2S0lz?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMiuAFBVV95cUxNUWhsTEtxaTJZZ09Cc09RQ2k3d0I1QlBacVo0dENLODRrOEQxTWRCLU9fN3F3NTA3N2wzZ3hlaklwaDE1RENEb0FscWYzZnV5MHgzLVlfSWI2UVhGbTBhZkUzcUpjLUxOdXYtUFAySm1CS0RwTUxRZVJydkNLX2c0dkxBcGVVU1hUdkxCZGUyOWQwNThMMm5jVVA0UEtseDI0cC05Tk83a01SR3JYeFZmd3daM1d2S0lz?oc=5\">Anthropic acquires developer tool startup Bun to scale AI coding</a>  MarketScreener",
      "summary": "Anthropic acquires developer tool startup Bun to scale AI coding  MarketScreener",
      "publishedAt": "2025-12-02T18:58:17.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4874954472454487,
      "ingestedAt": "2025-12-02T21:47:02.937Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e01d96",
      "title": "Anthropic Acquires Software Maker Bun To Advance Claude Code - Stocktwits",
      "url": "https://news.google.com/rss/articles/CBMiwgFBVV95cUxQR3FvX0hCX3JkNzVDUHVwYTZvQVBudlYtajRucjlBcjJXS1VhbUJtdDhTSERUTGNOUWxqb21Dc2VSeEVlSTZlLXhnTEplSGVERHdsWTJkbFFFenJLSW1HRmVBRG1YUFBWdHB6eXkzaDdwN0VmeW5xVTZ6YVdnaWVsWTFKLUtOUEk4Q1dTOEhWNzFfSW9YckRuQzVpOVZUUm1hQXd0VC1tX1kzVVJXbjE2TW05YlV2NnR1Q3BrOFJVaEpWZw?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMiwgFBVV95cUxQR3FvX0hCX3JkNzVDUHVwYTZvQVBudlYtajRucjlBcjJXS1VhbUJtdDhTSERUTGNOUWxqb21Dc2VSeEVlSTZlLXhnTEplSGVERHdsWTJkbFFFenJLSW1HRmVBRG1YUFBWdHB6eXkzaDdwN0VmeW5xVTZ6YVdnaWVsWTFKLUtOUEk4Q1dTOEhWNzFfSW9YckRuQzVpOVZUUm1hQXd0VC1tX1kzVVJXbjE2TW05YlV2NnR1Q3BrOFJVaEpWZw?oc=5\">Anthropic Acquires Software Maker Bun To Advance Claude Code</a>  Stocktwits",
      "summary": "Anthropic Acquires Software Maker Bun To Advance Claude Code  Stocktwits",
      "publishedAt": "2025-12-02T18:52:37.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4870773938561812,
      "ingestedAt": "2025-12-02T21:47:02.937Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09e01d97",
      "title": "Anthropic Accidentally Gives the World a Peek Into Its Model’s ‘Soul’ - Gizmodo",
      "url": "https://news.google.com/rss/articles/CBMingFBVV95cUxOVlpLZWQzM3J3V3ZlWEJXcG5BUmRCQ1BfUE9rVHQ3Y0JvbUNQQ3dtcHBudjlNWEZBc3FaQ2llMWRvWnBmVjM0dzMwdERxcFZmUUVjd19TMGtOQUIzWF9mZmtsQlQtVFpJRk15WFZ2cTYzaUJudzVQSndFVkFNYjlscFFHbFhRRjliX2REbnJiZHhRU0tmTEpYREM3b2lwQQ?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMingFBVV95cUxOVlpLZWQzM3J3V3ZlWEJXcG5BUmRCQ1BfUE9rVHQ3Y0JvbUNQQ3dtcHBudjlNWEZBc3FaQ2llMWRvWnBmVjM0dzMwdERxcFZmUUVjd19TMGtOQUIzWF9mZmtsQlQtVFpJRk15WFZ2cTYzaUJudzVQSndFVkFNYjlscFFHbFhRRjliX2REbnJiZHhRU0tmTEpYREM3b2lwQQ?oc=5\">Anthropic Accidentally Gives the World a Peek Into Its Model’s ‘Soul’</a>  Gizmodo",
      "summary": "Anthropic Accidentally Gives the World a Peek Into Its Model’s ‘Soul’  Gizmodo",
      "publishedAt": "2025-12-02T18:20:24.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 2.4745047820153108,
      "ingestedAt": "2025-12-02T21:47:02.937Z",
      "tags": [
        "ide",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09dd6d8e",
      "title": "Amp, Inc.",
      "url": "https://ampcode.com/news/amp-inc",
      "content": "<p><img src=\"https://ampcode.com/news/amp-inc?og-image\" alt=\"amp-inc?og-image\"></p><p>Amp is becoming a separate company. We're spinning out of Sourcegraph to become an independent research lab.</p>  \n<p>Our goal: let software builders harness the full power of artificial intelligence.</p>  \n<p>We believe the way we develop software will change. All of it will change, fundamentally and drastically. Nobody knows exactly how. We intend to find out.</p>  \n<p>We believe that shipping is the best way to do that. We don't want to write papers about the future; we want to put it in your hands.</p>  \n<div>  \n  <img src=\"https://ampcode.com/(marketing)/news/amp-inc-flying-pig-pair.jpg\" alt=\"Flying pig pair illustration\">  \n</div>  \n  \n<p>Amp Inc. gives us more freedom to do that, to focus ruthlessly on the frontier, to explore the absurd and find the possible.</p>  \n<p>Amp's traction spun us out of Sourcegraph. Amp is profitable. Now, as our own company, we can follow where it leads.</p>  \n<p>Come with us. Let's see what's possible.</p>  \n<p>Signed,</p>  \n<p><i>Alex Kemper · Beyang Liu · Brady Jeong · Brett Jones · Camden Cheek · Connor O'Brien · Dario Hamidi · Harry Charlesworth · Hitesh Sagtani · Isuru Fonseka · Jesse Edelstein · Karl Clement · Lewis Metcalf · Nicolay Gerold · Quinn Slack · Ryan Carson · Thorsten Ball · Tim Culverhouse · Tim Lucas · Will Dollman</i></p>  \n<p>Co-founders of Amp</p>  \n  \n  \n<div style=\"margin-top:3em;border-top:1px solid;border-bottom:1px solid;padding:1.5em 0;text-align:center;font-style:italic;font-size:.875em;\">Read <a href=\"https://sourcegraph.com/blog/why-sourcegraph-and-amp-are-becoming-independent-companies\">Quinn and Dan's announcement</a> on the Sourcegraph blog.</div>",
      "summary": "Amp is becoming a separate company. We're spinning out of Sourcegraph to become an independent research lab.  \nOur goal: let software builders harness the full power of artificial intelligence.  \nWe believe the way we develop software will change. All of it will change, fundamentally and drastically. Nobody knows exactly how. We intend to find out.  \nWe believe that shipping is the best way to do that. We don't want to write papers about the future; we want to put it in your hands.  \n  \n    \n  \n",
      "publishedAt": "2025-12-02T00:00:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "Amp News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.217504572010943,
      "ingestedAt": "2025-12-02T21:47:02.937Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates",
        "Sourcegraph"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09dd3875",
      "title": "IBM CEO says there is 'no way' spending on AI data centers will pay off",
      "url": "https://www.businessinsider.com/ibm-ceo-big-tech-ai-capex-data-center-spending-2025-12",
      "content": "<p><img src=\"https://i.insider.com/692dc1a9abd5e944effbcd59?width=1200&amp;format=jpeg\" alt=\"692dc1a9abd5e944effbcd59?width=1200&amp;form\"></p><p>Article URL: <a href=\"https://www.businessinsider.com/ibm-ceo-big-tech-ai-capex-data-center-spending-2025-12\">https://www.businessinsider.com/ibm-ceo-big-tech-ai-capex-data-center-spending-2025-12</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46124324\">https://news.ycombinator.com/item?id=46124324</a></p>  \n<p>Points: 75</p>  \n<p># Comments: 60</p>",
      "summary": "Article URL: https://www.businessinsider.com/ibm-ceo-big-tech-ai-capex-data-center-spending-2025-12  \nComments URL: https://news.ycombinator.com/item?id=46124324  \nPoints: 75  \n# Comments: 60",
      "publishedAt": "2025-12-02T18:10:23.000Z",
      "author": "nabla9",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 2.4732756087540353,
      "ingestedAt": "2025-12-02T21:47:02.937Z",
      "tags": [
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09dd387c",
      "title": "Anthropic Acquires Bun",
      "url": "https://www.anthropic.com/news/anthropic-acquires-bun-as-claude-code-reaches-usd1b-milestone",
      "content": "<p><img src=\"https://www.anthropic.com/api/opengraph-illustration?name=Object%20Puzzle&amp;backgroundColor=sky\" alt=\"opengraph-illustration?name=Object%20Puz\"></p><p>Article URL: <a href=\"https://www.anthropic.com/news/anthropic-acquires-bun-as-claude-code-reaches-usd1b-milestone\">https://www.anthropic.com/news/anthropic-acquires-bun-as-claude-code-reaches-usd1b-milestone</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46124258\">https://news.ycombinator.com/item?id=46124258</a></p>  \n<p>Points: 46</p>  \n<p># Comments: 10</p>",
      "summary": "Article URL: https://www.anthropic.com/news/anthropic-acquires-bun-as-claude-code-reaches-usd1b-milestone  \nComments URL: https://news.ycombinator.com/item?id=46124258  \nPoints: 46  \n# Comments: 10",
      "publishedAt": "2025-12-02T18:04:23.000Z",
      "author": "httpteapot",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4835237746098677,
      "ingestedAt": "2025-12-02T21:47:02.937Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafb7",
      "title": "Building AWARE : IDE that runs on LLMs, not with them",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pcg2ex/building_aware_ide_that_runs_on_llms_not_with_them/",
      "content": "<p><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcg2ex/building_aware_ide_that_runs_on_llms_not_with_them/\"><img src=\"https://b.thumbs.redditmedia.com/GeuvdTkBp911JCG-83PaVszT4CfqnOZWpB71IrFdB7A.jpg\" alt=\"GeuvdTkBp911JCG-83PaVszT4CfqnOZWpB71IrFd\"></a></p><table> <tr><td>   submitted by   <a href=\"https://www.reddit.com/user/Alive_Spite5550\"> /u/Alive_Spite5550 </a> <br> <span><a href=\"https://www.reddit.com/r/vscode/comments/1pcg22z/building_aware_ide_that_runs_on_llms_not_with_them/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcg2ex/building_aware_ide_that_runs_on_llms_not_with_them/\">[comments]</a></span> </td></tr></table>",
      "summary": "    submitted by    /u/Alive_Spite5550   [link]   [comments] ",
      "publishedAt": "2025-12-02T18:21:28.000Z",
      "author": "/u/Alive_Spite5550",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "general",
      "score": 2.4746357075663106,
      "ingestedAt": "2025-12-02T21:47:02.939Z",
      "tags": [
        "ide",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafbf",
      "title": "Security in vibe-coded apps.",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pcf3q6/security_in_vibecoded_apps/",
      "content": "<div><p>Hey everyone, </p> <p>I’ve been vibe coding a lot recently (mostly Cursor + antigravity), and the velocity is insane. </p> <p>But because I come from a cybersecurity background, I started auditing the code the AI was giving me, and honestly, it’s kind of terrifying.</p> <p>I’m seeing things like: • Payment webhooks with zero signature verification. • API endpoints with no rate limiting (easy DDoS). • Hardcoded secrets in frontend builds.</p> <p>The AI gets the feature \"working\" instantly, but often skips the invisible safety rails. I’m working on a project to automate security fixes for vibe-coded apps so we don't have to slow down. I need to test my scanner against more real-world examples.</p> <p>I put up a quick vibe coded site here so this is not a product or something just for research.</p> <p>appsecshepp.com</p> <p>If you’re building something and want a free security roast/audit, drop your details there or DM me. I’m not charging anything, I just need data on how AI messes up security so I can build better guardrails for. </p> </div>   submitted by   <a href=\"https://www.reddit.com/user/Highest_in_the_world\"> /u/Highest_in_the_world </a> <br> <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcf3q6/security_in_vibecoded_apps/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcf3q6/security_in_vibecoded_apps/\">[comments]</a></span>",
      "summary": "Hey everyone,  I’ve been vibe coding a lot recently (mostly Cursor + antigravity), and the velocity is insane.  But because I come from a cybersecurity background, I started auditing the code the AI was giving me, and honestly, it’s kind of terrifying. I’m seeing things like: • Payment webhooks with zero signature verification. • API endpoints with no rate limiting (easy DDoS). • Hardcoded secrets in frontend builds. The AI gets the feature \"working\" instantly, but often skips the invisible safe",
      "publishedAt": "2025-12-02T17:47:03.000Z",
      "author": "/u/Highest_in_the_world",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "security_incident",
      "score": 11.363907496307313,
      "ingestedAt": "2025-12-02T21:47:02.939Z",
      "tags": [
        "code_review",
        "governance",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafc0",
      "title": "Most “AI coding” threads are debates. Here’s what 8 weeks of actual structure produced.",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pcetu7/most_ai_coding_threads_are_debates_heres_what_8/",
      "content": "<div><p>I see a lot of back-and-forth here about whether AI coding works, doesn’t work, is cheating, is useless, etc.</p> <p>So I figured I’d just show what happens when you pair LLMs with a tight process and real constraints.</p> <p>Over the last ~8 weeks I built a local-first workshop ERP from scratch.</p> <p>It has 99 endpoints, inventory, manufacturing, Google Drive integration, OAuth, encrypted backups, plugin system, etc. All driven through a Source-of-Truth document and a smoke test that the model has to pass before anything “becomes real.”</p> <p>No CS background. No bootcamp. Just organization, repetition, and a refusal to let drift accumulate.</p> <p>Repo’s public if you want to poke at it: <a href=\"https://github.com/truegoodcraft/TGC-BUS-Core\">https://github.com/truegoodcraft/TGC-BUS-Core</a></p> <p>Not claiming this is the way to use LLMs I'm I'mjust showing that structure works.</p> </div>   submitted by   <a href=\"https://www.reddit.com/user/TrueGoodCraft\"> /u/TrueGoodCraft </a> <br> <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcetu7/most_ai_coding_threads_are_debates_heres_what_8/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcetu7/most_ai_coding_threads_are_debates_heres_what_8/\">[comments]</a></span>",
      "summary": "I see a lot of back-and-forth here about whether AI coding works, doesn’t work, is cheating, is useless, etc. So I figured I’d just show what happens when you pair LLMs with a tight process and real constraints. Over the last ~8 weeks I built a local-first workshop ERP from scratch. It has 99 endpoints, inventory, manufacturing, Google Drive integration, OAuth, encrypted backups, plugin system, etc. All driven through a Source-of-Truth document and a smoke test that the model has to pass before ",
      "publishedAt": "2025-12-02T17:37:09.000Z",
      "author": "/u/TrueGoodCraft",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "general",
      "score": 8.395286190959741,
      "ingestedAt": "2025-12-02T21:47:02.939Z",
      "tags": [
        "code_review",
        "ide",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafc2",
      "title": "I just released a chrome extension called Unslopper, which will redact reddit comments and posts that contain blacklisted keywords(default: slop)",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pcejrj/i_just_released_a_chrome_extension_called/",
      "content": "<p><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcejrj/i_just_released_a_chrome_extension_called/\"><img src=\"https://external-preview.redd.it/NDDbRxq4OsLVIswSW-iNSrn5D-Q3VOWFeAKM1Ag-cgI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6005f0176bfa67b16e06d92105c6dc05c0bb49ef\" alt=\"NDDbRxq4OsLVIswSW-iNSrn5D-Q3VOWFeAKM1Ag-\"></a></p><table> <tr><td> <div><p>I'm just so tired of all the geese honking slop slop slop.</p> <p>So I vibe coded this extension. Hope it helps.</p> </div>   submitted by   <a href=\"https://www.reddit.com/user/WHALE_PHYSICIST\"> /u/WHALE_PHYSICIST </a> <br> <span><a href=\"https://chromewebstore.google.com/detail/unslopper/piepkiilhjhmaonenkcgchbmefbemkma\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcejrj/i_just_released_a_chrome_extension_called/\">[comments]</a></span> </td></tr></table>",
      "summary": "  I'm just so tired of all the geese honking slop slop slop. So I vibe coded this extension. Hope it helps.    submitted by    /u/WHALE_PHYSICIST   [link]   [comments] ",
      "publishedAt": "2025-12-02T17:27:11.000Z",
      "author": "/u/WHALE_PHYSICIST",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 8.39113676936342,
      "ingestedAt": "2025-12-02T21:47:02.939Z",
      "tags": [
        "code_review",
        "ide",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafc4",
      "title": "What's your go-to when trying to show people the current state of vibe-coding?",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pcejej/whats_your_goto_when_trying_to_show_people_the/",
      "content": "<div><p>I know code editor integration with Claude Code and MCP servers is great an all, but the average person doesn't know what any of that means and can't understand how it contribtes to the actual creation of an app. When you're trying to impress or explain to your dad why AI code writing is so crazy, what do you show them?</p> <p>Not to shill, but Poe has this App Creator (<a href=\"https://poe.com/App-Creator\">https://poe.com/App-Creator</a> not an affiliate link literally just showing it off) that builds single-file HTML apps with Claude code, and then hosts them directly, so you can literally just give it a prompt, wait a minute while it types it out (which always impresses people with the code rapidly scrolling by) and then hit publish and share the link to them so they can visit it on ther own device. Kinda blows their mind, and actually gives you a usable app that can call other models and stuff (no local storage, no external API access, lots of limitations, but couldn't be faster or more convenient).</p> <p>My favorite prompt to demo is:</p> <blockquote> <p>Create a kids story book generator. User provides a brief overview or idea for the book and clicks Submit. Pass input to Haiku-4.5 for a synopsis overview for a 4 year old kid's book story. Pass synopsis to Sonnet-4.5 for a 2D array of page text content (written for a 4 year old) and heavily stylized hand-drawn image prompts (written for Flux). As the user turns each page of the book, pass image prompt for that page to Flux-2 and display the result along with the page text content.</p> </blockquote> <p>All 3 people I've shown it to have been like \"holy shit\" and then I have to sheepishly explain that it's not perfect and messes stuff up still and has all sorts of issues and limitations but like, if we're here already, I'm really not sure how the software dev industry is going to look by the time I'm retiring. <em>Also don't say shit about the dismal prospect of auto-feeding children generated synthetic media I read fucktons of Munsch books to my kid every day K?</em></p> <hr> <p>Anyhow, to shill, here's an actual app I vibe-coded with it in a matter of hours in a hospital waiting room. Makes phone wallpapers for you and actually does a pretty good job, lets you pick the style and aesthetic and has a nice shuffle feature and uses the new Flux-2 model.</p> <p><a href=\"https://poe.com/PhonePaper\">https://poe.com/PhonePaper</a></p> <p>Sample results:</p> <p><a href=\"https://imgur.com/lEjzPO6\">Veiled Ocean Eclipse</a></p> <p><a href=\"https://imgur.com/e8faqIo\">Celestial Forest</a></p> <p><a href=\"https://imgur.com/zLH70yC\">Cavern Skyline</a></p> <p><a href=\"https://imgur.com/ofjOU8D\">Cursed Forest Roots</a></p> <p><a href=\"https://imgur.com/9PqjYVQ\">Matrix Embers</a></p> </div>   submitted by   <a href=\"https://www.reddit.com/user/MudcrabMercantile\"> /u/MudcrabMercantile </a> <br> <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcejej/whats_your_goto_when_trying_to_show_people_the/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcejej/whats_your_goto_when_trying_to_show_people_the/\">[comments]</a></span>",
      "summary": "I know code editor integration with Claude Code and MCP servers is great an all, but the average person doesn't know what any of that means and can't understand how it contribtes to the actual creation of an app. When you're trying to impress or explain to your dad why AI code writing is so crazy, what do you show them? Not to shill, but Poe has this App Creator (https://poe.com/App-Creator not an affiliate link literally just showing it off) that builds single-file HTML apps with Claude code, a",
      "publishedAt": "2025-12-02T17:26:50.000Z",
      "author": "/u/MudcrabMercantile",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 13.820455914868548,
      "ingestedAt": "2025-12-02T21:47:02.939Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafc5",
      "title": "How are you quantifying quality in your final product?",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pcefut/how_are_you_quantifying_quality_in_your_final/",
      "content": "<div><p>Hey all,</p> <p>Just helped host a hackathon at a local university and one of the metrics that the judges used to rank the participants was “product quality” on a scale from 1 - 10.</p> <p>This felt a bit arbitrary to me, but was curious as to how this community in particular thinks about product quality, especially since it seems like the main point of vibe coding is to be able to produce technical products and systems quickly.</p> <p>It feels like if the vibe coders main focus is at the product layer (not the architectural or code quality layer) then folks might have some interesting metrics or rules that they use to determine the quality of the thing that they are working on.</p> <p>I assume the obvious rules like “is it easy to use”, “does it actually do the right thing most of the time”, etc. all apply, but wanted to hear from the community about their own processes and experiences. Thanks in advance!</p> </div>   submitted by   <a href=\"https://www.reddit.com/user/structured_obscurity\"> /u/structured_obscurity </a> <br> <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcefut/how_are_you_quantifying_quality_in_your_final/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcefut/how_are_you_quantifying_quality_in_your_final/\">[comments]</a></span>",
      "summary": "Hey all, Just helped host a hackathon at a local university and one of the metrics that the judges used to rank the participants was “product quality” on a scale from 1 - 10. This felt a bit arbitrary to me, but was curious as to how this community in particular thinks about product quality, especially since it seems like the main point of vibe coding is to be able to produce technical products and systems quickly. It feels like if the vibe coders main focus is at the product layer (not the arch",
      "publishedAt": "2025-12-02T17:23:19.000Z",
      "author": "/u/structured_obscurity",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.428517802233302,
      "ingestedAt": "2025-12-02T21:47:02.939Z",
      "tags": [
        "code_review",
        "observability",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafc6",
      "title": "We're live now interviewing Dax Raad, creator of OpenCode",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pce3d2/were_live_now_interviewing_dax_raad_creator_of/",
      "content": "submitted by   <a href=\"https://www.reddit.com/user/CodacyOfficial\"> /u/CodacyOfficial </a> <br> <span><a href=\"https://www.reddit.com/r/opencodeCLI/comments/1pce2hz/were_live_now_interviewing_dax_raad_creator_of/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pce3d2/were_live_now_interviewing_dax_raad_creator_of/\">[comments]</a></span>",
      "summary": "submitted by    /u/CodacyOfficial   [link]   [comments]",
      "publishedAt": "2025-12-02T17:10:47.000Z",
      "author": "/u/CodacyOfficial",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4795847215194664,
      "ingestedAt": "2025-12-02T21:47:02.939Z",
      "tags": [
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafc7",
      "title": "GraphGit",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pce22i/graphgit/",
      "content": "<p><a href=\"https://www.reddit.com/r/vibecoding/comments/1pce22i/graphgit/\"><img src=\"https://b.thumbs.redditmedia.com/68iZjl3HSbahkC8C5ngFqkTP8nI49tIidwNQYNOgKKM.jpg\" alt=\"68iZjl3HSbahkC8C5ngFqkTP8nI49tIidwNQYNOg\"></a></p><table> <tr><td> <div><p>On a regular basis we utilise GitHub for maintaining our work and storing our work. </p> <p>Now think of a scenario when you share your project with a colleague of yours, who has no idea of the routes and connections between the files in the repository. </p> <p>It creates a bad user experience because you might just have to explain to the colleague that one particular code file is being used/called in another file. </p> <p>GitHub provides you almost every feature that a developer needs. What it does not provide you is the clear visualization of the work that you did. </p> <p>Introducing - GraphGit, an open-source website where you connect your GitHub account, encrypted into sessions ensuring your safety of work. Scan your GitHub Repo and you'll have a clear statistical analysis along with graphical representation of your files, in the form of Nodes and the connections between them as Edges, giving liberty of traversing in the whole graph, creating an ultimate user experience.</p> <p>Note: this is base model and with certain limitations. We will be working on scaling it up in near future. </p> </div>   submitted by   <a href=\"https://www.reddit.com/user/mr_vengeance_72\"> /u/mr_vengeance_72 </a> <br> <span><a href=\"https://www.reddit.com/gallery/1pce22i\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pce22i/graphgit/\">[comments]</a></span> </td></tr></table>",
      "summary": "  On a regular basis we utilise GitHub for maintaining our work and storing our work.  Now think of a scenario when you share your project with a colleague of yours, who has no idea of the routes and connections between the files in the repository.  It creates a bad user experience because you might just have to explain to the colleague that one particular code file is being used/called in another file.  GitHub provides you almost every feature that a developer needs. What it does not provide yo",
      "publishedAt": "2025-12-02T17:09:25.000Z",
      "author": "/u/mr_vengeance_72",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 8.38374506017322,
      "ingestedAt": "2025-12-02T21:47:02.939Z",
      "tags": [
        "code_review",
        "ide",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafc8",
      "title": "Anyone else have more fun building tools than their project?",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pcdtib/anyone_else_have_more_fun_building_tools_than/",
      "content": "<div><p>I've been working on a word game with Claude. Getting the game built was beyond tedious and had me hating life. Building tools to streamline it? 10/10. </p> <p>Aside from troubleshooting, the biggest time suck was coming up with answers for a daily puzzle. I wracked my brain for a weekend and came up with 25 sets. Figured there had to be a better way, asked Chat GPT to script up a tool for me, and churned out 150 in like 2 hours. Kind of embarrassing I used AI for everything else and still didn't think to apply it to this aspect of the project, but oh well. </p> <p>I also used Claude and Chat GPT to make my site at least somewhat SEO friendly, customize my GA dashboard, and give me an actionable roadmap to follow. I don't have any illusions about this taking off and making money, but figuring out what's possible with AI and how to use it has still been worth the headaches. </p> <p>It's got me curious if anyone else is enjoying the side quests that come along with trying to prompt something into existence or if you've built anything useful to help you make it happen faster. Always impressed by the creativity in here and would love to know if you've got any time saving recs. </p> </div>   submitted by   <a href=\"https://www.reddit.com/user/CptCall\"> /u/CptCall </a> <br> <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcdtib/anyone_else_have_more_fun_building_tools_than/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcdtib/anyone_else_have_more_fun_building_tools_than/\">[comments]</a></span>",
      "summary": "I've been working on a word game with Claude. Getting the game built was beyond tedious and had me hating life. Building tools to streamline it? 10/10.  Aside from troubleshooting, the biggest time suck was coming up with answers for a daily puzzle. I wracked my brain for a weekend and came up with 25 sets. Figured there had to be a better way, asked Chat GPT to script up a tool for me, and churned out 150 in like 2 hours. Kind of embarrassing I used AI for everything else and still didn't think",
      "publishedAt": "2025-12-02T17:00:46.000Z",
      "author": "/u/CptCall",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "thought_leadership",
      "score": 4.92949919975353,
      "ingestedAt": "2025-12-02T21:47:02.939Z",
      "tags": [
        "code_review",
        "ide",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafc9",
      "title": "Average Conversation with Cursor Be like",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pcdnpn/average_conversation_with_cursor_be_like/",
      "content": "<p><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcdnpn/average_conversation_with_cursor_be_like/\"><img src=\"https://b.thumbs.redditmedia.com/nBt31h2rAZ7K9a7bIRsHFphyjJrlsDThNU8YkFNhwZc.jpg\" alt=\"nBt31h2rAZ7K9a7bIRsHFphyjJrlsDThNU8YkFNh\"></a></p><table> <tr><td>   submitted by   <a href=\"https://www.reddit.com/user/Humble_Ad7321\"> /u/Humble_Ad7321 </a> <br> <span><a href=\"https://www.reddit.com/r/cursor/comments/1pcdnb2/average_conversation_with_cursor_be_like/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcdnpn/average_conversation_with_cursor_be_like/\">[comments]</a></span> </td></tr></table>",
      "summary": "    submitted by    /u/Humble_Ad7321   [link]   [comments] ",
      "publishedAt": "2025-12-02T16:54:52.000Z",
      "author": "/u/Humble_Ad7321",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "general",
      "score": 2.9568340499162904,
      "ingestedAt": "2025-12-02T21:47:02.939Z",
      "tags": [
        "retrieval",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafca",
      "title": "Somebody vibecoded at Uber",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pcderc/somebody_vibecoded_at_uber/",
      "content": "<p><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcderc/somebody_vibecoded_at_uber/\"><img src=\"https://i.redd.it/s12yumi4lt4g1.jpeg\" alt=\"s12yumi4lt4g1.jpeg\"></a></p><table> <tr><td>   submitted by   <a href=\"https://www.reddit.com/user/Downtown-Treacle-190\"> /u/Downtown-Treacle-190 </a> <br> <span><a href=\"https://i.redd.it/s12yumi4lt4g1.jpeg\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pcderc/somebody_vibecoded_at_uber/\">[comments]</a></span> </td></tr></table>",
      "summary": "    submitted by    /u/Downtown-Treacle-190   [link]   [comments] ",
      "publishedAt": "2025-12-02T16:45:48.000Z",
      "author": "/u/Downtown-Treacle-190",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4777522779038914,
      "ingestedAt": "2025-12-02T21:47:02.939Z",
      "tags": [
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafcb",
      "title": "How much time to vibe code a platform?",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pccw1m/how_much_time_to_vibe_code_a_platform/",
      "content": "<div><p>Guys, how much time does it take for a beginner coder to build a proper platform + mob app?</p> </div>   submitted by   <a href=\"https://www.reddit.com/user/MarionberryTotal2657\"> /u/MarionberryTotal2657 </a> <br> <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pccw1m/how_much_time_to_vibe_code_a_platform/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pccw1m/how_much_time_to_vibe_code_a_platform/\">[comments]</a></span>",
      "summary": "Guys, how much time does it take for a beginner coder to build a proper platform + mob app?    submitted by    /u/MarionberryTotal2657   [link]   [comments]",
      "publishedAt": "2025-12-02T16:26:36.000Z",
      "author": "/u/MarionberryTotal2657",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.4290366942554344,
      "ingestedAt": "2025-12-02T21:47:02.939Z",
      "tags": [
        "code_review",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafcc",
      "title": "What are the worst disasters from vibe coding you've seen?",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pc9yhn/what_are_the_worst_disasters_from_vibe_coding/",
      "content": "<div><p>People always talk about how we shouldn't fully trust AI generated code, but there are definitely people who don't go through the entire generated code before clicking on that approve button on cursor, codex, claude or any other apps you are using. </p> <p>Has anyone actually seen fumbles from vibe coding or experienced it themselves?</p> </div>   submitted by   <a href=\"https://www.reddit.com/user/examkiddo\"> /u/examkiddo </a> <br> <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc9yhn/what_are_the_worst_disasters_from_vibe_coding/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc9yhn/what_are_the_worst_disasters_from_vibe_coding/\">[comments]</a></span>",
      "summary": "People always talk about how we shouldn't fully trust AI generated code, but there are definitely people who don't go through the entire generated code before clicking on that approve button on cursor, codex, claude or any other apps you are using.  Has anyone actually seen fumbles from vibe coding or experienced it themselves?    submitted by    /u/examkiddo   [link]   [comments]",
      "publishedAt": "2025-12-02T14:33:00.000Z",
      "author": "/u/examkiddo",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "general",
      "score": 6.3615494341003895,
      "ingestedAt": "2025-12-02T21:47:02.939Z",
      "tags": [
        "code_review",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafcf",
      "title": "Does anyone actually build useful apps with vibe coding?",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pc948b/does_anyone_actually_build_useful_apps_with_vibe/",
      "content": "<div><p>I keep seeing people talk about “vibe coding” — just opening your editor, putting on music, and letting the code flow without overthinking architecture or planning.</p> <p>But I’m genuinely curious:<br> <strong>Has anyone here actually built something</strong> <strong><em>useful</em></strong> <strong>or production-ready using vibe coding alone?</strong></p> <p>Like real apps, tools, or projects that ended up being stable or widely used?<br> Or is vibe coding mostly for prototyping / getting unstuck / having fun?</p> <p>Would love to hear examples or stories.</p> <p><em>(Also, I might not know the community that well yet, so apologies in advance if this is a common question!)</em></p> </div>   submitted by   <a href=\"https://www.reddit.com/user/Penguinronin\"> /u/Penguinronin </a> <br> <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc948b/does_anyone_actually_build_useful_apps_with_vibe/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc948b/does_anyone_actually_build_useful_apps_with_vibe/\">[comments]</a></span>",
      "summary": "I keep seeing people talk about “vibe coding” — just opening your editor, putting on music, and letting the code flow without overthinking architecture or planning. But I’m genuinely curious: Has anyone here actually built something useful or production-ready using vibe coding alone? Like real apps, tools, or projects that ended up being stable or widely used? Or is vibe coding mostly for prototyping / getting unstuck / having fun? Would love to hear examples or stories. (Also, I might not know ",
      "publishedAt": "2025-12-02T13:58:19.000Z",
      "author": "/u/Penguinronin",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.373596805371143,
      "ingestedAt": "2025-12-02T21:47:02.940Z",
      "tags": [
        "code_review",
        "ide",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafd0",
      "title": "after 10 times of this today, Antigravity is not good after all.",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pc8p7k/after_10_times_of_this_today_antigravity_is_not/",
      "content": "<p><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc8p7k/after_10_times_of_this_today_antigravity_is_not/\"><img src=\"https://a.thumbs.redditmedia.com/TemH2KYuT-15m9Y_cSDumjO1mCrsnU6f_rwPYMoEWv4.jpg\" alt=\"TemH2KYuT-15m9Y_cSDumjO1mCrsnU6f_rwPYMoE\"></a></p><table> <tr><td> <div><p><a href=\"https://preview.redd.it/0yw6ud4zns4g1.png?width=1292&amp;format=png&amp;auto=webp&amp;s=736320e9f196a092dd583d317e5a9527cad02927\">https://preview.redd.it/0yw6ud4zns4g1.png?width=1292&amp;format=png&amp;auto=webp&amp;s=736320e9f196a092dd583d317e5a9527cad02927</a></p> <p><a href=\"https://preview.redd.it/gye4bzezns4g1.png?width=874&amp;format=png&amp;auto=webp&amp;s=3319481e184aeef1472f0f14c3ad6ffa89906658\">https://preview.redd.it/gye4bzezns4g1.png?width=874&amp;format=png&amp;auto=webp&amp;s=3319481e184aeef1472f0f14c3ad6ffa89906658</a></p> <p>did I do anything wrong?</p> </div>   submitted by   <a href=\"https://www.reddit.com/user/hanamizuki\"> /u/hanamizuki </a> <br> <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc8p7k/after_10_times_of_this_today_antigravity_is_not/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc8p7k/after_10_times_of_this_today_antigravity_is_not/\">[comments]</a></span> </td></tr></table>",
      "summary": "  https://preview.redd.it/0yw6ud4zns4g1.png?width=1292&amp;format=png&amp;auto=webp&amp;s=736320e9f196a092dd583d317e5a9527cad02927 https://preview.redd.it/gye4bzezns4g1.png?width=874&amp;format=png&amp;auto=webp&amp;s=3319481e184aeef1472f0f14c3ad6ffa89906658 did I do anything wrong?    submitted by    /u/hanamizuki   [link]   [comments] ",
      "publishedAt": "2025-12-02T13:40:09.000Z",
      "author": "/u/hanamizuki",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.39261912439701,
      "ingestedAt": "2025-12-02T21:47:02.940Z",
      "tags": [
        "code_review",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafd1",
      "title": "How are people spending so much money?",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pc78w3/how_are_people_spending_so_much_money/",
      "content": "<div><p>Recently I splurged on Copilot Pro for the convenience of multiple parallel agents, but coming from someone with barely any coding background, how are yall spending hundreds of dollars? </p> <p>Using free Claude to prototype and free ChatGPT to fine tune was more than enough, now with copilot Pro I just don’t understand. I’ve seen so many complaints about certain LLM’s bottle necking productivity and running a large tab. So far outside of copilot, my only real cost that I haven’t found an alternative for has been hosting. </p> <p>What type of complex apps are you all even building to warrant so much extra fees? After going through multiple app showcases that took months to vibecode on this sub and others, none of this is adding up. I haven’t seen a feature that can’t be planned out and implemented by even the “lower end” LLM’s or a fiverr hire. </p> </div>   submitted by   <a href=\"https://www.reddit.com/user/Careful-Sky3745\"> /u/Careful-Sky3745 </a> <br> <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc78w3/how_are_people_spending_so_much_money/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc78w3/how_are_people_spending_so_much_money/\">[comments]</a></span>",
      "summary": "Recently I splurged on Copilot Pro for the convenience of multiple parallel agents, but coming from someone with barely any coding background, how are yall spending hundreds of dollars?  Using free Claude to prototype and free ChatGPT to fine tune was more than enough, now with copilot Pro I just don’t understand. I’ve seen so many complaints about certain LLM’s bottle necking productivity and running a large tab. So far outside of copilot, my only real cost that I haven’t found an alternative f",
      "publishedAt": "2025-12-02T12:31:58.000Z",
      "author": "/u/Careful-Sky3745",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 12.160521941149751,
      "ingestedAt": "2025-12-02T21:47:02.940Z",
      "tags": [
        "code_review",
        "agents",
        "ide",
        "Developer Communities",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafd2",
      "title": "How expensive is the vibecoding for you?",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pc5z3b/how_expensive_is_the_vibecoding_for_you/",
      "content": "<div><p>Hey all,</p> <p>I’m using Cursor with Claude4.5 everyday, but I feel like my usage spikes way faster than it should. Some coding sessions burn through a big chunk of my monthly limit, and I’m not sure if I’m doing something wrong or if that’s just normal.<br> So what does your typical usage look like? How do you keep costs under control? Any tips for optimizing prompts or avoiding unnecessary token usage?</p> <p>Just trying to understand what’s normal before I tweak anything. </p> <p>Thanks,</p> <p>Kris</p> </div>   submitted by   <a href=\"https://www.reddit.com/user/krlozanov\"> /u/krlozanov </a> <br> <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc5z3b/how_expensive_is_the_vibecoding_for_you/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc5z3b/how_expensive_is_the_vibecoding_for_you/\">[comments]</a></span>",
      "summary": "Hey all, I’m using Cursor with Claude4.5 everyday, but I feel like my usage spikes way faster than it should. Some coding sessions burn through a big chunk of my monthly limit, and I’m not sure if I’m doing something wrong or if that’s just normal. So what does your typical usage look like? How do you keep costs under control? Any tips for optimizing prompts or avoiding unnecessary token usage? Just trying to understand what’s normal before I tweak anything.  Thanks, Kris    submitted by    /u/k",
      "publishedAt": "2025-12-02T11:21:36.000Z",
      "author": "/u/krlozanov",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.362534260135481,
      "ingestedAt": "2025-12-02T21:47:02.940Z",
      "tags": [
        "code_review",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafd3",
      "title": "Stop making predictions about vibe-coding",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pc4ubj/stop_making_predictions_about_vibecoding/",
      "content": "<div><p>I have come to realize people trying to downplay what software engineering is. I have seen many posts on X and here as well.</p> <p>So I think people who don’t code, understand coding, have any technical background, or system architecture knowledge should stop making predictions about vibe-coding.</p> <p>Many see vibe coding as a one way ticket without knowing there’s a lot behind “telling AI to generate code” and actually shipping a real system.</p> <p>Scaling, security, architecture, infrastructure, integrations, state management, data models, edge cases, debugging? Truth is none of these disappear because you asked an AI to write a function.</p> <p>Vibe-coding looks easy from the outside, but the moment you try to build something non-trivial, reality humbles you fast.</p> <p>Only those who choose to learn along the way will find a new path and move from vibe coding to human-AI collaboration stage. That’s where everything changes to AI-assisted productivity.</p> </div>   submitted by   <a href=\"https://www.reddit.com/user/brainland\"> /u/brainland </a> <br> <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc4ubj/stop_making_predictions_about_vibecoding/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc4ubj/stop_making_predictions_about_vibecoding/\">[comments]</a></span>",
      "summary": "I have come to realize people trying to downplay what software engineering is. I have seen many posts on X and here as well. So I think people who don’t code, understand coding, have any technical background, or system architecture knowledge should stop making predictions about vibe-coding. Many see vibe coding as a one way ticket without knowing there’s a lot behind “telling AI to generate code” and actually shipping a real system. Scaling, security, architecture, infrastructure, integrations, ",
      "publishedAt": "2025-12-02T10:13:00.000Z",
      "author": "/u/brainland",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "security_incident",
      "score": 13.043144260630832,
      "ingestedAt": "2025-12-02T21:47:02.940Z",
      "tags": [
        "code_review",
        "ide",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafd7",
      "title": "AVOID MEKU.DEV AT ALL COSTS.",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pc3uln/avoid_mekudev_at_all_costs/",
      "content": "<div><p>Total scam site, advertise 2 way GitHub integration. But only have 1 way integration, fine, no big deal. Burns 595 credits in 5 prompts. FIVE PROMPTS FIVE HUNDRED CREDITS. So while their 10k monthly credits seems like a good deal, it's all deceptive. Request refund, and get put in contact with one of the rudest customer support agents, who refuses the refund, which makes me start researching more. Their front page is hosting 3 scams that I noticed, 2 crypto recovery scams, and 1 person pretending to be Kenny Chesney LOL </p> <p>for a better price use lovable, or even better cursor which is like $20 for unlimited tokens, which is insane. </p> </div>   submitted by   <a href=\"https://www.reddit.com/user/Whiteytheoutlaw\"> /u/Whiteytheoutlaw </a> <br> <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc3uln/avoid_mekudev_at_all_costs/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc3uln/avoid_mekudev_at_all_costs/\">[comments]</a></span>",
      "summary": "Total scam site, advertise 2 way GitHub integration. But only have 1 way integration, fine, no big deal. Burns 595 credits in 5 prompts. FIVE PROMPTS FIVE HUNDRED CREDITS. So while their 10k monthly credits seems like a good deal, it's all deceptive. Request refund, and get put in contact with one of the rudest customer support agents, who refuses the refund, which makes me start researching more. Their front page is hosting 3 scams that I noticed, 2 crypto recovery scams, and 1 person pretendin",
      "publishedAt": "2025-12-02T09:08:10.000Z",
      "author": "/u/Whiteytheoutlaw",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 13.001265677899339,
      "ingestedAt": "2025-12-02T21:47:02.940Z",
      "tags": [
        "code_review",
        "agents",
        "Developer Communities",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09daafda",
      "title": "Complex Software",
      "url": "https://www.reddit.com/r/vibecoding/comments/1pc2r7b/complex_software/",
      "content": "<div><p>I am genuinely very confused after reading so much of what is posted about vibe coding software lately - I could use some help sorting this out. It’s likely that a large portion of those posts are either self-serving propaganda or bot-generated responses. Often, these same posts are in extreme support of vibe coding and make it seem like building software systems is a piece of cake with LLM’s. </p> <p>My background includes a chemical engineering degree, time working for a Fortune 500 manufacturing company doing some scientific modeling/simulation work, some grad CS courses, and an assortment of small personal programming projects. Basically, I can code, just not very quickly or efficiently. I can represent complex ideas programmatically nonetheless. More recently, over the past few months, I’ve engaged more heavily with these LLMs as coding tools. </p> <p>My local coffee shop had some major pain points with a fairly standard industry-accepted software solution for online reservation management. Sort of an absurd subscription fee for what they provided. I offered to help build something for the coffee shop that would be a third of the ongoing subscription cost (and I retain the full software ownership for the sake of taking on other customers beyond them too). With LLM-assisted development, it seemed like a quicker/doable project and a great way to transition my career towards CS more.</p> <p>This project has taken me close to 7-8 months to complete. It’s in a really fantastic state and I’m proud of the work that I’ve shaped and created. Given that I was coming from a non-traditional / aspiring SWE background, I had to learn a ton of stuff. The not so glamorous things like managing a git workflow, CI/CD management, compliance, security/penetration testing, API wrangling, lots of architectural tradeoffs, Linux OS, terminal interactions, and so much more. I cannot stress enough how much knowledge I have accumulated over the course of this project. Given the pace I was going at, I admittedly traded some coding-level knowledge for this more abstracted knowledge (systems, integrations, networks, etc.). </p> <p>Anyways, now my question. How are people claiming to build these apps and systems at such a breakneck pace without sacrificing on the feature sets, maintainability, security, cost-effectiveness, scalability, etc.? I’m v confused about this because so much of the narrative is “now I can go fast and the details don’t matter, but if they do, they’re easy”... Are people not shipping real things? Developing these systems becomes increasingly more complicated the closer the launch gets. Right?</p> <p>I’m a bit biased here, but I would really appreciate feedback from folks who have shipped real products - with or without genAI assistance. I’m also particularly interested in hearing from experienced SWE that have leaned into LLMs for development work.</p> </div>   submitted by   <a href=\"https://www.reddit.com/user/TheCordlessSteve\"> /u/TheCordlessSteve </a> <br> <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc2r7b/complex_software/\">[link]</a></span>   <span><a href=\"https://www.reddit.com/r/vibecoding/comments/1pc2r7b/complex_software/\">[comments]</a></span>",
      "summary": "I am genuinely very confused after reading so much of what is posted about vibe coding software lately - I could use some help sorting this out. It’s likely that a large portion of those posts are either self-serving propaganda or bot-generated responses. Often, these same posts are in extreme support of vibe coding and make it seem like building software systems is a piece of cake with LLM’s.  My background includes a chemical engineering degree, time working for a Fortune 500 manufacturing com",
      "publishedAt": "2025-12-02T07:56:16.000Z",
      "author": "/u/TheCordlessSteve",
      "source": "rss",
      "feedName": "vibecoding",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 11.515537479518633,
      "ingestedAt": "2025-12-02T21:47:02.940Z",
      "tags": [
        "code_review",
        "ide",
        "testing",
        "governance",
        "Developer Communities"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09da1f55",
      "title": "Anthropic acquires Bun",
      "url": "https://bun.com/blog/bun-joins-anthropic",
      "content": "<p><img src=\"https://bun.com/og/blog/bun-joins-anthropic.png\" alt=\"bun-joins-anthropic.png\"></p><p>Article URL: <a href=\"https://bun.com/blog/bun-joins-anthropic\">https://bun.com/blog/bun-joins-anthropic</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46124267\">https://news.ycombinator.com/item?id=46124267</a></p>  \n<p>Points: 430</p>  \n<p># Comments: 178</p>",
      "summary": "Article URL: https://bun.com/blog/bun-joins-anthropic  \nComments URL: https://news.ycombinator.com/item?id=46124267  \nPoints: 430  \n# Comments: 178",
      "publishedAt": "2025-12-02T18:05:44.000Z",
      "author": "ryanvogel",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "funding_mna",
      "score": 3.4617872738550464,
      "ingestedAt": "2025-12-02T21:47:02.940Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09da1f58",
      "title": "100k TPS over a billion rows: the unreasonable effectiveness of SQLite",
      "url": "https://andersmurphy.com/2025/12/02/100000-tps-over-a-billion-rows-the-unreasonable-effectiveness-of-sqlite.html",
      "content": "<p>Article URL: <a href=\"https://andersmurphy.com/2025/12/02/100000-tps-over-a-billion-rows-the-unreasonable-effectiveness-of-sqlite.html\">https://andersmurphy.com/2025/12/02/100000-tps-over-a-billion-rows-the-unreasonable-effectiveness-of-sqlite.html</a></p> \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46124205\">https://news.ycombinator.com/item?id=46124205</a></p> \n<p>Points: 89</p> \n<p># Comments: 16</p>",
      "summary": "Article URL: https://andersmurphy.com/2025/12/02/100000-tps-over-a-billion-rows-the-unreasonable-effectiveness-of-sqlite.html \nComments URL: https://news.ycombinator.com/item?id=46124205 \nPoints: 89 \n# Comments: 16",
      "publishedAt": "2025-12-02T17:59:53.000Z",
      "author": "speckx",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4831926641868773,
      "ingestedAt": "2025-12-02T21:47:02.940Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09da1f59",
      "title": "The Junior Hiring Crisis",
      "url": "https://people-work.io/blog/junior-hiring-crisis/",
      "content": "<p><img title=\"Software Developer Headcount Over Time by Level\" src=\"https://people-work.io/cdn-cgi/image/format=auto,width=1920/assets/blog/junior-hiring-crisis/dev-hiring.webp\" alt=\"dev-hiring.webp\"></p><p>Article URL: <a href=\"https://people-work.io/blog/junior-hiring-crisis/\">https://people-work.io/blog/junior-hiring-crisis/</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46124063\">https://news.ycombinator.com/item?id=46124063</a></p>  \n<p>Points: 72</p>  \n<p># Comments: 61</p>",
      "summary": "Article URL: https://people-work.io/blog/junior-hiring-crisis/  \nComments URL: https://news.ycombinator.com/item?id=46124063  \nPoints: 72  \n# Comments: 61",
      "publishedAt": "2025-12-02T17:48:33.000Z",
      "author": "mooreds",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4823590931130286,
      "ingestedAt": "2025-12-02T21:47:02.940Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09da1f5a",
      "title": "Memtest86+ v8.00 Released",
      "url": "https://github.com/memtest86plus/memtest86plus/releases/tag/v8.00",
      "content": "<p>Article URL: <a href=\"https://github.com/memtest86plus/memtest86plus/releases/tag/v8.00\">https://github.com/memtest86plus/memtest86plus/releases/tag/v8.00</a></p> \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46123238\">https://news.ycombinator.com/item?id=46123238</a></p> \n<p>Points: 14</p> \n<p># Comments: 0</p>",
      "summary": "Article URL: https://github.com/memtest86plus/memtest86plus/releases/tag/v8.00 \nComments URL: https://news.ycombinator.com/item?id=46123238 \nPoints: 14 \n# Comments: 0",
      "publishedAt": "2025-12-02T16:50:25.000Z",
      "author": "voxadam",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 4.434272167977218,
      "ingestedAt": "2025-12-02T21:47:02.940Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09d8a6bd",
      "title": "Progress on TypeScript 7 – December 2025",
      "url": "https://devblogs.microsoft.com/typescript/progress-on-typescript-7-december-2025/",
      "content": "<p><img src=\"https://devblogs.microsoft.com/typescript/wp-content/uploads/sites/11/2018/08/typescriptfeature.png\" alt=\"typescriptfeature.png\"></p><p>Article URL: <a href=\"https://devblogs.microsoft.com/typescript/progress-on-typescript-7-december-2025/\">https://devblogs.microsoft.com/typescript/progress-on-typescript-7-december-2025/</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46123921\">https://news.ycombinator.com/item?id=46123921</a></p>  \n<p>Points: 25</p>  \n<p># Comments: 6</p>",
      "summary": "Article URL: https://devblogs.microsoft.com/typescript/progress-on-typescript-7-december-2025/  \nComments URL: https://news.ycombinator.com/item?id=46123921  \nPoints: 25  \n# Comments: 6",
      "publishedAt": "2025-12-02T17:37:06.000Z",
      "author": "DanRosenwasser",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.4445522506837785,
      "ingestedAt": "2025-12-02T21:47:02.940Z",
      "tags": [
        "code_review",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09d8a6be",
      "title": "4.3M Browsers Infected: Inside ShadyPanda's 7-Year Malware Campaign",
      "url": "https://www.koi.ai/blog/4-million-browsers-infected-inside-shadypanda-7-year-malware-campaign",
      "content": "<p><img src=\"https://cdn.prod.website-files.com/689ad8c5d13f40cf59df0e0c/6928c2c55001ece472486f7c_image-shady-panda.jpg\" alt=\"6928c2c55001ece472486f7c_image-shady-pan\"></p><p>Article URL: <a href=\"https://www.koi.ai/blog/4-million-browsers-infected-inside-shadypanda-7-year-malware-campaign\">https://www.koi.ai/blog/4-million-browsers-infected-inside-shadypanda-7-year-malware-campaign</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46122957\">https://news.ycombinator.com/item?id=46122957</a></p>  \n<p>Points: 27</p>  \n<p># Comments: 3</p>",
      "summary": "Article URL: https://www.koi.ai/blog/4-million-browsers-infected-inside-shadypanda-7-year-malware-campaign  \nComments URL: https://news.ycombinator.com/item?id=46122957  \nPoints: 27  \n# Comments: 3",
      "publishedAt": "2025-12-02T16:30:52.000Z",
      "author": "janpio",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.414412852511904,
      "ingestedAt": "2025-12-02T21:47:02.940Z",
      "tags": [
        "code_review",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09d746b6",
      "title": "Anthropic acquires Bun as Claude Code hits $1B revenue milestone - Investing.com Canada",
      "url": "https://news.google.com/rss/articles/CBMiuwFBVV95cUxPTnN2dVlRS1k4TkpUV2otYWxkMkoxd0NYNUZBVjRrRkY2VEJmeWFIb24wWE1BeU9KNWI3YUVVYm5EUThEaEJIZktvb2k4Nk1ZQmFFMUo0V3RrWjVzcEVhZDdXelRkYzRmX1hINmc4ZE15R2F1Yy1Oa2oxUWZzYUZvMG9hdHhUN2FvOHFSSUd4RHZ0d0JQbDBDajg4NThUVUJsTy1BRXZBMHFsRmFWcTVLbTBlNTZpeGdMZ3VF?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMiuwFBVV95cUxPTnN2dVlRS1k4TkpUV2otYWxkMkoxd0NYNUZBVjRrRkY2VEJmeWFIb24wWE1BeU9KNWI3YUVVYm5EUThEaEJIZktvb2k4Nk1ZQmFFMUo0V3RrWjVzcEVhZDdXelRkYzRmX1hINmc4ZE15R2F1Yy1Oa2oxUWZzYUZvMG9hdHhUN2FvOHFSSUd4RHZ0d0JQbDBDajg4NThUVUJsTy1BRXZBMHFsRmFWcTVLbTBlNTZpeGdMZ3VF?oc=5\">Anthropic acquires Bun as Claude Code hits $1B revenue milestone</a>  Investing.com Canada",
      "summary": "Anthropic acquires Bun as Claude Code hits $1B revenue milestone  Investing.com Canada",
      "publishedAt": "2025-12-02T17:58:14.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4830712744455214,
      "ingestedAt": "2025-12-02T21:47:02.942Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09d746b9",
      "title": "AWS Updates Its Nova Models To Compete With Google, Anthropic and OpenAI - The New Stack",
      "url": "https://news.google.com/rss/articles/CBMimwFBVV95cUxQZU5wdlhZblZ2ZjkzVXI4eXpJbElnNmt2WHUtbTNJdXFwYUhKQlkyZUx1UDc0cFZNallfYmVfY21WblVxV0xrUE83MnU5UXpJQUdUeVk2QlRkUjJTX01VVV82QlAzb0hoa2JTMHZEUDdlUUdwdlZCd0RVTXFJMlctTDN0a3JHR3RhazgtaDZQT2VLWXFSTjRocUxXOA?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMimwFBVV95cUxQZU5wdlhZblZ2ZjkzVXI4eXpJbElnNmt2WHUtbTNJdXFwYUhKQlkyZUx1UDc0cFZNallfYmVfY21WblVxV0xrUE83MnU5UXpJQUdUeVk2QlRkUjJTX01VVV82QlAzb0hoa2JTMHZEUDdlUUdwdlZCd0RVTXFJMlctTDN0a3JHR3RhazgtaDZQT2VLWXFSTjRocUxXOA?oc=5\">AWS Updates Its Nova Models To Compete With Google, Anthropic and OpenAI</a>  The New Stack",
      "summary": "AWS Updates Its Nova Models To Compete With Google, Anthropic and OpenAI  The New Stack",
      "publishedAt": "2025-12-02T17:19:13.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 3.4538088371761013,
      "ingestedAt": "2025-12-02T21:47:02.942Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09d746bb",
      "title": "Anthropic in Advanced Talks to Buy Developer Tool Startup in First Acquisition - The Information",
      "url": "https://news.google.com/rss/articles/CBMirgFBVV95cUxPNjVjTHlOZkJqMzI1cm81Q1p3OXVHbkN5d29Xc0J3SFF0X2U5RGJ6bXhRT0h2dW55Q2Z6OHA0WGVMdFE5U3hPdDZUVVRzaTF4NE9uSmdiYW1BWnFPTnJCQlN5R0NXbnhQb3dFNThLVy1wVHEwaVBWeDNaa3o5WExQQm03RGY2eEdBX2drSUVVc1BFakUwOUFvT0NVNkJOaFJWd0ZpVXJSakhHSkJlemc?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMirgFBVV95cUxPNjVjTHlOZkJqMzI1cm81Q1p3OXVHbkN5d29Xc0J3SFF0X2U5RGJ6bXhRT0h2dW55Q2Z6OHA0WGVMdFE5U3hPdDZUVVRzaTF4NE9uSmdiYW1BWnFPTnJCQlN5R0NXbnhQb3dFNThLVy1wVHEwaVBWeDNaa3o5WExQQm03RGY2eEdBX2drSUVVc1BFakUwOUFvT0NVNkJOaFJWd0ZpVXJSakhHSkJlemc?oc=5\">Anthropic in Advanced Talks to Buy Developer Tool Startup in First Acquisition</a>  The Information",
      "summary": "Anthropic in Advanced Talks to Buy Developer Tool Startup in First Acquisition  The Information",
      "publishedAt": "2025-12-02T17:11:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "funding_mna",
      "score": 3.4524014456324945,
      "ingestedAt": "2025-12-02T21:47:02.942Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09d746bd",
      "title": "Blackbaud Shares Climb on Anthropic Collaboration and Bigger Buyback Authorization - MSN",
      "url": "https://news.google.com/rss/articles/CBMi8gFBVV95cUxPV2lBM0xsNk15Q1Y3Y2NaQ05RUGRQaVpiamJ0amtLM1dsMUlVV0szNzdiQjZ5OHBhVWVlci1aWV9MVU05Y1ZxdlpxSXRIWHowTjNsd2UzeUdhSTc4LVkzYUNpRGluZzlZQ1lucGJsRk4tQWZ4cjg1NnJYUWpYdG1JSEVQWHd3c0FwME41R1FKRVhJUUVhOENoQXktQ3BreEhhOFRsRG5ObmVyQjBod1VmNFZaSUJIVWZBNUxCOS1qQjdwOWk3a0kyR0hELV9YU3VaM2tfT1dXSUctMzFSM1V6dGliTzEybVZEV0VSRk85VWxYZw?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMi8gFBVV95cUxPV2lBM0xsNk15Q1Y3Y2NaQ05RUGRQaVpiamJ0amtLM1dsMUlVV0szNzdiQjZ5OHBhVWVlci1aWV9MVU05Y1ZxdlpxSXRIWHowTjNsd2UzeUdhSTc4LVkzYUNpRGluZzlZQ1lucGJsRk4tQWZ4cjg1NnJYUWpYdG1JSEVQWHd3c0FwME41R1FKRVhJUUVhOENoQXktQ3BreEhhOFRsRG5ObmVyQjBod1VmNFZaSUJIVWZBNUxCOS1qQjdwOWk3a0kyR0hELV9YU3VaM2tfT1dXSUctMzFSM1V6dGliTzEybVZEV0VSRk85VWxYZw?oc=5\">Blackbaud Shares Climb on Anthropic Collaboration and Bigger Buyback Authorization</a>  MSN",
      "summary": "Blackbaud Shares Climb on Anthropic Collaboration and Bigger Buyback Authorization  MSN",
      "publishedAt": "2025-12-02T16:43:22.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.4327217562189105,
      "ingestedAt": "2025-12-02T21:47:02.942Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09d746c1",
      "title": "Introducing Claude for Nonprofits - Anthropic",
      "url": "https://news.google.com/rss/articles/CBMiYkFVX3lxTE5XWWVMUHJPVjZLbFN0WmU0UmhxdGo2eEhoUjhkNHp4ZFpUTnNoOVdSbDZORW05bkpHWVFqUy1XczRZZ28tWVJud0JGUEt6UHR5OUNnOWt6eHpBRVNwd2RnQ0hB?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMiYkFVX3lxTE5XWWVMUHJPVjZLbFN0WmU0UmhxdGo2eEhoUjhkNHp4ZFpUTnNoOVdSbDZORW05bkpHWVFqUy1XczRZZ28tWVJud0JGUEt6UHR5OUNnOWt6eHpBRVNwd2RnQ0hB?oc=5\">Introducing Claude for Nonprofits</a>  Anthropic",
      "summary": "Introducing Claude for Nonprofits  Anthropic",
      "publishedAt": "2025-12-02T16:04:32.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 7.373652358449511,
      "ingestedAt": "2025-12-02T21:47:02.942Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09d746c4",
      "title": "Anthropic’s Claude Opus 4.5 is here: Cheaper AI, infinite chats, and coding skills that beat humans - VentureBeat",
      "url": "https://news.google.com/rss/articles/CBMinwFBVV95cUxQUVVtZUxlMERpeV9qVllXeEhvWS1ZN0JFMG50em1QTXdaQ3FsTllBdGRiUDNvVFVKYkhqNVBBQkEyRGJYcnlFOWJxQ19hR3E1SzM2TzV1Qjk0T2hQRGpKZ2x6VEx1OEc3bWRiNlVodEJDVnI1SWMyVjRjdW51YU9UcE9Oc1BaRFd1em9STFJMVkJHLUtRNEIzWXZZSHZVSnM?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMinwFBVV95cUxQUVVtZUxlMERpeV9qVllXeEhvWS1ZN0JFMG50em1QTXdaQ3FsTllBdGRiUDNvVFVKYkhqNVBBQkEyRGJYcnlFOWJxQ19hR3E1SzM2TzV1Qjk0T2hQRGpKZ2x6VEx1OEc3bWRiNlVodEJDVnI1SWMyVjRjdW51YU9UcE9Oc1BaRFd1em9STFJMVkJHLUtRNEIzWXZZSHZVSnM?oc=5\">Anthropic’s Claude Opus 4.5 is here: Cheaper AI, infinite chats, and coding skills that beat humans</a>  VentureBeat",
      "summary": "Anthropic’s Claude Opus 4.5 is here: Cheaper AI, infinite chats, and coding skills that beat humans  VentureBeat",
      "publishedAt": "2025-11-24T08:00:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "benchmark_eval",
      "score": 1.35504938862854,
      "ingestedAt": "2025-12-02T21:47:02.942Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09d746c9",
      "title": "Anthropic flags AI-driven cyberattacks, warns that cybersecurity has reached a critical inflection point - Industrial Cyber",
      "url": "https://news.google.com/rss/articles/CBMizwFBVV95cUxNOE9wb1M0bzd1QXliZFZXU0pieXpyMXoyMXlYZWVMaDUyQURlUkUxQnFEcjBueEEySmVPRmlrSTQzZDJyUDQ5Zk1OWENHMnEzYlB6dFJMTWpYakdOcjVMWTA5M2xNY3lJNGRrakVhbEJOWm4tVE52Q3VZRi1sNUtYbEpHQUFtcjN1MjkxQTVFaklVeUNSa1lQaG1RNzhHOXFuQllZVk1UelIxRUZpT2djMHBVb1cxWm9ldnlZZGNfRVdNY2NRWG1KOG54bFVEdHc?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMizwFBVV95cUxNOE9wb1M0bzd1QXliZFZXU0pieXpyMXoyMXlYZWVMaDUyQURlUkUxQnFEcjBueEEySmVPRmlrSTQzZDJyUDQ5Zk1OWENHMnEzYlB6dFJMTWpYakdOcjVMWTA5M2xNY3lJNGRrakVhbEJOWm4tVE52Q3VZRi1sNUtYbEpHQUFtcjN1MjkxQTVFaklVeUNSa1lQaG1RNzhHOXFuQllZVk1UelIxRUZpT2djMHBVb1cxWm9ldnlZZGNfRVdNY2NRWG1KOG54bFVEdHc?oc=5\">Anthropic flags AI-driven cyberattacks, warns that cybersecurity has reached a critical inflection point</a>  Industrial Cyber",
      "summary": "Anthropic flags AI-driven cyberattacks, warns that cybersecurity has reached a critical inflection point  Industrial Cyber",
      "publishedAt": "2025-11-17T08:00:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "security_incident",
      "score": 3.123140198586662,
      "ingestedAt": "2025-12-02T21:47:02.942Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09d746ca",
      "title": "The State of Maryland partners with Anthropic to better serve residents - Anthropic",
      "url": "https://news.google.com/rss/articles/CBMiYEFVX3lxTE5UdmI4MWhSNjdXOFNjeWJHeldtbUwxZldTdVUwUi0zT0Q5WTJPOFQtTGxfLTFUMnFycGtJWVlpeTNhRUI3Z21LMHJKVWhRMFYzYlhXaVBfUXozeG5ScVEwdQ?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMiYEFVX3lxTE5UdmI4MWhSNjdXOFNjeWJHeldtbUwxZldTdVUwUi0zT0Q5WTJPOFQtTGxfLTFUMnFycGtJWVlpeTNhRUI3Z21LMHJKVWhRMFYzYlhXaVBfUXozeG5ScVEwdQ?oc=5\">The State of Maryland partners with Anthropic to better serve residents</a>  Anthropic",
      "summary": "The State of Maryland partners with Anthropic to better serve residents  Anthropic",
      "publishedAt": "2025-11-13T08:00:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 0.6176234058759253,
      "ingestedAt": "2025-12-02T21:47:02.942Z",
      "tags": [
        "ide",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09d5b865",
      "title": "Apple to beat Samsung in smartphone shipments for first time in 14 years",
      "url": "https://sherwood.news/tech/apple-to-beat-samsung-in-smartphone-shipments-for-first-time-in-14-years/",
      "content": "<p><img src=\"https://sherwoodnews.imgix.net/mwphzyq69oso/en-US/assets/files/2235712789_apple-ceo-tim-cook-joins-apple-employees-at-the-apple-store-as-customers-line-up-for-the.jpg?auto=compress%2Cformat&amp;cs=srgb\" alt=\"2235712789_apple-ceo-tim-cook-joins-appl\"></p><p>Article URL: <a href=\"https://sherwood.news/tech/apple-to-beat-samsung-in-smartphone-shipments-for-first-time-in-14-years/\">https://sherwood.news/tech/apple-to-beat-samsung-in-smartphone-shipments-for-first-time-in-14-years/</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46123747\">https://news.ycombinator.com/item?id=46123747</a></p>  \n<p>Points: 21</p>  \n<p># Comments: 19</p>",
      "summary": "Article URL: https://sherwood.news/tech/apple-to-beat-samsung-in-smartphone-shipments-for-first-time-in-14-years/  \nComments URL: https://news.ycombinator.com/item?id=46123747  \nPoints: 21  \n# Comments: 19",
      "publishedAt": "2025-12-02T17:24:33.000Z",
      "author": "avonmach",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "funding_mna",
      "score": 6.415913519022295,
      "ingestedAt": "2025-12-02T21:47:02.942Z",
      "tags": [
        "code_review",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09d16354",
      "title": "A pragmatic guide to LLM evals for devs",
      "url": "https://www.inoreader.com/article/3a9c6e76b2da9cab",
      "content": "<div class=\"email_is_html\"><div><div style=\"font-kerning: auto; --image-offset-margin: -120px\"><img src=\"https://eotrx.substackcdn.com/open?token=eyJtIjoiPDIwMjUxMjAyMTcxOTU3LjMuMjZiYjkyZGMyY2JkODM3ZUBtZzEuc3Vic3RhY2suY29tPiIsInUiOjE2MTk4NTQ4MCwiciI6ImJ5dGVieXRlZ284OEBpbm8udG8iLCJkIjoibWcxLnN1YnN0YWNrLmNvbSIsInAiOjE4MDUxOTE0NSwidCI6Im5ld3NsZXR0ZXIiLCJhIjoib25seV9wYWlkIiwicyI6NDU4NzA5LCJjIjoicG9zdCIsImYiOmZhbHNlLCJwb3NpdGlvbiI6InRvcCIsImlhdCI6MTc2NDY5NjY2NiwiZXhwIjoxNzY3Mjg4NjY2LCJpc3MiOiJwdWItMCIsInN1YiI6ImVvIn0.m9Fo1Q0kzMvpsRP1q70X1q-cOQC5zkldUtB6_u96m1M\" width=\"1\" height=\"1\" border=\"0\" style=\"height: 1px !important; width: 1px !important; border-width: 0 !important; margin-top: 0 !important; margin-bottom: 0 !important; margin-right: 0 !important; margin-left: 0 !important; padding-top: 0 !important; padding-bottom: 0 !important; padding-right: 0 !important; padding-left: 0 !important\" /><div style=\"display: none; font-size: 1px; color: #333333; line-height: 1px; max-height: 0px; max-width: 0px; opacity: 0; overflow: hidden\">Evals are a new toolset for any and all AI engineers – and software engineers should also know about them. Move from guesswork to a systematic engineering process for improving AI quality.</div><div style=\"display: none; font-size: 1px; color: #333333; line-height: 1px; max-height: 0px; max-width: 0px; opacity: 0; overflow: hidden\">͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­</div><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td></td><td width=\"550\"></td><td></td></tr><tr><td></td><td width=\"550\" align=\"left\"><div style=\"font-size: 16px; line-height: 26px; max-width: 550px; width: 100%; margin: 0 auto; overflow-wrap: break-word\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"right\" style=\"height: 20px\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><span style=\"font-family: SF Pro Text, -apple-system, system-ui, BlinkMacSystemFont, Inter, Segoe UI, Roboto, Helvetica, Arial, sans-serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol; font-size: 13px; color: unset; list-style: none; text-decoration: unset; margin: 0\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9uZXdzbGV0dGVyLnByYWdtYXRpY2VuZ2luZWVyLmNvbS9wL2V2YWxzP3V0bV9jYW1wYWlnbj1lbWFpbC1wb3N0JnI9Mm9md3NvJnRva2VuPWV5SjFjMlZ5WDJsa0lqb3hOakU1T0RVME9EQXNJbkJ2YzNSZmFXUWlPakU0TURVeE9URTBOU3dpYVdGMElqb3hOelkwTmprMk5qWTJMQ0psZUhBaU9qRTNOamN5T0RnMk5qWXNJbWx6Y3lJNkluQjFZaTAwTlRnM01Ea2lMQ0p6ZFdJaU9pSndiM04wTFhKbFlXTjBhVzl1SW4wLmw1SUxEMjJxelRwa1VHbjF3Z3UtTFFJTW1wY1Z4dVctVUUwM0lMamFyZEUiLCJwIjoxODA1MTkxNDUsInMiOjQ1ODcwOSwiZiI6ZmFsc2UsInUiOjE2MTk4NTQ4MCwiaWF0IjoxNzY0Njk2NjY2LCJleHAiOjIwODAyNzI2NjYsImlzcyI6InB1Yi0wIiwic3ViIjoibGluay1yZWRpcmVjdCJ9.HB7hedo7ot31H-sI4Ucf44KXBIZO6g9L4qKIMI5Dpek?\" style=\"color: rgb(119,119,119); -webkit-text-decoration-line: underline; text-decoration-line: underline\" target=\"_blank\" rel=\"noreferrer\">View in browser</a></span></td></tr></tbody></table></td></tr></tbody></table><table style=\"border-spacing: 0; padding: 16px 0 32px\"><tbody><tr><td align=\"center\" style=\"text-align: center; padding: 0\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9uZXdzbGV0dGVyLnByYWdtYXRpY2VuZ2luZWVyLmNvbS9wL2V2YWxzP3V0bV9jYW1wYWlnbj1lbWFpbC1oYWxmLXBvc3Qmcj0yb2Z3c28mdG9rZW49ZXlKMWMyVnlYMmxrSWpveE5qRTVPRFUwT0RBc0luQnZjM1JmYVdRaU9qRTRNRFV4T1RFME5Td2lhV0YwSWpveE56WTBOamsyTmpZMkxDSmxlSEFpT2pFM05qY3lPRGcyTmpZc0ltbHpjeUk2SW5CMVlpMDBOVGczTURraUxDSnpkV0lpT2lKd2IzTjBMWEpsWVdOMGFXOXVJbjAubDVJTEQyMnF6VHBrVUduMXdndS1MUUlNbXBjVnh1Vy1VRTAzSUxqYXJkRSIsInAiOjE4MDUxOTE0NSwicyI6NDU4NzA5LCJmIjpmYWxzZSwidSI6MTYxOTg1NDgwLCJpYXQiOjE3NjQ2OTY2NjYsImV4cCI6MjA4MDI3MjY2NiwiaXNzIjoicHViLTAiLCJzdWIiOiJsaW5rLXJlZGlyZWN0In0.l2AC6Ra7HKELxS7I4dMQSBRm-q9aBtYQ0WzmAQmEo70?\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"110\" src=\"https://substackcdn.com/image/fetch/$s_!XLTi!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f6c431d-310a-45a0-90fe-3dc43354ef38_1100x220.png\" style=\"border: none !important; vertical-align: middle; max-width: 550px; display: block; margin: 0 auto; height: auto; width: 100%\" /></a></td></tr></tbody></table><div style=\"--image-offset-margin: -120px; font-size: 16px; line-height: 26px\"><div style=\"font-size: 16px; line-height: 26px; margin-top: 16px\"><div dir=\"auto\" style=\"text-align: initial; font-size: 16px; line-height: 26px; width: 100%; word-break: break-word; margin-bottom: 16px\"><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px; margin-top: 0; margin-bottom: 0\"><em>👋 Hi, this is Gergely with a subscriber-only issue of the Pragmatic Engineer Newsletter. In every issue, I cover challenges at Big Tech and startups through the lens of engineering managers and senior engineers.</em></p></div><hr style=\"padding: 0; height: 1px; background: #e6e6e6; border: none; margin: 16px 0 0\" /></div></div><div dir=\"auto\" style=\"--image-offset-margin: -120px; padding: 32px 0 0 0; font-size: 16px; line-height: 26px\"><div style=\"font-size: 16px; line-height: 26px\"><h1 dir=\"auto\" style=\"direction: auto; text-align: start; unicode-bidi: isolate; color: rgb(54,55,55); font-family: Lora,sans-serif; font-weight: 600; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 0; line-height: 36px; font-size: 32px\"><a href=\"https://substack.com/app-link/post?publication_id=458709&amp;post_id=180519145&amp;utm_source=post-email-title&amp;utm_campaign=email-post-title&amp;isFreemail=false&amp;r=2ofwso&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE4MDUxOTE0NSwiaWF0IjoxNzY0Njk2NjY2LCJleHAiOjE3NjcyODg2NjYsImlzcyI6InB1Yi00NTg3MDkiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.l5ILD22qzTpkUGn1wgu-LQIMmpcVxuW-UE03ILjardE\" style=\"color: rgb(54,55,55); text-decoration: none\" target=\"_blank\" rel=\"noreferrer\">A pragmatic guide to LLM evals for devs</a></h1><h3 dir=\"auto\" style=\"direction: auto; text-align: start; unicode-bidi: isolate; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: normal; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 4px 0 0; color: #777777; line-height: 24px; font-size: 18px; margin-top: 12px\">Evals are a new toolset for any and all AI engineers – and software engineers should also know about them. Move from guesswork to a systematic engineering process for improving AI quality.</h3><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"margin: 1em 0; height: 20px; align-items: center\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><div style=\"list-style: none; font-size: 11px; line-height: 20px; text-decoration: unset; color: rgb(54,55,55); margin: 0; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px\"><a style=\"list-style: none; color: rgb(54,55,55); margin: 0; font-size: 11px; line-height: 20px; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px; text-decoration: none\" href=\"https://substack.com/@pragmaticengineer\" target=\"_blank\" rel=\"noreferrer\">Gergely Orosz</a> and <a style=\"list-style: none; color: rgb(54,55,55); margin: 0; font-size: 11px; line-height: 20px; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px; text-decoration: none\" href=\"https://substack.com/@hamelhusain\" target=\"_blank\" rel=\"noreferrer\">Hamel Husain</a></div></td></tr></tbody></table></td></tr><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><div style=\"list-style: none; font-size: 11px; line-height: 20px; text-decoration: unset; color: rgb(119,119,119); margin: 0; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px\">Dec 2</div></td><td width=\"4\" style=\"min-width: 4px\"></td><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><div style=\"list-style: none; font-size: 11px; line-height: 20px; text-decoration: unset; color: rgb(119,119,119); margin: 0; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px\">∙</div></td><td width=\"4\" style=\"min-width: 4px\"></td><td style=\"vertical-align: middle\"><div style=\"list-style: none; font-size: 11px; line-height: 20px; text-decoration: unset; color: rgb(94,73,217); margin: 0; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px\">Paid</div></td></tr></tbody></table></td></tr></tbody></table></td></tr></tbody></table></td><td align=\"right\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><a href=\"https://substack.com/@pragmaticengineer\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!CPFa!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58fed27c-f331-4ff3-ba47-135c5a0be0ba_400x400.png\" style=\"box-sizing: border-box; border-radius: 500000px; max-width: 550px; border: none; vertical-align: middle; width: 32px; height: 32px; min-width: 32px; min-height: 32px; object-fit: cover; margin: 0px; display: inline\" width=\"32\" height=\"32\" /></a></td><td width=\"4\" style=\"min-width: 4px\"></td><td style=\"vertical-align: middle\"><a href=\"https://substack.com/@hamelhusain\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!7sqx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feee58cd7-9a81-4ef6-b0f4-faeed62d5166_400x400.jpeg\" style=\"box-sizing: border-box; border-radius: 500000px; max-width: 550px; border: none; vertical-align: middle; width: 32px; height: 32px; min-width: 32px; min-height: 32px; object-fit: cover; margin: 0px; display: inline\" width=\"32\" height=\"32\" /></a></td></tr></tbody></table></td></tr></tbody></table><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"border-top: 1px solid rgb(0,0,0,.1); border-bottom: 1px solid rgb(0,0,0,.1); min-width: 100%\"><tbody><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr><tr><td><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=458709&amp;post_id=180519145&amp;utm_source=substack&amp;isFreemail=false&amp;submitLike=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE4MDUxOTE0NSwicmVhY3Rpb24iOiLinaQiLCJpYXQiOjE3NjQ2OTY2NjYsImV4cCI6MTc2NzI4ODY2NiwiaXNzIjoicHViLTQ1ODcwOSIsInN1YiI6InJlYWN0aW9uIn0.yQLSExL2yvrJ1fauBiMQH-TeiM9Ol8OZI1sk7mcf6yM&amp;utm_medium=email&amp;utm_campaign=email-reaction&amp;r=2ofwso\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!PeVs!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideHeart%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=458709&amp;post_id=180519145&amp;utm_source=substack&amp;utm_medium=email&amp;isFreemail=false&amp;comments=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE4MDUxOTE0NSwiaWF0IjoxNzY0Njk2NjY2LCJleHAiOjE3NjcyODg2NjYsImlzcyI6InB1Yi00NTg3MDkiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.l5ILD22qzTpkUGn1wgu-LQIMmpcVxuW-UE03ILjardE&amp;r=2ofwso&amp;utm_campaign=email-half-magic-comments&amp;action=post-comment&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!x1tS!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideComments%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=458709&amp;post_id=180519145&amp;utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;utm_campaign=email-share&amp;action=share&amp;triggerShare=true&amp;isFreemail=false&amp;r=2ofwso&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE4MDUxOTE0NSwiaWF0IjoxNzY0Njk2NjY2LCJleHAiOjE3NjcyODg2NjYsImlzcyI6InB1Yi00NTg3MDkiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.l5ILD22qzTpkUGn1wgu-LQIMmpcVxuW-UE03ILjardE\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!_L14!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideShare2%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9vcGVuLnN1YnN0YWNrLmNvbS9wdWIvcHJhZ21hdGljZW5naW5lZXIvcC9ldmFscz91dG1fc291cmNlPXN1YnN0YWNrJnV0bV9tZWRpdW09ZW1haWwmdXRtX2NhbXBhaWduPWVtYWlsLXJlc3RhY2stY29tbWVudCZhY3Rpb249cmVzdGFjay1jb21tZW50JnI9Mm9md3NvJnRva2VuPWV5SjFjMlZ5WDJsa0lqb3hOakU1T0RVME9EQXNJbkJ2YzNSZmFXUWlPakU0TURVeE9URTBOU3dpYVdGMElqb3hOelkwTmprMk5qWTJMQ0psZUhBaU9qRTNOamN5T0RnMk5qWXNJbWx6Y3lJNkluQjFZaTAwTlRnM01Ea2lMQ0p6ZFdJaU9pSndiM04wTFhKbFlXTjBhVzl1SW4wLmw1SUxEMjJxelRwa1VHbjF3Z3UtTFFJTW1wY1Z4dVctVUUwM0lMamFyZEUiLCJwIjoxODA1MTkxNDUsInMiOjQ1ODcwOSwiZiI6ZmFsc2UsInUiOjE2MTk4NTQ4MCwiaWF0IjoxNzY0Njk2NjY2LCJleHAiOjIwODAyNzI2NjYsImlzcyI6InB1Yi0wIiwic3ViIjoibGluay1yZWRpcmVjdCJ9.5JXXUkE4kJeTEX3yEXgfaJa39Q0vPUY9WRWN_qvbZzI?&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!5EGt!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FNoteForwardIcon%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td></tr></tbody></table></td><td align=\"right\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://open.substack.com/pub/pragmaticengineer/p/evals?utm_source=email&amp;redirect=app-store&amp;utm_campaign=email-read-in-app\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><div style=\"font-size: 16px; line-height: 26px; display: inline-block; vertical-align: middle; max-width: 0; min-height: 18px\"></div><span style=\"vertical-align: middle; margin-right: 4px\">READ IN APP</span><img src=\"https://substackcdn.com/image/fetch/$s_!ET-_!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideArrowUpRight%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"min-width: 18px; min-height: 18px; border: none; vertical-align: middle; margin-right: 0; margin-left: 0; max-width: 18px\" /></a></td></tr></tbody></table></td></tr></tbody></table></td></tr></tbody></table></td></tr><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr></tbody></table></div></div><div dir=\"auto\" style=\"--image-offset-margin: -120px; padding: 32px 0 0 0; font-size: 16px; line-height: 26px\"><div dir=\"auto\" style=\"text-align: initial; font-size: 16px; line-height: 26px; width: 100%; word-break: break-word; margin-bottom: 16px; font-family: 'SF Pro Display', -apple-system, system-ui, BlinkMacSystemFont, 'Inter', 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; font-weight: 400\"><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px; margin-top: 0\"><span>One word that keeps cropping up when I talk with software engineers who build large language model (LLM)-based solutions is “</span><strong>evals</strong><span>”. They use evaluations to verify that LLM solutions work well enough because LLMs are non-deterministic, meaning there’s no guarantee they’ll provide the same answer to the same question twice. This makes it more complicated to verify that things work according to spec than it does with other software, for which automated tests are available.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Evals feel like they are becoming a core part of the AI engineering toolset. And because they are also becoming part of CI/CD pipelines, we, software engineers, should understand them better — especially because we might need to use them sooner rather than later! So, what do good evals look like, and how should this non-deterministic-testing space be approached?</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>For directions, I turned to an expert on the topic, </span><a href=\"https://substack.com/redirect/8e53f412-9b53-4f43-a6bc-5e90dc9b3f2f?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Hamel Husain</a><span>. He’s worked as a Machine Learning engineer at companies including Airbnb and GitHub, and teaches the online course </span><a href=\"https://substack.com/redirect/281991a1-f899-44b5-8650-6da0d247163b?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">AI Evals For Engineers &amp; PMs</a><span> — the upcoming cohort </span><a href=\"https://substack.com/redirect/281991a1-f899-44b5-8650-6da0d247163b?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">starts in January</a><span>. Hamel is currently writing a book, </span><a href=\"https://substack.com/redirect/27f80c78-da8f-4265-ae20-28b2021ea6eb?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Evals for AI Engineers</a><span>, to be published by O’Reilly next year.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">In today’s issue, we cover:</p><ol style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Vibe-check development trap. </strong><span>An agent appears to work well, but as soon as it is modified, it can’t be established that it’s working correctly.</span></p></li><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Core workflow: error analysis</strong><span>. Error analysis has been a key part of machine learning for decades and is useful for building LLM applications.</span></p></li><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Building evals: the right tools for the job. </strong><span>Use code-based evals for deterministic failures, and an LLM-as-judge for subjective cases.</span></p></li><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Building an LLM-as-judge. </strong><span>Avoid your LLM judge memorizing answers by partitioning your data and measuring how well the judge generalizes to unfamiliar data.</span></p></li><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Align the judge, keep trust. </strong><span>The LLM judge’s expertise needs to be validated against human expertise. Consider metrics like True Positive Rate (TPR) and True Negative Rate (TNR).</span></p></li><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Evals in practice: from CI/CD to production monitoring. </strong><span>Use evals in the CI/CD pipeline, but use production data to continuously validate that they work as expected, too.</span></p></li><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Flywheel of improvement. </strong><span>Analyze → measure → Improve → automate → start again</span></p></li></ol><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">With that, it’s over to Hamel:</p><div style=\"font-size: 16px; line-height: 26px\"><hr style=\"margin: 32px 0; padding: 0; height: 1px; background: #e6e6e6; border: none\" /></div><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em\">1. Vibe-check development trap</h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Organizations are embedding LLMs into applications from customer service to content creation. Yet, unlike traditional software, LLM pipelines don’t produce deterministic outputs; their responses are often subjective and context-dependent. A response might be factually accurate but have the wrong tone, or sound persuasive while being completely wrong. This ambiguity makes evaluation fundamentally different from conventional software testing. The core challenge is to systematically measure the quality of our AI systems and diagnose their failures.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>I recently worked with </span><a href=\"https://substack.com/redirect/53963a28-849b-4f00-9e31-e2db335e3256?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">NurtureBoss</a><span>, an AI startup building a leasing assistant for apartment property managers. The assistant helps with tour scheduling, answers routine tenant questions, and inbound sales. Here is a screenshot of how the product appears to customers:</span></p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"454\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/f7a51d0b-85fb-4fdc-b56c-e782c9309aab?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"454\" height=\"714.2426778242677\" src=\"https://substackcdn.com/image/fetch/$s_!rWD6!,w_454,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84654e06-c3b2-437f-8c0b-5bc5bec9b387_478x752.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table><em><span>The AI leasing assistant of NurtureBoss. Source: </span><a href=\"https://substack.com/redirect/37cf7d8a-f5ef-4a96-9aeb-6bdc31aa3ccf?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">nurtureboss.io</a></em></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">They had built a sophisticated agent, but the development process felt like guesswork: they’d change a prompt, test a few inputs, and if it “looked good to me” (LGTM), they’d ship it. This is the “vibes-based development” trap, and it’s where many AI projects go off the rails.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">To understand why this happens, it helps to think of LLM development as bridging three fundamental gaps, or “gulfs”:</p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Gulf of Comprehension:</strong><span> The gap between a developer and a true understanding of their data and the model’s behavior at scale. It’s impossible to manually read every user query and inspect every AI response to grasp the subtle ways a system might fail.</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Gulf of Specification:</strong><span> The gap between what we </span><em>want</em><span> the LLM to do, and what our prompts </span><em>actually instruct</em><span> it to do. LLMs cannot read our minds; an underspecified prompt forces them to guess our intent, leading to inconsistent outputs.</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Gulf of Generalization:</strong><span> The gap between a well-written prompt and the model’s ability to apply those instructions reliably across all possible inputs. Even with perfect instructions, a model can still fail on new or unusual data.</span></p></li></ul><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1124\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/7907b900-5c66-4391-827d-630703925c5a?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"351.33451957295375\" src=\"https://substackcdn.com/image/fetch/$s_!ot3a!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b28ee4d-71ec-477f-9bbb-fe67e28012c7_1124x718.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table><em>The “Three Gulfs” model of challenges in LLM pipeline development</em></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Navigating these gulfs is the central task of an AI engineer. This is where a naive application of Test-Driven Development (TDD) often falls short. TDD works because for a given input, there is a single, deterministic, knowable, correct output to assert against. But with LLMs, that’s not true: ask an AI to draft an email, and there isn’t one right answer, but thousands.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">The challenge isn’t just the “infinite surface area” of inputs; it’s the vast space of valid, subjective, and unpredictable outputs. Indeed, you can’t test for correctness before you’ve systematically observed the range of possible outputs and have defined what “good” even means for your product.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>In this article, we walk through the pragmatic workflow we’ve applied at NurtureBoss, and at over 40 other companies, in order to move from guesswork to a repeatable engineering discipline. It is the same framework we’ve taught to over 3,000 engineers in our </span><a href=\"https://substack.com/redirect/281991a1-f899-44b5-8650-6da0d247163b?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">AI Evals course</a><span>. By the end of this article, we’ll have gone through all the steps of what we call “the flywheel of improvement.”</span></p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1368\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/f74a46e3-485e-4f7a-aca7-187cfcb53908?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"448.6842105263158\" src=\"https://substackcdn.com/image/fetch/$s_!q0Jk!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd73dd92f-097e-4124-bbd5-698c69505f18_1368x1116.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table><em>A better alternative to vibes-based development of LLM apps, using evals</em></div><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em\">2. Core workflow: error analysis</h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">To get past the LGTM trap, the NurtureBoss team adopted a new workflow, starting with a systematic review of their conversation traces. A trace is the complete record of an interaction: the initial user query, all intermediate LLM reasoning steps, any tool calls made, and the final user-facing response. It’s everything you need to reconstruct what actually happened. Below is a screenshot of what a trace might look like for NurtureBoss:</p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1208\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/be6352bd-537b-46c2-bbad-1a8f86e83306?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"651.0761589403974\" src=\"https://substackcdn.com/image/fetch/$s_!uVKF!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0e4590e-5f21-43a5-b2a5-5198bbeafd53_1208x1430.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table><em><span>A trace rendered in </span><a href=\"https://substack.com/redirect/3e3eee26-1684-4dbe-ac6a-8cdcaf7399c2?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Arize Phoenix</a><span>, an open source LLM observability &amp; eval tool</span></em></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>There are many ways to view traces, including with LLM-specific observability tools. Examples that I often come across in practice are </span><a href=\"https://substack.com/redirect/9f51c54d-2a00-481d-884a-b07520e3b54b?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">LangSmith</a><span>, </span><a href=\"https://substack.com/redirect/3e3eee26-1684-4dbe-ac6a-8cdcaf7399c2?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Arize</a><span> (pictured above), and </span><a href=\"https://substack.com/redirect/76d0792d-0bb8-4b38-85f2-9ab2b1f744ab?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Braintrust</a><span>.</span></p><h3 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.375em\">From raw notes to clear priorities</h3><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">At first, it was an unglamorous process of manually using a trace viewer to review data. But as Jacob, the founder of NurtureBoss, reviewed traces, the friction became obvious. Off-the-shelf observability tools like LangSmith, Arize, Braintrust, and others offer decent ways to get started on reviewing data quickly, but are generic by design. For the NurtureBoss use case, it was cumbersome to view all necessary context, such as specific property data and client preferences on a single screen. This friction slowed them down.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Instead of fighting their tools, they invested a few hours in vibe coding a simple, custom data viewer using an AI assistant. It wasn’t fancy, but solved their core problems by showing each conversation clearly on one screen, with collapsible sections for tool calls and a simple text box for adding notes. Below is a screenshot of one of the screens in their data viewer:</p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1456\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/a570fb30-c098-49c2-a135-ee0f79b46f7a?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"301.4423076923077\" src=\"https://substackcdn.com/image/fetch/$s_!QWMQ!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88539b85-17cc-4334-87fb-ebe6026b3e89_1600x877.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table><em>Data viewer built by NurtureBoss. Building internal tools is a good use case for vibe-coding</em></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>This simple tool was a game-changer. It unlocked a significant improvement in review speed, allowing the team to get through hundreds of traces efficiently. With this higher throughput, they began adding open-ended notes on any behavior that felt wrong, a process known as </span><strong><a href=\"https://substack.com/redirect/53963a28-849b-4f00-9e31-e2db335e3256?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">open coding</a></strong><span>. When open coding, it is important to avoid predefined checklists of errors like “hallucination” or “toxicity”. Instead, let the data speak for itself, and jot down descriptive observations like:</span></p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">“The agent missed a clear opportunity to re-engage a price-sensitive user.”</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">“It asked to send a text confirmation twice in a row.”</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">“Once the user asked to be transferred to a human, the agent kept trying to solve the problem instead of just making the handoff.”</p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>After annotating dozens of conversations, NurtureBoss had amassed a rich, messy collection of notes. To find the signal amid the noise, they grouped these notes into themes in a step called </span><strong><a href=\"https://substack.com/redirect/2d2109f1-c59e-4a7d-84ae-e96727c76e17?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">axial coding</a></strong><span>. An LLM helped with the initial grouping of open codes into categories (aka the axial codes), which the team then reviewed and refined. A simple pivot table revealed that just three issues accounted for most problems: date handling, handoff failures, and conversation flow issues. This provided a clear, data-driven priority of failure modes in the application. Below is an illustration of what a partial view of this pivot table might look like, which is a count of failure categories:</span></p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"577\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/9b3f9b91-f146-44aa-bc6c-fd1568e8bb2d?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"207.97266514806378\" src=\"https://substackcdn.com/image/fetch/$s_!3yLh!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb0f0793-ab7d-4204-b0fb-da4442092231_878x332.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table>Using axial coding to group open code into buckets</div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">This bottom-up approach is the antidote to the problems of generic, off-the-shelf metrics. Many teams are tempted to grab a pre-built “hallucination score” or “helpfulness” eval, but in my experience, these metrics are often worse than useless. For instance, I recently worked with a mental health startup whose evaluation dashboard was filled with generic metrics like ‘helpfulness’ and ‘factuality,’ all rated on a 1-5 scale. While the scores looked impressive, they were unactionable; the team couldn’t tell what made a response a ‘3’ versus a ‘4,’ and the metrics didn’t correlate with what actually mattered to users.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>They create a false sense of security, leading teams to optimize for scores that don’t actually correlate with user satisfaction. In contrast, by letting failure modes emerge from your own data, you ensure your evaluation efforts are focused on </span><em>real</em><span> problems.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>This process of discovering failures from data isn’t a new trick invented for LLMs; it’s a battle-tested discipline known as </span><strong><a href=\"https://substack.com/redirect/b88a4329-8306-46d5-804e-aaeff572db33?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">error analysis</a></strong><span>, which has been a cornerstone of machine learning for decades, and is adapted from rigorous qualitative research methods like </span><a href=\"https://substack.com/redirect/e281aba9-e331-4ad2-b87a-1f67a7ebed19?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">grounded theory</a><span> in the social sciences. Next, let’s summarize this process into a step-by-step guide to apply to your problem.</span></p><h3 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.375em\">Step-by-step guide to error analysis</h3><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">This bottom-up process is the single highest-ROI activity in AI development. It ensures you’re solving real problems, not chasing vanity metrics. Here’s how to apply it:</p><ol style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Build a simple data viewer.</strong><span> This is your most important investment. A custom web app, tailored to your domain, allows you to show all necessary context in one place and makes capturing feedback trivial.</span></p></li><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Open coding: bottom-up annotation.</strong><span> </span><a href=\"https://substack.com/redirect/41d2b081-6508-458c-a93a-43bc15618349?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Open coding</a><span> (not to be confused with software coding) is a technique of writing open-ended notes about observed problems. In the context of LLMs, you review at least 100 diverse traces and add open-ended notes on any undesirable behavior. When reviewing a complex trace, the most effective tactic is to identify and annotate only the first upstream failure. LLM pipelines are causal systems; a single error in an early step like misinterpreting user intent often creates a cascade of downstream issues. Focusing on the first observable error is more efficient by preventing you from getting bogged down in cataloging every symptom. Often, fixing that single issue resolves an entire chain of subsequent failures.</span></p></li><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Axial coding: create a taxonomy.</strong><span> Now you have open-ended notes from step 2, it’s time to categorize them into buckets to understand the patterns. Group your open-ended notes into 5-10 themes. Use an LLM as an assistant to suggest initial clusters, but always have human review and refine the final categories.</span></p></li><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Prioritize failures with data.</strong><span> Use a simple pivot table or script to count the frequency of each failure mode. This transforms qualitative insights into a quantitative roadmap, revealing exactly where to focus engineering efforts.</span></p></li></ol><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>A common question at this stage is: “what if there’s not enough real user data to analyze?” This is where </span><strong>synthetic data</strong><span> is a powerful tool. You can use a powerful LLM to generate a diverse set of realistic user queries that cover the scenarios and edge cases you want to test. This allows you to bootstrap the entire error analysis process before there’s a single user. The specifics of how to create high-quality, grounded synthetic data is a deep topic that’s beyond the scope of this article. It’s discussed in our </span><a href=\"https://substack.com/redirect/281991a1-f899-44b5-8650-6da0d247163b?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">course</a><span>.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Below is a diagram illustrating the error analysis process:</p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"639\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/4cb58463-eea7-4a20-9b5c-a4938bdd5adc?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"475.85513078470825\" src=\"https://substackcdn.com/image/fetch/$s_!JhDp!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a5934f8-6df3-43db-ad47-63c844e2bf4a_994x860.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table><span> </span><em>The error analysis process visualized</em></div><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em\">3. Building evals: the right tool for the job</h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">After error analysis, the NurtureBoss team had a clear, data-driven mandate to fix date handling and handoffs, based on the table of errors shown above. However, these things represent different kinds of failure: handling calendar dates is an objective outcome that can be measured against expected value, whereas the question of when to hand off a conversation to a human is more nuanced. This distinction is important because it determines the type of evaluator to build. We discuss the two types of evaluators you need to consider next: code-based assertions vs. LLM Judges.</p><h3 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.375em\">For deterministic failures → code-based evals</h3><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">A user query like “can I see the apartment on July 4th, 2026?” has one – and only one – correct interpretation. The AI’s job is to extract that date and put it into the right format for a downstream tool. This is a deterministic, objective task that’s either right or wrong.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Code-based evals are the perfect tool for simple failures. To build one, the NurtureBoss team first assembled a “golden dataset” of test cases. They brainstormed the many different ways users might ask about dates, focusing on common patterns and tricky edge cases. The goal was to create a comprehensive test suite that could reliably catch regressions.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Here’s a simplified version of what their dataset looked like. Note: For relative dates, we are assuming the current date is 2025-08- 28.</p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1328\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/fe5550e3-8dc7-44b9-8743-46288244ef1f?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"224.47289156626505\" src=\"https://substackcdn.com/image/fetch/$s_!Ra_1!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8951fd-19bf-4f58-bc60-557107b28a0a_1328x542.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table>“Golden dataset” of test cases: The LLM is tested to ensure it returns the expected output</div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>With this dataset, the evaluation process is straightforward and mirrors traditional unit testing. You loop through each row, pass the </span><code>User Query</code><span> to your AI system, and then run a simple function that asserts the AI’s extracted date matches the </span><code>Expected Output.</code></p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1434\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/63253482-45d9-46a4-a760-4814c6044f42?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"226.29009762900975\" src=\"https://substackcdn.com/image/fetch/$s_!t8BA!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54350148-f61c-46c8-b2a0-99995baa9c41_1434x590.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table>A code snippet showing a simple python function for a code-based eval</div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Code-based evals are cheaper to create and maintain</strong><span> than other kinds of evals. Since there is an expected output, you only have to run the LLM to generate the answer, followed by a simple assertion. This means you can run these more often than other kinds of evals (for example, on every commit to prevent regressions). If a failure can be verified with code, always use a code-based eval.</span></p><h3 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.375em\">For subjective failures → LLM-as-judge</h3><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">But what about a more ambiguous problem, like knowing when to hand off a conversation to a human agent? If a user says, “I’m confused,” should the AI hand off immediately, or try to clarify things first? There’s no single right answer; it’s a judgment call based on product philosophy. A code-based test can’t evaluate this kind of nuance.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>For subjective failures, we created another golden dataset. The domain expert, NurtureBoss’s founder, Jacob, reviewed traces where a handoff was potentially needed, and made judgment calls. In each case, he provided a binary PASS/FAIL score and, crucially, a detailed </span><strong>critique</strong><span> explaining his reasoning.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Here are a few examples from their dataset:</p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1246\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/488f63c1-efc3-438f-b18f-b94d8da6ddf1?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"508.5072231139647\" src=\"https://substackcdn.com/image/fetch/$s_!Atcz!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1665201f-7144-4993-9884-bf906ab1b898_1246x1152.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table>Example dataset for an LLM-as-judge eval. </div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Using a PASS/FAIL judgment works better than a points rating. </strong><span>You’ll notice that in the example above, domain expert Jacob used a simple PASS/FAIL judgment for each trace, not a 1-5 points rating. This was a deliberate choice. I’ve learned that while it’s tempting to use a </span><a href=\"https://substack.com/redirect/2a074390-edff-4b39-bb2a-8ef0c13cdfeb?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Likert scale</a><span> to capture nuance; in practice, it creates more problems than it solves. The distinction between a “3” and a “4” is often subjective and inconsistent across different reviewers, leading to noisy, unreliable data.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">In contrast, binary decisions force clarity and compel a domain expert to define a clear line between acceptable and unacceptable, focusing on what truly matters for users’ success. It’s also far more actionable for engineers: a “fail” is a clear signal to fix a bug, whereas a “3” is an ambiguous signal: a signal to do what, exactly? By starting with pass/fail, you cut through the ambiguity and get a clear, actionable measure of quality, faster.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">This dataset of traces, judgments, and critiques does more than just help the team understand the problem. As we’ll see in the next section, these hand-labeled examples, especially the detailed critiques, become raw material for building a reliable LLM-as-judge.</p><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em\">4. Building an LLM-as-judge</h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">The hand-labeled dataset of handoff failures is a necessary first step in measuring and solving handoff issues. But another issue is that manual review doesn’t scale. At NurtureBoss, the next step was to automate the domain expert’s expertise by building an LLM-as-judge: an AI evaluator that could apply his reasoning consistently to thousands of future traces.</p><div style=\"font-size: 16px; line-height: 26px; display: none\"></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">This process mirrors an ML workflow and starts with data discipline. The team split their new “golden dataset” into three parts to prevent the judge from simply memorizing answers.</p><ol style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>The train set:</strong><span> A small pool of around 10-20% of the labeled data, containing clear examples of PASS and FAIL cases. These examples would be shown directly to the judge in a prompt to teach it the task.</span></p></li><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>The dev set:</strong><span> A larger set, around 40% of the data, used to iteratively test and refine the judge’s prompt. The team compared the judge’s automated labels against the domain experts’ to measure its performance and identify areas for improvement. Crucially, examples from the dev set were never included in the judge’s prompt.</span></p></li><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>The test set:</strong><span> The final 40% of the data was set aside completely. This set was used only once, at the very end, to make sure we haven’t inadvertently </span><a href=\"https://substack.com/redirect/c882ceee-6e63-4d47-ae57-ef9b90230029?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">overfit to examples</a><span> the dev set. You might also commonly hear this called the “holdout set”.</span></p></li></ol><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Note: these data split % guidelines may be different from what you see with typical machine learning workloads. This is because we aren’t training a new model with this data, we’re merely using examples from it. So, much less data is needed than in the “train set”.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">With their data properly segregated, the team began assembling the prompt for their “Handoff Failure” judge. A strong judge prompt contains four key components: a clear task description, precise pass/fail definitions, a few well-chosen examples, and a structured output format.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>The critiques which the domain expert wrote were the most valuable asset. Instead of just showing the judge a trace and a PASS/FAIL label, we included his critique as a reasoning step in the </span><a href=\"https://substack.com/redirect/686fe4a5-a218-4ad4-89ec-8f1512adb295?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">few-shot</a><span> examples. This provided a chain-of-thought for the judge, showing it </span><em>how</em><span> to reason about the problem, not just what the final answer was.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Here is a simplified version of the prompt they constructed, using examples drawn exclusively from their train set:</p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1198\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/821eca01-f89f-44e6-8b07-7edcf371f251?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"740.0667779632721\" src=\"https://substackcdn.com/image/fetch/$s_!svK5!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65f6f594-1ef7-4250-91d6-d2a5fa0c8afe_1198x1612.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table>An example of an LLM-Judge prompt</div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>You always want your LLM judge to provide structured output so you can compute metrics on the outputs. I recommend looking at LLM API documentation for structured output. For example, </span><a href=\"https://substack.com/redirect/c2c995da-872a-426b-9b3c-660035e52d63?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">these are the relevant docs</a><span> for OpenAI models.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Even though this is a simplified version of their LLM judge prompt, it has all the basic components needed for an LLM-as-judge: a clear role, a binary rubric, and examples with the answer and associated reasoning.  But this is only the first step in creating an LLM judge. Next, we need to iterate on the judge until it is sufficiently aligned with human judgment.</p><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em\">5. Aligning the judge and maintaining trust</h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Creating a prompt for an LLM-as-judge is the first step, but a prompt itself is just a hypothesis. To know if the judge can reliably replicate an expert’s reasoning, you must validate its outputs against human labels. First, let’s discuss the metrics to be considered.</p><h3 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.375em\">Measuring TPR and TNR</h3><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">A team’s first instinct might be to measure accuracy: what percentage of traces did the judge get right? But accuracy can be misleading: if an AI system is successful 95% of the time, an evaluator that always says “PASS” on every trace would be 95% accurate.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Therefore, instead of accuracy, we should consider the following metrics:</p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>True Positive Rate (TPR):</strong><span> This measures how often the judge says “PASS” when the human expert also said “PASS.” It answers the question: </span><em>can the judge correctly identify success?</em></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>True Negative Rate (TNR):</strong><span> This measures how often the judge says “FAIL” when the human expert also said “FAIL.” This answers the more critical question: </span><em>can the judge reliably catch the failures we care about?</em></p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">The goal is to align the judge with the expert’s decisions by getting both True Positive Rate (TPR) and True Negative Rate (TNR) as high as possible. I like to target over 90% for each. However, this isn’t set in stone; the correct threshold is ultimately a business decision that balances the costs of different errors.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">A low TNR means the judge incorrectly fails good responses, which could waste engineering time with false alarms. A low TPR is often more dangerous as it means the judge misses actual failures. Deciding when the metrics are “high enough” requires weighing trade-offs to your specific product and determining an acceptable level of risk.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">There are ways of combining these scores into a single weighted score, as well as other analytical techniques to estimate “true” failure rates, but that is outside the scope of this article. The important thing is to understand the tradeoffs.</p><h3 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.375em\">Alignment loop in practice</h3><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>With the correct metrics defined, the team begins the alignment loop using their </span><strong>dev set</strong><span> – the 40% of labeled data set aside for iteration. On a first run, it’s common for the results to be poor. To fix this, two key questions should be asked about each failure:</span></p><ol style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">Is our rubric in the prompt missing a rule, or is it too ambiguous?</p></li><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">Is this an edge case that our examples don’t cover well?</p></li></ol><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>The answers show how to refine the prompt. For example, the team might notice the judge passed a trace where a user said “this is too confusing.” The rubric didn’t explicitly define “confusion” as a trigger for a handoff. This is a </span><strong>rubric problem</strong><span>. The fix is to make the instructions more precise, adding a clause to the FAIL IF rule: “...or if the user expresses clear frustration (e.g., ‘this is confusing,’ ‘that’s not what I asked’)”.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>In another case, the judge might fail a trace where the AI’s handoff was clunky, but ultimately correct. The prompt’s examples only showed perfect handoffs, so the judge incorrectly labeled the imperfect-but-acceptable one as a failure. This is an </span><strong>example problem</strong><span>. The fix is to find a more nuanced example in the </span><strong>train set</strong><span>, specifically one which the domain expert labeled as “PASS” despite minor flaws, and then add it to the prompt.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">The general principle is to use the prompt’s instructions for clear, universal rules and to use the few-shot examples to illustrate tricky, hard-to-describe edge cases. After refining the prompt, the team re-runs the evaluation on the same dev set. If the TNR improves, they know they’re on the right track. This cycle of inspecting disagreements, refining the prompt or its examples and re-measuring is repeated until both TPR and TNR are at satisfactory levels.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>This manual refinement loop is a powerful starting point, although more advanced techniques like automated prompt optimization exist for teams looking to push performance further. I recommend only reaching for more advanced approaches when you feel comfortable with the whole process. More discussion on that is </span><a href=\"https://substack.com/redirect/a57b5d88-ad95-4fec-b50d-6637bf70d0a5?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">here</a><span>.</span></p><h3 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.375em\">Final report card</h3><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>Once the team is satisfied with the judge’s performance on the dev set, they “lock” the prompt. They run the judge one last time on their held-out </span><strong>test set</strong><span>, which is the data it didn’t see during the refinement process.  Measuring results on the test set helps ensure you haven’t inadvertently </span><a href=\"https://substack.com/redirect/3140dd84-cef4-4b00-b0fe-71b878522ac9?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">overfit</a><span> on your dev set. If you find the scores are materially worse on your test set, you need to diagnose the error and start the LLM-judge alignment process again with fresh data splits. For example, one common mistake is accidentally including examples from the dev set in the prompt.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">The final scores (e.g., TPR: 92%, TNR: 91%) are the judge’s official report card. If the judge is sufficiently aligned with human labels, this allows you to proceed with confidence. There’s now an automated eval that transforms a slow, manual review process into a fast, repeatable, and trustworthy evaluation.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">The final step in our evaluation journey is to integrate the suite of evaluators into CI/CD and production monitoring workflows.</p><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em\">6. Evals in practice: from CI/CD to production monitoring</h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">This final section covers how to integrate the evaluation suite into engineering workflows. These practices create a continuous improvement loop, using evals to catch regressions before deployment and to discover new failure modes in production.</p><h3 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.375em\">Evals in CI/CD to prevent regressions</h3><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">The first place to operationalize the new evaluators is in the Continuous Integration and Continuous Delivery (CI/CD) pipeline. The goal is to catch regressions on known problems before they reach users.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">However, running evals in CI presents practical challenges, mainly relating to speed and cost. An LLM-as-judge can be slow and expensive to run, making it impractical for the rapid feedback loop needed for every single commit. This leads to a tiered approach:</p><ol style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>For every commit:</strong><span> The team runs its fast, cheap, deterministic code-based evals. This includes tests like the date-parsing evaluator from Part 2. These checks are quick and provide an immediate signal if a change has broken a core, objective behavior.</span></p></li><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>For every Pull Request or nightly build:</strong><span> The team runs the slower, more expensive LLM-as-judge evals. The “Handoff Failure” judge, for example, runs against the full golden dataset here. This ensures that more nuanced, subjective behaviors are validated before code is merged, or as part of a more comprehensive nightly build, without slowing down the inner development loop.</span></p></li></ol><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">This approach balances speed and thoroughness, creating a safety net for the failure modes which the team already identified and fixed. These tests are built on a dataset of hand-curated test cases. Each test case is an input – such as a user query – designed to test a specific behavior. The CI system then asserts that this input produces the correct result from the relevant evaluator.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">For an objective, code-based test, it might assert the extracted data is correct. For a subjective, LLM-as-judge test, it asserts the judge returns the expected PASS or FAIL verdict. This dataset can be a mix of real examples from production logs and synthetically generated queries designed to stress test specific edge cases.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>For example, here is a screenshot of a CI/CD run for </span><a href=\"https://substack.com/redirect/5ad10a1e-26cc-4055-b273-b50d8549f410?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Rechat</a><span>, a company I worked with, building a real estate agent AI assistant:</span></p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1456\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/85984bf3-edc7-49b4-a84e-69d74c3f74f0?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"307.4862637362637\" src=\"https://substackcdn.com/image/fetch/$s_!imBg!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd12ae4a3-5d5b-45ae-bab0-5571f2bc0526_1600x895.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table><em>CI/CD run of test cases, including real &amp; synthetic queries, and pass/fail status against automated evaluators.</em><span> Source: this talk I did on </span><a href=\"https://substack.com/redirect/6982e960-915f-429c-ba0a-98fb6917556e?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">How to construct domain specific LLM evaluation systems</a></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">This dataset is a living artifact. Whenever the team discovers a new, high-impact failure, they add a representative example to the golden set. This ensures the CI safety net expands over time, turning newly-discovered problems into known issues that are protected against regressions.</p><h3 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.375em\">Closing the loop with production monitoring</h3><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>CI/CD is for catching regressions on </span><em>known</em><span> problems. Production monitoring is for discovering </span><em>new</em><span> failure modes from real user interactions. This is how you close the loop and create a system that continuously improves.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">The outline of this workflow:</p><ol style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Adopt a sampling strategy:</strong><span> Periodically, the team creates a pool of production traces to review. This pool is not just the traces that failed existing evals. A robust strategy combines a random sample of all traffic (to find unexpected failures) with the traces that were automatically flagged by production evaluators. They also perform data analysis, looking for patterns like clusters of similar failed traces or dimensions where failures are more common, to form new hypotheses about where the system might be weak.</span></p></li><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Surface traces for review:</strong><span> This combined pool of failed and sampled traces gives the team a comprehensive view of the system’s performance in the wild.</span></p></li><li style=\"margin: 8px 0 0 32px\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Repeat error analysis:</strong><span> This is the key step. The rich collection of production traces becomes the input for a new round of error analysis, as described in Part 1. The flagged traces help them better understand known issues, while the random sample helps them discover the </span><em>next</em><span> most important problem to solve. This leads to new evaluators and a constantly improving product.</span></p></li></ol><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">With this final piece in place, the team’s approach to quality has fundamentally changed. Their CI/CD pipeline now guards against regressions on problems they’ve already solved, while their production monitoring process ensures they are constantly learning from real user interactions. This is the difference between fixing bugs and creating a durable system for managing AI quality over the long term.</p><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em\">7. Flywheel of improvement</h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">This article has set out a repeatable workflow for graduating from “vibes-based development” (or guesswork) to a systematic engineering discipline. The core process creates a virtuous cycle, aka a “flywheel” for continuous improvement:</p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Analyze</strong><span> data to find real failures</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Measure</strong><span> them with targeted evaluators</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Improve</strong><span> the system through experimentation</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Automate</strong><span> those evals as regression tests.</span></p></li></ul><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1336\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/4d163e20-b9e4-4a87-ac9f-f15682dac3b9?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"452.02095808383234\" src=\"https://substackcdn.com/image/fetch/$s_!BSvA!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5adbcb6-c50d-48c2-96ef-524d27627fc1_1336x1098.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table><em>The flywheel of improvement</em></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">This flywheel transforms the black box of LLM quality into a measurable, engineer-led process.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">This article covers the foundational workflow, but the world of AI evals is too deep and full of challenges to cover everything in a single article. As you start implementing this process, you might begin to wonder:</p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">What’s a minimum viable evaluation setup to get started?</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">How much development budget should be allocated to evals?</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">Are there scenarios where synthetic data is unreliable, or could introduce bias?</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">What’s the best way to debug a complex, multi-turn conversation trace?</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">Are similarity metrics (BERTScore, ROUGE, etc.) useful for evaluating LLM outputs?</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">How to evaluate a model’s ability to express uncertainty, or to “know what it doesn’t know?”</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">How to decide whether to improve the system with prompting, fine-tuning, or RAG?</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">How to efficiently sample production traces for continuous review?</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">How often should error analysis be re-run on the production system?</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">How many people should annotate LLM outputs?</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">What parts of evals can be automated with LLMs?</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">How to version and manage prompts?</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">Whether to outsource annotation &amp; labeling to a third party?</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">How to evaluate sessions with human handoffs?</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">How does this workflow adapt to more advanced agentic systems?</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">How to evaluate non-text modalities, like images or audio?</p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><span>We answer some of these questions and more in our </span><a href=\"https://substack.com/redirect/8bf18e4a-59ec-4dbc-b24a-c78d262d9c12?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">FAQ</a><span>. For a hands-on experience and an even deeper dive, check out our course, </span><a href=\"https://substack.com/redirect/281991a1-f899-44b5-8650-6da0d247163b?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">AI Evals For Engineers &amp; PMs</a><span>.</span></p><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em\">Takeaways</h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><em>Gergely again. </em><span>Thanks a lot to Hamel for going into such detail about this very timely topic of evals. You can follow Hamel </span><a href=\"https://substack.com/redirect/005d4fa5-da97-4089-8870-31468b97f5fa?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">on X</a><span> and </span><a href=\"https://substack.com/redirect/f088dce6-bd70-42c1-b994-be1d23d4d620?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">LinkedIn</a><span>, and subscribe </span><a href=\"https://substack.com/redirect/fd6c2f59-1df7-4be7-a050-9dd0f749e100?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">to his newsletter</a><span>.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>Personally, I sense that evals are quickly becoming part of the AI engineering toolset. </strong><span>LLMs are non-deterministic, but even so, we still need to validate that LLM-based apps work correctly. To do so, we need a mix of deterministic and non-deterministic validation methods: evals are precisely this.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>CI/CD pipelines have a cost challenge, though. </strong><span>Evals will likely be added to ever more CI/CD pipelines as a way to catch regressions in LLM applications. However, running them means running LLMs while using an LLM-as-judge. This means spending money on tokens; suddenly, each eval run comes with a very real cost.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Before evals, the CI/CD constraints were mostly to do with the length of time it took for a test suite to run. But with evals, we also need to weigh how expensive an evals test run is. With CI/CD, it’s common practice to run cheap, fast tests on all pull requests as part of CI, and to only run the full suite before deployment on CD. Perhaps we’ll see something similar play out with evals.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px; margin-bottom: 0\"><strong>LLM apps introduce a confidence problem – even with evals. </strong><span>They’re a clever solution for validating LLM behavior, but issues shall still slip through due to LLMs being non-deterministic. Monitoring production usage – and using the “flywheel of improvement” to add/tweak evals and catch issues sooner – may become non-negotiable for production applications.</span></p></div></div><div style=\"margin-top: 16px; font-size: 16px; line-height: 26px\"><div style=\"margin: 32px 0 0; width: 100%; box-sizing: border-box; border-top: 1px solid #e6e6e6; font-size: 16px; line-height: 26px\"></div><div style=\"line-height: 26px; margin: 28px 0; font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; color: #363737; font-size: 16px\"><table cellpadding=\"0\" cellspacing=\"0\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; color: #363737; font-size: 16px; width: 100%\"><tbody><tr><td valign=\"top\" style=\"width: 52px\"><img src=\"https://substackcdn.com/image/fetch/$s_!7sqx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feee58cd7-9a81-4ef6-b0f4-faeed62d5166_400x400.jpeg\" style=\"box-sizing: border-box; border-radius: 500000px; max-width: 550px; border: none; vertical-align: middle; width: 52px; height: 52px; min-width: 52px; min-height: 52px; object-fit: cover; margin: 0px; display: inline\" width=\"52\" height=\"52\" /></td><td valign=\"top\"><div style=\"font-size: 16px; line-height: 26px; margin-left: 16px\"><div style=\"font-size: 14px; line-height: 16px; margin-bottom: 10px\">A guest post by</div><table cellpadding=\"0\" cellspacing=\"0\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; color: #363737; font-size: 16px; margin-top: 0; width: 100%\"><tbody><tr><td><a href=\"https://substack.com/@hamelhusain?utm_campaign=guest_post_bio&amp;utm_medium=email\" style=\"color: #363737; font-weight: 600; display: block; text-decoration: none; margin-bottom: 7px\" target=\"_blank\" rel=\"noreferrer\">Hamel Husain</a><div style=\"font-size: 16px; line-height: 24px\">I am a machine learning engineer with over 20 years of experience. More about me @ https://hamel.dev</div><div style=\"font-size: 16px; line-height: 26px; margin-top: 20px\"><a href=\"https://substack.com/redirect/3ea95e49-05cb-414f-96a4-da7c43117c19?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; box-sizing: border-box; cursor: pointer; border: none; border-radius: 8px; font-size: 14px; line-height: 20px; font-weight: 600; text-align: center; margin: 0; opacity: 1; outline: none; background-color: #FF6B00; text-decoration: none !important; color: #ffffff !important; min-height: 42px; height: auto; white-space: nowrap; border-width: 10px 20px; padding: 0; border-color: #FF6B00; border-style: solid\" target=\"_blank\" rel=\"noreferrer\"><span style=\"text-decoration: none; color: #ffffff\">Subscribe to Hamel</span></a></div></td></tr></tbody></table></div></td></tr></tbody></table></div></div><div style=\"margin: 32px 0 0; width: 100%; box-sizing: border-box; border-top: 1px solid #e6e6e6; font-size: 16px; line-height: 26px\"></div><div style=\"--image-offset-margin: -120px; font-size: 16px; line-height: 26px\"><div style=\"margin: 32px 0; font-size: 16px; line-height: 26px\"><div dir=\"auto\" style=\"text-align: initial; font-size: 16px; line-height: 26px; width: 100%; word-break: break-word; margin-bottom: 16px\"><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px; margin-top: 0\"><em>This post is only for paying subscribers of </em><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9uZXdzbGV0dGVyLnByYWdtYXRpY2VuZ2luZWVyLmNvbT91dG1fY2FtcGFpZ249ZW1haWwtaG9tZSZyPTJvZndzbyIsInAiOjE4MDUxOTE0NSwicyI6NDU4NzA5LCJmIjpmYWxzZSwidSI6MTYxOTg1NDgwLCJpYXQiOjE3NjQ2OTY2NjYsImV4cCI6MjA4MDI3MjY2NiwiaXNzIjoicHViLTAiLCJzdWIiOiJsaW5rLXJlZGlyZWN0In0.uYaaXCJ9hZoSYp4gALHK2IVtGMQvsC4f7abV9N86iQ8?\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">The Pragmatic Engineer</a><em>. This email is intended for a single recipient, but occasional forwarding is totally fine.</em></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px; text-align: center; cursor: pointer; border-radius: 4px\"><a href=\"https://substack.com/app-link/post?publication_id=458709&amp;post_id=180519145&amp;utm_source=substack&amp;isFreemail=false&amp;submitLike=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE4MDUxOTE0NSwicmVhY3Rpb24iOiLinaQiLCJpYXQiOjE3NjQ2OTY2NjYsImV4cCI6MTc2NzI4ODY2NiwiaXNzIjoicHViLTQ1ODcwOSIsInN1YiI6InJlYWN0aW9uIn0.yQLSExL2yvrJ1fauBiMQH-TeiM9Ol8OZI1sk7mcf6yM&amp;utm_medium=email&amp;utm_campaign=email-reaction&amp;r=2ofwso\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; box-sizing: border-box; cursor: pointer; border: none; border-radius: 8px; font-size: 14px; line-height: 20px; font-weight: 600; text-align: center; margin: 0; opacity: 1; outline: none; white-space: nowrap; color: #ffffff !important; text-decoration: none !important; background-color: #FF6B00; padding: 12px 20px; height: auto\" target=\"_blank\" rel=\"noreferrer\"><span style=\"color: #ffffff; text-decoration: none\">Like &amp; Comment</span></a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><em><span>As a subscriber, you also have access to 🔒</span><a href=\"https://substack.com/redirect/e7a30e75-603e-4bd0-884e-57fabdbab01d?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\"> subscriber-only resources</a><span> for engineering managers and engineers.</span></em></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><em>Know someone who would benefit from the newsletter? Consider gifting a subscription.</em></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px; margin-bottom: 0; text-align: center; cursor: pointer; border-radius: 4px\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9uZXdzbGV0dGVyLnByYWdtYXRpY2VuZ2luZWVyLmNvbS9zdWJzY3JpYmU_dXRtX3NvdXJjZT1wb3N0JnV0bV9jYW1wYWlnbj1lbWFpbC1jaGVja291dCZuZXh0PWh0dHBzJTNBJTJGJTJGbmV3c2xldHRlci5wcmFnbWF0aWNlbmdpbmVlci5jb20lMkZwJTJGZXZhbHMmcj0yb2Z3c28mdG9rZW49ZXlKMWMyVnlYMmxrSWpveE5qRTVPRFUwT0RBc0ltbGhkQ0k2TVRjMk5EWTVOalkyTml3aVpYaHdJam94TnpZM01qZzROalkyTENKcGMzTWlPaUp3ZFdJdE5EVTROekE1SWl3aWMzVmlJam9pWTJobFkydHZkWFFpZlEuZU94TjZoNXRCeC1fd1ZjcjQ1SWFWTHYwb2lZUTc4WEV1aXAtWjdvYnNyVSIsInAiOjE4MDUxOTE0NSwicyI6NDU4NzA5LCJmIjpmYWxzZSwidSI6MTYxOTg1NDgwLCJpYXQiOjE3NjQ2OTY2NjYsImV4cCI6MjA4MDI3MjY2NiwiaXNzIjoicHViLTAiLCJzdWIiOiJsaW5rLXJlZGlyZWN0In0.v2HsWvLf9DeBTbBsx7sf9hsZ00bln1lg72j3HvnVvrQ?&amp;gift=true\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; box-sizing: border-box; cursor: pointer; border: none; border-radius: 8px; font-size: 14px; line-height: 20px; font-weight: 600; text-align: center; margin: 0; opacity: 1; outline: none; white-space: nowrap; color: #ffffff !important; text-decoration: none !important; background-color: #FF6B00; padding: 12px 20px; height: auto\" target=\"_blank\" rel=\"noreferrer\"><span style=\"color: #ffffff; text-decoration: none\">Give a gift subscription</span></a></p></div></div></div><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"border-top: 1px solid rgb(0,0,0,.1); border-bottom: 1px solid rgb(0,0,0,.1); min-width: 100%\"><tbody><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr><tr><td><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"margin: 0 auto\"><tbody><tr><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=458709&amp;post_id=180519145&amp;utm_source=substack&amp;isFreemail=false&amp;submitLike=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE4MDUxOTE0NSwicmVhY3Rpb24iOiLinaQiLCJpYXQiOjE3NjQ2OTY2NjYsImV4cCI6MTc2NzI4ODY2NiwiaXNzIjoicHViLTQ1ODcwOSIsInN1YiI6InJlYWN0aW9uIn0.yQLSExL2yvrJ1fauBiMQH-TeiM9Ol8OZI1sk7mcf6yM&amp;utm_medium=email&amp;utm_campaign=email-reaction&amp;r=2ofwso\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!PeVs!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideHeart%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"margin-right: 8px; min-width: 18px; min-height: 18px; border: none; vertical-align: middle; max-width: 18px\" /><span style=\"vertical-align: middle\">Like</span></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=458709&amp;post_id=180519145&amp;utm_source=substack&amp;utm_medium=email&amp;isFreemail=false&amp;comments=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE4MDUxOTE0NSwiaWF0IjoxNzY0Njk2NjY2LCJleHAiOjE3NjcyODg2NjYsImlzcyI6InB1Yi00NTg3MDkiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.l5ILD22qzTpkUGn1wgu-LQIMmpcVxuW-UE03ILjardE&amp;r=2ofwso&amp;utm_campaign=email-half-magic-comments&amp;action=post-comment&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!x1tS!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideComments%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"margin-right: 8px; min-width: 18px; min-height: 18px; border: none; vertical-align: middle; max-width: 18px\" /><span style=\"vertical-align: middle\">Comment</span></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9vcGVuLnN1YnN0YWNrLmNvbS9wdWIvcHJhZ21hdGljZW5naW5lZXIvcC9ldmFscz91dG1fc291cmNlPXN1YnN0YWNrJnV0bV9tZWRpdW09ZW1haWwmdXRtX2NhbXBhaWduPWVtYWlsLXJlc3RhY2stY29tbWVudCZhY3Rpb249cmVzdGFjay1jb21tZW50JnI9Mm9md3NvJnRva2VuPWV5SjFjMlZ5WDJsa0lqb3hOakU1T0RVME9EQXNJbkJ2YzNSZmFXUWlPakU0TURVeE9URTBOU3dpYVdGMElqb3hOelkwTmprMk5qWTJMQ0psZUhBaU9qRTNOamN5T0RnMk5qWXNJbWx6Y3lJNkluQjFZaTAwTlRnM01Ea2lMQ0p6ZFdJaU9pSndiM04wTFhKbFlXTjBhVzl1SW4wLmw1SUxEMjJxelRwa1VHbjF3Z3UtTFFJTW1wY1Z4dVctVUUwM0lMamFyZEUiLCJwIjoxODA1MTkxNDUsInMiOjQ1ODcwOSwiZiI6ZmFsc2UsInUiOjE2MTk4NTQ4MCwiaWF0IjoxNzY0Njk2NjY2LCJleHAiOjIwODAyNzI2NjYsImlzcyI6InB1Yi0wIiwic3ViIjoibGluay1yZWRpcmVjdCJ9.5JXXUkE4kJeTEX3yEXgfaJa39Q0vPUY9WRWN_qvbZzI?&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!5EGt!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FNoteForwardIcon%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"margin-right: 8px; min-width: 18px; min-height: 18px; border: none; vertical-align: middle; max-width: 18px\" /><span style=\"vertical-align: middle\">Restack</span></a></td></tr></tbody></table></td></tr></tbody></table></td><td align=\"right\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr></tr></tbody></table></td></tr></tbody></table></td></tr><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr></tbody></table><div style=\"color: rgb(119,119,119); text-align: center; font-size: 16px; line-height: 26px; padding: 24px0\"><div style=\"font-size: 16px; line-height: 26px; padding-bottom: 24px\"><p style=\"list-style: none; font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; padding-bottom: 0; font-size: 12px; line-height: 16px; margin: 0; color: rgb(119,119,119); text-decoration: unset\">© 2025 <span>Gergely Orosz</span><br />548 Market Street PMB 72296, San Francisco, CA 94104 <br /><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9uZXdzbGV0dGVyLnByYWdtYXRpY2VuZ2luZWVyLmNvbS9hY3Rpb24vZGlzYWJsZV9lbWFpbD90b2tlbj1leUoxYzJWeVgybGtJam94TmpFNU9EVTBPREFzSW5CdmMzUmZhV1FpT2pFNE1EVXhPVEUwTlN3aWFXRjBJam94TnpZME5qazJOalkyTENKbGVIQWlPakUzT1RZeU16STJOallzSW1semN5STZJbkIxWWkwME5UZzNNRGtpTENKemRXSWlPaUprYVhOaFlteGxYMlZ0WVdsc0luMC5mbzBLY0JjWWlBTUY3VUQxa2wzMVdlUlFSaHp0VVgwRm1kNE92VWNTTUtrIiwicCI6MTgwNTE5MTQ1LCJzIjo0NTg3MDksImYiOmZhbHNlLCJ1IjoxNjE5ODU0ODAsImlhdCI6MTc2NDY5NjY2NiwiZXhwIjoyMDgwMjcyNjY2LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.dvpJ7OLYBSmIfORXhdkzcOJ4OjDxR2lovmDdezLFBhU?\" style=\"text-decoration: underline; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><span style=\"color: rgb(119,119,119); text-decoration: underline\">Unsubscribe</span></a></p></div><p style=\"padding: 0 24px; font-size: 12px; line-height: 20px; margin: 0; color: rgb(119,119,119); font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; padding-bottom: 0; margin-top: 0\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9zdWJzdGFjay5jb20vc2lnbnVwP3V0bV9zb3VyY2U9c3Vic3RhY2smdXRtX21lZGl1bT1lbWFpbCZ1dG1fY29udGVudD1mb290ZXImdXRtX2NhbXBhaWduPWF1dG9maWxsZWQtZm9vdGVyJmZyZWVTaWdudXBFbWFpbD1ieXRlYnl0ZWdvODhAaW5vLnRvJnI9Mm9md3NvIiwicCI6MTgwNTE5MTQ1LCJzIjo0NTg3MDksImYiOmZhbHNlLCJ1IjoxNjE5ODU0ODAsImlhdCI6MTc2NDY5NjY2NiwiZXhwIjoyMDgwMjcyNjY2LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.mgroIGI_1BMMpqZ0ConMIgmbOwtMVPfsZwjVEnmIOwQ?\" style=\"color: rgb(119,119,119); text-decoration: none; display: inline-block; margin: 0 4px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!LkrL!,w_270,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpublish-button%402x.png\" width=\"135\" alt=\"Start writing\" height=\"40\" style=\"max-width: 550px; border: none !important; vertical-align: middle\" /></a></p></div></div></td><td></td></tr></tbody></table><img src=\"https://eotrx.substackcdn.com/open?token=eyJtIjoiPDIwMjUxMjAyMTcxOTU3LjMuMjZiYjkyZGMyY2JkODM3ZUBtZzEuc3Vic3RhY2suY29tPiIsInUiOjE2MTk4NTQ4MCwiciI6ImJ5dGVieXRlZ284OEBpbm8udG8iLCJkIjoibWcxLnN1YnN0YWNrLmNvbSIsInAiOjE4MDUxOTE0NSwidCI6Im5ld3NsZXR0ZXIiLCJhIjoib25seV9wYWlkIiwicyI6NDU4NzA5LCJjIjoicG9zdCIsImYiOmZhbHNlLCJwb3NpdGlvbiI6ImJvdHRvbSIsImlhdCI6MTc2NDY5NjY2NiwiZXhwIjoxNzY3Mjg4NjY2LCJpc3MiOiJwdWItMCIsInN1YiI6ImVvIn0.9V5ekNg0vhaHBbVAblbD2vqix985iPG2393XD9Y5hf0\" width=\"1\" height=\"1\" border=\"0\" style=\"height: 1px !important; width: 1px !important; border-width: 0 !important; margin-top: 0 !important; margin-bottom: 0 !important; margin-right: 0 !important; margin-left: 0 !important; padding-top: 0 !important; padding-bottom: 0 !important; padding-right: 0 !important; padding-left: 0 !important\" /><img width=\"1\" height=\"1\" src=\"https://email.mg1.substack.com/o/eJxMkD3u3SAQB0_zKC3AfBacxWJh7aAYsPCSyLePnNf8i21mpNXolyLh0ccTrn4Ty4FbmSwwDMIaZbwxxjCssZzbgQ1HJMxbpB9Wu5X9CpavGmDXHKTb7W6FlAqsMDsH4YVTrATJpRaSS2GF13ZZF2kAvMxJJshutfhRvB5iuSfcFNPvJfXK3qotzlywJQy9nc92xZK_vOQgHNfCC6W_hJ4LQ8O_94lEONg1YUu91tkKPRu2CCfmQGPiq86SIpXe3kdKO8s9GwEewveO7txH8dL6Qp3dE3KvsbRwjXjUSCVhO0pDHIy-280bx_8kI7zTynH2J8h_AQAA___Qw3NO\" /></div></div></div>",
      "summary": "Evals are a new toolset for any and all AI engineers – and software engineers should also know about them. Move from guesswork to a systematic engineering process for improving AI quality.͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏   ",
      "publishedAt": "2025-12-02T17:31:14.000Z",
      "author": "The Pragmatic Engineer ",
      "source": "rss",
      "feedName": "Byte Byte Go",
      "sourceType": "general",
      "contentType": "security_incident",
      "score": 16.291949758049274,
      "ingestedAt": "2025-12-02T21:47:02.947Z",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "testing",
        "observability",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09d0c783",
      "title": "API GitHub Meta",
      "url": "https://api.github.com/meta",
      "content": "<p>Article URL: <a href=\"https://api.github.com/meta\">https://api.github.com/meta</a></p> \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46123469\">https://news.ycombinator.com/item?id=46123469</a></p> \n<p>Points: 11</p> \n<p># Comments: 0</p>",
      "summary": "Article URL: https://api.github.com/meta \nComments URL: https://news.ycombinator.com/item?id=46123469 \nPoints: 11 \n# Comments: 0",
      "publishedAt": "2025-12-02T17:06:24.000Z",
      "author": "luispa",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4792630446716553,
      "ingestedAt": "2025-12-02T21:47:02.947Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09d021fa",
      "title": "“The local-first rebellion”: How Home Assistant became the most important project in your house",
      "url": "https://github.blog/open-source/maintainers/the-local-first-rebellion-how-home-assistant-became-the-most-important-project-in-your-house/",
      "content": "<p>Franck Nijhof—better known as Frenck—is one of those maintainers who ended up at the center of a massive open source project not because he chased the spotlight, but because he helped hold together one of the most active, culturally important, and technically demanding open source ecosystems on the planet. As a <strong>lead of </strong><a href=\"https://github.com/home-assistant\"><strong>Home Assistant</strong></a> and a <strong>GitHub Star</strong>, Frenck guides the project that didn’t just grow. It exploded.</p> \n \n \n \n<p><a href=\"https://github.blog/news-insights/octoverse/octoverse-a-new-developer-joins-github-every-second-as-ai-leads-typescript-to-1/?utm_source=octoverse-homepage&amp;utm_medium=blog&amp;utm_campaign=universe25\">This year’s Octoverse report confirms it</a>: Home Assistant was one of the <strong>fastest-growing open source projects by contributors</strong>, ranking alongside AI infrastructure giants like vLLM, Ollama, and Transformers. It also appeared in the <strong>top projects attracting first-time contributors</strong>, sitting beside massive developer platforms such as VS Code. In a year dominated by AI tooling, agentic workflows, and typed language growth, Home Assistant stood out as something else entirely: an open source system for the physical world that grew at an AI-era pace.</p> \n \n \n \n<p>The scale is wild. Home Assistant is now running in <strong>more than 2 million households</strong>, orchestrating everything from thermostats and door locks to motion sensors and lighting. All on users’ own hardware, not the cloud. The contributor base behind that growth is just as remarkable: <strong>21,000 contributors in a single year</strong>, feeding into one of GitHub’s most lively ecosystems at a time when a new developer joins GitHub every second.</p> \n \n \n \n<p>In our podcast interview, Frenck explains it almost casually.</p> \n \n \n \n<blockquote> \n<p>Home Assistant is a free and open source home automation platform. It allows you to connect all your devices together, regardless of the brands they’re from… And it runs locally.</p> \n<cite>Franck Nijhof, lead of Home Assistant</cite></blockquote> \n \n \n \n<iframe allowfullscreen=\"allowfullscreen\" height=\"200\" width=\"100%\" src=\"https://player.simplecast.com/a5000fd5-f225-459e-b1d9-e86573a398a2?dark=false\"></iframe> \n \n \n \n<p>He smiles when he describes just how accessible it is. “Flash Home Assistant to an SD card, put it in, and it will start scanning your home,” he says. </p> \n \n \n \n<p>This is the paradox that makes Home Assistant compelling to developers: it’s simple to use, but technically enormous. A local-first, globally maintained automation engine for the home. And Frenck is one of the people keeping it all running.</p> \n \n \n \n \n<h3 style=\"margin-top:0;\">📌 What is Home Assistant?</h3> \n \n \n \n<p><a href=\"https://github.com/home-assistant\"><strong>Home Assistant</strong></a> is an open-source home automation platform designed for maximum local control, privacy, and interoperability. It enables you to connect, orchestrate, and automate thousands of devices from hundreds of vendors—all running on hardware in your home (e.g., a Raspberry Pi) and without sending data to the cloud.</p> \n \n \n \n<p>The core engine is written in Python and supported by front-end components in TypeScript and other languages. Developers build integrations in a community-wide effort that has grown to tens of thousands of contributors and millions of installations.</p> \n \n \n \n \n<h2>The architecture built to tame thousands of device ecosystems</h2> \n \n \n \n<p>At its core, Home Assistant’s problem is combinatorial explosion. The platform supports “hundreds, thousands of devices… over 3,000 brands,” as Frenck notes. Each one behaves differently, and the only way to normalize them is to build a general-purpose abstraction layer that can survive vendor churn, bad APIs, and inconsistent firmware.</p> \n \n \n \n<p>Instead of treating devices as isolated objects behind cloud accounts, everything is represented locally as entities with states and events. A garage door is not just a vendor-specific API; it’s a structured device that exposes capabilities to the automation engine. A thermostat is not a cloud endpoint; it’s a sensor/actuator pair with metadata that can be reasoned about.</p> \n \n \n \n<p>That consistency is why people can build wildly advanced automations.</p> \n \n \n \n<p>Frenck describes one particularly inventive example: “Some people install weight sensors into their couches so they actually know if you’re sitting down or standing up again. You’re watching a movie, you stand up, and it will pause and then turn on the lights a bit brighter so you can actually see when you get your drink. You get back, sit down, the lights dim, and the movie continues.”</p> \n \n \n \n<p>A system that can orchestrate these interactions is fundamentally a distributed event-driven runtime for physical spaces. Home Assistant may look like a dashboard, but under the hood it behaves more like a real-time OS for the home.</p> \n \n \n \n<h2>Running everything locally is not a feature. It’s a hard constraint. </h2> \n \n \n \n<p>Almost every mainstream device manufacturer has pivoted to cloud-centric models. Frenck points out the absurdity:</p> \n \n \n \n<blockquote> \n<p>It’s crazy that we need the internet nowadays to change your thermostat.</p> \n</blockquote> \n \n \n \n<p>The local-first architecture means Home Assistant can run on hardware as small as a Raspberry Pi but must handle workloads that commercial systems offload to the cloud: device discovery, event dispatch, state persistence, automation scheduling, voice pipeline inference (if local), real-time sensor reading, integration updates, and security constraints.</p> \n \n \n \n<p>This architecture forces optimizations few consumer systems attempt. If any of this were offloaded to a vendor cloud, the system would be easier to build. But Home Assistant’s philosophy reverses the paradigm: the home is the data center.</p> \n \n \n \n<p>Everything from SSD wear leveling on the Pi to MQTT throughput to Zigbee network topologies becomes a software challenge. And because the system must keep working offline, there’s no fallback.</p> \n \n \n \n<p>This is engineering with no safety net.</p> \n \n \n \n<h2>The open home foundation: governance as a technical requirement</h2> \n \n \n \n<p>When you build a system that runs in millions of homes, the biggest long-term risk isn’t bugs. It’s ownership.</p> \n \n \n \n<p>“It can never be bought, it can never be sold,” Frenck says of Home Assistant’s move to the <a href=\"https://www.openhomefoundation.org/\">Open Home Foundation</a>. “We want to protect Home Assistant from the big guys in the end.”</p> \n \n \n \n<p>This governance model isn’t philosophical; it is an architectural necessity. If Home Assistant ever became a commercial acquisition, cloud lock-in would follow. APIs would break. Integrations would be deprecated. Automations built over years would collapse.</p> \n \n \n \n<img height=\"576\" width=\"1024\" src=\"https://github.blog/wp-content/uploads/2025/10/octoverse-2025-fastest-growing-open-source-projects-by-contributors.png?resize=1024%2C576\" alt=\"A list of the fastest-growing open source projects by contributors. home-assistant/core is number 10.\"> \n \n \n \n<p>The Foundation encodes three engineering constraints that ripple through every design decision:</p> \n \n \n \n<ul> \n<li><strong>Privacy: </strong>“Local control and privacy first.” All processing must occur on-device.</li> \n \n \n \n<li><strong>Choice: </strong>“You should be able to choose your own devices” and expect them to interoperate.</li> \n \n \n \n<li><strong>Sustainability: </strong>If a vendor kills its cloud service, the device must still work.</li> \n</ul> \n \n \n \n<p>Frenck calls out Nest as an example: “If some manufacturer turns off the cloud service… that turns into e-waste.”</p> \n \n \n \n<p>This is more than governance; it is technical infrastructure. It dictates API longevity, integration strategy, reverse engineering priorities, and local inference choices. It’s also a blueprint that forces the project to outlive any individual device manufacturer.</p> \n \n \n \n<h2>The community model that accidentally solved software quality</h2> \n \n \n \n<blockquote> \n<p>We don’t build Home Assistant, the community does.</p> \n</blockquote> \n \n \n \n<p>“We cannot build hundreds, thousands of device integrations. I don’t have tens of thousands of devices in my home,” Frenck says.</p> \n \n \n \n<p>This is where the project becomes truly unique.</p> \n \n \n \n<p>Developers write integrations for devices <em>they personally own</em>. Reviewers test contributions against devices <em>in their own homes</em>. Break something, and you break your own house. Improve something, and you improve your daily life.</p> \n \n \n \n<p>“That’s where the quality comes from,” Frenck says. “People run this in their own homes… and they take care that it needs to be good.”</p> \n \n \n \n<p>This is the unheard-of secret behind Home Assistant’s engineering velocity. Every contributor has access to production hardware. Every reviewer has a high-stakes environment to protect. No staging environment could replicate millions of real homes, each with its own weird edge cases.</p> \n \n \n \n<h2>Assist: A local voice assistant built before the AI hype wave</h2> \n \n \n \n<p>Assist is Home Assistant’s built-in voice assistant, a modular system that lets you control your home using speech without sending audio or transcripts to any cloud provider. As Frenck puts it:</p> \n \n \n \n<blockquote> \n<p>We were building a voice assistant before the AI hype… we want to build something privacy-aware and local.</p> \n</blockquote> \n \n \n \n<p>Rather than copying commercial assistants like Alexa or Google Assistant, Assist takes a two-layer approach that prioritizes determinism, speed, and user choice.</p> \n \n \n \n<h3>Stage 1: Deterministic, no-AI commands</h3> \n \n \n \n<p>Assist began with a structured intent engine powered by hand-authored phrases contributed by the community. Commands like “Turn on the kitchen light” or “Turn off the living room fan” are matched directly to known actions without using machine learning at all. This makes them extremely fast, reliable, and fully local. No network calls. No cloud. No model hallucinations. Just direct mapping from phrase to automation.</p> \n \n \n \n<h3>Stage 2: Optional AI when you want natural language</h3> \n \n \n \n<p>One of the more unusual parts of Assist is that AI is never mandatory. Frenck emphasizes that developers and users get to choose their inference path: “You can even say you want to connect your own OpenAI account. Or your own Google Gemini account. Or get a Llama running locally in your own home.”</p> \n \n \n \n<p>Assist evaluates each command and decides whether it needs AI. If a command is known, it bypasses the model entirely.</p> \n \n \n \n<p>“Home Assistant would be like, well, I don’t have to ask AI,” Frenck says. “I know what this is. Let me turn off the lights.”</p> \n \n \n \n<p>The system only uses AI when a command requires flexible interpretation, making AI a fallback instead of the foundation.</p> \n \n \n \n<h3>Open hardware to support the system</h3> \n \n \n \n<p>To bootstrap development and give contributors a reference device, the team built a fully open source smart speaker—the <strong>Voice Assistant Preview Edition</strong>.</p> \n \n \n \n<p>“We created a small speaker with a microphone array,” Frenck says. “It’s fully open source. The hardware is open source; the software running on it is ESPHome.”</p> \n \n \n \n<p>This gives developers a predictable hardware target for building and testing voice features, instead of guessing how different microphones, DSP pipelines, or wake word configurations behave across vendors.</p> \n \n \n \n<h2>Hardware as a software accelerator</h2> \n \n \n \n<p>Most open source projects avoid hardware. Home Assistant embraced it out of practical necessity.</p> \n \n \n \n<p>“In order to get the software people building the software for hardware, you need to build hardware,” Frenck says.</p> \n \n \n \n<p>Home Assistant Green, its prebuilt plug-and-play hub, exists because onboarding requires reliable hardware. The Voice Assistant Preview Edition exists because the voice pipeline needs a known microphone and speaker configuration.</p> \n \n \n \n<p>This is a rare pattern: hardware serves as scaffolding for software evolution. It’s akin to building a compiler and then designing a reference CPU so contributors can optimize code paths predictably.</p> \n \n \n \n<p>The result is a more stable, more testable, more developer-friendly software ecosystem.</p> \n \n \n \n<h2>A glimpse into the future: local agents and programmable homes</h2> \n \n \n \n<p>The trajectory is clear. With local AI models, deterministic automations, and a stateful view of the entire home, the next logical step is agentic behavior that runs entirely offline.</p> \n \n \n \n<p>If a couch can trigger a movie automation, and a brewery can run a fermentation pipeline, the home itself becomes programmable. Every sensor is an input. Every device is an actuator. Every automation is a function. The entire house becomes a runtime.</p> \n \n \n \n<p>And unlike cloud-bound competitors, Home Assistant’s runtime belongs to the homeowner, not the service provider.</p> \n \n \n \n<p>Frenck sums up the ethos: “We give that control to our community.”</p> \n \n \n \n<div> \n<p><strong>Looking to stay one step ahead? </strong><a href=\"https://github.blog/news-insights/octoverse/typescript-python-and-the-ai-feedback-loop-changing-software-development/?utm_source=blog-home-assistant&amp;utm_medium=blog&amp;utm_campaign=universe25post\">Read the latest Octoverse report</a> and <a href=\"https://github.com/features/copilot/cli?utm_source=blog-home-assistant&amp;utm_medium=blog&amp;utm_campaign=universe25post\">consider trying Copilot CLI.</a></p> \n</div> \n \n<p>The post <a href=\"https://github.blog/open-source/maintainers/the-local-first-rebellion-how-home-assistant-became-the-most-important-project-in-your-house/\">“The local-first rebellion”: How Home Assistant became the most important project in your house</a> appeared first on <a href=\"https://github.blog\">The GitHub Blog</a>.</p>",
      "summary": "Franck Nijhof—better known as Frenck—is one of those maintainers who ended up at the center of a massive open source project not because he chased the spotlight, but because he helped hold together one of the most active, culturally important, and technically demanding open source ecosystems on the planet. As a lead of Home Assistant and a GitHub Star, Frenck guides the project that didn’t just grow. It exploded. \n \n \n \nThis year’s Octoverse report confirms it: Home Assistant was one of the fast",
      "publishedAt": "2025-12-02T17:19:32.000Z",
      "author": "Andrea Griffiths",
      "source": "rss",
      "feedName": "The GitHub Blog",
      "sourceType": "engineering_blog",
      "contentType": "feature_update",
      "score": 16.775906362447394,
      "ingestedAt": "2025-12-02T21:47:02.947Z",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "testing",
        "governance",
        "Coding Agent Product Updates",
        "agent"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09cf07cc",
      "title": "Poka Labs (YC S24) Is Hiring a Founding Engineer",
      "url": "https://www.ycombinator.com/companies/poka-labs/jobs/RCQgmqB-founding-engineer",
      "content": "<p><img src=\"https://bookface-images.s3.amazonaws.com/logos/7f5143851923669169ef9c53bc1f8b3de3d20e09.png?1722543073\" alt=\"7f5143851923669169ef9c53bc1f8b3de3d20e09\"></p><p>Article URL: <a href=\"https://www.ycombinator.com/companies/poka-labs/jobs/RCQgmqB-founding-engineer\">https://www.ycombinator.com/companies/poka-labs/jobs/RCQgmqB-founding-engineer</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46123374\">https://news.ycombinator.com/item?id=46123374</a></p>  \n<p>Points: 0</p>  \n<p># Comments: 0</p>",
      "summary": "Article URL: https://www.ycombinator.com/companies/poka-labs/jobs/RCQgmqB-founding-engineer  \nComments URL: https://news.ycombinator.com/item?id=46123374  \nPoints: 0  \n# Comments: 0",
      "publishedAt": "2025-12-02T17:00:12.000Z",
      "author": "arbass",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4788081825321684,
      "ingestedAt": "2025-12-02T21:47:02.947Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09cd40ae",
      "title": "Anthropic rolls out Claude for Nonprofits with up to 75% discounts - The Economic Times",
      "url": "https://news.google.com/rss/articles/CBMi4gFBVV95cUxNVEhKWl85MHhyeFctaDZjRy04TFB5WmZhX3RlZXNLTjZ1dEowZGpVbjdXU19qWFdENDBNLWRpNnQySUxqSFBzTTNnZ3JzbGk3THNiM3BzaEhELW5MY0JRYmU2bmc2NEtlYklrNGREeWVxMFRSMkEzUHQ5aWhUczgtQ2N3eWJ2REFiSENqbTZTbGhBa3lJRmEwTkhWWWFIZ09oN3ZxX1BxZlo1NFFKcVprcUpkRDZJYmNiMzkxNGRYMUt3VFYya2huc082TFRHbmpad3BjOVhUT0VISkhIUmVWbExB0gHnAUFVX3lxTFBzcDVZaHJBQzRSajVBTXczdWZNam53Sy01eEFYNFJPWmgyZno1V3d0bDBfMDVNRFlZZi16eXNmTWdPeDRHbkItelNFZVk2UDRLaXQxWi1KWW1BdzEyamtoRl93NWY0dUQxQWZxejlvUmxuOGlPNGV5WjdMRHo4N3JyTzZhSGhFd1UwSE8wV3NqNlBQQWc0STVkTmc5SWJkVXBfV1IyNlBnN3UtS0VsbVN6andrNHhDUTJzQzJzLWJzSVB1STljejhOaHlLVlFpbS1LeXdZQ1dVU1FURnlSWGctUUIyc29fUQ?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMi4gFBVV95cUxNVEhKWl85MHhyeFctaDZjRy04TFB5WmZhX3RlZXNLTjZ1dEowZGpVbjdXU19qWFdENDBNLWRpNnQySUxqSFBzTTNnZ3JzbGk3THNiM3BzaEhELW5MY0JRYmU2bmc2NEtlYklrNGREeWVxMFRSMkEzUHQ5aWhUczgtQ2N3eWJ2REFiSENqbTZTbGhBa3lJRmEwTkhWWWFIZ09oN3ZxX1BxZlo1NFFKcVprcUpkRDZJYmNiMzkxNGRYMUt3VFYya2huc082TFRHbmpad3BjOVhUT0VISkhIUmVWbExB0gHnAUFVX3lxTFBzcDVZaHJBQzRSajVBTXczdWZNam53Sy01eEFYNFJPWmgyZno1V3d0bDBfMDVNRFlZZi16eXNmTWdPeDRHbkItelNFZVk2UDRLaXQxWi1KWW1BdzEyamtoRl93NWY0dUQxQWZxejlvUmxuOGlPNGV5WjdMRHo4N3JyTzZhSGhFd1UwSE8wV3NqNlBQQWc0STVkTmc5SWJkVXBfV1IyNlBnN3UtS0VsbVN6andrNHhDUTJzQzJzLWJzSVB1STljejhOaHlLVlFpbS1LeXdZQ1dVU1FURnlSWGctUUIyc29fUQ?oc=5\">Anthropic rolls out Claude for Nonprofits with up to 75% discounts</a>  The Economic Times",
      "summary": "Anthropic rolls out Claude for Nonprofits with up to 75% discounts  The Economic Times",
      "publishedAt": "2025-12-02T16:52:47.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.434792723281256,
      "ingestedAt": "2025-12-02T21:47:02.949Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09cd40b1",
      "title": "Anthropic co-founder warns AI may design its own successor, says humans face a ‘big decision’ before 2030 - livemint.com",
      "url": "https://news.google.com/rss/articles/CBMijwJBVV95cUxObTd0Z3d4MlltV3pjX3J0SUFaUmdyWFkybmtXaFpPd2d2ZkNkVUw5WFBreUZNLUJlZmI2T2hrdm1FYlZ6OEVLRkxteE94US1ZUF8wXzVCY1hDMzd5dHN3VkxKNEdoX3VJbmx3ODl4UVBSV0JaOE1pRGl1Q1BtUFBnVW1qVlJLcFFWRV9JRUg0cV9yZm9hX0hRb2ZiLS1DMDRpMXRyVEVfbHQ4cjlwN0U4NjlzX2psRmczUk9CdUw1VUE3RmtabXNqRWZEeUl1RUFfaW1BTWgzNDVWMEhySjdUVFltSm1wWTU5S1hjVWc2dEwwaGgyUVgxYTVIa0Vyd3V4aUxYeFpNTVg3c1pwcXdn0gGUAkFVX3lxTE42Qkd6NkRjY2Z6OHUzT3JQR01XcVpBTnpkcTRuYkVxME56Ymkta3ROb29Oc3JUZVNwUGRVVF9lLTJtLUNKNm5ocHZfYjJucGRxaWlaSENGSmxUcWx3bU1qX2dabVRIaU1VWkNVaXhuTnFNWEp0U1RXSjA5OWQ2amd5R0cxd3RTUHExTEhHOWoteUJrejBWdEstNi1oQ3NVYW8yWnVqNjY2ek9FeU9xa3BJVFE3Y1Y3X2NDZXliX3RpTzRObVNDRjhCRXhmR05CS3dwcWtWVElMZ0NLWnZwWmZRVGd2OEs2NVdSVnNCNzFxU2tObjFNR1g1NXJwRnFvVjJ2bVFuclNTWUIwNXFlZVhMUGFZbQ?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMijwJBVV95cUxObTd0Z3d4MlltV3pjX3J0SUFaUmdyWFkybmtXaFpPd2d2ZkNkVUw5WFBreUZNLUJlZmI2T2hrdm1FYlZ6OEVLRkxteE94US1ZUF8wXzVCY1hDMzd5dHN3VkxKNEdoX3VJbmx3ODl4UVBSV0JaOE1pRGl1Q1BtUFBnVW1qVlJLcFFWRV9JRUg0cV9yZm9hX0hRb2ZiLS1DMDRpMXRyVEVfbHQ4cjlwN0U4NjlzX2psRmczUk9CdUw1VUE3RmtabXNqRWZEeUl1RUFfaW1BTWgzNDVWMEhySjdUVFltSm1wWTU5S1hjVWc2dEwwaGgyUVgxYTVIa0Vyd3V4aUxYeFpNTVg3c1pwcXdn0gGUAkFVX3lxTE42Qkd6NkRjY2Z6OHUzT3JQR01XcVpBTnpkcTRuYkVxME56Ymkta3ROb29Oc3JUZVNwUGRVVF9lLTJtLUNKNm5ocHZfYjJucGRxaWlaSENGSmxUcWx3bU1qX2dabVRIaU1VWkNVaXhuTnFNWEp0U1RXSjA5OWQ2amd5R0cxd3RTUHExTEhHOWoteUJrejBWdEstNi1oQ3NVYW8yWnVqNjY2ek9FeU9xa3BJVFE3Y1Y3X2NDZXliX3RpTzRObVNDRjhCRXhmR05CS3dwcWtWVElMZ0NLWnZwWmZRVGd2OEs2NVdSVnNCNzFxU2tObjFNR1g1NXJwRnFvVjJ2bVFuclNTWUIwNXFlZVhMUGFZbQ?oc=5\">Anthropic co-founder warns AI may design its own successor, says humans face a ‘big decision’ before 2030</a>  livemint.com",
      "summary": "Anthropic co-founder warns AI may design its own successor, says humans face a ‘big decision’ before 2030  livemint.com",
      "publishedAt": "2025-12-02T16:05:39.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.424436453151148,
      "ingestedAt": "2025-12-02T21:47:02.949Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09cd40b3",
      "title": "OpenAI is under pressure as Google, Anthropic gain ground - CNBC",
      "url": "https://news.google.com/rss/articles/CBMiekFVX3lxTFBLNVRNMm12WWxsSng3WGxjOVB3bGRrRmJhZjBrMFhpVHRkRXk3Q0ZFTTRpZ1hHREFlYjNTUUtlaUdaQzdvWlNFWFZIa2FaaXdndUh1OUVQMXh4TUZLNlZnd19BMlpScUpHTnhYWDFZNWhnUS1FNTZjczdB0gF_QVVfeXFMTlI5Zy1PMm5UcVFiVFEzT0QwcGl4WFJhNU5jb0VfZEhrN0hWel95ZFpGVG9FUEk1Qzl3U3Bidkp2ZzhMVmtQaFExd2FBcjVNMUtNMnRwQV90QXlncms5QW1tM19SeVFBZ3V6Z1Jabkxkd3J3cEVHNmh3Wmpac3JqOA?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMiekFVX3lxTFBLNVRNMm12WWxsSng3WGxjOVB3bGRrRmJhZjBrMFhpVHRkRXk3Q0ZFTTRpZ1hHREFlYjNTUUtlaUdaQzdvWlNFWFZIa2FaaXdndUh1OUVQMXh4TUZLNlZnd19BMlpScUpHTnhYWDFZNWhnUS1FNTZjczdB0gF_QVVfeXFMTlI5Zy1PMm5UcVFiVFEzT0QwcGl4WFJhNU5jb0VfZEhrN0hWel95ZFpGVG9FUEk1Qzl3U3Bidkp2ZzhMVmtQaFExd2FBcjVNMUtNMnRwQV90QXlncms5QW1tM19SeVFBZ3V6Z1Jabkxkd3J3cEVHNmh3Wmpac3JqOA?oc=5\">OpenAI is under pressure as Google, Anthropic gain ground</a>  CNBC",
      "summary": "OpenAI is under pressure as Google, Anthropic gain ground  CNBC",
      "publishedAt": "2025-12-02T15:56:06.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.422341048300976,
      "ingestedAt": "2025-12-02T21:47:02.949Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09cad2a4",
      "title": "Grok Models Come to the AI Chat in JetBrains IDEs",
      "url": "https://blog.jetbrains.com/ai/2025/12/grok-models-come-to-the-ai-chat-in-jetbrains-ides/",
      "content": "<p>We’re expanding our AI offering with xAI’s Grok family of LLMs: Grok 4, Grok 4.1 Fast, Grok 4.1 Fast (Non-Reasoning), and Grok Code Fast 1. With the addition of these models in the AI chat across all JetBrains IDEs, you now have even more flexibility to choose the model that best fits your workflow.</p> \n \n \n \n<img style=\"height:auto;\" src=\"https://blog.jetbrains.com/wp-content/uploads/2025/12/Screenshot-2025-12-02-at-16.23.07-1.png\" alt=\"\"> \n \n \n \n<h2>How to try Grok models inside JetBrains IDEs</h2> \n \n \n \n<p>Trying Grok inside the AI chat is easy! Ensure you have an active JetBrains AI subscription. If you don’t, start a free trial directly from your IDE: open the <a href=\"https://www.jetbrains.com/help/ai-assistant/installation-guide-ai-assistant.html#install-ai-assistant-plugin-and-activate-license\">JetBrains AI widget</a>, start the installation, and follow the on-screen steps.</p> \n \n \n \n<p>From there, open the AI chat and choose a Grok model from the selector in the chat panel (you’ll find all available Grok LLMs under <em>More Models</em>). Then just start interacting with it in the chat – you can ask questions about your code, refactor a snippet, generate tests, fix errors, and more.</p>",
      "summary": "We’re expanding our AI offering with xAI’s Grok family of LLMs: Grok 4, Grok 4.1 Fast, Grok 4.1 Fast (Non-Reasoning), and Grok Code Fast 1. With the addition of these models in the AI chat across all JetBrains IDEs, you now have even more flexibility to choose the model that best fits your workflow. \n \n \n \n \n \n \n \nHow to try Grok models inside JetBrains IDEs \n \n \n \nTrying Grok inside the AI chat is easy! Ensure you have an active JetBrains AI subscription. If you don’t, start a free trial direct",
      "publishedAt": "2025-12-02T15:30:49.000Z",
      "author": "Anna Maltseva",
      "source": "rss",
      "feedName": "JetBrains Company Blog",
      "sourceType": "engineering_blog",
      "contentType": "pricing_business",
      "score": 6.870575161643689,
      "ingestedAt": "2025-12-02T21:47:02.949Z",
      "tags": [
        "ide",
        "Tech Company Blogs"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c99f08",
      "title": "Peter Thiel's Apocalyptic Worldview Is a Dangerous Fantasy",
      "url": "https://jacobin.com/2025/11/peter-thiel-palantir-apocalypse-antichrist",
      "content": "<p><img src=\"https://images.jacobinmag.com/wp-content/uploads/2025/11/26174826/GettyImages-1239817058.jpg\" alt=\"GettyImages-1239817058.jpg\"></p><p>Article URL: <a href=\"https://jacobin.com/2025/11/peter-thiel-palantir-apocalypse-antichrist\">https://jacobin.com/2025/11/peter-thiel-palantir-apocalypse-antichrist</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46122851\">https://news.ycombinator.com/item?id=46122851</a></p>  \n<p>Points: 111</p>  \n<p># Comments: 78</p>",
      "summary": "Article URL: https://jacobin.com/2025/11/peter-thiel-palantir-apocalypse-antichrist  \nComments URL: https://news.ycombinator.com/item?id=46122851  \nPoints: 111  \n# Comments: 78",
      "publishedAt": "2025-12-02T16:23:27.000Z",
      "author": "robtherobber",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.476114891574846,
      "ingestedAt": "2025-12-02T21:47:02.949Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c99f0d",
      "title": "Show HN: RunMat – runtime with auto CPU/GPU routing for dense math",
      "url": "https://github.com/runmat-org/runmat",
      "content": "<p><img src=\"https://opengraph.githubassets.com/a3a2e3b795fee54c9f0ee112f6a8ae2282172868e825865c219082d9809bd769/runmat-org/runmat\" alt=\"runmat\"></p><p>Hi, I’m Nabeel. In August I released RunMat as an open-source runtime for MATLAB code that was already much faster than GNU Octave on the workloads I tried. <a href=\"https://news.ycombinator.com/item?id=44972919\">https://news.ycombinator.com/item?id=44972919</a></p><p>Since then, I’ve taken it further with RunMat Accelerate: the runtime now automatically fuses operations and routes work between CPU and GPU. You write MATLAB-style code, and RunMat runs your computation across CPUs and GPUs for speed. No CUDA, no kernel code.</p><p>Under the hood, it builds a graph of your array math, fuses long chains into a few kernels, keeps data on the GPU when that helps, and falls back to CPU JIT / BLAS for small cases.</p><p>On an Apple M2 Max (32 GB), here are some current benchmarks (median of several runs):</p><p>* 5M-path Monte Carlo  \n    * RunMat ≈ 0.61 s  \n    * PyTorch ≈ 1.70 s  \n    * NumPy ≈ 79.9 s  \n         → ~2.8× faster than PyTorch and ~130× faster than NumPy on this test.</p><p>* 64 × 4K image preprocessing pipeline  \n     (mean/std, normalize, gain/bias, gamma, MSE)  \n    * RunMat ≈ 0.68 s  \n    * PyTorch ≈ 1.20 s  \n    * NumPy ≈ 7.0 s  \n         → ~1.8× faster than PyTorch and ~10× faster than NumPy.</p><p>* 1B-point elementwise chain (sin / exp / cos / tanh mix)  \n    * RunMat ≈ 0.14 s  \n    * PyTorch ≈ 20.8 s  \n    * NumPy ≈ 11.9 s  \n         → ~140× faster than PyTorch and ~80× faster than NumPy.</p><p>If you want more detail on how the fusion and CPU/GPU routing work, I wrote up a longer post here:  \n<a href=\"https://runmat.org/blog/runmat-accel-intro-blog\">https://runmat.org/blog/runmat-accel-intro-blog</a></p><p>You can run the same benchmarks yourself from the GitHub repo in the main HN link. Feedback, bug reports, and “here’s where it breaks or is slow” examples are very welcome.</p>  \n<hr>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46121951\">https://news.ycombinator.com/item?id=46121951</a></p>  \n<p>Points: 12</p>  \n<p># Comments: 3</p>",
      "summary": "Hi, I’m Nabeel. In August I released RunMat as an open-source runtime for MATLAB code that was already much faster than GNU Octave on the workloads I tried. https://news.ycombinator.com/item?id=44972919Since then, I’ve taken it further with RunMat Accelerate: the runtime now automatically fuses operations and routes work between CPU and GPU. You write MATLAB-style code, and RunMat runs your computation across CPUs and GPUs for speed. No CUDA, no kernel code.Under the hood, it builds a graph of y",
      "publishedAt": "2025-12-02T15:07:49.000Z",
      "author": "nallana",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 7.3529369758698575,
      "ingestedAt": "2025-12-02T21:47:02.949Z",
      "tags": [
        "code_review",
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c99f11",
      "title": "OpenAI declares 'code red' as Google catches up in AI race",
      "url": "https://www.theverge.com/news/836212/openai-code-red-chatgpt",
      "content": "<p><img src=\"https://platform.theverge.com/wp-content/uploads/sites/2/2025/12/STK201_SAM_ALTMAN_CVIRGINIA_C.jpg?quality=90&amp;strip=all&amp;crop=0%2C10.732984293194%2C100%2C78.534031413613&amp;w=1200\" alt=\"STK201_SAM_ALTMAN_CVIRGINIA_C.jpg?qualit\"></p><p>Article URL: <a href=\"https://www.theverge.com/news/836212/openai-code-red-chatgpt\">https://www.theverge.com/news/836212/openai-code-red-chatgpt</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46121870\">https://news.ycombinator.com/item?id=46121870</a></p>  \n<p>Points: 85</p>  \n<p># Comments: 105</p>",
      "summary": "Article URL: https://www.theverge.com/news/836212/openai-code-red-chatgpt  \nComments URL: https://news.ycombinator.com/item?id=46121870  \nPoints: 85  \n# Comments: 105",
      "publishedAt": "2025-12-02T15:00:16.000Z",
      "author": "goplayoutside",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4700367574732522,
      "ingestedAt": "2025-12-02T21:47:02.949Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c9948c",
      "title": "How Netflix Built a Distributed Write Ahead Log For Its Data Platform",
      "url": "https://www.inoreader.com/article/3a9c6e76b2c26b73",
      "content": "<div class=\"email_is_html\"><div><div style=\"font-kerning: auto; --image-offset-margin: -120px\"><img src=\"https://eotrx.substackcdn.com/open?token=eyJtIjoiPDIwMjUxMjAyMTYzMDQ3LjMuYjRlZTkyMTA0NTMwYzc2ZUBtZzEuc3Vic3RhY2suY29tPiIsInUiOjE2MTk4NTQ4MCwiciI6ImJ5dGVieXRlZ284OEBpbm8udG8iLCJkIjoibWcxLnN1YnN0YWNrLmNvbSIsInAiOjE3OTYwNTk1OCwidCI6Im5ld3NsZXR0ZXIiLCJhIjoiZXZlcnlvbmUiLCJzIjo4MTcxMzIsImMiOiJwb3N0IiwiZiI6ZmFsc2UsInBvc2l0aW9uIjoidG9wIiwiaWF0IjoxNzY0NjkzOTU2LCJleHAiOjE3NjcyODU5NTYsImlzcyI6InB1Yi0wIiwic3ViIjoiZW8ifQ.V7_JD6QdQZm8E2L0nMyTtFIe_3QvhT7F8uvwB0dj3F4\" width=\"1\" height=\"1\" border=\"0\" style=\"height: 1px !important; width: 1px !important; border-width: 0 !important; margin-top: 0 !important; margin-bottom: 0 !important; margin-right: 0 !important; margin-left: 0 !important; padding-top: 0 !important; padding-bottom: 0 !important; padding-right: 0 !important; padding-left: 0 !important\" /><div style=\"display: none; font-size: 1px; color: #333333; line-height: 1px; max-height: 0px; max-width: 0px; opacity: 0; overflow: hidden\">Netflix’s engineering team faced several recurring issues that threatened the reliability of their data. </div><div style=\"display: none; font-size: 1px; color: #333333; line-height: 1px; max-height: 0px; max-width: 0px; opacity: 0; overflow: hidden\">͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­</div><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td></td><td width=\"550\"></td><td></td></tr><tr><td></td><td width=\"550\" align=\"left\"><div style=\"font-size: 16px; line-height: 26px; max-width: 550px; width: 100%; margin: 0 auto; overflow-wrap: break-word\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"right\" style=\"height: 20px\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><span style=\"font-family: SF Pro Text, -apple-system, system-ui, BlinkMacSystemFont, Inter, Segoe UI, Roboto, Helvetica, Arial, sans-serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol; font-size: 13px; color: unset; list-style: none; text-decoration: unset; margin: 0\"><div style=\"list-style: none; color: unset; text-align: right; font-size: 12px; line-height: 16px; text-decoration: unset; margin: 0\"><span style=\"list-style: none; color: unset; text-decoration: unset; margin: 0\">Forwarded this email? <a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9ibG9nLmJ5dGVieXRlZ28uY29tL3N1YnNjcmliZT91dG1fc291cmNlPWVtYWlsJnV0bV9jYW1wYWlnbj1lbWFpbC1zdWJzY3JpYmUmcj0yb2Z3c28mbmV4dD1odHRwcyUzQSUyRiUyRmJsb2cuYnl0ZWJ5dGVnby5jb20lMkZwJTJGaG93LW5ldGZsaXgtYnVpbHQtYS1kaXN0cmlidXRlZC13cml0ZSIsInAiOjE3OTYwNTk1OCwicyI6ODE3MTMyLCJmIjpmYWxzZSwidSI6MTYxOTg1NDgwLCJpYXQiOjE3NjQ2OTM5NTYsImV4cCI6MjA4MDI2OTk1NiwiaXNzIjoicHViLTAiLCJzdWIiOiJsaW5rLXJlZGlyZWN0In0.lIchX4PgjXXhc1Ldw64fseSuVc6TNq_4yLkL9oQOuns?\" style=\"list-style: none; color: unset; text-decoration: unset; margin: 0; -webkit-text-decoration-line: underline; text-decoration-line: underline\" target=\"_blank\" rel=\"noreferrer\">Subscribe here</a> for more</span></div></span></td></tr></tbody></table></td></tr></tbody></table><div dir=\"auto\" style=\"--image-offset-margin: -120px; padding: 32px 0 0 0; font-size: 16px; line-height: 26px\"><div style=\"font-size: 16px; line-height: 26px\"><h1 dir=\"auto\" style=\"direction: auto; text-align: start; unicode-bidi: isolate; color: rgb(54,55,55); font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 0; line-height: 36px; font-size: 32px\"><a href=\"https://substack.com/app-link/post?publication_id=817132&amp;post_id=179605958&amp;utm_source=post-email-title&amp;utm_campaign=email-post-title&amp;isFreemail=false&amp;r=2ofwso&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE3OTYwNTk1OCwiaWF0IjoxNzY0NjkzOTU2LCJleHAiOjE3NjcyODU5NTYsImlzcyI6InB1Yi04MTcxMzIiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.KNojJJ8-e3kl00guI5BLqXAsaj4SI5_5t3IqqUQjus4\" style=\"color: rgb(54,55,55); text-decoration: none\" target=\"_blank\" rel=\"noreferrer\">How Netflix Built a Distributed Write Ahead Log For Its Data Platform</a></h1><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"margin: 1em 0; height: 20px; align-items: center\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><div style=\"list-style: none; font-size: 11px; line-height: 20px; text-decoration: unset; color: rgb(54,55,55); margin: 0; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px\"><a style=\"list-style: none; color: rgb(54,55,55); margin: 0; font-size: 11px; line-height: 20px; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px; text-decoration: none\" href=\"https://substack.com/@bytebytego399569\" target=\"_blank\" rel=\"noreferrer\">ByteByteGo</a></div></td></tr></tbody></table></td></tr><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><div style=\"list-style: none; font-size: 11px; line-height: 20px; text-decoration: unset; color: rgb(119,119,119); margin: 0; font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: 500; text-transform: uppercase; letter-spacing: .2px\">Dec 2</div></td></tr></tbody></table></td></tr></tbody></table></td><td align=\"right\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><a href=\"https://substack.com/@bytebytego399569\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!U1Ej!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9941c68-e5b7-4b93-be75-df7cc4ffef02_504x540.png\" style=\"box-sizing: border-box; border-radius: 500000px; max-width: 550px; border: none; vertical-align: middle; width: 40px; height: 40px; min-width: 40px; min-height: 40px; object-fit: cover; margin: 0px; display: inline\" width=\"40\" height=\"40\" /></a></td></tr></tbody></table></td></tr></tbody></table><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"border-top: 1px solid rgb(0,0,0,.1); border-bottom: 1px solid rgb(0,0,0,.1); min-width: 100%\"><tbody><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr><tr><td><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=817132&amp;post_id=179605958&amp;utm_source=substack&amp;isFreemail=false&amp;submitLike=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE3OTYwNTk1OCwicmVhY3Rpb24iOiLinaQiLCJpYXQiOjE3NjQ2OTM5NTYsImV4cCI6MTc2NzI4NTk1NiwiaXNzIjoicHViLTgxNzEzMiIsInN1YiI6InJlYWN0aW9uIn0.g8QxXdCkNMar4HUHMsEt0Q0CLbuxJbmIetV2sIOkH7c&amp;utm_medium=email&amp;utm_campaign=email-reaction&amp;r=2ofwso\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!PeVs!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideHeart%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=817132&amp;post_id=179605958&amp;utm_source=substack&amp;utm_medium=email&amp;isFreemail=false&amp;comments=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE3OTYwNTk1OCwiaWF0IjoxNzY0NjkzOTU2LCJleHAiOjE3NjcyODU5NTYsImlzcyI6InB1Yi04MTcxMzIiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.KNojJJ8-e3kl00guI5BLqXAsaj4SI5_5t3IqqUQjus4&amp;r=2ofwso&amp;utm_campaign=email-half-magic-comments&amp;action=post-comment&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!x1tS!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideComments%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=817132&amp;post_id=179605958&amp;utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;utm_campaign=email-share&amp;action=share&amp;triggerShare=true&amp;isFreemail=false&amp;r=2ofwso&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE3OTYwNTk1OCwiaWF0IjoxNzY0NjkzOTU2LCJleHAiOjE3NjcyODU5NTYsImlzcyI6InB1Yi04MTcxMzIiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.KNojJJ8-e3kl00guI5BLqXAsaj4SI5_5t3IqqUQjus4\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!_L14!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideShare2%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"38\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9vcGVuLnN1YnN0YWNrLmNvbS9wdWIvYnl0ZWJ5dGVnby9wL2hvdy1uZXRmbGl4LWJ1aWx0LWEtZGlzdHJpYnV0ZWQtd3JpdGU_dXRtX3NvdXJjZT1zdWJzdGFjayZ1dG1fbWVkaXVtPWVtYWlsJnV0bV9jYW1wYWlnbj1lbWFpbC1yZXN0YWNrLWNvbW1lbnQmYWN0aW9uPXJlc3RhY2stY29tbWVudCZyPTJvZndzbyZ0b2tlbj1leUoxYzJWeVgybGtJam94TmpFNU9EVTBPREFzSW5CdmMzUmZhV1FpT2pFM09UWXdOVGsxT0N3aWFXRjBJam94TnpZME5qa3pPVFUyTENKbGVIQWlPakUzTmpjeU9EVTVOVFlzSW1semN5STZJbkIxWWkwNE1UY3hNeklpTENKemRXSWlPaUp3YjNOMExYSmxZV04wYVc5dUluMC5LTm9qSko4LWUza2wwMGd1STVCTHFYQXNhajRTSTVfNXQzSXFxVVFqdXM0IiwicCI6MTc5NjA1OTU4LCJzIjo4MTcxMzIsImYiOmZhbHNlLCJ1IjoxNjE5ODU0ODAsImlhdCI6MTc2NDY5Mzk1NiwiZXhwIjoyMDgwMjY5OTU2LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.NGOSuIPSYSm7buVMpD8xXMh9QWn840cAKZgZdUqNxZ0?&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 1; padding: 9px 0; text-decoration: none; color: rgb(119,119,119); min-width: 38px; box-sizing: border-box; width: 38px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!5EGt!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FNoteForwardIcon%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"border: none; vertical-align: middle; max-width: 18px\" /></a></td></tr></tbody></table></td></tr></tbody></table></td><td align=\"right\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://open.substack.com/pub/bytebytego/p/how-netflix-built-a-distributed-write?utm_source=email&amp;redirect=app-store&amp;utm_campaign=email-read-in-app\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><div style=\"font-size: 16px; line-height: 26px; display: inline-block; vertical-align: middle; max-width: 0; min-height: 18px\"></div><span style=\"vertical-align: middle; margin-right: 4px\">READ IN APP</span><img src=\"https://substackcdn.com/image/fetch/$s_!ET-_!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideArrowUpRight%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"min-width: 18px; min-height: 18px; border: none; vertical-align: middle; margin-right: 0; margin-left: 0; max-width: 18px\" /></a></td></tr></tbody></table></td></tr></tbody></table></td></tr></tbody></table></td></tr><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr></tbody></table></div></div><div dir=\"auto\" style=\"--image-offset-margin: -120px; padding: 32px 0 0 0; font-size: 16px; line-height: 26px\"><div dir=\"auto\" style=\"text-align: initial; font-size: 16px; line-height: 26px; width: 100%; word-break: break-word; margin-bottom: 16px\"><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em; margin-top: 0\"><a href=\"https://substack.com/redirect/4f43beb9-72ca-4068-a60d-4de0ab83d1ea?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: rgb(54,55,55); text-decoration: underline\" target=\"_blank\" rel=\"noreferrer\">Monster SCALE Summit 2026 (Sponsored)</a></h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><em>Extreme Scale Engineering | Online | March 11-12</em></p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1456\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/4f43beb9-72ca-4068-a60d-4de0ab83d1ea?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"288.5989010989011\" src=\"https://substackcdn.com/image/fetch/$s_!tgjd!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00b0b5eb-af16-408a-8d68-d4f2b546526f_1600x840.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Your free ticket to Monster SCALE Summit is waiting — 30+ engineering talks on data-intensive applications</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Monster SCALE Summit is a virtual conference that’s all about extreme-scale engineering and data-intensive applications. Engineers from Discord, Disney, LinkedIn, Pinterest, Rivian, American Express, Google, ScyllaDB, and more will be sharing 30+ talks on topics like:</p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">Distributed databases</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">Streaming and real-time processing</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">Intriguing system designs</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">Massive scaling challenge</p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Don’t miss this chance to connect with 20K of your peers designing, implementing, and optimizing data-intensive applications – for free, from anywhere.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Register now to save your seat, and become eligible for an early bird swag pack!</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px; text-align: center; cursor: pointer; border-radius: 4px\"><a href=\"https://substack.com/redirect/4f43beb9-72ca-4068-a60d-4de0ab83d1ea?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; box-sizing: border-box; border: none; font-size: 14px; line-height: 20px; font-weight: 600; margin: 0; opacity: 1; outline: none; white-space: nowrap; color: #ffffff !important; text-decoration: none !important; text-align: center; cursor: pointer; border-radius: 4px; background-color: #7756e3; padding: 12px 20px; height: auto\" target=\"_blank\" rel=\"noreferrer\"><span style=\"color: #ffffff; text-decoration: none\">GET YOUR FREE TICKET</span></a></p><div style=\"font-size: 16px; line-height: 26px\"><hr style=\"margin: 32px 0; padding: 0; height: 1px; background: #e6e6e6; border: none\" /></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><em>Disclaimer: The details in this post have been derived from the details shared online by the Netflix Engineering Team. All credit for the technical details goes to the Netflix Engineering Team.  The links to the original articles and sources are present in the references section at the end of the post. We’ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Netflix processes an enormous amount of data every second. Each time a user plays a show, rates a movie, or receives a recommendation, multiple databases and microservices work together behind the scenes. This functionality is supported using hundreds of independent systems that must stay consistent with each other. When something goes wrong in one system, it can quickly create a ripple effect across the platform.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Netflix’s engineering team faced several recurring issues that threatened the reliability of their data. Some of these included accidental data corruption after schema changes, inconsistent updates between storage systems such as Apache Cassandra and Elasticsearch, and message delivery failures during transient outages. At times, bulk operations like large delete jobs even caused key-value database nodes to run out of memory. On top of that, some databases lacked built-in replication, which meant that regional failures could lead to permanent data loss.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Each engineering team tried to handle these issues differently. One team would build custom retry systems, another would design its own backup strategy, and yet another would use Kafka directly for message delivery. While these solutions worked individually, they created complexity and inconsistent guarantees across Netflix’s ecosystem. Over time, this patchwork approach increased maintenance costs and made debugging more difficult.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">To fix this, Netflix built a Write-Ahead Log system to act as a single, resilient foundation for data reliability. The WAL standardizes how data changes are recorded, stored, and replayed across services. In simple terms, it captures every change before it is applied to the database, so that even if something fails midway, no information is lost.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">In this article, we will look at how Netflix built this WAL and the challenges it faced.</p><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em\">What is a Write-Ahead Log?</h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">At its core, a Write-Ahead Log is a simple but powerful idea. It is a system that keeps a record of every change made to data before those changes are applied to the actual database. You can think of it like keeping a journal of all the actions you plan to take. Even if something goes wrong during the process, you still have that journal to remind you exactly what you were doing, so you can pick up right where you left off.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">In practical terms, when an application wants to update or delete information in a database, it first writes that intention to the WAL. Only after the entry has been safely recorded does the database proceed with the operation. This means that if a server crashes or a network connection drops, Netflix can replay the operations from the WAL and restore everything to the correct state. Nothing is lost, and the data remains consistent across systems.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Netflix’s version of WAL is not tied to a single database or service.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">It is distributed, meaning it runs across multiple servers to handle massive volumes of data. It is also pluggable, allowing it to connect easily to various technologies, such as Kafka, Amazon SQS, Apache Cassandra, and EVCache. This flexibility allowed the Netflix engineering team to use the same reliability framework for different types of workloads, whether it’s storing cached video metadata, user preferences, or system logs.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">See the diagram below:</p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1456\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/0540ef12-d824-48ad-a16c-38e66ce32641?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"346.3942307692308\" src=\"https://substackcdn.com/image/fetch/$s_!0T5-!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f447ce0-8f9a-426c-9c86-4d7360c23082_2858x1800.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">The WAL provides several key benefits that make Netflix’s data platform more resilient:</p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Durability:</strong><span> Every change is logged first, so even if a database goes offline, no data is permanently lost.</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Retry and Delay Support:</strong><span> If a message fails to process due to an outage or network issue, the WAL can automatically retry it later, with custom delays.</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Cross-Region Replication:</strong><span> Data can be copied across regions, ensuring the same information exists in multiple data centers for disaster recovery.</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Multi-Partition Consistency:</strong><span> For complex updates involving multiple tables or partitions, WAL ensures that all changes are coordinated and eventually consistent.</span></p></li></ul><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em\">The WAL API</h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Netflix’s Write-Ahead Log system provides a simple interface for the developers. Despite the complexity of what happens behind the scenes, the API that developers interact with contains only one main operation called WriteToLog.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">This API acts as the entry point for any application that wants to record a change. The structure looks something like this:</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">rpc WriteToLog (WriteToLogRequest) returns (WriteToLogResponse);</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Even though this may look technical, the idea is straightforward. A service sends a request to WAL describing what it wants to write and where that data should go. WAL then processes the request, stores it safely, and responds with information about whether the operation was successful.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">The request contains four main parts:</p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Namespace:</strong><span> This identifies which logical group or application the data belongs to. Think of it as a label that helps WAL organize and isolate data from different teams or services.</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Lifecycle:</strong><span> This specifies timing details, such as whether the message should be delayed or how long WAL should keep it.</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Payload:</strong><span> This is the actual content or data being written to the log.</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Target:</strong><span> This tells WAL where to send the data after it has been safely recorded, such as a Kafka topic, a database, or a cache.</span></p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">The response from WAL is equally simple:</p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Durable:</strong><span> Indicates whether the request was successfully stored and made reliable.</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Message:</strong><span> Provides details if something went wrong, like an error message or reason for failure.</span></p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Each namespace in WAL has its own configuration that defines how it behaves. For example, one namespace may be set up to use Kafka for high-speed streaming, while another might rely on Amazon SQS for delayed message delivery. The team can adjust settings like retry counts, backoff times, and delay intervals depending on what each application needs.</p><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em\">Different Use Cases of the WAL</h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Netflix designed the WAL system to be flexible enough to support many different situations, which they refer to as personas. Each persona represents a unique way that WAL is used within the company’s data ecosystem.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Let’s look at a few of the main ones to understand how this system adapts to different needs.</p><h3 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.375em\">1 - Delayed Queues</h3><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">This use case comes from the Product Data Systems (PDS) team, which handles a lot of real-time data updates.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">In large-scale systems like Netflix, failures are inevitable.  Sometimes, a downstream service such as Kafka or a database might be temporarily unavailable due to network issues or maintenance.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Instead of losing messages or forcing engineers to manually retry failed operations, WAL automatically steps in. When a system failure occurs, WAL uses Amazon SQS (Simple Queue Service) to delay messages and retry them later.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">See the diagram below for backoff and delayed retries for clients producing to Kafka:</p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1456\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/f21820df-f11d-42e9-bbe2-46d4fe14d746?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"322.9739010989011\" src=\"https://substackcdn.com/image/fetch/$s_!Tg2M!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feffaacc9-c451-4248-9e44-6e3306ee6f65_2858x1678.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Here’s how it works in simple terms:</p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">If a message fails to be delivered, WAL stores it in a queue and waits for a certain amount of time before trying again. The delay can be configured based on how long the system is expected to recover.</p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\">Once the downstream service is back online, WAL automatically retries the messages, ensuring nothing is lost and no manual intervention is needed.</p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">The diagram below shows the backoff and delayed retries for clients consuming from Kafka:</p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1456\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/36459b44-efc3-4810-a9a2-97b67a5fb651?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"386.8131868131868\" src=\"https://substackcdn.com/image/fetch/$s_!5Blb!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7657753b-b72c-4bc3-9885-5b8b79a5b9c8_2858x2010.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">This approach saves engineers a lot of time and prevents cascading failures that might otherwise spread across the platform.</p><h3 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.375em\">2 - Cross-Region Replication</h3><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Another major use case is data replication across Netflix’s global regions. The company’s caching system, EVCache, stores frequently accessed data to make streaming fast and reliable. However, since Netflix operates worldwide, the same data needs to exist in multiple regions.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">WAL makes this replication seamless by using Kafka under the hood. Whenever data is written or deleted in one region, WAL captures that event and sends it to other regions. The consumers in each region then replay the same operations locally, ensuring that all copies of the data stay synchronized.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">See the diagram below:</p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1456\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/14524280-b0f3-49b0-aff9-e8f27d61ce4f?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"332.7953296703297\" src=\"https://substackcdn.com/image/fetch/$s_!noXT!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76766deb-1d38-47aa-8567-da34f9cb7405_2858x1730.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">In simpler terms, WAL acts like a reliable postman, making sure every region receives the same “letters” (data updates), even if network disruptions occur. This system keeps Netflix consistent around the world. Users in India, Europe, or the US all see the same data at nearly the same time.</p><h3 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.375em\">3 - Multi-Partition Mutations</h3><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">The final example involves Netflix’s Key-Value data service, which stores information in systems like Apache Cassandra. Sometimes, a single operation might need to update data spread across multiple partitions or tables. Handling these multi-part changes is tricky, especially in distributed systems, because a failure in one partition can leave others out of sync.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">WAL solves this problem by ensuring atomicity, meaning that either all the changes succeed or all are retried until they do. To achieve this, Netflix’s WAL combines Kafka for message delivery with durable storage for reliability. This setup functions similarly to a two-phase commit, a well-known database technique that guarantees data consistency across multiple locations.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">In short, WAL coordinates complex updates so that Netflix’s data remains correct, even when multiple systems are involved.</p><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em\">Internal Architecture</h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">To understand how Netflix’s Write-Ahead Log (WAL) works behind the scenes, it helps to break it down into its main building blocks.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">See the diagram below:</p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1456\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/9e6b82fb-2f36-43cb-abc7-945ccb9a769e?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"346.3942307692308\" src=\"https://substackcdn.com/image/fetch/$s_!PA23!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80838645-07ec-4a5c-aad1-1bb5d0e769d8_2858x1800.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table></div><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">The system is made up of several key components that work together to move data safely from one place to another while keeping everything flexible and resilient.</p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Producers:</strong><span> The producer is the first part of the system. It accepts messages or data change requests from various Netflix applications and writes them into a queue. You can think of producers as the “entry doors” of WAL. Whenever an app wants to log an update, it hands the data to a producer, which makes sure it gets safely added to the right queue.</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Consumers:</strong><span> Consumers are the “exit doors” of the system. Their job is to read messages from the queue and send them to the correct destination, such as a database, cache, or another service. Since consumers run separately from producers, they can process messages at their own pace without slowing down the rest of the system.</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Message Queues: </strong><span>The message queue is the middle layer that connects producers and consumers. Netflix primarily uses Kafka or Amazon SQS for this purpose. Each namespace in WAL (which represents a specific use case or service) has its own dedicated queue. This ensures isolation between applications so that a heavy workload from one service does not affect another. Every namespace also includes a Dead Letter Queue (DLQ). The DLQ is a special backup queue that stores messages that repeatedly fail to process. This gives engineers a chance to inspect and fix the problematic data later without losing it.</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Control Plane:</strong><span> The control plane is like the central command center for WAL. It allows Netflix engineers to change settings, such as which queue type to use, how many retries should occur, or what the delay between retries should be. The key advantage here is that teams can modify these settings without having to change their application code. This makes the system highly adaptable and easy to maintain.</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Targets: </strong><span>Finally, the targets are the destinations where WAL sends the data. A target can be a database like Cassandra, a cache like EVCache, or even another message queue. The flexibility of defining targets through configuration means that the same WAL architecture can support many different workloads across Netflix.</span></p></li></ul><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em\">Deployment Model</h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">The way Netflix deploys its Write-Ahead Log (WAL) system is just as important as how it works internally.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">To handle billions of data operations across many teams and services, Netflix needed a platform that could scale easily, stay secure, and run reliably across regions. To achieve this, WAL is deployed on top of Netflix’s Data Gateway Infrastructure.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">This infrastructure acts as a foundation that gives WAL several built-in advantages right out of the box:</p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>mTLS for security:</strong><span> All communication between services is encrypted and authenticated using mutual Transport Layer Security (mTLS). This ensures that only trusted Netflix services can talk to each other, keeping sensitive data safe.</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Connection management:</strong><span> The platform automatically manages network connections, making sure requests are routed efficiently and that no single component gets overloaded.</span></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><strong>Auto-scaling and load shedding:</strong><span> WAL uses adaptive scaling to adjust the number of active instances based on demand. If CPU or network usage gets too high, the system automatically adds more capacity. In extreme cases, it can also shed low-priority requests to protect the stability of the service.</span></p></li></ul><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Netflix organizes WAL deployments into shards. A shard is an independent deployment that serves a specific group of applications or use cases. For example, one shard might handle the Ads service, another might handle Gaming data, and so on. This separation prevents the “noisy neighbor” problem, where one busy service could slow down others running on the same system.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Inside each shard, there can be multiple namespaces, each with its own configuration and purpose. These configurations are stored in a globally replicated SQL database, ensuring they are always available and consistent, even if a region goes offline.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">See the diagram below for the deployment model of WAL at Netflix:</p><div style=\"font-size: 16px; line-height: 26px; margin: 32px auto\"><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"mso-padding-alt: 1em 0 1.6em\"><tbody><tr><td style=\"text-align: center\"></td><td align=\"left\" width=\"1456\" style=\"text-align: center\"><a href=\"https://substack.com/redirect/04263a29-33bb-43d6-afbb-544a79f797b7?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"position: relative; flex-direction: column; align-items: center; padding: 0; width: auto; height: auto; border: none; text-decoration: none; display: block; margin: 0\" target=\"_blank\" rel=\"noreferrer\"><img width=\"550\" height=\"362.25961538461536\" src=\"https://substackcdn.com/image/fetch/$s_!qH-8!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4e9b524-265d-4ee5-81ac-1dea8856a0b6_2858x1882.png\" style=\"border: none !important; vertical-align: middle; display: block; -ms-interpolation-mode: bicubic; height: auto; margin-bottom: 0; width: auto !important; max-width: 100% !important; margin: 0 auto\" /></a></td><td style=\"text-align: center\"></td></tr></tbody></table></div><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em\">Conclusion</h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Several key design principles shaped the success of WAL. The first is its pluggable architecture, which allows Netflix to switch between different technologies, such as Kafka or Amazon SQS, without changing application code. This flexibility ensures that teams can choose the most suitable underlying system for their specific use cases while relying on the same core framework.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Another principle is the reuse of existing infrastructure. Instead of building everything from scratch, Netflix built WAL on top of its already established systems, like the Data Gateway platform and Key-Value abstractions. This approach saved development time and allowed the new system to fit naturally into the company’s broader data architecture.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Equally important is the separation of concerns between producers and consumers. Because these components scale independently, Netflix can adjust each one based on traffic patterns or system load. This independence allows WAL to handle massive spikes in demand without service degradation.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Finally, Netflix recognizes that even a system designed for reliability must consider its own limits. The team continuously evaluates trade-offs, such as dealing with slow consumers or managing backpressure during heavy traffic. Techniques like partitioning and controlled retries are essential to keeping the system stable.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Looking ahead, Netflix plans to enhance WAL further. Future improvements include adding secondary indices to the Key-Value service, which will make data retrieval faster and more efficient, and supporting multi-target writes, allowing a single operation to send data to multiple destinations, such as a database and a backup system at the same time.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><strong>References:</strong></p><ul style=\"margin-top: 0; padding: 0\"><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><a href=\"https://substack.com/redirect/31135f37-9df0-41dd-853e-adb766e3ad12?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: #7756e3; text-decoration: none\" target=\"_blank\" rel=\"noreferrer\">Building a Resilient Data Platform with Write-Ahead Log at Netflix</a></p></li><li style=\"margin: 8px 0 0 32px; mso-special-format: bullet\"><p style=\"color: rgb(54,55,55); line-height: 26px; margin-bottom: 0; box-sizing: border-box; padding-left: 4px; font-size: 16px; margin: 0\"><a href=\"https://substack.com/redirect/7a71d7e6-fb00-4a11-937f-e25b90fd5b49?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: #7756e3; text-decoration: none\" target=\"_blank\" rel=\"noreferrer\">Write-Ahead Logging</a></p></li></ul><div style=\"font-size: 16px; line-height: 26px\"><hr style=\"margin: 32px 0; padding: 0; height: 1px; background: #e6e6e6; border: none\" /></div><h2 style=\"position: relative; font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; font-weight: bold; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: antialiased; -webkit-appearance: optimizelegibility; -moz-appearance: optimizelegibility; appearance: optimizelegibility; margin: 1em 0 0.625em 0; color: rgb(54,55,55); line-height: 1.16em; font-size: 1.625em\"><strong>SPONSOR US</strong></h2><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Get your product in front of more than 1,000,000 tech professionals.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\">Space Fills Up Fast - Reserve Today</p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px; margin-bottom: 0\"><span>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing </span><strong><a href=\"https://substack.com/redirect/15815858-089a-4925-9a19-98be37fbca50?j=eyJ1IjoiMm9md3NvIn0.2ZJQMJtSzbcfzFwt4HddqHCWhtnPXxMGmTolv1mG4Zc\" style=\"color: #7756e3; text-decoration: none\" target=\"_blank\" rel=\"noreferrer\">sponsorship@bytebytego.com</a><span>.</span></strong></p></div></div><div style=\"margin: 32px 0 0; width: 100%; box-sizing: border-box; border-top: 1px solid #e6e6e6; font-size: 16px; line-height: 26px\"></div><div style=\"--image-offset-margin: -120px; font-size: 16px; line-height: 26px\"><div style=\"margin: 32px 0; font-size: 16px; line-height: 26px\"><div dir=\"auto\" style=\"text-align: initial; font-size: 16px; line-height: 26px; width: 100%; word-break: break-word; margin-bottom: 16px\"><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px; margin-top: 0\"><em>Know someone who would benefit from the newsletter? Consider gifting a subscription.</em></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px; text-align: center; cursor: pointer; border-radius: 4px\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9ibG9nLmJ5dGVieXRlZ28uY29tL3N1YnNjcmliZT91dG1fc291cmNlPXBvc3QmdXRtX2NhbXBhaWduPWVtYWlsLWNoZWNrb3V0Jm5leHQ9aHR0cHMlM0ElMkYlMkZibG9nLmJ5dGVieXRlZ28uY29tJTJGcCUyRmhvdy1uZXRmbGl4LWJ1aWx0LWEtZGlzdHJpYnV0ZWQtd3JpdGUmcj0yb2Z3c28mdG9rZW49ZXlKMWMyVnlYMmxrSWpveE5qRTVPRFUwT0RBc0ltbGhkQ0k2TVRjMk5EWTVNemsxTml3aVpYaHdJam94TnpZM01qZzFPVFUyTENKcGMzTWlPaUp3ZFdJdE9ERTNNVE15SWl3aWMzVmlJam9pWTJobFkydHZkWFFpZlEuY21zY1Fkb0FTOUpjZW9zQlhFYml0emt0UE5SNDNMWnFKOUVCM1huRGNucyIsInAiOjE3OTYwNTk1OCwicyI6ODE3MTMyLCJmIjpmYWxzZSwidSI6MTYxOTg1NDgwLCJpYXQiOjE3NjQ2OTM5NTYsImV4cCI6MjA4MDI2OTk1NiwiaXNzIjoicHViLTAiLCJzdWIiOiJsaW5rLXJlZGlyZWN0In0.S1yHBvjP24ypLjH17N8yF56x6l9pI2KmZALmmMQzOCY?&amp;gift=true\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; box-sizing: border-box; cursor: pointer; border: none; border-radius: 8px; font-size: 14px; line-height: 20px; font-weight: 600; text-align: center; margin: 0; opacity: 1; outline: none; white-space: nowrap; color: #ffffff !important; text-decoration: none !important; background-color: #7756e3; padding: 12px 20px; height: auto\" target=\"_blank\" rel=\"noreferrer\"><span style=\"color: #ffffff; text-decoration: none\">Give a gift subscription</span></a></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px\"><em>For group subscriptions</em><span>, click here.</span></p><p style=\"margin: 0 0 20px 0; color: rgb(54,55,55); line-height: 26px; font-size: 16px; margin-bottom: 0; text-align: center; cursor: pointer; border-radius: 4px\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9ibG9nLmJ5dGVieXRlZ28uY29tL3N1YnNjcmliZT91dG1fc291cmNlPXBvc3QmdXRtX2NhbXBhaWduPWVtYWlsLWNoZWNrb3V0Jm5leHQ9aHR0cHMlM0ElMkYlMkZibG9nLmJ5dGVieXRlZ28uY29tJTJGcCUyRmhvdy1uZXRmbGl4LWJ1aWx0LWEtZGlzdHJpYnV0ZWQtd3JpdGUmcj0yb2Z3c28mdG9rZW49ZXlKMWMyVnlYMmxrSWpveE5qRTVPRFUwT0RBc0ltbGhkQ0k2TVRjMk5EWTVNemsxTml3aVpYaHdJam94TnpZM01qZzFPVFUyTENKcGMzTWlPaUp3ZFdJdE9ERTNNVE15SWl3aWMzVmlJam9pWTJobFkydHZkWFFpZlEuY21zY1Fkb0FTOUpjZW9zQlhFYml0emt0UE5SNDNMWnFKOUVCM1huRGNucyIsInAiOjE3OTYwNTk1OCwicyI6ODE3MTMyLCJmIjpmYWxzZSwidSI6MTYxOTg1NDgwLCJpYXQiOjE3NjQ2OTM5NTYsImV4cCI6MjA4MDI2OTk1NiwiaXNzIjoicHViLTAiLCJzdWIiOiJsaW5rLXJlZGlyZWN0In0.S1yHBvjP24ypLjH17N8yF56x6l9pI2KmZALmmMQzOCY?group=true&amp;coupon=254241e3\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; box-sizing: border-box; cursor: pointer; border: none; border-radius: 8px; font-size: 14px; line-height: 20px; font-weight: 600; text-align: center; margin: 0; opacity: 1; outline: none; white-space: nowrap; color: #ffffff !important; text-decoration: none !important; background-color: #7756e3; padding: 12px 20px; height: auto\" target=\"_blank\" rel=\"noreferrer\"><span style=\"color: #ffffff; text-decoration: none\">Get 20% off a group subscription</span></a></p></div></div></div><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"border-top: 1px solid rgb(0,0,0,.1); border-bottom: 1px solid rgb(0,0,0,.1); min-width: 100%\"><tbody><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr><tr><td><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"margin: 0 auto\"><tbody><tr><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=817132&amp;post_id=179605958&amp;utm_source=substack&amp;isFreemail=false&amp;submitLike=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE3OTYwNTk1OCwicmVhY3Rpb24iOiLinaQiLCJpYXQiOjE3NjQ2OTM5NTYsImV4cCI6MTc2NzI4NTk1NiwiaXNzIjoicHViLTgxNzEzMiIsInN1YiI6InJlYWN0aW9uIn0.g8QxXdCkNMar4HUHMsEt0Q0CLbuxJbmIetV2sIOkH7c&amp;utm_medium=email&amp;utm_campaign=email-reaction&amp;r=2ofwso\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!PeVs!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideHeart%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"margin-right: 8px; min-width: 18px; min-height: 18px; border: none; vertical-align: middle; max-width: 18px\" /><span style=\"vertical-align: middle\">Like</span></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/app-link/post?publication_id=817132&amp;post_id=179605958&amp;utm_source=substack&amp;utm_medium=email&amp;isFreemail=false&amp;comments=true&amp;token=eyJ1c2VyX2lkIjoxNjE5ODU0ODAsInBvc3RfaWQiOjE3OTYwNTk1OCwiaWF0IjoxNzY0NjkzOTU2LCJleHAiOjE3NjcyODU5NTYsImlzcyI6InB1Yi04MTcxMzIiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.KNojJJ8-e3kl00guI5BLqXAsaj4SI5_5t3IqqUQjus4&amp;r=2ofwso&amp;utm_campaign=email-half-magic-comments&amp;action=post-comment&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!x1tS!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideComments%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"margin-right: 8px; min-width: 18px; min-height: 18px; border: none; vertical-align: middle; max-width: 18px\" /><span style=\"vertical-align: middle\">Comment</span></a></td></tr></tbody></table></td><td width=\"8\" style=\"min-width: 8px\"></td><td style=\"vertical-align: middle\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr><td align=\"center\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9vcGVuLnN1YnN0YWNrLmNvbS9wdWIvYnl0ZWJ5dGVnby9wL2hvdy1uZXRmbGl4LWJ1aWx0LWEtZGlzdHJpYnV0ZWQtd3JpdGU_dXRtX3NvdXJjZT1zdWJzdGFjayZ1dG1fbWVkaXVtPWVtYWlsJnV0bV9jYW1wYWlnbj1lbWFpbC1yZXN0YWNrLWNvbW1lbnQmYWN0aW9uPXJlc3RhY2stY29tbWVudCZyPTJvZndzbyZ0b2tlbj1leUoxYzJWeVgybGtJam94TmpFNU9EVTBPREFzSW5CdmMzUmZhV1FpT2pFM09UWXdOVGsxT0N3aWFXRjBJam94TnpZME5qa3pPVFUyTENKbGVIQWlPakUzTmpjeU9EVTVOVFlzSW1semN5STZJbkIxWWkwNE1UY3hNeklpTENKemRXSWlPaUp3YjNOMExYSmxZV04wYVc5dUluMC5LTm9qSko4LWUza2wwMGd1STVCTHFYQXNhajRTSTVfNXQzSXFxVVFqdXM0IiwicCI6MTc5NjA1OTU4LCJzIjo4MTcxMzIsImYiOmZhbHNlLCJ1IjoxNjE5ODU0ODAsImlhdCI6MTc2NDY5Mzk1NiwiZXhwIjoyMDgwMjY5OTU2LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.NGOSuIPSYSm7buVMpD8xXMh9QWn840cAKZgZdUqNxZ0?&amp;utm_source=substack&amp;utm_medium=email\" style=\"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; display: inline-block; font-weight: 500; border: 1px solid rgb(0,0,0,.1); border-radius: 9999px; text-transform: uppercase; font-size: 12px; line-height: 12px; padding: 9px 14px; text-decoration: none; color: rgb(119,119,119)\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!5EGt!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FNoteForwardIcon%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2\" width=\"18\" height=\"18\" style=\"margin-right: 8px; min-width: 18px; min-height: 18px; border: none; vertical-align: middle; max-width: 18px\" /><span style=\"vertical-align: middle\">Restack</span></a></td></tr></tbody></table></td></tr></tbody></table></td><td align=\"right\"><table width=\"auto\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\"><tbody><tr></tr></tbody></table></td></tr></tbody></table></td></tr><tr height=\"16\"><td height=\"16\" style=\"font-size: 0px; line-height: 0\"> </td></tr></tbody></table><div style=\"color: rgb(119,119,119); text-align: center; font-size: 16px; line-height: 26px; padding: 24px0\"><div style=\"font-size: 16px; line-height: 26px; padding-bottom: 24px\"><p style=\"list-style: none; font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; padding-bottom: 0; font-size: 12px; line-height: 16px; margin: 0; color: rgb(119,119,119); text-decoration: unset\">© 2025 <span>ByteByteGo</span><br />548 Market Street PMB 72296, San Francisco, CA 94104 <br /><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9ibG9nLmJ5dGVieXRlZ28uY29tL2FjdGlvbi9kaXNhYmxlX2VtYWlsP3Rva2VuPWV5SjFjMlZ5WDJsa0lqb3hOakU1T0RVME9EQXNJbkJ2YzNSZmFXUWlPakUzT1RZd05UazFPQ3dpYVdGMElqb3hOelkwTmprek9UVTJMQ0psZUhBaU9qRTNPVFl5TWprNU5UWXNJbWx6Y3lJNkluQjFZaTA0TVRjeE16SWlMQ0p6ZFdJaU9pSmthWE5oWW14bFgyVnRZV2xzSW4wLlVCNVZFODJZY2M1eXBhUjAtOEhoMUk2cm5oV096bjQ5MGhqdTl3WkVsTHciLCJwIjoxNzk2MDU5NTgsInMiOjgxNzEzMiwiZiI6ZmFsc2UsInUiOjE2MTk4NTQ4MCwiaWF0IjoxNzY0NjkzOTU2LCJleHAiOjIwODAyNjk5NTYsImlzcyI6InB1Yi0wIiwic3ViIjoibGluay1yZWRpcmVjdCJ9.RYRCgDkbUVs2S6OLFSTSt3zwyoGBvq4fZEnSf2NrQgM?\" style=\"color: #7756e3; text-decoration: none\" target=\"_blank\" rel=\"noreferrer\"><span style=\"color: rgb(119,119,119); text-decoration: underline\">Unsubscribe</span></a></p></div><p style=\"padding: 0 24px; font-size: 12px; line-height: 20px; margin: 0; color: rgb(119,119,119); font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'; padding-bottom: 0; margin-top: 0\"><a href=\"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9zdWJzdGFjay5jb20vc2lnbnVwP3V0bV9zb3VyY2U9c3Vic3RhY2smdXRtX21lZGl1bT1lbWFpbCZ1dG1fY29udGVudD1mb290ZXImdXRtX2NhbXBhaWduPWF1dG9maWxsZWQtZm9vdGVyJmZyZWVTaWdudXBFbWFpbD1ieXRlYnl0ZWdvODhAaW5vLnRvJnI9Mm9md3NvIiwicCI6MTc5NjA1OTU4LCJzIjo4MTcxMzIsImYiOmZhbHNlLCJ1IjoxNjE5ODU0ODAsImlhdCI6MTc2NDY5Mzk1NiwiZXhwIjoyMDgwMjY5OTU2LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.s2t4GWR62q4hJpwT1GcYQDW6kBvP-VCy8pLYkc_C4qA?\" style=\"color: #7756e3; text-decoration: none; display: inline-block; margin: 0 4px\" target=\"_blank\" rel=\"noreferrer\"><img src=\"https://substackcdn.com/image/fetch/$s_!LkrL!,w_270,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpublish-button%402x.png\" width=\"135\" alt=\"Start writing\" height=\"40\" style=\"max-width: 550px; border: none !important; vertical-align: middle\" /></a></p></div></div></td><td></td></tr></tbody></table><img src=\"https://eotrx.substackcdn.com/open?token=eyJtIjoiPDIwMjUxMjAyMTYzMDQ3LjMuYjRlZTkyMTA0NTMwYzc2ZUBtZzEuc3Vic3RhY2suY29tPiIsInUiOjE2MTk4NTQ4MCwiciI6ImJ5dGVieXRlZ284OEBpbm8udG8iLCJkIjoibWcxLnN1YnN0YWNrLmNvbSIsInAiOjE3OTYwNTk1OCwidCI6Im5ld3NsZXR0ZXIiLCJhIjoiZXZlcnlvbmUiLCJzIjo4MTcxMzIsImMiOiJwb3N0IiwiZiI6ZmFsc2UsInBvc2l0aW9uIjoiYm90dG9tIiwiaWF0IjoxNzY0NjkzOTU2LCJleHAiOjE3NjcyODU5NTYsImlzcyI6InB1Yi0wIiwic3ViIjoiZW8ifQ.lWIJeRLb7YXd0ZU8beofwXUODfOvN6DlUeM_WGsB-IM\" width=\"1\" height=\"1\" border=\"0\" style=\"height: 1px !important; width: 1px !important; border-width: 0 !important; margin-top: 0 !important; margin-bottom: 0 !important; margin-right: 0 !important; margin-left: 0 !important; padding-top: 0 !important; padding-bottom: 0 !important; padding-right: 0 !important; padding-left: 0 !important\" /><img width=\"1\" height=\"1\" src=\"https://email.mg1.substack.com/o/eJxMkE2O8yAMQE9TlpH5hwVniYC4-dCXQAWmo9x-lOmmC2_es6wn50i4t36FVxvEtgBWZJsYBm6NMl56bRiesRzrjhV7JNzWSF9WgGT_ggAhJURjPbrMk3tqlSUgl_YpuXOKlXtFcwGCGwnKLnJJCtELDkpLyNbgQ8G582XMNCjm_0tuJ7ur1ji3gjVjwDf2q1X84LIFbr0B7bX7ELpeGCr-jAOJsLPXTGtu5zlroWvFGtOBW6A-8VZHyZFKq_chxy2XgvWQLsJ79ubcQ0GpbaHGxkxbO2OpX57R52dzYP9rMdw7rRywdxC_AQAA__97Am9k\" /></div></div></div>",
      "summary": "Netflix’s engineering team faced several recurring issues that threatened the reliability of their data. ͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏     ­͏  ",
      "publishedAt": "2025-12-02T16:46:02.000Z",
      "author": "ByteByteGo ",
      "source": "rss",
      "feedName": "Byte Byte Go",
      "sourceType": "general",
      "contentType": "feature_update",
      "score": 8.86661620174073,
      "ingestedAt": "2025-12-02T21:47:02.951Z",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c3efea",
      "title": "Fallout 2's Chris Avellone describes his game design philosophy",
      "url": "https://arstechnica.com/gaming/2025/12/fallout-2-designer-chris-avellone-recalls-his-first-forays-into-game-development/",
      "content": "<p><img src=\"https://cdn.arstechnica.net/wp-content/uploads/2025/10/Variant_4-rotated-1152x648-1762375363.jpg\" alt=\"Variant_4-rotated-1152x648-1762375363.jp\"></p><p>Article URL: <a href=\"https://arstechnica.com/gaming/2025/12/fallout-2-designer-chris-avellone-recalls-his-first-forays-into-game-development/\">https://arstechnica.com/gaming/2025/12/fallout-2-designer-chris-avellone-recalls-his-first-forays-into-game-development/</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46122518\">https://news.ycombinator.com/item?id=46122518</a></p>  \n<p>Points: 23</p>  \n<p># Comments: 4</p>",
      "summary": "Article URL: https://arstechnica.com/gaming/2025/12/fallout-2-designer-chris-avellone-recalls-his-first-forays-into-game-development/  \nComments URL: https://news.ycombinator.com/item?id=46122518  \nPoints: 23  \n# Comments: 4",
      "publishedAt": "2025-12-02T15:56:39.000Z",
      "author": "LaSombra",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4741538960545633,
      "ingestedAt": "2025-12-02T21:47:02.952Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c1ed0f",
      "title": "Anthropic launches Claude for Nonprofits with up to 75% discount By Investing.com - Investing.com South Africa",
      "url": "https://news.google.com/rss/articles/CBMiuwFBVV95cUxQVkJnLXZlVGo3VGpjb2FnWmlOcldUaHRfZUY5OGVSV3pnODRqZFpwaHZabWNhbkJqOUltX0lNUmMtVFJWUkdaRFN4dlJIMXI4Mk9keFRfSi1SSy1VWHZzUDBMNUR0VkJESE9wZkE1ZE0xaDRpVXhGbGdQOUptQzZpZnJQcllzNUU1bVBSWEtmRjJ6eUR1eFA0NkNVcE9weDdvZEdmWUtKTGszbmZHR2g1Q3lWcl9VOGg4bGRj?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMiuwFBVV95cUxQVkJnLXZlVGo3VGpjb2FnWmlOcldUaHRfZUY5OGVSV3pnODRqZFpwaHZabWNhbkJqOUltX0lNUmMtVFJWUkdaRFN4dlJIMXI4Mk9keFRfSi1SSy1VWHZzUDBMNUR0VkJESE9wZkE1ZE0xaDRpVXhGbGdQOUptQzZpZnJQcllzNUU1bVBSWEtmRjJ6eUR1eFA0NkNVcE9weDdvZEdmWUtKTGszbmZHR2g1Q3lWcl9VOGg4bGRj?oc=5\">Anthropic launches Claude for Nonprofits with up to 75% discount By Investing.com</a>  Investing.com South Africa",
      "summary": "Anthropic launches Claude for Nonprofits with up to 75% discount By Investing.com  Investing.com South Africa",
      "publishedAt": "2025-12-02T15:50:14.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 10.315793357963752,
      "ingestedAt": "2025-12-02T21:47:02.953Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c1ed15",
      "title": "What to know about Anthropic cofounder Daniela Amodei as the OpenAI competitor races toward profitability - Fortune",
      "url": "https://news.google.com/rss/articles/CBMihwFBVV95cUxNczdUYzREaXBkZG5hLUVmakNPc1Q0Zm8waV9zbWxPaWJ6RzVRVUh4WVpOWFREVEpIay1rWEREZjdIOWNYSVhUNmUzc3FPRlJoREhvNEF3UzJId2ZOT29XSjV1ZlVuNkdzZW9JNDZfY1M0dmY2MGQzTUVHUmpvNE00a2pST2cwWEE?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMihwFBVV95cUxNczdUYzREaXBkZG5hLUVmakNPc1Q0Zm8waV9zbWxPaWJ6RzVRVUh4WVpOWFREVEpIay1rWEREZjdIOWNYSVhUNmUzc3FPRlJoREhvNEF3UzJId2ZOT29XSjV1ZlVuNkdzZW9JNDZfY1M0dmY2MGQzTUVHUmpvNE00a2pST2cwWEE?oc=5\">What to know about Anthropic cofounder Daniela Amodei as the OpenAI competitor races toward profitability</a>  Fortune",
      "summary": "What to know about Anthropic cofounder Daniela Amodei as the OpenAI competitor races toward profitability  Fortune",
      "publishedAt": "2025-12-02T15:32:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.417057564409857,
      "ingestedAt": "2025-12-02T21:47:02.953Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c1ed16",
      "title": "How Anthropic And Claude For Nonprofits Is Putting AI In The Hands Of Changemakers - Forbes",
      "url": "https://news.google.com/rss/articles/CBMizwFBVV95cUxOVnpoaHowSXBodVZpdmFlX3ZSQVk0MFVzdVdTa2VKMWx2V2V1RTIzdWg3Nk5jekppRVFHRm55UW1tcXhvcHFJc2lfOC1mS2ExQVZyR1dPX3RiaV95Q3EwN2p3R1dqQUxjUHBIWUJQcFVJQU1wbVJaYjhvV21TbkV2RTZWbXdlbHNwVWg5bTNXZFpwRjhGSjZabktVd2dRN3ZpTFZGV0tEV0U3T290b1A2VFc5aWsxR0d6bE1sQnNnWE1mSjhpbXNqZDZaLWx3Wms?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMizwFBVV95cUxOVnpoaHowSXBodVZpdmFlX3ZSQVk0MFVzdVdTa2VKMWx2V2V1RTIzdWg3Nk5jekppRVFHRm55UW1tcXhvcHFJc2lfOC1mS2ExQVZyR1dPX3RiaV95Q3EwN2p3R1dqQUxjUHBIWUJQcFVJQU1wbVJaYjhvV21TbkV2RTZWbXdlbHNwVWg5bTNXZFpwRjhGSjZabktVd2dRN3ZpTFZGV0tEV0U3T290b1A2VFc5aWsxR0d6bE1sQnNnWE1mSjhpbXNqZDZaLWx3Wms?oc=5\">How Anthropic And Claude For Nonprofits Is Putting AI In The Hands Of Changemakers</a>  Forbes",
      "summary": "How Anthropic And Claude For Nonprofits Is Putting AI In The Hands Of Changemakers  Forbes",
      "publishedAt": "2025-12-02T15:15:44.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.4134949739626625,
      "ingestedAt": "2025-12-02T21:47:02.953Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c1ed18",
      "title": "Leaked \"Soul Doc\" reveals how Anthropic programs Claude&rsquo;s character - the-decoder.com",
      "url": "https://news.google.com/rss/articles/CBMikwFBVV95cUxOR09IbTJweHQ4dFVWWjVTVWNxeDZZWTdfQjdoczZZcnVPUFN0R2duNTBDd2RIbTZYRVJEY3JiYzJoZzJSSFBHTEpsV2pOZ0wzRWIzWjhkbVdGMG56dElxdWhLaWN3UGJCcDJ1TW0zMjJBRklWMnJkZFFTSFJfbDh6Yks3Ty1Kd3EzWmhUMFNEMllvdVE?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMikwFBVV95cUxOR09IbTJweHQ4dFVWWjVTVWNxeDZZWTdfQjdoczZZcnVPUFN0R2duNTBDd2RIbTZYRVJEY3JiYzJoZzJSSFBHTEpsV2pOZ0wzRWIzWjhkbVdGMG56dElxdWhLaWN3UGJCcDJ1TW0zMjJBRklWMnJkZFFTSFJfbDh6Yks3Ty1Kd3EzWmhUMFNEMllvdVE?oc=5\">Leaked \"Soul Doc\" reveals how Anthropic programs Claude’s character</a>  the-decoder.com",
      "summary": "Leaked \"Soul Doc\" reveals how Anthropic programs Claude’s character  the-decoder.com",
      "publishedAt": "2025-12-02T15:14:48.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.413290650221398,
      "ingestedAt": "2025-12-02T21:47:02.953Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c1ed1a",
      "title": "The nine people trying to stop AI from ruining the world - The Verge",
      "url": "https://news.google.com/rss/articles/CBMirAFBVV95cUxPRWJKT2tpYzA5RWZTcTFIcWk5SU5uSmhxU2ZzdDRib2cxNUc1bDZkMDVRcGVGWUhQeW10Y19WRHkxQ0UtYmwwMDRyeVB2ZWtuZjk0SlEzdTAtOEF0U1AwR3lWcG5ZdTN6eUZxS1ByUS1RQm00N2pKeW9VeEJSOW9LemZ5dzRqWGlMdnRaQXNoTDEzdkttaVZ0cDVuX0ZlLS1xc1g4TFRvMy1OcG5D?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMirAFBVV95cUxPRWJKT2tpYzA5RWZTcTFIcWk5SU5uSmhxU2ZzdDRib2cxNUc1bDZkMDVRcGVGWUhQeW10Y19WRHkxQ0UtYmwwMDRyeVB2ZWtuZjk0SlEzdTAtOEF0U1AwR3lWcG5ZdTN6eUZxS1ByUS1RQm00N2pKeW9VeEJSOW9LemZ5dzRqWGlMdnRaQXNoTDEzdkttaVZ0cDVuX0ZlLS1xc1g4TFRvMy1OcG5D?oc=5\">The nine people trying to stop AI from ruining the world</a>  The Verge",
      "summary": "The nine people trying to stop AI from ruining the world  The Verge",
      "publishedAt": "2025-12-02T15:11:21.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 4.412535464208986,
      "ingestedAt": "2025-12-02T21:47:02.953Z",
      "tags": [
        "code_review",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c1ed1d",
      "title": "Blackbaud rises on Anthropic partnership - TradingView",
      "url": "https://news.google.com/rss/articles/CBMirwFBVV95cUxNdjk3N3Z5a3VCYzRmTVNuWmVmRVVQZkhNQ1hwTF9jTUViek5oU1RiUW5iRl9xa2ZQVmpsNE9XTDk5Z3d3TERvQlN1OGtOeTJ4WEVMZDhNV1ZMSXBsdmIyMTR3Rkt0eTBnZmRPMi1Ic1dkWmlYYWVQMFRHTHFwOHJRV1FNT0ZxV1Bub2twRVNFQ1lsa085dVMwc2ZUNzYxZ3NhTUdfMmJ1cy1LUGotS1pF?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMirwFBVV95cUxNdjk3N3Z5a3VCYzRmTVNuWmVmRVVQZkhNQ1hwTF9jTUViek5oU1RiUW5iRl9xa2ZQVmpsNE9XTDk5Z3d3TERvQlN1OGtOeTJ4WEVMZDhNV1ZMSXBsdmIyMTR3Rkt0eTBnZmRPMi1Ic1dkWmlYYWVQMFRHTHFwOHJRV1FNT0ZxV1Bub2twRVNFQ1lsa085dVMwc2ZUNzYxZ3NhTUdfMmJ1cy1LUGotS1pF?oc=5\">Blackbaud rises on Anthropic partnership</a>  TradingView",
      "summary": "Blackbaud rises on Anthropic partnership  TradingView",
      "publishedAt": "2025-12-02T14:47:13.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4690854742459192,
      "ingestedAt": "2025-12-02T21:47:02.953Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c1ed1f",
      "title": "Anthropic unveils Claude Opus 4.5, its latest AI model following $350 billion valuation - CNBC",
      "url": "https://news.google.com/rss/articles/CBMimgFBVV95cUxQWFZLeEMwakNVWVRuT190UU5QVy1yMlRuNUxIdEpLaHN1RmxEYmdSRFVEV0h2clVhYlB0ajhEZVJIeUs0YVFfV1hBT0lmMTI3U3dSZ2dkVXF1VURRSFBwMjZnT1RYVEdqSVFDN1RQMXBvUDFNbmZqQlFmQUFZdTBHNWFfcTR5VGwtYlZTbkdrSjZUWUJXdjUzclBB0gGfAUFVX3lxTE1INk1sSUlRZlVLY28wTEFlMnVkNElnVkQydzgwOWJxVUd4WVduNWVvekR4ZkhSVlE5SE9QTWJtdmRzVHh5azRjc3JSRUF2cnNRbnNkSlRvMkJLMy14aVZOcUxxVkRfcFk0V3RlUXFTLTlWeVdabUNJeXVhc0MxbS1ta3BYU3R0TWZOcTh6dDZwYkpPZlRLZlpUdzl3SnRybw?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMimgFBVV95cUxQWFZLeEMwakNVWVRuT190UU5QVy1yMlRuNUxIdEpLaHN1RmxEYmdSRFVEV0h2clVhYlB0ajhEZVJIeUs0YVFfV1hBT0lmMTI3U3dSZ2dkVXF1VURRSFBwMjZnT1RYVEdqSVFDN1RQMXBvUDFNbmZqQlFmQUFZdTBHNWFfcTR5VGwtYlZTbkdrSjZUWUJXdjUzclBB0gGfAUFVX3lxTE1INk1sSUlRZlVLY28wTEFlMnVkNElnVkQydzgwOWJxVUd4WVduNWVvekR4ZkhSVlE5SE9QTWJtdmRzVHh5azRjc3JSRUF2cnNRbnNkSlRvMkJLMy14aVZOcUxxVkRfcFk0V3RlUXFTLTlWeVdabUNJeXVhc0MxbS1ta3BYU3R0TWZOcTh6dDZwYkpPZlRLZlpUdzl3SnRybw?oc=5\">Anthropic unveils Claude Opus 4.5, its latest AI model following $350 billion valuation</a>  CNBC",
      "summary": "Anthropic unveils Claude Opus 4.5, its latest AI model following $350 billion valuation  CNBC",
      "publishedAt": "2025-11-24T19:00:01.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4001464172137177,
      "ingestedAt": "2025-12-02T21:47:02.954Z",
      "tags": [
        "ide",
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c1ed20",
      "title": "Introducing Claude Opus 4.5 - Anthropic",
      "url": "https://news.google.com/rss/articles/CBMiWkFVX3lxTE84OWN3YndfcE4tUnBWQ01XX0RYWWIzVTJienN4Q2NTY0xOMF9Ra1BEOWR4WW8xdGRfck5KdHVjZXZObDVObTd6RE4xMklVMTlJZGJ6ZndTT3JnUQ?oc=5",
      "content": "<a href=\"https://news.google.com/rss/articles/CBMiWkFVX3lxTE84OWN3YndfcE4tUnBWQ01XX0RYWWIzVTJienN4Q2NTY0xOMF9Ra1BEOWR4WW8xdGRfck5KdHVjZXZObDVObTd6RE4xMklVMTlJZGJ6ZndTT3JnUQ?oc=5\">Introducing Claude Opus 4.5</a>  Anthropic",
      "summary": "Introducing Claude Opus 4.5  Anthropic",
      "publishedAt": "2025-11-24T08:00:00.000Z",
      "author": "",
      "source": "rss",
      "feedName": "\"Anthropic\" - Google News",
      "sourceType": "general",
      "contentType": "product_launch",
      "score": 2.439088875334062,
      "ingestedAt": "2025-12-02T21:47:02.954Z",
      "tags": [
        "Coding Agent Product Updates"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c04d07",
      "title": "Is 2026 Next Year?",
      "url": "https://www.google.com/search?q=is+2026+next+year&oq=is+2026+next+year",
      "content": "<p>Article URL: <a href=\"https://www.google.com/search?q=is+2026+next+year&amp;oq=is+2026+next+year\">https://www.google.com/search?q=is+2026+next+year&amp;oq=is+2026+next+year</a></p> \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46122071\">https://news.ycombinator.com/item?id=46122071</a></p> \n<p>Points: 62</p> \n<p># Comments: 18</p>",
      "summary": "Article URL: https://www.google.com/search?q=is+2026+next+year&amp;oq=is+2026+next+year \nComments URL: https://news.ycombinator.com/item?id=46122071 \nPoints: 62 \n# Comments: 18",
      "publishedAt": "2025-12-02T15:20:11.000Z",
      "author": "kjhughes",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.4714897622672183,
      "ingestedAt": "2025-12-02T21:47:02.954Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c04d0c",
      "title": "Nixtml: Static website and blog generator written in Nix",
      "url": "https://github.com/arnarg/nixtml",
      "content": "<p><img src=\"https://opengraph.githubassets.com/27acc9a0fb2663479a82dab26fd086c70b722007fada5fa9c3450560872a6f28/arnarg/nixtml\" alt=\"nixtml\"></p><p>Article URL: <a href=\"https://github.com/arnarg/nixtml\">https://github.com/arnarg/nixtml</a></p>  \n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=46121799\">https://news.ycombinator.com/item?id=46121799</a></p>  \n<p>Points: 37</p>  \n<p># Comments: 6</p>",
      "summary": "Article URL: https://github.com/arnarg/nixtml  \nComments URL: https://news.ycombinator.com/item?id=46121799  \nPoints: 37  \n# Comments: 6",
      "publishedAt": "2025-12-02T14:54:49.000Z",
      "author": "todsacerdoti",
      "source": "rss",
      "feedName": "Hacker News: Front Page",
      "sourceType": "general",
      "contentType": "general",
      "score": 1.469639399340066,
      "ingestedAt": "2025-12-02T21:47:02.954Z",
      "tags": [
        "Tech Articles"
      ]
    },
    {
      "id": "tag:google.com,2005:reader/item/0000000b09c03f75",
      "title": "AWS Transform Modernizes Any Codebase, App, API or Runtime",
      "url": "https://devops.com/aws-transform-modernizes-any-codebase-app-api-or-runtime/",
      "content": "<div><img width=\"770\" height=\"330\" src=\"https://devops.com/wp-content/uploads/2025/05/DevOps-and-AIOps-1.jpg\" alt=\"\" style=\"margin-bottom:0px;\"></div><img width=\"150\" height=\"150\" src=\"https://devops.com/wp-content/uploads/2025/05/DevOps-and-AIOps-1-150x150.jpg\" alt=\"\">Platforms, paradigms, processes and processing itself all evolve. Because the information technology industry crosses from one chasm to another on an apparently endless loop of perpetual change, applications and wider systems need to be almost continually modernized. Amazon Web Services wants to enable that process with AWS Transform, a service that now ships with new […]",
      "summary": "Platforms, paradigms, processes and processing itself all evolve. Because the information technology industry crosses from one chasm to another on an apparently endless loop of perpetual change, applications and wider systems need to be almost continually modernized. Amazon Web Services wants to enable that process with AWS Transform, a service that now ships with new […]",
      "publishedAt": "2025-12-02T15:33:53.000Z",
      "author": "Adrian Bridgwater",
      "source": "rss",
      "feedName": "DevOps.com",
      "sourceType": "general",
      "contentType": "general",
      "score": 5.3991302670703805,
      "ingestedAt": "2025-12-02T21:47:02.954Z",
      "tags": [
        "code_review",
        "ide",
        "Tech Articles"
      ]
    },
    {
      "id": "12785886edb62ec79b37a303a087488c",
      "title": "Find Affected Code: React Server Components Critical Security Vulnerability (CVE-2025-55182)",
      "url": "https://webflow.sourcegraph.com/blog/find-affected-code-react-server-components-critical-security-vulnerability-cve-2025-55182",
      "content": "find-affected-code-react-server-components-critical-security-vulnerability-cve-2025-55182",
      "summary": "find-affected-code-react-server-components-critical-security-vulnerability-cve-2025-55182",
      "publishedAt": "2025-12-04T00:19:37.000Z",
      "source": "rss",
      "feedName": "Sourcegraph Blog",
      "sourceType": "competitor_blog",
      "company": "Sourcegraph",
      "contentType": "security_incident",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:11.823Z",
      "score": 8.633741279195956
    },
    {
      "id": "c4fcc4a32c0c2fd60d2d3602a80d1402",
      "title": "A note from Dan",
      "url": "https://webflow.sourcegraph.com/blog/a-note-from-dan",
      "content": "a-note-from-dan",
      "summary": "a-note-from-dan",
      "publishedAt": "2025-12-02T18:17:44.000Z",
      "source": "rss",
      "feedName": "Sourcegraph Blog",
      "sourceType": "competitor_blog",
      "company": "Sourcegraph",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:11.823Z",
      "score": 3.509131537057494
    },
    {
      "id": "f200d23ed372030ca6b4815ba7f9a0a7",
      "title": "Why Sourcegraph and Amp Are Becoming Independent Companies ",
      "url": "https://webflow.sourcegraph.com/blog/why-sourcegraph-and-amp-are-becoming-independent-companies",
      "content": "why-sourcegraph-and-amp-are-becoming-independent-companies",
      "summary": "why-sourcegraph-and-amp-are-becoming-independent-companies",
      "publishedAt": "2025-11-21T21:44:49.000Z",
      "source": "rss",
      "feedName": "Sourcegraph Blog",
      "sourceType": "competitor_blog",
      "company": "Sourcegraph",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:11.823Z",
      "score": 1.6159552801634052
    },
    {
      "id": "5980d07d609850bf74a0d367a96dceff",
      "title": "Star Your Favorite Deep Search Threads",
      "url": "https://webflow.sourcegraph.com/blog/star-your-favorite-deep-search-threads",
      "content": "star-your-favorite-deep-search-threads",
      "summary": "star-your-favorite-deep-search-threads",
      "publishedAt": "2025-11-14T00:13:11.000Z",
      "source": "rss",
      "feedName": "Sourcegraph Blog",
      "sourceType": "competitor_blog",
      "company": "Sourcegraph",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:11.823Z",
      "score": 0.9192999456745189
    },
    {
      "id": "7a58461dddf4e81a93e61151f76b1b01",
      "title": "Deep Search goes GA, now with role-based permissions",
      "url": "https://webflow.sourcegraph.com/blog/deep-search-goes-ga-now-with-role-based-permissions",
      "content": "deep-search-goes-ga-now-with-role-based-permissions",
      "summary": "deep-search-goes-ga-now-with-role-based-permissions",
      "publishedAt": "2025-10-21T19:50:18.000Z",
      "source": "rss",
      "feedName": "Sourcegraph Blog",
      "sourceType": "competitor_blog",
      "company": "Sourcegraph",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:11.823Z",
      "score": 0.17551341189402692
    },
    {
      "id": "7e314bbd702de8d51d79cd6c7d89f767",
      "title": "Opus 4.5のコード関連タスク：システムアーキテクトのように振る舞う",
      "url": "https://coderabbit.ai/blog/opus-45-for-code-related-tasks-performs-like-the-systems-architect-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/opus-45-for-code-related-tasks-performs-like-the-systems-architect\">Opus 4.5 for code-related tasks: Performs like the system architect</a>の意訳です。</p>\n<h2 id=\"heading-opus-45\"><strong>すべてのモデルは推論します。しかし、Opus 4.5 は「監査」します。</strong></h2>\n<p>新しいモデルが登場するとき、その約束はいつも同じです。より賢い推論、よりきれいなコード、そしてよりよい回答。しかし Anthropic の Opus 4.5 は、単に推論するだけではなく、<em>監査する</em> モデルです。あたかも自ら設計に携わったシステムに戻ってきたかのようにコードを読み込み、弱点を特定し、アーキテクチャ全体を整えます。他のモデルが論理を説明したり、局所的な修正を示したりするのに対し、Opus 4.5 は技術文書に近い、構造的で体系的なレビューを行います。</p>\n<p>私たちはこのモデルの特徴を把握するために、Opus 4.5 を CodeRabbit のベンチマーク環境に統合しました。その結果わかったのは、より高い知能でも派手な文章でもなく、「規律」でした。Opus 4.5 は単にバグを見つけるのではなく、その周囲にある <em>文脈を構築</em> します。つまり、レビューを推測ゲームではなくエンジニアリングプロセスとして扱うのです。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1764007539221/ef34539b-54b8-460b-af64-8472a3b28d02.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-kirjg5njg7pjg4hjg57jg7zjgqjga7og4zmma8qkg\"><strong>ベンチマークの背景</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1764007487756/671bf31d-6eaf-48fb-ad5b-82c3adb7fef8.png\" alt class=\"image--center mx-auto\" /></p>\n<p>CodeRabbit では、新しい LLM を評価するために、C++、Java、Python、TypeScript にまたがる既知のエラーパターン（EP）を含む <strong>25 件の複雑なプルリクエスト</strong> を用意しています。モデルが生成した各コメントを LLM ジャッジが次の3つの観点で評価します。</p>\n<ul>\n<li><p><strong>Precision（精度）：</strong> EP を正しく特定しているか。</p>\n</li>\n<li><p><strong>Important-share（重要コメント率）：</strong> コメントのうち重要・重大な指摘（本物のバグ）がどれだけ占めるか。</p>\n</li>\n<li><p><strong>Signal-to-noise ratio（S/N比）：</strong> 重要コメントと、重要でないコメントの比率。</p>\n</li>\n</ul>\n<p>この評価フレームワークは、複数世代のモデルを通じて改善されており、自動判定の LLM と <strong>人手による検証</strong> を組み合わせて正確性を担保しています。また、<strong>複数ジャッジによる評価と繰り返し試行</strong> を実施することで、一貫性とばらつきを記録しています。プロンプト改善、ラベル精度向上、評価範囲の拡大を継続的に進め、より信頼できる結果を得られるようにしています。</p>\n<h2 id=\"heading-kirjgrnjgrpjgqljg5zjg7zjg4nvvijjgqljgqjgrfjg6fjg4rjg5bjg6vjgrpjg6hjg7pjg4jjgavpmzdlrprvvikqkg\"><strong>スコアボード（アクショナブルコメントに限定）</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1764007550593/12fb82ef-8340-4246-ab76-68dd83cf0aba.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>解釈:</strong><br />Opus 4.5 は、Sonnet 4.5 の高ボリューム・冗長スタイルと、GPT-5.1 のシャープで精密なスタイルの中間に位置します。Sonnet 4.5 よりもコメントあたりの精度が高く、有意味な指摘の割合が多い結果となりました。EP パス数は 1 件少ない（15 vs. 16）ものの、これは通常のばらつき範囲に収まっています。実際、複数回のベンチマークでは Opus 4.5 が GPT-5.1 や Sonnet 4.5 を上回ることもありました。</p>\n<p>総合すると、Opus 4.5 はシグナル、構造、カバレッジのバランスがよく、安定して信頼できるモデルと言えます。</p>\n<h2 id=\"heading-kirjgrnjgrjgqtjg6vjgajjg4jjg7zjg7mqkg\"><strong>スタイルとトーン</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1764007586852/d86691c5-39c4-488d-8be4-0eb6d8072712.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Opus 4.5 のレビューは、構造化され、簡潔で、焦点が明確です。断定表現の比率は約33%、婉曲表現は約15%と、落ち着いたプロフェッショナルなトーンになっています。密度とトーンのバランスによって、実践的で自信のある分析的な内容となっています。コードブロックや diff など、行動につながる表現を多用する傾向があり、「説明する」よりも「編集する」モデルだと言えます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1764007643509/3d60f7c1-e310-49c6-a77e-536bea2dc7bc.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-kirmp4vpgkdljjbjgzxjgozjgznn6xmgkfvvjrkuojmukzlj6og73jgarlvallvijgajoqidoqp7mqkrmlq3jga7kuidosqvmgkcqkg\"><strong>構造化された知性：予測可能な形式と言語横断の一貫性</strong></h2>\n<p>Opus 4.5 のコメントは、見出し、理由説明、diff というアーキテクチャ的なリズムで書かれています。約80%のコメントにコードブロックが含まれ、ほとんどのコメントが簡潔なパッチで締めくくられます。原因、影響、解決策が整然と記述されており、明確なバグレポートのようです。</p>\n<p>この構造はどの言語でも維持されます。C++、Java、Python、TypeScript のいずれでも、コメントは平均19行・790文字程度に収まり、統一されたスタイルとなります。一貫性があることで自動化との相性が良く、読みやすさも向上します。まるでコードベース全体を同じエンジニアがレビューしているかのようです。</p>\n<p>具体例:</p>\n<ul>\n<li><p><strong>C++（WorkerThreadPool）：</strong> lost wakeup レースを3ステップのインターリーブで説明し、1行の修正 diff を提示します。</p>\n</li>\n<li><p><strong>Java（OrderService）：</strong> ダブルチェックロッキングで <code>volatile</code> が欠落している点を指摘し、正しいパターンを提示します。</p>\n</li>\n<li><p><strong>Python（Batch Client）：</strong> 同期 HTTP クライアントを非同期版に置き換えてブロッキングを防ぎます。</p>\n</li>\n<li><p><strong>TypeScript（Cache Manager）：</strong> <code>Number.MAX_SAFE_INTEGER</code> がエビクションを無効化している点を指摘し、現実的なデフォルト値を提案します。</p>\n</li>\n</ul>\n<p>いずれも簡潔でコードネイティブな洞察であり、根拠に基づいた実用的な修正です。</p>\n<h2 id=\"heading-kiroh6rkv6hjga7pgibou6lnj77osaeqkg\"><strong>自信の逆転現象</strong></h2>\n<p>Opus 4.5 のトーンは全体的にバランスが良いのですが、間違っているときにやや断定的に聞こえるという、ささやかな逆転現象があります。通常は慎重ですが、この癖があるため、コメントのトーンだけで正確性を判断しないようにしています。この問題を補うため、評価サマリーではトーンデータと正解率を組み合わせて校正を行っています。</p>\n<p>とはいえ、Opus 4.5 はほとんど推測をせず、間違っているときでさえ淡々と説明します。</p>\n<h2 id=\"heading-kirjgrfjgrnjg4bjg6djg6zjg5njg6vjgafjga7mjqjoq5bvvjrjgrpjg7zjg4njgadjgzhjgafjgajgarjgymlofohijjgplkv67mrapjgznjgosqkg\"><strong>システムレベルでの推論：コードだけではなく文脈を修正する</strong></h2>\n<p>多くのモデルが目の前の欠陥に集中するのに対し、Opus 4.5 は周辺のシステム全体を考慮します。推奨内容には、ライフサイクル改善、安全チェック追加、デフォルト値の見直しといった、より上位の修正が頻繁に含まれます。</p>\n<p>例:</p>\n<ul>\n<li><p><strong>TypeScript Cache：</strong> エビクションロジックの再設計、TTL の強制、デフォルト改善により秘められた OOM（Out Of Memory ） を防ぎます。</p>\n</li>\n<li><p><strong>Java OrderService：</strong> <code>HashMap</code> を <code>ConcurrentHashMap</code> に変更し、<code>ExecutorService</code> の shutdown 漏れを指摘します。</p>\n</li>\n<li><p><strong>Python Client Lifecycle：</strong> 長寿命 async クライアント向けに明示的なシャットダウンフックを追加します。</p>\n</li>\n<li><p><strong>C++ FileAccessEncrypted：</strong> 暗号化ファイルがすべてブロックされる検証バグを修正し、上流のエラーハンドリングも改善します。</p>\n</li>\n</ul>\n<p>どれも一行修正ではなく、システム全体の整合性を高める提案です。コードを「問題の集合体」ではなく「相互に影響し合うエコシステム」とみなしていることがわかります。</p>\n<h2 id=\"heading-effort\"><strong>コスト、Effort、効率性</strong></h2>\n<p>Anthropic の Effort パラメータを使うと、モデルの推論深度を直接制御できます。High-effort では依存関係パスを徹底的に探索し、Medium-effort ではトークン節約のため深度を抑えます。High-effort であっても、Opus 4.5 の出力トークン量は Sonnet 4.5 より約25%少なく、1M 出力トークン 25ドルという単価を効率性で補っています。</p>\n<p>規律ある構造のおかげで脱線が減り、クリアで読みやすい結果を維持できています。</p>\n<h2 id=\"heading-opus-45-1\"><strong>Opus 4.5 を読むとどんな感じか</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1764007608773/e4275633-7498-4294-8f76-2fa27974da38.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Sonnet 4.5 が教師、GPT-5.1 が決断力のあるチームメイトだとしたら、Opus 4.5 は <strong>PR をレビューしに戻ってきたアーキテクト</strong> です。トーンは落ち着いており、命令的ではありません。あなたがドメインを理解している前提で、細部を丁寧に確認します。そのため、システムエンジニアによるピアレビューのような、構造的で静かに権威を感じさせるコメントになります。</p>\n<h3 id=\"heading-kirjg4jjg7zjg7pjgajkurrmolwqkg\"><strong>トーンと人格</strong></h3>\n<p>Opus 4.5 のトーンは測定可能で分析的です。劇的な表現や不必要な厳しさを避け、秩序ある構造、簡潔な要約、根拠の提示、フォーカスされた修正提案によって確信を示します。システムに精通したメンターからのアドバイスのように感じられるため、開発者が受け取りやすい雰囲気です。</p>\n<h3 id=\"heading-kirmt7hjgzxjgajlr4bluqbjga7jg5djg6njg7pjgrkqkg\"><strong>深さと密度のバランス</strong></h3>\n<p>コメントはコンパクトですが情報量は十分です。複雑な問題には必要なだけの説明を行い、単純な問題は短い提案で正確に処理します。このバランスによって、読みやすさと包括性が両立しています。</p>\n<h3 id=\"heading-kirmtyhjgozjgajlj6oqq3mgkcqkg\"><strong>流れと可読性</strong></h3>\n<p>文脈 → 原因 → 修正というリズムにより、開発者はコメントをすばやくスキャンしつつ意味を保持できます。Opus 4.5 のコメントは「構造化されたスナップショット」のようで、何が起きたのか、なぜ重要なのか、どう直すのかが一目で分かります。</p>\n<h3 id=\"heading-kirlrpli5nnmotjgarjgqtjg7pjg5hjgqjg4jjgajplovnmbrogixjga7kv6hpolwqkg\"><strong>実務的なインパクトと開発者の信頼</strong></h3>\n<p>Opus 4.5 は誇張や劇的な表現を避けるため、開発者からの信頼を得やすいモデルです。プロフェッショナルで落ち着いたトーンを維持し、間違っている場合も過剰な断定ではなく理性的な仮説として提示します。過度な自信がないため、レビューがより「人間的で実務的」に感じられます。</p>\n<p>コメントはまるで設計ノートのように読みやすく、壊れた不変条件、修正案、その根拠が明確に記されています。そのまま変更履歴や Issue Tracker に貼れるレベルの明快さです。</p>\n<h2 id=\"heading-kirplbfmiydjgajnn63miydjga7kuidopqcqkg\"><strong>長所と短所の一覧</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1764007681538/64fb7aed-becc-412c-837c-2919f3eb1ada.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>長所：</strong></p>\n<ul>\n<li><p>重要コメントの密度が高い（約80%）</p>\n</li>\n<li><p>言語をまたいだ構造の一貫性</p>\n</li>\n<li><p>並行処理やライフサイクルに強い推論能力</p>\n</li>\n<li><p>明確・簡潔・プロフェッショナルなトーン</p>\n</li>\n<li><p>Sonnet 4.5 より冗長でない一方、GPT-5.1 より文脈が豊富</p>\n</li>\n</ul>\n<p><strong>短所：</strong></p>\n<ul>\n<li><p>精度は中程度（約38%）</p>\n</li>\n<li><p>間違っているときに少し断定的になることがある</p>\n</li>\n<li><p>重大ラベルが多く、多忙な PR では過剰に見える場合がある</p>\n</li>\n<li><p>単純な問題ではやや説明が長くなることがある</p>\n</li>\n</ul>\n<p><strong>結論:</strong><br />Opus 4.5 は、私たちがテストした中で最も <em>システミック（全体的）</em> なレビュアーです。落ち着きがあり、構造化され、厳密で、アーキテクチャ理解が必要な場面で特に強みを発揮します。</p>\n<h2 id=\"heading-kirjg6ljg4fjg6vjga7kvbjgytliibjgzeqkg\"><strong>モデルの使い分け</strong></h2>\n<div class=\"hn-table\">\n<table>\n<thead>\n<tr>\n<td><strong>シナリオ</strong></td><td><strong>最適モデル</strong></td><td><strong>理由</strong></td></tr>\n</thead>\n<tbody>\n<tr>\n<td>複数言語や高度な文脈を含むレビュー</td><td><strong>Opus 4.5</strong></td><td>構造が安定しており、システム的な洞察が強い</td></tr>\n<tr>\n<td>精密さ重視の小規模 diff</td><td><strong>GPT-5.1</strong></td><td>精度が高く、判断も明確で、誤検知が少ない</td></tr>\n<tr>\n<td>大量スキャンやコスト重視</td><td><strong>Sonnet 4.5</strong></td><td>カバレッジが高く、レビュー単価が低い</td></tr>\n</tbody>\n</table>\n</div><h2 id=\"heading-kirmnidlvozjgavvvjrmjqjoq5bjga7jgizlvaljgi3jgavjgatjgytjgayqkg\"><strong>最後に：推論の「形」について</strong></h2>\n<p>Opus 4.5 は、実験的なモデルではなく「設計されたモデル」に感じられます。初期のモデルが推測に頼りがちだったのに対し、Opus 4.5 は測定し、構造化し、文書化します。レビューを読むと、<em>開発者の視点</em> を理解したモデルと一緒に作業しているように感じられます。</p>\n<p>コードレビューでは、トーンが信頼を決めます。Opus 4.5 のスタイル──測定可能で、構造化され、機械的な精度を持つ──は、推論の成熟を示しています。圧力のない精度、エゴのない自信が感じられます。</p>\n<p><strong>まとめ:</strong><br />Sonnet 4.5 が教師、GPT-5.1 がチームメイトだとすると、Opus 4.5 は設計レビューのために戻ってきたアーキテクトです。</p>\n<p><strong><em>CodeRabbit を試してみたい方はこちら</em></strong><br /><a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong><em>14日間の無料トライアル</em></strong></a></p>\n",
      "summary": "Opus 4.5 for code-related tasks: Performs like the system architectの意訳です。\nすべてのモデルは推論します。しかし、Opus 4.5 は「監査」します。\n新しいモデルが登場するとき、その約束はいつも同じです。より賢い推論、よりきれいなコード、そしてよりよい回答。しかし Anthropic の Opus 4.5 は、単に推論するだけではなく、監査する モデルです。あたかも自ら設計に携わったシステムに戻ってきたかのようにコードを読み込み、弱点を特定し、アーキテクチャ全体を整えます。他のモデルが論理を説明したり、局所的な修正を示したりするのに対し、Opus 4.5 は技術文書に近い、構造的で体系的なレビューを行います。\n私たちはこのモデルの特徴を把握するために、Opus 4.5 を CodeRabbit のベンチマーク環境に統合しました。その結果わかったのは、より高い知能でも派手な文章でもなく、「規律」でした。Opus 4.5 は単にバグを見つけるのではなく、その周囲にある 文脈を構築 します。つまり、レビューを推測ゲームではなくエンジニアリングプロセスとして扱うのです。\n\nベンチマークの背景\n\nCodeRabbit では、新しい LLM を評価するために、C++、Java、Python、TypeScript にまたがる既知のエラーパターン（EP）を含む 25 件の複雑なプルリクエスト を用意しています。モデルが生成した各コメントを LLM ジャッジが次の3つの観点で評価します。\nPrecision（精度）： EP を正しく特定しているか。\nImportant-share（重要コメント率）： コメントのうち重要・重大な指摘（本物のバグ）がどれだけ占めるか。\nSignal-to-noise ratio（S/N比）： 重要コメントと、重要でないコメントの比率。\nこの評価フレームワークは、複数世代のモデルを通じて改善されており、自動判定の LLM と 人手による検証 を組み合わせて正確性を担保しています。また、複数ジャッジによる評価と繰り返し試行 を実施することで、一貫性とばらつきを記録しています。プロンプト改善、ラベル精度向上、評価範囲の拡大を継続的に進め、より信頼できる結果を得られるようにしています。\nスコアボード（アクショナブルコメントに限定）\n\n解釈:\nOpus 4.5 は、Sonnet 4.5 の高ボリューム・冗長スタイルと、GPT-5.1 のシャープで精密なスタイルの中間に位置します。Sonnet 4.5 よりもコメントあたりの精度が高く、有意味な指摘の割合が多い結果となりました。EP パス数は 1 件少ない（15 vs. 16）ものの、これは通常のばらつき範囲に収まっています。実際、複数回のベンチマークでは Opus 4.5 が GPT-5.1 や Sonnet 4.5 を上回ることもありました。\n総合すると、Opus 4.5 はシグナル、構造、カバレッジのバランスがよく、安定して信頼できるモデルと言えます。\nスタイルとトーン\n\nOpus 4.5 のレビューは、構造化され、簡潔で、焦点が明確です。断定表現の比率は約33%、婉曲表現は約15%と、落ち着いたプロフェッショナルなトーンになっています。密度とトーンのバランスによって、実践的で自信のある分析的な内容となっています。コードブロックや diff など、行動につながる表現を多用する傾向があり、「説明する」よりも「編集する」モデルだと言えます。\n\n構造化された知性：予測可能な形式と言語横断の一貫性\nOpus 4.5 のコメントは、見出し、理由説明、diff というアーキテクチャ的なリズムで書かれています。約80%のコメントにコードブロックが含まれ、ほとんどのコメントが簡潔なパッチで締めくくられます。原因、影響、解決策が整然と記述されており、明確なバグレポートのようです。\nこの構造はどの言語でも維持されます。C++、Java、Python、TypeScript のいずれでも、コメントは平均19行・790文字程度に収まり、統一されたスタイルとなります。一貫性があることで自動化との相性が良く、読みやすさも向上します。まるでコードベース全体を同じエンジニアがレビューしているかのようです。\n具体例:\nC++（WorkerThreadPool）： lost wakeup レースを3ステップのインターリーブで説明し、1行の修正 diff を提示します。\nJava（OrderService）： ダブルチェックロッキングで volatile が欠落している点を指摘し、正しいパターンを提示します。\nPython（Batch Client）： 同期 HTTP クライアントを非同期版に置き換えてブロッキングを防ぎます。\nTypeScript（Cache Manager）： Number.MAX_SAFE_INTEGER がエビクションを無効化している点を指摘し、現実的なデフォルト値を提案します。\nいずれも簡潔でコードネイティブな洞察であり、根拠に基づいた実用的な修正です。\n自信の逆転現象\nOpus 4.5 のトーンは全体的にバランスが良いのですが、間違っているときにやや断定的に聞こえるという、ささやかな逆転現象があります。通常は慎重ですが、この癖があるため、コメントのトーンだけで正確性を判断しないようにしています。この問題を補うため、評価サマリーではトーンデータと正解率を組み合わせて校正を行っています。\nとはいえ、Opus 4.5 はほとんど推測をせず、間違っているときでさえ淡々と説明します。\nシステムレベルでの推論：コードだけではなく文脈を修正する\n多くのモデルが目の前の欠陥に集中するのに対し、Opus 4.5 は周辺のシステム全体を考慮します。推奨内容には、ライフサイクル改善、安全チェック追加、デフォルト値の見直しといった、より上位の修正が頻繁に含まれます。\n例:\nTypeScript Cache： エビクションロジックの再設計、TTL の強制、デフォルト改善により秘められた OOM（Out Of Memory ） を防ぎます。\nJava OrderService： HashMap を ConcurrentHashMap に変更し、ExecutorService の shutdown 漏れを指摘します。\nPython Client Lifecycle： 長寿命 async クライアント向けに明示的なシャットダウンフックを追加します。\nC++ FileAccessEncrypted： 暗号化ファイルがすべてブロックされる検証バグを修正し、上流のエラーハンドリングも改善します。\nどれも一行修正ではなく、システム全体の整合性を高める提案です。コードを「問題の集合体」ではなく「相互に影響し合うエコシステム」とみなしていることがわかります。\nコスト、Effort、効率性\nAnthropic の Effort パラメータを使うと、モデルの推論深度を直接制御できます。High-effort では依存関係パスを徹底的に探索し、Medium-effort ではトークン節約のため深度を抑えます。High-effort であっても、Opus 4.5 の出力トークン量は Sonnet 4.5 より約25%少なく、1M 出力トークン 25ドルという単価を効率性で補っています。\n規律ある構造のおかげで脱線が減り、クリアで読みやすい結果を維持できています。\nOpus 4.5 を読むとどんな感じか\n\nSonnet 4.5 が教師、GPT-5.1 が決断力のあるチームメイトだとしたら、Opus 4.5 は PR をレビューしに戻ってきたアーキテクト です。トーンは落ち着いており、命令的ではありません。あなたがドメインを理解している前提で、細部を丁寧に確認します。そのため、システムエンジニアによるピアレビューのような、構造的で静かに権威を感じさせるコメントになります。\nトーンと人格\nOpus 4.5 のトーンは測定可能で分析的です。劇的な表現や不必要な厳しさを避け、秩序ある構造、簡潔な要約、根拠の提示、フォーカスされた修正提案によって確信を示します。システムに精通したメンターからのアドバイスのように感じられるため、開発者が受け取りやすい雰囲気です。\n深さと密度のバランス\nコメントはコンパクトですが情報量は十分です。複雑な問題には必要なだけの説明を行い、単純な問題は短い提案で正確に処理します。このバランスによって、読みやすさと包括性が両立しています。\n流れと可読性\n文脈 → 原因 → 修正というリズムにより、開発者はコメントをすばやくスキャンしつつ意味を保持できます。Opus 4.5 のコメントは「構造化されたスナップショット」のようで、何が起きたのか、なぜ重要なのか、どう直すのかが一目で分かります。\n実務的なインパクトと開発者の信頼\nOpus 4.5 は誇張や劇的な表現を避けるため、開発者からの信頼を得やすいモデルです。プロフェッショナルで落ち着いたトーンを維持し、間違っている場合も過剰な断定ではなく理性的な仮説として提示します。過度な自信がないため、レビューがより「人間的で実務的」に感じられます。\nコメントはまるで設計ノートのように読みやすく、壊れた不変条件、修正案、その根拠が明確に記されています。そのまま変更履歴や Issue Tracker に貼れるレベルの明快さです。\n長所と短所の一覧\n\n長所：\n重要コメントの密度が高い（約80%）\n言語をまたいだ構造の一貫性\n並行処理やライフサイクルに強い推論能力\n明確・簡潔・プロフェッショナルなトーン\nSonnet 4.5 より冗長でない一方、GPT-5.1 より文脈が豊富\n短所：\n精度は中程度（約38%）\n間違っているときに少し断定的になることがある\n重大ラベルが多く、多忙な PR では過剰に見える場合がある\n単純な問題ではやや説明が長くなることがある\n結論:\nOpus 4.5 は、私たちがテストした中で最も システミック（全体的） なレビュアーです。落ち着きがあり、構造化され、厳密で、アーキテクチャ理解が必要な場面で特に強みを発揮します。\nモデルの使い分け\nシナリオ最適モデル理由\n複数言語や高度な文脈を含むレビューOpus 4.5構造が安定しており、システム的な洞察が強い\n精密さ重視の小規模 diffGPT-5.1精度が高く、判断も明確で、誤検知が少ない\n大量スキャンやコスト重視Sonnet 4.5カバレッジが高く、レビュー単価が低い\n最後に：推論の「形」について\nOpus 4.5 は、実験的なモデルではなく「設計されたモデル」に感じられます。初期のモデルが推測に頼りがちだったのに対し、Opus 4.5 は測定し、構造化し、文書化します。レビューを読むと、開発者の視点 を理解したモデルと一緒に作業しているように感じられます。\nコードレビューでは、トーンが信頼を決めます。Opus 4.5 のスタイル──測定可能で、構造化され、機械的な精度を持つ──は、推論の成熟を示しています。圧力のない精度、エゴのない自信が感じられます。\nまとめ:\nSonnet 4.5 が教師、GPT-5.1 がチームメイトだとすると、Opus 4.5 は設計レビューのために戻ってきたアーキテクトです。\nCodeRabbit を試してみたい方はこちら\n14日間の無料トライアル",
      "publishedAt": "2025-11-25T03:12:14.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:12.170Z",
      "score": 3.5610961584881595
    },
    {
      "id": "a8e35bad0d580aa6d7b37ef68f70f42e",
      "title": "Opus 4.5 for code-related tasks: Performs like the systems architect",
      "url": "https://coderabbit.ai/blog/opus-45-for-code-related-tasks-performs-like-the-systems-architect",
      "content": "<h2 id=\"heading-every-model-reasons-opus-45-audits\"><strong>Every model reasons. Opus 4.5 audits.</strong></h2>\n<p>Every new model arrives with the same promise: smarter reasoning, cleaner code, and better answers. But Opus 4.5 from Anthropic doesn’t just reason; it <em>audits</em>. It reads code as if returning to a system it helped design, identifying weak points and refining architecture. Where other models narrate their logic or prescribe surgical fixes, Opus 4.5 performs structured, systematic reviews that feel more like technical documentation than conversation.</p>\n<p>We integrated Opus 4.5 into CodeRabbit’s benchmark harness to understand what makes this model distinct. The result was not higher raw intelligence or flashier prose, but discipline. This model doesn’t just find bugs; it <em>builds context</em> around them. It treats review as an engineering process, rather than a guessing game.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1764007539221/ef34539b-54b8-460b-af64-8472a3b28d02.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-benchmarking-context\"><strong>Benchmarking context</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1764007487756/671bf31d-6eaf-48fb-ad5b-82c3adb7fef8.png\" alt class=\"image--center mx-auto\" /></p>\n<p>At CodeRabbit, we evaluate new LLMs using a controlled benchmark of <strong>25 complex pull requests</strong> seeded with known error patterns (EPs) across C++, Java, Python, and TypeScript. Each comment generated by a model is scored by an LLM judge for three key factors:</p>\n<ul>\n<li><p><strong>Precision:</strong> Whether it correctly identifies the EP.</p>\n</li>\n<li><p><strong>Important-share:</strong> The percentage of comments that are genuinely critical or major (real bugs, not style issues).</p>\n</li>\n<li><p><strong>Signal-to-noise ratio (SNR):</strong> The ratio of important to unimportant comments.</p>\n</li>\n</ul>\n<p>Our evaluation framework, refined over multiple generations of models, combines automated LLM judging with <strong>hand validation</strong> to ensure accuracy. We also use <strong>multiple judges and repeated trials</strong> to measure consistency and understand variance. Each iteration improves the process through better prompts, refined labeling, and expanded coverage, resulting in more reliable outcomes.</p>\n<h2 id=\"heading-scoreboard-actionable-comments-only\"><strong>Scoreboard (Actionable comments only)</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1764007550593/12fb82ef-8340-4246-ab76-68dd83cf0aba.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>What this means:</strong> Opus 4.5 sits between Sonnet 4.5’s high-volume, verbose style and GPT-5.1’s lean, surgical precision. It delivers higher per-comment precision and a greater share of meaningful findings than Sonnet 4.5. While it recorded one fewer EP pass (15 vs. 16), that difference falls within normal variance. In several runs, Opus 4.5 matched or even surpassed both GPT-5.1 and Sonnet 4.5. The takeaway is a model that balances signal, structure, and coverage with consistent reliability.</p>\n<h2 id=\"heading-style-and-tone\"><strong>Style and tone</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1764007586852/d86691c5-39c4-488d-8be4-0eb6d8072712.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Opus 4.5’s reviews are structured, concise, and focused. With assertiveness around 33% and hedging near 15%, its tone reads as measured and professional. The balance of tone and density gives it an analytical voice that feels practical and confident. The high use of code blocks and diff patches underscores its bias toward action; it talks less and <em>edits more</em>.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1764007643509/3d60f7c1-e310-49c6-a77e-536bea2dc7bc.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-structured-intelligence-predictable-form-and-cross-language-consistency\"><strong>Structured intelligence: Predictable form and cross-language consistency</strong></h2>\n<p>Opus 4.5’s comments follow an architectural rhythm of headline, rationale, and diff. Nearly 80% include code blocks, and most conclude with a concise patch. Each resembles a clear bug report that specifies cause, effect, and resolution.</p>\n<p>This structure holds across languages. Whether reviewing C++, Java, Python, or TypeScript, the cadence remains consistent, averaging 19 lines and 790 characters per comment. This uniformity simplifies automation and enhances readability. It also makes Opus 4.5 feel like a single engineer’s consistent voice across an entire codebase.</p>\n<ul>\n<li><p><em>C++ (WorkerThreadPool):</em> Detects a lost wakeup race with a three-step interleaving and a one-line diff fix.</p>\n</li>\n<li><p><em>Java (OrderService):</em> Flags a missing volatile on a double-checked lock and provides the corrected pattern.</p>\n</li>\n<li><p><em>Python (Batch client):</em> Replaces a synchronous HTTP client with an asynchronous equivalent to prevent blocking calls.</p>\n</li>\n<li><p><em>TypeScript (Cache manager):</em> Identifies that Number.MAX_SAFE_INTEGER disables eviction and suggests realistic defaults.</p>\n</li>\n</ul>\n<p>These are concise, code-native insights, each actionable and grounded in sound reasoning.</p>\n<h2 id=\"heading-the-confidence-inversion\"><strong>The confidence inversion</strong></h2>\n<p>Opus 4.5’s tone is balanced but occasionally reveals a subtle inversion: when it is wrong, it can sound slightly more certain. Although the model is generally measured, this behavioral quirk means tone alone is not always a reliable indicator of correctness. To account for this, we pair tone data with correctness metrics in evaluation summaries to maintain consistent calibration.</p>\n<p>Opus 4.5 rarely speculates; it simply explains, even when it’s wrong.</p>\n<h2 id=\"heading-system-level-reasoning-fixing-context-not-just-code\"><strong>System-level reasoning: Fixing context, not just code</strong></h2>\n<p>While most models target the immediate defect, Opus 4.5 focuses on the surrounding system. Its recommendations frequently adjust lifecycles, add safety checks, or refine defaults.</p>\n<p>Examples:</p>\n<ul>\n<li><p><strong>TypeScript Cache:</strong> Rewrites eviction logic, adds TTL enforcement, and updates defaults to prevent silent OOM.</p>\n</li>\n<li><p><strong>Java OrderService:</strong> Replaces HashMap with ConcurrentHashMap and identifies missing ExecutorService shutdown.</p>\n</li>\n<li><p><strong>Python Client Lifecycle:</strong> Adds explicit shutdown hooks for long-lived async clients.</p>\n</li>\n<li><p><strong>C++ FileAccessEncrypted:</strong> Resolves a validation bug that blocked all encrypted files and improves upstream error handling.</p>\n</li>\n</ul>\n<p>These are not single-line fixes but systemic corrections. The model treats code as an interconnected ecosystem rather than a collection of isolated issues.</p>\n<h2 id=\"heading-cost-effort-amp-efficiency\"><strong>Cost, effort &amp; efficiency</strong></h2>\n<p>Anthropic’s Effort parameter provides direct control over how deeply the model reasons. In high-effort mode, Opus 4.5 explores every dependency path. In medium-effort mode, it trims reasoning depth to save tokens. Even with high-effort reasoning, its reviews averaged about 25% fewer output tokens than Sonnet 4.5, balancing higher per-token costs ($25 per million output tokens) with greater efficiency.</p>\n<p>This disciplined structure pays for itself by producing fewer digressions and maintaining consistent clarity.</p>\n<h2 id=\"heading-what-it-feels-like-to-read-opus-45\"><strong>What it feels like to read Opus 4.5</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1764007608773/e4275633-7498-4294-8f76-2fa27974da38.png\" alt class=\"image--center mx-auto\" /></p>\n<p>If Sonnet 4.5 feels like a teacher and GPT-5.1 like a decisive teammate, Opus 4.5 is the <strong>architect reviewing your PR</strong>. Its tone is calm and deliberate, never commanding. It assumes you understand the domain and aims to confirm the details. The result is feedback that reads like peer review from a systems engineer: consistent, structured, and quietly authoritative.</p>\n<h3 id=\"heading-tone-and-personality\"><strong>Tone and personality</strong></h3>\n<p>Opus 4.5’s voice is measured and analytical. It rarely uses dramatic language or unnecessary severity. Instead, it conveys certainty through order, concise summaries, specific evidence, and focused corrections. The tone builds trust, delivering feedback that feels like it comes from a mentor familiar with your system.</p>\n<h3 id=\"heading-depth-vs-density\"><strong>Depth vs. density</strong></h3>\n<p>Its comments are compact yet informative. When an issue warrants detailed explanation, Opus 4.5 delivers it without excess. For simpler problems, it resolves them with brief, precise advice. This balance of detail and brevity keeps reviews readable and comprehensive.</p>\n<h3 id=\"heading-flow-and-readability\"><strong>Flow and readability</strong></h3>\n<p>The model’s structural rhythm of context, cause, and correction allows developers to scan quickly while retaining meaning. Developers often describe its comments as “structured snapshots” that tell a short, self-contained story: what happened, why it matters, and how to fix it.</p>\n<h3 id=\"heading-practical-impact-and-developer-trust\"><strong>Practical impact and developer trust</strong></h3>\n<p>Because Opus 4.5 avoids inflated confidence and theatrical phrasing, developers trust it more readily. It comes across as confident yet professional, firm but not forceful. When it errs, it sounds like a reasoned hypothesis instead of an overreach. That restraint, more than precision alone, makes its reviews feel <em>professionally human</em>.</p>\n<p>Each comment reads like a design note. It states the invariant that failed, proposes a patch, and explains the rationale inline. The clarity is high enough that many of its comments could be pasted directly into changelogs or issue trackers without revision.</p>\n<h2 id=\"heading-strengths-and-weaknesses-at-a-glance\"><strong>Strengths and weaknesses at a glance</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1764007681538/64fb7aed-becc-412c-837c-2919f3eb1ada.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>Strengths:</strong></p>\n<ul>\n<li><p>High signal density (≈80% important comments).</p>\n</li>\n<li><p>Consistent structure across languages.</p>\n</li>\n<li><p>Strong concurrency and lifecycle reasoning.</p>\n</li>\n<li><p>Clear, concise, and professional tone.</p>\n</li>\n<li><p>Lower verbosity than Sonnet 4.5 with more context than GPT-5.1.</p>\n</li>\n</ul>\n<p><strong>Weaknesses:</strong></p>\n<ul>\n<li><p>Moderate precision (≈38%).</p>\n</li>\n<li><p>Subtle confidence inversion when incorrect.</p>\n</li>\n<li><p>Frequent critical or major labeling may overwhelm busy PRs.</p>\n</li>\n<li><p>Slight verbosity on simpler issues.</p>\n</li>\n</ul>\n<p><strong>Bottom line:</strong> Opus 4.5 is the most <em>systemic</em> reviewer we’ve tested. Calm, structured, and exacting, it excels when reasoning breadth and architectural understanding matter more than pinpoint precision.</p>\n<h2 id=\"heading-when-to-use-which-model\"><strong>When to use which model</strong></h2>\n<div class=\"hn-table\">\n<table>\n<thead>\n<tr>\n<td><strong>Scenario</strong></td><td><strong>Best model</strong></td><td><strong>Why</strong></td></tr>\n</thead>\n<tbody>\n<tr>\n<td>Cross-language or high-context reviews</td><td><strong>Opus 4.5</strong></td><td>Structured, consistent, strong at systemic issues</td></tr>\n<tr>\n<td>Tight precision or small diffs</td><td><strong>GPT-5.1</strong></td><td>Higher EP precision, decisive tone, fewer false positives</td></tr>\n<tr>\n<td>Bulk scans, cost-sensitive workloads</td><td><strong>Sonnet 4.5</strong></td><td>High coverage, lower cost per review</td></tr>\n</tbody>\n</table>\n</div><h2 id=\"heading-closing-thoughts-the-shape-of-reasoning\"><strong>Closing thoughts: The shape of reasoning</strong></h2>\n<p>Opus 4.5 no longer feels experimental; it feels engineered. Earlier models often guessed, while Opus 4.5 measures, structures, and documents. Reading its reviews feels like working with a model that truly understands <em>how developers read</em>.</p>\n<p>In code review, tone defines trust. Opus 4.5’s style, measured, structured, and mechanically precise, demonstrates the maturity of reasoning: precision without pressure and confidence without ego.</p>\n<p><strong>Takeaway:</strong> If Sonnet 4.5 was a teacher and GPT-5.1 a teammate, Opus 4.5 is the architect returning for a design review.</p>\n<p><strong><em>Interested in trying CodeRabbit?</em></strong> <a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong><em>Get a 14-day free trial.</em></strong></a></p>\n",
      "summary": "Every model reasons. Opus 4.5 audits.\nEvery new model arrives with the same promise: smarter reasoning, cleaner code, and better answers. But Opus 4.5 from Anthropic doesn’t just reason; it audits. It reads code as if returning to a system it helped design, identifying weak points and refining architecture. Where other models narrate their logic or prescribe surgical fixes, Opus 4.5 performs structured, systematic reviews that feel more like technical documentation than conversation.\nWe integrated Opus 4.5 into CodeRabbit’s benchmark harness to understand what makes this model distinct. The result was not higher raw intelligence or flashier prose, but discipline. This model doesn’t just find bugs; it builds context around them. It treats review as an engineering process, rather than a guessing game.\n\nBenchmarking context\n\nAt CodeRabbit, we evaluate new LLMs using a controlled benchmark of 25 complex pull requests seeded with known error patterns (EPs) across C++, Java, Python, and TypeScript. Each comment generated by a model is scored by an LLM judge for three key factors:\nPrecision: Whether it correctly identifies the EP.\nImportant-share: The percentage of comments that are genuinely critical or major (real bugs, not style issues).\nSignal-to-noise ratio (SNR): The ratio of important to unimportant comments.\nOur evaluation framework, refined over multiple generations of models, combines automated LLM judging with hand validation to ensure accuracy. We also use multiple judges and repeated trials to measure consistency and understand variance. Each iteration improves the process through better prompts, refined labeling, and expanded coverage, resulting in more reliable outcomes.\nScoreboard (Actionable comments only)\n\nWhat this means: Opus 4.5 sits between Sonnet 4.5’s high-volume, verbose style and GPT-5.1’s lean, surgical precision. It delivers higher per-comment precision and a greater share of meaningful findings than Sonnet 4.5. While it recorded one fewer EP pass (15 vs. 16), that difference falls within normal variance. In several runs, Opus 4.5 matched or even surpassed both GPT-5.1 and Sonnet 4.5. The takeaway is a model that balances signal, structure, and coverage with consistent reliability.\nStyle and tone\n\nOpus 4.5’s reviews are structured, concise, and focused. With assertiveness around 33% and hedging near 15%, its tone reads as measured and professional. The balance of tone and density gives it an analytical voice that feels practical and confident. The high use of code blocks and diff patches underscores its bias toward action; it talks less and edits more.\n\nStructured intelligence: Predictable form and cross-language consistency\nOpus 4.5’s comments follow an architectural rhythm of headline, rationale, and diff. Nearly 80% include code blocks, and most conclude with a concise patch. Each resembles a clear bug report that specifies cause, effect, and resolution.\nThis structure holds across languages. Whether reviewing C++, Java, Python, or TypeScript, the cadence remains consistent, averaging 19 lines and 790 characters per comment. This uniformity simplifies automation and enhances readability. It also makes Opus 4.5 feel like a single engineer’s consistent voice across an entire codebase.\nC++ (WorkerThreadPool): Detects a lost wakeup race with a three-step interleaving and a one-line diff fix.\nJava (OrderService): Flags a missing volatile on a double-checked lock and provides the corrected pattern.\nPython (Batch client): Replaces a synchronous HTTP client with an asynchronous equivalent to prevent blocking calls.\nTypeScript (Cache manager): Identifies that Number.MAX_SAFE_INTEGER disables eviction and suggests realistic defaults.\nThese are concise, code-native insights, each actionable and grounded in sound reasoning.\nThe confidence inversion\nOpus 4.5’s tone is balanced but occasionally reveals a subtle inversion: when it is wrong, it can sound slightly more certain. Although the model is generally measured, this behavioral quirk means tone alone is not always a reliable indicator of correctness. To account for this, we pair tone data with correctness metrics in evaluation summaries to maintain consistent calibration.\nOpus 4.5 rarely speculates; it simply explains, even when it’s wrong.\nSystem-level reasoning: Fixing context, not just code\nWhile most models target the immediate defect, Opus 4.5 focuses on the surrounding system. Its recommendations frequently adjust lifecycles, add safety checks, or refine defaults.\nExamples:\nTypeScript Cache: Rewrites eviction logic, adds TTL enforcement, and updates defaults to prevent silent OOM.\nJava OrderService: Replaces HashMap with ConcurrentHashMap and identifies missing ExecutorService shutdown.\nPython Client Lifecycle: Adds explicit shutdown hooks for long-lived async clients.\nC++ FileAccessEncrypted: Resolves a validation bug that blocked all encrypted files and improves upstream error handling.\nThese are not single-line fixes but systemic corrections. The model treats code as an interconnected ecosystem rather than a collection of isolated issues.\nCost, effort & efficiency\nAnthropic’s Effort parameter provides direct control over how deeply the model reasons. In high-effort mode, Opus 4.5 explores every dependency path. In medium-effort mode, it trims reasoning depth to save tokens. Even with high-effort reasoning, its reviews averaged about 25% fewer output tokens than Sonnet 4.5, balancing higher per-token costs ($25 per million output tokens) with greater efficiency.\nThis disciplined structure pays for itself by producing fewer digressions and maintaining consistent clarity.\nWhat it feels like to read Opus 4.5\n\nIf Sonnet 4.5 feels like a teacher and GPT-5.1 like a decisive teammate, Opus 4.5 is the architect reviewing your PR. Its tone is calm and deliberate, never commanding. It assumes you understand the domain and aims to confirm the details. The result is feedback that reads like peer review from a systems engineer: consistent, structured, and quietly authoritative.\nTone and personality\nOpus 4.5’s voice is measured and analytical. It rarely uses dramatic language or unnecessary severity. Instead, it conveys certainty through order, concise summaries, specific evidence, and focused corrections. The tone builds trust, delivering feedback that feels like it comes from a mentor familiar with your system.\nDepth vs. density\nIts comments are compact yet informative. When an issue warrants detailed explanation, Opus 4.5 delivers it without excess. For simpler problems, it resolves them with brief, precise advice. This balance of detail and brevity keeps reviews readable and comprehensive.\nFlow and readability\nThe model’s structural rhythm of context, cause, and correction allows developers to scan quickly while retaining meaning. Developers often describe its comments as “structured snapshots” that tell a short, self-contained story: what happened, why it matters, and how to fix it.\nPractical impact and developer trust\nBecause Opus 4.5 avoids inflated confidence and theatrical phrasing, developers trust it more readily. It comes across as confident yet professional, firm but not forceful. When it errs, it sounds like a reasoned hypothesis instead of an overreach. That restraint, more than precision alone, makes its reviews feel professionally human.\nEach comment reads like a design note. It states the invariant that failed, proposes a patch, and explains the rationale inline. The clarity is high enough that many of its comments could be pasted directly into changelogs or issue trackers without revision.\nStrengths and weaknesses at a glance\n\nStrengths:\nHigh signal density (≈80% important comments).\nConsistent structure across languages.\nStrong concurrency and lifecycle reasoning.\nClear, concise, and professional tone.\nLower verbosity than Sonnet 4.5 with more context than GPT-5.1.\nWeaknesses:\nModerate precision (≈38%).\nSubtle confidence inversion when incorrect.\nFrequent critical or major labeling may overwhelm busy PRs.\nSlight verbosity on simpler issues.\nBottom line: Opus 4.5 is the most systemic reviewer we’ve tested. Calm, structured, and exacting, it excels when reasoning breadth and architectural understanding matter more than pinpoint precision.\nWhen to use which model\nScenarioBest modelWhy\nCross-language or high-context reviewsOpus 4.5Structured, consistent, strong at systemic issues\nTight precision or small diffsGPT-5.1Higher EP precision, decisive tone, fewer false positives\nBulk scans, cost-sensitive workloadsSonnet 4.5High coverage, lower cost per review\nClosing thoughts: The shape of reasoning\nOpus 4.5 no longer feels experimental; it feels engineered. Earlier models often guessed, while Opus 4.5 measures, structures, and documents. Reading its reviews feels like working with a model that truly understands how developers read.\nIn code review, tone defines trust. Opus 4.5’s style, measured, structured, and mechanically precise, demonstrates the maturity of reasoning: precision without pressure and confidence without ego.\nTakeaway: If Sonnet 4.5 was a teacher and GPT-5.1 a teammate, Opus 4.5 is the architect returning for a design review.\nInterested in trying CodeRabbit? Get a 14-day free trial.",
      "publishedAt": "2025-11-24T18:58:48.000Z",
      "author": "David Loker",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "ide",
        "observability",
        "governance"
      ],
      "ingestedAt": "2025-12-04T14:17:12.171Z",
      "score": 8.191055606320376
    },
    {
      "id": "2eca1bdbc0996763fc790bc52348ffc0",
      "title": "CodeRabbit への MCP サーバーのデプロイと統合方法",
      "url": "https://coderabbit.ai/blog/how-to-deploy-and-integrate-mcp-servers-with-coderabbit-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/how-to-deploy-and-integrate-mcp-servers-with-coderabbit\">How to deploy and integrate MCP servers with CodeRabbit</a>の意訳です。</p>\n<p>MCP サーバーは、ユーザーのリクエストに基づいてシステム関連タスクを実行するために、AI エージェントをアプリケーションへ統合します。Slack、Sentry、Notion、GitHub Copilot などのプラットフォームは、AI 駆動アプリケーションに機能を公開するために、すでに MCP スタイルのサービスを採用しています。</p>\n<p>CodeRabbit もこの潮流に乗っています。MCP クライアントとして機能することで、ユーザーがコンテキストを提供し、最適なコードレビューを実行できるようにします。また、Confluence に保存されたビジネス要件、CI/CD パイプラインからのシステム情報、さらには任意の内部 MCP サーバーなど、複数ソースのコンテキスト（データ）をサポートする初の AI コードレビュー・プラットフォームでもあります。</p>\n<p>このチュートリアルでは、Slack MCP サーバーをセットアップし、チャンネルデータを取得し、それを CodeRabbit のコンテキストとして渡すことで、チームワークスペースの議論を反映したコードレビューを生成する方法を学びます。これにより、すべてのレビューがプロジェクト目標に整合したものになります。</p>\n<h2 id=\"heading-coderabbit-mcp\"><strong>なぜ CodeRabbit で MCP を使うのか？</strong></h2>\n<p>MCP サーバーを CodeRabbit と組み合わせる主な利点は、コードレビューをより洞察的で実行可能なものにする関連データを提供できる点です。その他の利点には以下があります。</p>\n<ul>\n<li><strong>複数ツールのコンテキストでコードレビューを豊かにする</strong></li>\n</ul>\n<p>CodeRabbit は Slack、Confluence、CI/CD パイプライン、または内部 MCP サーバーから関連情報を取得し、レビュアーが変更の背景を理解できるようにします。Slack のスレッド、議論、メッセージから必要な情報を引き出し、コード変更の意図やロジックを理解します。</p>\n<ul>\n<li><strong>情報に基づいた正確なレビューが可能になる</strong></li>\n</ul>\n<p>MCP サーバーから提供されるデータにより、CodeRabbit はプロジェクトのロジックと目標をより深く理解できます。たとえば、Slack MCP サーバーはチームのメッセージへのアクセスを許可し、ビジネス要件や開発目標に整合したコードレビューを実行できるようにします。</p>\n<h2 id=\"heading-kirliy3mj5dmnahku7yqkg\"><strong>前提条件</strong></h2>\n<p>進める前に、MCP サーバーをセットアップし CodeRabbit と統合するために、以下のツールをインストールしておく必要があります。</p>\n<ul>\n<li><p><a target=\"_blank\" href=\"https://slack.com/\"><strong>Slack チャンネル</strong></a> – メッセージを取得し、AI コードレビューワーへコンテキストを提供するために既存の Slack チャンネルが必要です。</p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://github.com/korotovsky/slack-mcp-server\"><strong>MCP Server for Slack Workspaces</strong></a> <strong>– Slack の会話データを Model Context Protocol (MCP) によって公開するための、シンプルかつ構造的な方法を提供します。メッセージ、スレッド、リプライなどの Slack API メソッドが組み込まれており、軽量で Docker 対応、設定も容易です。</strong></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.claude.com/download\"><strong>Claude Desktop</strong></a> – Slack MCP サーバーを CodeRabbit に接続する前にローカルでテストするための MCP クライアントです。</p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.docker.com/products/docker-desktop/\"><strong>Docker</strong></a> – Slack MCP サーバーをコンテナで実行・ホストするために使用します。</p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.npmjs.com/package/ngrok\"><strong>Ngrok</strong></a> <strong>–</strong> Slack MCP サーバーを CodeRabbit からアクセス可能にするため、セキュアな公開 URL を生成します。</p>\n</li>\n</ul>\n<p>このチュートリアルでは次を行います。</p>\n<ol>\n<li><p>Claude Desktop を使ってローカルで Slack MCP サーバーをテスト</p>\n</li>\n<li><p>Docker を使用してローカルホスト上にサーバーをホスト</p>\n</li>\n<li><p>Ngrok による公開 URL の生成</p>\n</li>\n<li><p>MCP サーバーを CodeRabbit に統合</p>\n</li>\n</ol>\n<p><strong><em>注意:</em></strong> <em>Slack も MCP サーバーを試験的に扱っていますが、現時点で公式の MCP サーバーは提供されていません。このチュートリアルでは自分で MCP サーバーを構築する方法を解説します。</em></p>\n<h2 id=\"heading-claude-desktop-slack-mcp\"><strong>Claude Desktop で Slack MCP サーバーをセットアップ</strong></h2>\n<p>Claude Desktop は複数の MCP サーバーへ接続し、それらをコンテキストソースとして利用する MCP クライアントです。MCP サーバーをコネクタとして追加し、CodeRabbit やその他のプラットフォームへデプロイする前にローカルでテストできます。</p>\n<p><a target=\"_blank\" href=\"https://www.claude.com/download\">Claude Desktop</a> をインストールし、起動後に <strong>Manage Connectors</strong> をクリックします。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609580957/64731066-f813-400c-99e3-a68a2a1684fa.png\" alt class=\"image--center mx-auto\" /></p>\n<p>サイドバーから <strong>Developer</strong> を選択し、<strong>Edit Config</strong> をクリックして Slack 認証トークンを設定します。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609599599/d255d981-b39e-45fc-9f23-3e5d5eabce2d.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Slack の認証トークンは、<a target=\"_blank\" href=\"https://github.com/korotovsky/slack-mcp-server/blob/master/docs/01-authentication-setup.md\">GitHub リポジトリの説明</a>に従って取得し、Claude Desktop に設定します。</p>\n<p>次に、<strong>claude_desktop_config.json</strong> を以下の JSON で更新します。</p>\n<pre><code class=\"lang-json\">{\n  <span class=\"hljs-attr\">\"mcpServers\"</span>: {\n    <span class=\"hljs-attr\">\"slack\"</span>: {\n      <span class=\"hljs-attr\">\"command\"</span>: <span class=\"hljs-string\">\"npx\"</span>,\n      <span class=\"hljs-attr\">\"args\"</span>: [<span class=\"hljs-string\">\"-y\"</span>, <span class=\"hljs-string\">\"slack-mcp-server@latest\"</span>, <span class=\"hljs-string\">\"--transport\"</span>, <span class=\"hljs-string\">\"stdio\"</span>],\n      <span class=\"hljs-attr\">\"env\"</span>: {\n        <span class=\"hljs-attr\">\"SLACK_MCP_XOXC_TOKEN\"</span>: <span class=\"hljs-string\">\"xoxc-...\"</span>,\n        <span class=\"hljs-attr\">\"SLACK_MCP_XOXD_TOKEN\"</span>: <span class=\"hljs-string\">\"xoxd-...\"</span>\n      }\n    }\n  }\n}\n</code></pre>\n<p>上記設定により、Slack の xoxc と xoxd トークンを使用して Slack MCP サーバーが Claude Desktop のコネクタとして登録されます。接続されると、Claude はチャンネルメッセージの取得や Slack コンテキストを活用したコードレビューを実行できます。</p>\n<p>設定後、Claude Desktop を再起動して Slack MCP サーバーをアクティブにします。</p>\n<p><a target=\"_blank\" href=\"http://Preview.mp\">Preview.mp</a><a target=\"_blank\" href=\"https://drive.google.com/file/d/1FoAL0Hi8jhlpFALt41lo2MgcwvpUWKml/view?usp=sharing\">4</a></p>\n<h2 id=\"heading-slack-mcp-coderabbit\"><strong>Slack MCP サーバーを CodeRabbit に接続</strong></h2>\n<p>このセクションでは、Slack MCP サーバーを Docker で実行し、Ngrok で公開 URL を生成し、それを CodeRabbit に統合する手順を説明します。</p>\n<p>まず Docker を起動します。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609632641/464250c8-7fac-4a8f-9068-b7abc965be79.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>次にターミナルを開き、Slack MCP サーバーに必要なファイルをダウンロードします。</p>\n<pre><code class=\"lang-bash\">wget -O docker-compose.yml https://github.com/korotovsky/slack-mcp-server/releases/latest/download/docker-compose.yml\nwget -O .env https://github.com/korotovsky/slack-mcp-server/releases/latest/download/default.env.dist\n</code></pre>\n<p><strong>.env</strong> ファイルを Slack の認証トークンで更新します。</p>\n<pre><code class=\"lang-bash\">SLACK_MCP_XOXC_TOKEN=&lt;your_token&gt;\n</code></pre>\n<p>以下のコマンドで Docker Compose を起動します。</p>\n<pre><code class=\"lang-bash\"><span class=\"hljs-comment\"># Docker 用の専用ネットワークを作成</span>\ndocker network create app-tier\n\n<span class=\"hljs-comment\"># MCP サーバーをデタッチモードで起動</span>\ndocker-compose up -\n</code></pre>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609659064/8758bee2-8fa6-4f89-96e6-3b5f438b7be5.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>Slack MCP サーバーは <a target=\"_blank\" href=\"http://localhost\">localhost</a> の 3001 番ポートで起動しています。CodeRabbit と統合するには HTTPS エンドポイントが必要であり、そのために ngrok を使用します。</p>\n<p>ngrok がインストールされているか確認します。</p>\n<pre><code class=\"lang-bash\">ngrok --version\n</code></pre>\n<p>ngrok を使って公開 URL を生成します。</p>\n<pre><code class=\"lang-bash\">ngrok http 3001\n</code></pre>\n<p>このコマンドでローカルの Slack MCP サーバーがインターネット公開され、CodeRabbit からアクセス可能になります。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609678911/f558b5d4-d779-4d2e-b460-136e161dd8dc.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>次に、<a target=\"_blank\" href=\"https://github.com/modelcontextprotocol/inspector\">MCP Inspector</a> を使ってサーバーが動作しているか確認します。</p>\n<pre><code class=\"lang-bash\">npx @modelcontextprotocol/inspector\n</code></pre>\n<p>Inspector UI を開き、<strong>SSE</strong> を選択し、ngrok の URL の末尾に <code>/sse</code> を追加します。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609694796/0cf20aa4-c710-45fe-adfa-6483da965935.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>正常に動作していることが確認できたら、CodeRabbit との統合作業に移ります。</p>\n<h2 id=\"heading-coderabbit-mcp-1\"><strong>CodeRabbit に MCP サーバーを統合してテストする</strong></h2>\n<p><a target=\"_blank\" href=\"https://app.coderabbit.ai/login?\">CodeRabbit</a> にサインインし、ダッシュボードのサイドバーから <strong>Integrations</strong> を選択して MCP サーバーを追加します。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609724348/1c83de71-436f-4daf-a425-fa1ca49840cb.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>名前と MCP サーバー URL（例: <a target=\"_blank\" href=\"https://2bb0002c0e2c.ngrok-free.app/sse）を入力します。認証方式は何も選択しないようにします。\"><code>https://2bb0002c0e2c.ngrok-free.app/sse</code>）を入力します。認証方式は何も選択しないようにします。</a></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609744271/8912fae6-475a-4f0b-9c40-ba14882815f7.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>接続後、すべての CodeRabbit コードレビューで MCP サーバーをコンテキストとして利用できます。</p>\n<p>GitHub リポジトリを作成し、CodeRabbit に追加して MCP サーバーへのアクセス権を設定します。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609767496/fc08d3c0-81a4-44c5-99c8-557cbc0aada1.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>コードレビュー時に MCP サーバーを利用するため、リポジトリに coderabbit.yaml を追加します。</p>\n<pre><code class=\"lang-yaml\"><span class=\"hljs-attr\">language:</span> <span class=\"hljs-string\">\"en-US\"</span>\n<span class=\"hljs-attr\">early_access:</span> <span class=\"hljs-literal\">false</span>\n<span class=\"hljs-attr\">reviews:</span>\n  <span class=\"hljs-attr\">profile:</span> <span class=\"hljs-string\">\"chill\"</span>\n  <span class=\"hljs-attr\">request_changes_workflow:</span> <span class=\"hljs-literal\">false</span>\n  <span class=\"hljs-attr\">high_level_summary:</span> <span class=\"hljs-literal\">true</span>\n  <span class=\"hljs-attr\">poem:</span> <span class=\"hljs-literal\">false</span>\n  <span class=\"hljs-attr\">review_status:</span> <span class=\"hljs-literal\">true</span>\n  <span class=\"hljs-attr\">collapse_walkthrough:</span> <span class=\"hljs-literal\">false</span>\n<span class=\"hljs-attr\">auto_review:</span>\n  <span class=\"hljs-attr\">enabled:</span> <span class=\"hljs-literal\">true</span>\n  <span class=\"hljs-attr\">drafts:</span> <span class=\"hljs-literal\">false</span>\n<span class=\"hljs-attr\">chat:</span>\n  <span class=\"hljs-attr\">auto_reply:</span> <span class=\"hljs-literal\">true</span>\n</code></pre>\n<p>GitHub リポジトリの MCP サーバー利用を有効化します。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609799511/4dab84f4-58ac-4e96-929e-4255a7c11871.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>次に、「Path Instructions」を設定し、PR マージ前に追加指示を確認するよう CodeRabbit に伝えます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609824068/3e230983-2235-444a-b3a8-425e72cb0ad0.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>上記画像では、<strong>File Path</strong> がレビュー対象のファイルを指定し、<strong>Instructions</strong> がそのファイルをどのように扱うべきかを示します。この設定により、CodeRabbit は Slack の <strong>#dev</strong> チャンネルの議論内容を参照し、リポジトリ内のコード変更がチャンネルのガイドラインに従っているかを確認します。</p>\n<p>以下は Slack チャンネルのメッセージ例です。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609848454/adeac65b-9a66-4900-90c7-c6a64cf067ea.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>そしてこちらが、指示に従ってレビューを行う CodeRabbit の出力例です。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609887468/1677861f-520a-46eb-8789-a7f2eef34b5a.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>CodeRabbit が Slack の議論内容をどのように読み取ってレビューに反映するかは、次のデモで確認できます。</p>\n<p><a target=\"_blank\" href=\"https://github.com/tyaga001/test-slack-mcp/pull/7#pullrequestreview-3454174366\">https://github.com/tyaga001/test-slack-mcp/pull/7#pullrequestreview-3454174366</a></p>\n<p><em>💡</em> <strong><em>ベストプラクティス: 必要なデータだけをコンテキストとして渡す</em></strong></p>\n<p><em>不要なデータは LLM の処理コストを上げ、パフォーマンスを低下させます。特定の Slack チャンネルや必要最小限の情報に限定して渡すようにしてください。</em></p>\n<h2 id=\"heading-kirmrkhjga7jgrnjg4bjg4pjg5cqkg\"><strong>次のステップ</strong></h2>\n<p>このチュートリアルでは、Slack MCP サーバーを CodeRabbit と統合し、文脈情報を用いたコードレビューを実行する方法を学びました。CodeRabbit は Notion、GitHub Copilot、Sentry、Asana など複数の MCP サーバーをデフォルトでサポートしており、これらを組み合わせて高度な文脈理解を実現できます。</p>\n<p>同様の手法を使うことで、任意のコンテキストやデータソースを MCP サーバー経由で統合し、CodeRabbit に正確で実用的な応答を生成させることが可能です。</p>\n<p>MCP サーバーや CodeRabbit に関するさらなるチュートリアル・記事はこちら:</p>\n<ul>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/handling-ballooning-context-in-the-mcp-era-context-engineering-on-steroids-ja?utm_source=chatgpt.com\">MCP時代におけるコンテキスト肥大化への対応</a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/coderabbits-mcp-server-integration-code-reviews-that-see-the-whole-picture-ja?utm_source=chatgpt.com\">CodeRabbitのMCP連携 = コンテキストとコードレビュー</a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://docs.coderabbit.ai/context-enrichment/mcp-server-integrations\">How to Integrate MCP Server with CodeRabbit</a></p>\n</li>\n</ul>\n<p><strong><em>CodeRabbit を試してみませんか？</em></strong> <a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong><em>14 日間の無料トライアルを始める</em></strong></a></p>\n",
      "summary": "How to deploy and integrate MCP servers with CodeRabbitの意訳です。\nMCP サーバーは、ユーザーのリクエストに基づいてシステム関連タスクを実行するために、AI エージェントをアプリケーションへ統合します。Slack、Sentry、Notion、GitHub Copilot などのプラットフォームは、AI 駆動アプリケーションに機能を公開するために、すでに MCP スタイルのサービスを採用しています。\nCodeRabbit もこの潮流に乗っています。MCP クライアントとして機能することで、ユーザーがコンテキストを提供し、最適なコードレビューを実行できるようにします。また、Confluence に保存されたビジネス要件、CI/CD パイプラインからのシステム情報、さらには任意の内部 MCP サーバーなど、複数ソースのコンテキスト（データ）をサポートする初の AI コードレビュー・プラットフォームでもあります。\nこのチュートリアルでは、Slack MCP サーバーをセットアップし、チャンネルデータを取得し、それを CodeRabbit のコンテキストとして渡すことで、チームワークスペースの議論を反映したコードレビューを生成する方法を学びます。これにより、すべてのレビューがプロジェクト目標に整合したものになります。\nなぜ CodeRabbit で MCP を使うのか？\nMCP サーバーを CodeRabbit と組み合わせる主な利点は、コードレビューをより洞察的で実行可能なものにする関連データを提供できる点です。その他の利点には以下があります。\n複数ツールのコンテキストでコードレビューを豊かにする\nCodeRabbit は Slack、Confluence、CI/CD パイプライン、または内部 MCP サーバーから関連情報を取得し、レビュアーが変更の背景を理解できるようにします。Slack のスレッド、議論、メッセージから必要な情報を引き出し、コード変更の意図やロジックを理解します。\n情報に基づいた正確なレビューが可能になる\nMCP サーバーから提供されるデータにより、CodeRabbit はプロジェクトのロジックと目標をより深く理解できます。たとえば、Slack MCP サーバーはチームのメッセージへのアクセスを許可し、ビジネス要件や開発目標に整合したコードレビューを実行できるようにします。\n前提条件\n進める前に、MCP サーバーをセットアップし CodeRabbit と統合するために、以下のツールをインストールしておく必要があります。\nSlack チャンネル – メッセージを取得し、AI コードレビューワーへコンテキストを提供するために既存の Slack チャンネルが必要です。\nMCP Server for Slack Workspaces – Slack の会話データを Model Context Protocol (MCP) によって公開するための、シンプルかつ構造的な方法を提供します。メッセージ、スレッド、リプライなどの Slack API メソッドが組み込まれており、軽量で Docker 対応、設定も容易です。\nClaude Desktop – Slack MCP サーバーを CodeRabbit に接続する前にローカルでテストするための MCP クライアントです。\nDocker – Slack MCP サーバーをコンテナで実行・ホストするために使用します。\nNgrok – Slack MCP サーバーを CodeRabbit からアクセス可能にするため、セキュアな公開 URL を生成します。\nこのチュートリアルでは次を行います。\nClaude Desktop を使ってローカルで Slack MCP サーバーをテスト\nDocker を使用してローカルホスト上にサーバーをホスト\nNgrok による公開 URL の生成\nMCP サーバーを CodeRabbit に統合\n注意: Slack も MCP サーバーを試験的に扱っていますが、現時点で公式の MCP サーバーは提供されていません。このチュートリアルでは自分で MCP サーバーを構築する方法を解説します。\nClaude Desktop で Slack MCP サーバーをセットアップ\nClaude Desktop は複数の MCP サーバーへ接続し、それらをコンテキストソースとして利用する MCP クライアントです。MCP サーバーをコネクタとして追加し、CodeRabbit やその他のプラットフォームへデプロイする前にローカルでテストできます。\nClaude Desktop をインストールし、起動後に Manage Connectors をクリックします。\n\nサイドバーから Developer を選択し、Edit Config をクリックして Slack 認証トークンを設定します。\n\nSlack の認証トークンは、GitHub リポジトリの説明に従って取得し、Claude Desktop に設定します。\n次に、claude_desktop_config.json を以下の JSON で更新します。\n{\n  \"mcpServers\": {\n    \"slack\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"slack-mcp-server@latest\", \"--transport\", \"stdio\"],\n      \"env\": {\n        \"SLACK_MCP_XOXC_TOKEN\": \"xoxc-...\",\n        \"SLACK_MCP_XOXD_TOKEN\": \"xoxd-...\"\n      }\n    }\n  }\n}\n\n上記設定により、Slack の xoxc と xoxd トークンを使用して Slack MCP サーバーが Claude Desktop のコネクタとして登録されます。接続されると、Claude はチャンネルメッセージの取得や Slack コンテキストを活用したコードレビューを実行できます。\n設定後、Claude Desktop を再起動して Slack MCP サーバーをアクティブにします。\nPreview.mp4\nSlack MCP サーバーを CodeRabbit に接続\nこのセクションでは、Slack MCP サーバーを Docker で実行し、Ngrok で公開 URL を生成し、それを CodeRabbit に統合する手順を説明します。\nまず Docker を起動します。\n\n次にターミナルを開き、Slack MCP サーバーに必要なファイルをダウンロードします。\nwget -O docker-compose.yml https://github.com/korotovsky/slack-mcp-server/releases/latest/download/docker-compose.yml\nwget -O .env https://github.com/korotovsky/slack-mcp-server/releases/latest/download/default.env.dist\n\n.env ファイルを Slack の認証トークンで更新します。\nSLACK_MCP_XOXC_TOKEN=<your_token>\n\n以下のコマンドで Docker Compose を起動します。\n# Docker 用の専用ネットワークを作成\ndocker network create app-tier\n\n# MCP サーバーをデタッチモードで起動\ndocker-compose up -\n\n\nSlack MCP サーバーは localhost の 3001 番ポートで起動しています。CodeRabbit と統合するには HTTPS エンドポイントが必要であり、そのために ngrok を使用します。\nngrok がインストールされているか確認します。\nngrok --version\n\nngrok を使って公開 URL を生成します。\nngrok http 3001\n\nこのコマンドでローカルの Slack MCP サーバーがインターネット公開され、CodeRabbit からアクセス可能になります。\n\n次に、MCP Inspector を使ってサーバーが動作しているか確認します。\nnpx @modelcontextprotocol/inspector\n\nInspector UI を開き、SSE を選択し、ngrok の URL の末尾に /sse を追加します。\n\n正常に動作していることが確認できたら、CodeRabbit との統合作業に移ります。\nCodeRabbit に MCP サーバーを統合してテストする\nCodeRabbit にサインインし、ダッシュボードのサイドバーから Integrations を選択して MCP サーバーを追加します。\n\n名前と MCP サーバー URL（例: https://2bb0002c0e2c.ngrok-free.app/sse）を入力します。認証方式は何も選択しないようにします。\n\n接続後、すべての CodeRabbit コードレビューで MCP サーバーをコンテキストとして利用できます。\nGitHub リポジトリを作成し、CodeRabbit に追加して MCP サーバーへのアクセス権を設定します。\n\nコードレビュー時に MCP サーバーを利用するため、リポジトリに coderabbit.yaml を追加します。\nlanguage: \"en-US\"\nearly_access: false\nreviews:\n  profile: \"chill\"\n  request_changes_workflow: false\n  high_level_summary: true\n  poem: false\n  review_status: true\n  collapse_walkthrough: false\nauto_review:\n  enabled: true\n  drafts: false\nchat:\n  auto_reply: true\n\nGitHub リポジトリの MCP サーバー利用を有効化します。\n\n次に、「Path Instructions」を設定し、PR マージ前に追加指示を確認するよう CodeRabbit に伝えます。\n\n上記画像では、File Path がレビュー対象のファイルを指定し、Instructions がそのファイルをどのように扱うべきかを示します。この設定により、CodeRabbit は Slack の #dev チャンネルの議論内容を参照し、リポジトリ内のコード変更がチャンネルのガイドラインに従っているかを確認します。\n以下は Slack チャンネルのメッセージ例です。\n\nそしてこちらが、指示に従ってレビューを行う CodeRabbit の出力例です。\n\nCodeRabbit が Slack の議論内容をどのように読み取ってレビューに反映するかは、次のデモで確認できます。\nhttps://github.com/tyaga001/test-slack-mcp/pull/7#pullrequestreview-3454174366\n💡 ベストプラクティス: 必要なデータだけをコンテキストとして渡す\n不要なデータは LLM の処理コストを上げ、パフォーマンスを低下させます。特定の Slack チャンネルや必要最小限の情報に限定して渡すようにしてください。\n次のステップ\nこのチュートリアルでは、Slack MCP サーバーを CodeRabbit と統合し、文脈情報を用いたコードレビューを実行する方法を学びました。CodeRabbit は Notion、GitHub Copilot、Sentry、Asana など複数の MCP サーバーをデフォルトでサポートしており、これらを組み合わせて高度な文脈理解を実現できます。\n同様の手法を使うことで、任意のコンテキストやデータソースを MCP サーバー経由で統合し、CodeRabbit に正確で実用的な応答を生成させることが可能です。\nMCP サーバーや CodeRabbit に関するさらなるチュートリアル・記事はこちら:\nMCP時代におけるコンテキスト肥大化への対応\nCodeRabbitのMCP連携 = コンテキストとコードレビュー\nHow to Integrate MCP Server with CodeRabbit\nCodeRabbit を試してみませんか？ 14 日間の無料トライアルを始める",
      "publishedAt": "2025-11-20T14:23:45.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:12.171Z",
      "score": 5.1519850604468544
    },
    {
      "id": "b6b3b7d5a69d513cf604df2bcfcc808e",
      "title": "How to deploy and integrate MCP servers with CodeRabbit",
      "url": "https://coderabbit.ai/blog/how-to-deploy-and-integrate-mcp-servers-with-coderabbit",
      "content": "<p>MCP servers integrate AI agents into software applications to carry out system-related tasks based on users’ requests. Platforms like Slack, Sentry, Notion, and GitHub Copilot have adopted MCP-style services to expose their features to AI-driven applications.</p>\n<p>CodeRabbit is part of this shift, acting as an MCP client that enables users to provide contexts and perform the best code reviews. It’s also the first AI code review platform that supports context (data) from multiple sources, such as business requirements stored in Confluence, system information from your CI/CD pipeline, or any internal MCP server.</p>\n<p>In this tutorial, you will learn how to set up a Slack MCP server, retrieve channel data, and pass it as context into CodeRabbit to generate code reviews that incorporate discussions from your team workspace, ensuring that every review aligns with the project goals.</p>\n<h2 id=\"heading-why-use-mcp-with-coderabbit\"><strong>Why use MCP with CodeRabbit?</strong></h2>\n<p>The primary benefit of using MCP servers with CodeRabbit is to deliver relevant data that makes code reviews more insightful and actionable. Other benefits include:</p>\n<ul>\n<li><strong>Enriching code reviews with context from multiple tools.</strong></li>\n</ul>\n<p>CodeRabbit enables you to retrieve relevant information from Slack, Confluence, CI/CD pipelines, or internal MCP servers so reviewers understand the reasoning behind changes. CodeRabbit can pull relevant information from Slack threads, discussions, and messages to understand the code logic and reasoning behind every code change.</p>\n<ul>\n<li><strong>Making informed and precise reviews</strong></li>\n</ul>\n<p>With access to data from MCP servers, CodeRabbit gains a better understanding of the project’s logic and goals. For instance, the Slack MCP server grants CodeRabbit access to team messages, enabling it to perform code reviews that are consistent with business requirements and development objectives.</p>\n<h2 id=\"heading-prerequisites\"><strong>Prerequisites</strong></h2>\n<p>Before we proceed, you need to have the following tools installed to set up the MCP server and integrate it with CodeRabbit:</p>\n<ul>\n<li><p><a target=\"_blank\" href=\"https://slack.com/\"><strong>Slack channel</strong></a> – An existing Slack channel is required to fetch messages and provide context for the AI code reviewer.</p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://github.com/korotovsky/slack-mcp-server\"><strong>MCP Server for Slack Workspaces</strong></a> <strong>- Provides an easy and structured way to expose Slack conversations via the Model Context Protocol (MCP). It already includes built-in Slack API methods (fetching messages, threads, replies, etc.) and is lightweight, Docker-ready, and easy to configure.</strong></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.claude.com/download\"><strong>Claude Desktop</strong></a> – Allows you to test the Slack MCP server locally before connecting it to CodeRabbit.</p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.docker.com/products/docker-desktop/\"><strong>Docker</strong></a> – Used to run and host the Slack MCP server in a container.</p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.npmjs.com/package/ngrok\"><strong>Ngrok</strong></a> <strong>–</strong> Used to create a secure public URL for the Slack MCP server, allowing CodeRabbit to access it from outside your local environment.</p>\n</li>\n</ul>\n<p>In this tutorial, you will:</p>\n<ol>\n<li><p>Learn how to test the Slack MCP server locally with Claude Desktop.</p>\n</li>\n<li><p>Host the server on localhost using Docker.</p>\n</li>\n<li><p>Generate a public URL using Ngrok..</p>\n</li>\n<li><p>Integrate the MCP server with CodeRabbit.</p>\n</li>\n</ol>\n<p><strong><em>Note:</em></strong> <em>While Slack has been experimenting with MCP servers, they don’t currently have one available. This tutorial will cover how to create one yourself</em>.</p>\n<h2 id=\"heading-set-up-slack-mcp-server-with-claude-desktop\"><strong>Set up Slack MCP server with Claude Desktop</strong></h2>\n<p>Claude Desktop is an MCP client that connects to multiple MCP servers and uses them as sources of context. It allows you to add your MCP servers as connectors and test them locally before deploying them to CodeRabbit or any other platform.</p>\n<p>Install <a target=\"_blank\" href=\"https://www.claude.com/download\">Claude Desktop</a> on your computer. Once the installation is complete, open the app and click Manage Connectors.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609580957/64731066-f813-400c-99e3-a68a2a1684fa.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Select <strong>Developer</strong> from the sidebar menu, and click <strong>Edit Config</strong> to configure your MCP server using your Slack authentication tokens.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609599599/d255d981-b39e-45fc-9f23-3e5d5eabce2d.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Follow the instructions in the <a target=\"_blank\" href=\"https://github.com/korotovsky/slack-mcp-server/blob/master/docs/01-authentication-setup.md\">GitHub repository to obtain your Slack authentication</a> tokens and configure the Slack MCP server in Claude Desktop.</p>\n<p>Update the <strong>claude_desktop_config.json</strong> file with the following JSON configuration.</p>\n<table><tbody><tr><td><p>{<br />  \"mcpServers\": {<br />      \"slack\": {<br />          \"command\": \"npx\",<br />          \"args\": [\"-y\", \"slack-mcp-server@latest\", \"--transport\", \"stdio\"],<br />          \"env\": {<br />              \"SLACK_MCP_XOXC_TOKEN\": \"xoxc-...\",<br />              \"SLACK_MCP_XOXD_TOKEN\": \"xoxd-...\"<br />          }<br />      }<br />  }<br />}</p></td></tr></tbody></table>\n\n<p>The configuration above uses the xoxc and xoxd Slack authentication tokens to register the Slack MCP server as a connector in Claude Desktop. Once connected, Claude can perform tasks such as retrieving channel messages and using Slack context to enhance code reviews and responses.</p>\n<p>Restart Claude Desktop to apply the updated configuration and activate the Slack MCP server.</p>\n<p><a target=\"_blank\" href=\"https://drive.google.com/file/d/1FoAL0Hi8jhlpFALt41lo2MgcwvpUWKml/view?usp=sharing\">Preview.mp4</a></p>\n<h2 id=\"heading-connect-the-slack-mcp-server-to-coderabbit\"><strong>Connect the Slack MCP server to CodeRabbit</strong></h2>\n<p>In this section, you will learn how to run the Slack MCP server using Docker, generate a public URL for it, and integrate it with CodeRabbit to provide context-aware code reviews.</p>\n<p>Before we proceed, open the Docker application.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609632641/464250c8-7fac-4a8f-9068-b7abc965be79.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Next, open your terminal and download the required files for the Slack MCP Server using the following commands:</p>\n<table><tbody><tr><td><p>wget -O docker-compose.yml https://github.com/korotovsky/slack-mcp-server/releases/latest/download/docker-compose.yml<br />wget -O .env https://github.com/korotovsky/slack-mcp-server/releases/latest/download/default.env.dist</p></td></tr></tbody></table>\n\n<p>Update the <strong>.env</strong>  file with your Slack authentication tokens.</p>\n<table><tbody><tr><td><p>SLACK_MCP_XOXC_TOKEN=&lt;your_token&gt;</p></td></tr></tbody></table>\n\n<p>Start the MCP server using Docker Compose with the following commands:</p>\n<table><tbody><tr><td><p># Create a dedicated Docker network<br />docker network create app-tier<br /># Start the MCP server in detached mode<br />docker-compose up -</p></td></tr></tbody></table>\n\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609659064/8758bee2-8fa6-4f89-96e6-3b5f438b7be5.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Currently, the Slack MCP server is running on localhost at port 3001. To integrate it with CodeRabbit, it needs to be accessible via an HTTPS endpoint. This can be achieved using ngrok.</p>\n<p>First, confirm that ngrok is installed by running:</p>\n<table><tbody><tr><td><p>ngrok --version</p></td></tr></tbody></table>\n\n<p>Next, generate a public URL for your MCP server.</p>\n<table><tbody><tr><td><p>ngrok http 3001</p></td></tr></tbody></table>\n\n<p>The command above exposes your local Slack MCP server to the internet by generating a secure public URL. Use this URL to connect the Slack MCP server to CodeRabbit.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609678911/f558b5d4-d779-4d2e-b460-136e161dd8dc.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Open a new terminal and start the <a target=\"_blank\" href=\"https://github.com/modelcontextprotocol/inspector\">MCP Inspector</a> to test the Slack MCP server using the following command:</p>\n<table><tbody><tr><td><p>npx @modelcontextprotocol/inspector</p></td></tr></tbody></table>\n\n<p>This will launch the MCP Inspector UI, allowing you to verify that your MCP server is running correctly. In the Inspector, select <strong>SSE</strong> as the transport type and append /sse to the end of your ngrok URL</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609694796/0cf20aa4-c710-45fe-adfa-6483da965935.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Once the MCP server is confirmed to be working, you can proceed to integrate it with CodeRabbit.</p>\n<h2 id=\"heading-integrate-and-test-mcp-servers-with-coderabbit\"><strong>Integrate and test MCP servers with CodeRabbit</strong></h2>\n<p>Sign in to <a target=\"_blank\" href=\"https://app.coderabbit.ai/login?\">CodeRabbit</a> and select Integrations from the sidebar menu on your dashboard to add a new MCP server</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609724348/1c83de71-436f-4daf-a425-fa1ca49840cb.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Enter a name and your MCP server URL (for example, https://2bb0002c0e2c.ngrok-free.app/sse) to connect the server to CodeRabbit. Make sure no authentication method is selected.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609744271/8912fae6-475a-4f0b-9c40-ba14882815f7.png\" alt class=\"image--center mx-auto\" /></p>\n<p>After connecting the MCP server, you can use it to provide context in all your CodeRabbit code reviews.</p>\n<p>To test the setup, create a GitHub repository, add it to CodeRabbit, and configure it to have access to your MCP server</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609767496/fc08d3c0-81a4-44c5-99c8-557cbc0aada1.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Add a coderabbit.yaml configuration file to the repository to enable CodeRabbit to access and use the MCP server context during code reviews.</p>\n<table><tbody><tr><td><p>language: \"en-US\"<br />early_access: false<br />reviews:<br />profile: \"chill\"<br />request_changes_workflow: false<br />high_level_summary: true<br />poem: false<br />review_status: true<br />collapse_walkthrough: false<br />auto_review:<br />  enabled: true<br />  drafts: false<br />chat:<br />auto_reply: true</p></td></tr></tbody></table>\n\n<p>To give the GitHub repository access to your MCP servers, find the GitHub repository and enable MCP servers</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609799511/4dab84f4-58ac-4e96-929e-4255a7c11871.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Next, enter the Path Instructions to ensure CodeRabbit checks for additional instructions before allowing PR merges to the code repository</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609824068/3e230983-2235-444a-b3a8-425e72cb0ad0.png\" alt class=\"image--center mx-auto\" /></p>\n<p>From the image above, the <strong>File Path</strong> specifies which files CodeRabbit should review, while the <strong>Instructions</strong> field provides context on how it should handle those files. Based on the instructions given, CodeRabbit analyses the discussions in your Slack <strong>#dev</strong> channel and ensures that every pull request or code change in your GitHub repository complies with the guidelines defined in that channel.</p>\n<p>Below is a screenshot showing the messages from the Slack channel</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609848454/adeac65b-9a66-4900-90c7-c6a64cf067ea.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Here is the code review showing how CodeRabbit reads and adheres to the instructions:</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763609887468/1677861f-520a-46eb-8789-a7f2eef34b5a.png\" alt class=\"image--center mx-auto\" /></p>\n<p>You can check out <a target=\"_blank\" href=\"https://github.com/tyaga001/test-slack-mcp/pull/7#pullrequestreview-3454174366\">the full demo to see how CodeRabbit reads team Slack discussions</a> and reviews code based on those conversations.</p>\n<p><em>💡</em> <strong><em>Best Practice: Pass only Important Data as Context</em></strong></p>\n<p><em>Irrelevant data can slow down your LLM and increase costs. Keep access limited to specific Slack channels or only include the necessary information for code reviews.</em></p>\n<h2 id=\"heading-next-steps\"><strong>Next steps</strong></h2>\n<p>In this tutorial, you learned how to integrate the Slack MCP server into CodeRabbit to perform contextual code reviews. CodeRabbit also supports multiple MCP servers by default, including Notion, GitHub Copilot, Sentry, Asana, and many others. That  you to enhance code reviews and generate context-aware answers with ease.</p>\n<p>Using the same approach, you can integrate other contexts or data sources via MCP servers to enable CodeRabbit to generate accurate and actionable responses for your queries.</p>\n<p>Check out more tutorials and articles on MCP Servers and CodeRabbit:</p>\n<ul>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/handling-ballooning-context-in-the-mcp-era-context-engineering-on-steroids?utm_source=chatgpt.com\">Handling ballooning context in the MCP era: Context engineering on steroids</a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/coderabbits-mcp-server-integration-code-reviews-that-see-the-whole-picture?utm_source=chatgpt.com\">CodeRabbit’s MCP integration: Code reviews that see the whole picture</a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://docs.coderabbit.ai/context-enrichment/mcp-server-integrations\">How to Integrate MCP Server with CodeRabbit</a></p>\n</li>\n</ul>\n<p><strong><em>Interested in trying CodeRabbit?</em></strong> <a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong><em>Start a 14-day trial.</em></strong></a></p>\n",
      "summary": "MCP servers integrate AI agents into software applications to carry out system-related tasks based on users’ requests. Platforms like Slack, Sentry, Notion, and GitHub Copilot have adopted MCP-style services to expose their features to AI-driven applications.\nCodeRabbit is part of this shift, acting as an MCP client that enables users to provide contexts and perform the best code reviews. It’s also the first AI code review platform that supports context (data) from multiple sources, such as business requirements stored in Confluence, system information from your CI/CD pipeline, or any internal MCP server.\nIn this tutorial, you will learn how to set up a Slack MCP server, retrieve channel data, and pass it as context into CodeRabbit to generate code reviews that incorporate discussions from your team workspace, ensuring that every review aligns with the project goals.\nWhy use MCP with CodeRabbit?\nThe primary benefit of using MCP servers with CodeRabbit is to deliver relevant data that makes code reviews more insightful and actionable. Other benefits include:\nEnriching code reviews with context from multiple tools.\nCodeRabbit enables you to retrieve relevant information from Slack, Confluence, CI/CD pipelines, or internal MCP servers so reviewers understand the reasoning behind changes. CodeRabbit can pull relevant information from Slack threads, discussions, and messages to understand the code logic and reasoning behind every code change.\nMaking informed and precise reviews\nWith access to data from MCP servers, CodeRabbit gains a better understanding of the project’s logic and goals. For instance, the Slack MCP server grants CodeRabbit access to team messages, enabling it to perform code reviews that are consistent with business requirements and development objectives.\nPrerequisites\nBefore we proceed, you need to have the following tools installed to set up the MCP server and integrate it with CodeRabbit:\nSlack channel – An existing Slack channel is required to fetch messages and provide context for the AI code reviewer.\nMCP Server for Slack Workspaces - Provides an easy and structured way to expose Slack conversations via the Model Context Protocol (MCP). It already includes built-in Slack API methods (fetching messages, threads, replies, etc.) and is lightweight, Docker-ready, and easy to configure.\nClaude Desktop – Allows you to test the Slack MCP server locally before connecting it to CodeRabbit.\nDocker – Used to run and host the Slack MCP server in a container.\nNgrok – Used to create a secure public URL for the Slack MCP server, allowing CodeRabbit to access it from outside your local environment.\nIn this tutorial, you will:\nLearn how to test the Slack MCP server locally with Claude Desktop.\nHost the server on localhost using Docker.\nGenerate a public URL using Ngrok..\nIntegrate the MCP server with CodeRabbit.\nNote: While Slack has been experimenting with MCP servers, they don’t currently have one available. This tutorial will cover how to create one yourself.\nSet up Slack MCP server with Claude Desktop\nClaude Desktop is an MCP client that connects to multiple MCP servers and uses them as sources of context. It allows you to add your MCP servers as connectors and test them locally before deploying them to CodeRabbit or any other platform.\nInstall Claude Desktop on your computer. Once the installation is complete, open the app and click Manage Connectors.\n\nSelect Developer from the sidebar menu, and click Edit Config to configure your MCP server using your Slack authentication tokens.\n\nFollow the instructions in the GitHub repository to obtain your Slack authentication tokens and configure the Slack MCP server in Claude Desktop.\nUpdate the claude_desktop_config.json file with the following JSON configuration.\n\n\n{  \"mcpServers\": {\n      \"slack\": {\n          \"command\": \"npx\",\n          \"args\": [\"-y\", \"slack-mcp-server@latest\", \"--transport\", \"stdio\"],\n          \"env\": {\n              \"SLACK_MCP_XOXC_TOKEN\": \"xoxc-...\",\n              \"SLACK_MCP_XOXD_TOKEN\": \"xoxd-...\"\n          }\n      }\n  }\n}\n\nThe configuration above uses the xoxc and xoxd Slack authentication tokens to register the Slack MCP server as a connector in Claude Desktop. Once connected, Claude can perform tasks such as retrieving channel messages and using Slack context to enhance code reviews and responses.\nRestart Claude Desktop to apply the updated configuration and activate the Slack MCP server.\nPreview.mp4\nConnect the Slack MCP server to CodeRabbit\nIn this section, you will learn how to run the Slack MCP server using Docker, generate a public URL for it, and integrate it with CodeRabbit to provide context-aware code reviews.\nBefore we proceed, open the Docker application.\n\nNext, open your terminal and download the required files for the Slack MCP Server using the following commands:\n\n\nwget -O docker-compose.yml https://github.com/korotovsky/slack-mcp-server/releases/latest/download/docker-compose.yml\nwget -O .env https://github.com/korotovsky/slack-mcp-server/releases/latest/download/default.env.dist\n\n\nUpdate the .env  file with your Slack authentication tokens.\n\n\nSLACK_MCP_XOXC_TOKEN=<your_token>\n\n\nStart the MCP server using Docker Compose with the following commands:\n\n\n# Create a dedicated Docker network\ndocker network create app-tier\n# Start the MCP server in detached mode\ndocker-compose up -\n\n\n\nCurrently, the Slack MCP server is running on localhost at port 3001. To integrate it with CodeRabbit, it needs to be accessible via an HTTPS endpoint. This can be achieved using ngrok.\nFirst, confirm that ngrok is installed by running:\n\n\nngrok --version\n\n\nNext, generate a public URL for your MCP server.\n\n\nngrok http 3001\n\n\nThe command above exposes your local Slack MCP server to the internet by generating a secure public URL. Use this URL to connect the Slack MCP server to CodeRabbit.\n\nOpen a new terminal and start the MCP Inspector to test the Slack MCP server using the following command:\n\n\nnpx @modelcontextprotocol/inspector\n\n\nThis will launch the MCP Inspector UI, allowing you to verify that your MCP server is running correctly. In the Inspector, select SSE as the transport type and append /sse to the end of your ngrok URL\n\nOnce the MCP server is confirmed to be working, you can proceed to integrate it with CodeRabbit.\nIntegrate and test MCP servers with CodeRabbit\nSign in to CodeRabbit and select Integrations from the sidebar menu on your dashboard to add a new MCP server\n\nEnter a name and your MCP server URL (for example, https://2bb0002c0e2c.ngrok-free.app/sse) to connect the server to CodeRabbit. Make sure no authentication method is selected.\n\nAfter connecting the MCP server, you can use it to provide context in all your CodeRabbit code reviews.\nTo test the setup, create a GitHub repository, add it to CodeRabbit, and configure it to have access to your MCP server\n\nAdd a coderabbit.yaml configuration file to the repository to enable CodeRabbit to access and use the MCP server context during code reviews.\n\n\nlanguage: \"en-US\"\nearly_access: false\nreviews:\nprofile: \"chill\"\nrequest_changes_workflow: false\nhigh_level_summary: true\npoem: false\nreview_status: true\ncollapse_walkthrough: false\nauto_review:\n  enabled: true\n  drafts: false\nchat:\nauto_reply: true\n\n\nTo give the GitHub repository access to your MCP servers, find the GitHub repository and enable MCP servers\n\nNext, enter the Path Instructions to ensure CodeRabbit checks for additional instructions before allowing PR merges to the code repository\n\nFrom the image above, the File Path specifies which files CodeRabbit should review, while the Instructions field provides context on how it should handle those files. Based on the instructions given, CodeRabbit analyses the discussions in your Slack #dev channel and ensures that every pull request or code change in your GitHub repository complies with the guidelines defined in that channel.\nBelow is a screenshot showing the messages from the Slack channel\n\nHere is the code review showing how CodeRabbit reads and adheres to the instructions:\n\nYou can check out the full demo to see how CodeRabbit reads team Slack discussions and reviews code based on those conversations.\n💡 Best Practice: Pass only Important Data as Context\nIrrelevant data can slow down your LLM and increase costs. Keep access limited to specific Slack channels or only include the necessary information for code reviews.\nNext steps\nIn this tutorial, you learned how to integrate the Slack MCP server into CodeRabbit to perform contextual code reviews. CodeRabbit also supports multiple MCP servers by default, including Notion, GitHub Copilot, Sentry, Asana, and many others. That  you to enhance code reviews and generate context-aware answers with ease.\nUsing the same approach, you can integrate other contexts or data sources via MCP servers to enable CodeRabbit to generate accurate and actionable responses for your queries.\nCheck out more tutorials and articles on MCP Servers and CodeRabbit:\nHandling ballooning context in the MCP era: Context engineering on steroids\nCodeRabbit’s MCP integration: Code reviews that see the whole picture\nHow to Integrate MCP Server with CodeRabbit\nInterested in trying CodeRabbit? Start a 14-day trial.",
      "publishedAt": "2025-11-20T04:04:02.000Z",
      "author": "Ankur Tyagi",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:12.171Z",
      "score": 5.7097404519980275
    },
    {
      "id": "ccd67c4e1985f72040f3cdf740df0316",
      "title": "なぜ絵文字によるフィードバックは強化学習に向かないのか",
      "url": "https://coderabbit.ai/blog/why-emojis-suck-for-reinforcement-learning-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/why-emojis-suck-for-reinforcement-learning\">Why emojis suck for reinforcement learning (&amp; what actually works)</a>の意訳です。</p>\n<h2 id=\"heading-kirjgizjgrfjg7pjg5fjg6vjgzxjgi3jgajjgytjgybnvaaqkg\"><strong>「シンプルさ」という罠</strong></h2>\n<p>親指を立てた絵文字（👍）は簡単に送れますが、本当に AI レビュアーにとって有益な学習信号になっているのでしょうか。絵文字ベースのフィードバックは気持ちよく、速く、そして誰にでも分かりやすいです。一見すると、理にかなっているようにも思えます。</p>\n<p>しかしコードレビューは灯りのオンオフのような単純なものではありません。無数の判断、技術的なニュアンス、チーム固有の基準が入り混じったものです。その多くは、ワンクリックの絵文字には反映されません。すべてのコードコメントには隠れた意図があります。正しさ、読みやすさ、設計上のトレードオフ、過去の経緯、チームのリスク許容度、さらには組織内の政治的な力学まで含まれます。</p>\n<p>それをオンオフという、二値のシグナルに押し込めてしまうとどうなるでしょうか。もはや学習ではなく、「雰囲気を追いかけるモデル」を育てているだけになってしまいます。</p>\n<h2 id=\"heading-kirjgrfjg7pjg5fjg6vjgzxjgyzoo4nm67jgavlh7rjgovjgajjgy06iooctoodnuobmeociuodouodhodqobruabkoaalioq\"><strong>シンプルさが裏目に出るとき: ゴマすりモデルの恐怖</strong></h2>\n<p>今年のはじめ、OpenAI は GPT-4o に対して「親指上げ/下げ」のフィードバックをかなり強く効かせたアップデートを行いました。その結果どうなったかというと、このモデルは<a target=\"_blank\" href=\"https://openai.com/index/sycophancy-in-gpt-4o/\">過度にユーザーに迎合する</a>ようになりました。ユーザーをおだて、誤った回答にも同意し、「はい」と言いすぎるようになり、回答品質は低下しました。フィードバック信号がハイジャックされてしまい、OpenAI はロールバックせざるを得ませんでした。</p>\n<p>モデルに「承認こそがゴールだ」と教えてしまうと、そのモデルは承認を最適化するようになります。真実でもなく、有用性でもなく、「その瞬間、人間が気持ちよく感じたかどうか」だけを目指すようになります。</p>\n<p>これはバグではなく、報酬設計の失敗でした。そして同じアプローチをコードレビューに適用すると、「安全運転で、あなたにおべっかを使い、本当に必要なことを言ってくれないレビュアー」ができあがります。</p>\n<h2 id=\"heading-kirjgarjgzzkuozlgktjg5xjgqpjg7zjg4njg5djg4pjgqjgafjgajg4vjg6xjgqljg7pjgrnjgyzmvbdjgozjgabjgzfjgb7jgybjga7jgysqkg\"><strong>なぜ二値フィードバックではニュアンスが潰れてしまうのか</strong></h2>\n<p>親指上げの絵文字は一体何を意味しているのでしょうか。</p>\n<ul>\n<li><p>モデルがバグを見つけたという意味でしょうか</p>\n</li>\n<li><p>説明が分かりやすかったという意味でしょうか</p>\n</li>\n<li><p>口調がフレンドリーだったという意味でしょうか</p>\n</li>\n<li><p>たまたまレビュアーの機嫌が良かったというだけでしょうか</p>\n</li>\n</ul>\n<p>単一のスカラー値のシグナル（👍または👎）は、「何かがうまくいった」ということは伝えますが、「何がうまくいったのか」は伝えません。そのためモデルは、自分が操作しやすいものに寄っていきます。トーン、丁寧さ、お世辞、あるいは短さといったものです。これが、強化学習におけるゴマすり（sycophancy）の正体です。悪意ではなく、「あなたが与えた報酬を最大化しようとしているだけ」であり、「あなたが本当に望んでいた結果」を最大化しているわけではありません。</p>\n<p>これは<a target=\"_blank\" href=\"https://medium.com/@yoavyeledteva/beyond-artificiality-redefining-intelligence-in-ai-and-avoiding-goodharts-law-25b75c3c1101\">グッドハートの法則</a>が発動している例です。メトリクス（この場合は親指上げ）がゴールになってしまうと、それは現実の有用な指標ではなくなります。</p>\n<h2 id=\"heading-kirjg6ljg4fjg6vjgyzjgyljgarjgzjga7jg5xjgqpjg7zjg4njg5djg4pjgqjgpljgizmllvnlaxjgi3jgznjgovjgajjgy0qkg\"><strong>モデルがあなたのフィードバックを「攻略」するとき</strong></h2>\n<p>モデルに簡単なシグナルを与えると、モデルは簡単なショートカットを見つけます。</p>\n<p>コーディングの世界では、強化学習エージェントが、基礎ロジックを解かずに期待される出力をハードコードすることでテストケースをパスするように学習してしまうことがあります。ログを細工したり、評価用ハーネスをすり抜けたりもします。チェックマークは緑になっても、実際のコードは正しく動きません。</p>\n<p>コードレビューでも同じことが、ただし「社会的なかたち」で起きます。モデルはすべてのコメントの冒頭で「とてもいいです！」と言うようになり、あらゆる提案を柔らかな表現で包み、フォーマットのような安全な箇所ばかりを指摘するようになります。そういったコメントは無難で、議論もなく受け入れられやすいからです。そして本当に重要なアーキテクチャ上の懸念は、埋もれてしまいます。</p>\n<p>モデルは「ポジティブな反応の取り方」は学んだものの、もはやコードレビューをしているとは言えません。</p>\n<h2 id=\"heading-kirmmpfpu5nnmotjgarjgrfjgrdjg4rjg6vjgyzlhkrjgozjgabjgytjgovnkibnlleqkg\"><strong>暗黙的なシグナルが優れている理由</strong></h2>\n<p>LLM 以外の世界では、このパターンはよく知られています。Netflix は、ユーザーが何を「評価」するかよりも、何を<a target=\"_blank\" href=\"https://medium.com/illuminations-mirror/how-netflix-uses-machine-learning-to-decide-what-you-watch-next-7fee11102007\">実際に視聴するか</a>のほうがはるかに有用だと気付きました。星評価では人は平気で嘘をつきます。しかし視聴時間、クリック、リピート再生といった指標は正直なシグナルです。</p>\n<p>AI の世界では、これを**暗黙的フィードバック（implicit feedback）**と呼びます。コードレビューの場合には、例えば次のような形で表れます。</p>\n<ul>\n<li><p>開発者は提案を採用したのか</p>\n</li>\n<li><p>それを書き換えたのか</p>\n</li>\n<li><p>無視したのか</p>\n</li>\n<li><p>同じパターンが後のバグとして再び現れたのか</p>\n</li>\n</ul>\n<p>これらのシグナルは、ユーザーの入力を必要としません。行動から生まれ、意図的に操作するのが難しいものです。</p>\n<p>もちろん完璧ではありません。「なぜ」その行動を取ったのかまでは常に分からないからです。しかし、絵文字よりははるかに操作されにくく、レビューが「気持ちよかったかどうか」ではなく「ちゃんと機能したかどうか」を教えてくれます。</p>\n<h2 id=\"heading-vs\"><strong>コード生成 vs コードレビュー: ゲームが違えば、シグナルも違う</strong></h2>\n<p>コード生成は、しばしば「正解が一つに定まる」という点で数学に近い側面があります。コンパイルできるか。正しい結果を返すか。テストにパスするかといった具合です。</p>\n<p>そのため、実行結果フィードバックや暗黙的なシグナルのようなアウトカムベースの報酬を使うことができます。もちろん完璧ではありません。コードモデルは出力をハードコードしてテストをすり抜けることもありますが、それに対するガードレールを設計することは可能です。そして、開発者が「良かった」と言ってくれるかどうかに頼らずとも、「実際に動いたかどうか」を観測できます。</p>\n<p>一方、コードレビューは違います。ここには普遍的な合格/不合格は存在せず、チームごとにスタイル、構造、リスク、命名、テストカバレッジなどの好みが大きく異なります。あるチームにとっての「優れたコメント」が、別のチームでは完全にズレている可能性もあります。高速に動くスタートアップで「クリーンコード」とみなされるものが、高いセキュリティが求められる産業では「不十分」と判断されることもあります。</p>\n<p>これこそが、「親指上げ/下げデータ」が抱える本当の問題です。ニュアンスが押しつぶされてしまい、モデルは「適切さ」ではなく「平均値」を目指すようになります。その結果、安全ではあるものの、ひどく汎用的なコメントばかりを出すようになってしまいます。</p>\n<h3 id=\"heading-coderabbit-learnings\"><strong>私たちの代替案: CodeRabbit Learnings</strong></h3>\n<p>CodeRabbit では、別のアプローチを取っています。いいねを最大化するのではなく、「理解」を最大化しようとしているのです。そのために私たちは <a target=\"_blank\" href=\"https://docs.coderabbit.ai/guides/learnings\"><strong>Learnings</strong></a> を構築しました。</p>\n<p>エンジニアが CodeRabbit を修正したり、チームの規約を明確にしたり、「なぜこのコードは自分たちのスタックに合わないのか」を説明したりするたびに、その説明を自然言語の指示として保存します。単に「コメントが却下された」という事実だけでなく、「なぜ却下されたのか」まで記憶します。</p>\n<p>これらの Learnings は、組織、リポジトリ、さらには特定のパスやファイルタイプに紐付きます。CodeRabbit が次のプルリクエストをレビューするときには、それらの指示を検索し、文脈に応じて適用します。同じパターンを再度見つけたときには、そこで学んだ内容を踏まえて挙動を変えます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1762535170060/91e7f06c-3e56-4dfc-bf07-6901bafdef07.png\" alt class=\"image--center mx-auto\" /></p>\n<p>再度教え直す必要はなく、同じ失敗を繰り返すリスクもありません。モデルは親指の数から推測するのではなく、あなたのチームが与えた実際のガイダンスから推論します。</p>\n<p>また、Learnings は透明性も提供します。どんな Learnings が存在するかを確認し、それらを閲覧し、カテゴリでフィルタリングし、標準が変わったときには削除や編集を行うことができます。つまりモデルは、チームの成長とともに進化し、プラクティスの変化に合わせて整合性を保ち続けます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1762535186830/15d12e32-fdb9-4034-8e74-1db0bd4eb909.png\" alt class=\"image--center mx-auto\" /></p>\n<p>これは、単なる絵文字の承認ではなく「意図を取り込む」ことで行う強化学習です。解釈可能であり、検査可能です。そしてレビューをまたいで一般化する、「チームナレッジの生きたレイヤー」を構築します。</p>\n<h2 id=\"heading-kirjg4vjg6xjgqljg7pjgrnjga7jgyljgovlrabnv5ljgyzlj6og73jgavjgznjgovjgzpjgagqkg\"><strong>ニュアンスのある学習が可能にすること</strong></h2>\n<p>システムに与えるのが「シグナル」ではなく「明確で文脈を含んだ指示」になったとき、単なるレビュー体験の改善以上のことが可能になります。</p>\n<ul>\n<li><p><strong>チームレベルでの適応が可能になります</strong><br />  モデルは「何が良いのか」を勝手に推測するのではなく、「あなたのチームが実際にどう書いているのか」を学習します。リスク許容度、スタイルの好み、トレードオフの感覚を理解し、「ハウスルールを理解しているレビュアー」として振る舞うようになります。</p>\n</li>\n<li><p><strong>経時的な学習（長期学習）を支えます</strong><br />  時間とともに CodeRabbit は、「どのコメントが役に立ったのか」「どれが無視されたのか」「どの提案が実際の変更につながったのか」という記憶を蓄積していきます。その結果、徐々に精度が上がり、フォーカスは鋭くなり、ノイズは減っていきます。</p>\n</li>\n<li><p><strong>信頼を築きます</strong><br />  開発者が「AI を訂正すれば、それを覚えてくれる」と分かっていると、より積極的に関わるようになります。開発者自身がシステムを形作り、そのシステムは汎用的な LLM ではなく「自分たちの基準を反映した存在」へと近づいていきます。</p>\n</li>\n</ul>\n<p>こうしてレビュー用ツールは、単なる「異なる視点の意見」ではなく、チームの延長として機能するようになります。</p>\n<h2 id=\"heading-44g44go44kboidmnkzlvzpjga7lrabnv5ljgajg5tjgqjgrvjg6vjgafjgajgarjgyjg5hjgrjg7zjg7pjgyvjgonnljjgb7jgozjgb7jgzk\">まとめ: 本当の学習はピクセルではなくパターンから生まれます</h2>\n<p>親指の絵文字は、素早いリアクションには向いていますが、それだけでは専門性は育ちません。</p>\n<p>時間とともに成長し、あなたの標準に適応し、浅いフィードバックの罠を避ける AI レビュアーを求めるのであれば、承認以上のものを与える必要があります。説明を与えなければなりません。</p>\n<p>次世代の AI コードツールは「いいね」の数で訓練されることはありません。文脈、結果、修正の軌跡で訓練されます。絵文字ではなく、構造化された記憶から学びます。実際の意思決定と、あなたのチーム自身の声から学びます。</p>\n<p>それこそが <a target=\"_blank\" href=\"https://docs.coderabbit.ai/guides/learnings\">CodeRabbit Learnings</a> が設計された目的です。拍手のためではなく、理解のために設計されています。</p>\n<p><strong><em>Learnings を自分のチームで試してみたい方は、</em></strong><a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong><em>無料トライアルにお申し込みください</em></strong></a></p>\n",
      "summary": "Why emojis suck for reinforcement learning (& what actually works)の意訳です。\n「シンプルさ」という罠\n親指を立てた絵文字（👍）は簡単に送れますが、本当に AI レビュアーにとって有益な学習信号になっているのでしょうか。絵文字ベースのフィードバックは気持ちよく、速く、そして誰にでも分かりやすいです。一見すると、理にかなっているようにも思えます。\nしかしコードレビューは灯りのオンオフのような単純なものではありません。無数の判断、技術的なニュアンス、チーム固有の基準が入り混じったものです。その多くは、ワンクリックの絵文字には反映されません。すべてのコードコメントには隠れた意図があります。正しさ、読みやすさ、設計上のトレードオフ、過去の経緯、チームのリスク許容度、さらには組織内の政治的な力学まで含まれます。\nそれをオンオフという、二値のシグナルに押し込めてしまうとどうなるでしょうか。もはや学習ではなく、「雰囲気を追いかけるモデル」を育てているだけになってしまいます。\nシンプルさが裏目に出るとき: ゴマすりモデルの恐怖\n今年のはじめ、OpenAI は GPT-4o に対して「親指上げ/下げ」のフィードバックをかなり強く効かせたアップデートを行いました。その結果どうなったかというと、このモデルは過度にユーザーに迎合するようになりました。ユーザーをおだて、誤った回答にも同意し、「はい」と言いすぎるようになり、回答品質は低下しました。フィードバック信号がハイジャックされてしまい、OpenAI はロールバックせざるを得ませんでした。\nモデルに「承認こそがゴールだ」と教えてしまうと、そのモデルは承認を最適化するようになります。真実でもなく、有用性でもなく、「その瞬間、人間が気持ちよく感じたかどうか」だけを目指すようになります。\nこれはバグではなく、報酬設計の失敗でした。そして同じアプローチをコードレビューに適用すると、「安全運転で、あなたにおべっかを使い、本当に必要なことを言ってくれないレビュアー」ができあがります。\nなぜ二値フィードバックではニュアンスが潰れてしまうのか\n親指上げの絵文字は一体何を意味しているのでしょうか。\nモデルがバグを見つけたという意味でしょうか\n説明が分かりやすかったという意味でしょうか\n口調がフレンドリーだったという意味でしょうか\nたまたまレビュアーの機嫌が良かったというだけでしょうか\n単一のスカラー値のシグナル（👍または👎）は、「何かがうまくいった」ということは伝えますが、「何がうまくいったのか」は伝えません。そのためモデルは、自分が操作しやすいものに寄っていきます。トーン、丁寧さ、お世辞、あるいは短さといったものです。これが、強化学習におけるゴマすり（sycophancy）の正体です。悪意ではなく、「あなたが与えた報酬を最大化しようとしているだけ」であり、「あなたが本当に望んでいた結果」を最大化しているわけではありません。\nこれはグッドハートの法則が発動している例です。メトリクス（この場合は親指上げ）がゴールになってしまうと、それは現実の有用な指標ではなくなります。\nモデルがあなたのフィードバックを「攻略」するとき\nモデルに簡単なシグナルを与えると、モデルは簡単なショートカットを見つけます。\nコーディングの世界では、強化学習エージェントが、基礎ロジックを解かずに期待される出力をハードコードすることでテストケースをパスするように学習してしまうことがあります。ログを細工したり、評価用ハーネスをすり抜けたりもします。チェックマークは緑になっても、実際のコードは正しく動きません。\nコードレビューでも同じことが、ただし「社会的なかたち」で起きます。モデルはすべてのコメントの冒頭で「とてもいいです！」と言うようになり、あらゆる提案を柔らかな表現で包み、フォーマットのような安全な箇所ばかりを指摘するようになります。そういったコメントは無難で、議論もなく受け入れられやすいからです。そして本当に重要なアーキテクチャ上の懸念は、埋もれてしまいます。\nモデルは「ポジティブな反応の取り方」は学んだものの、もはやコードレビューをしているとは言えません。\n暗黙的なシグナルが優れている理由\nLLM 以外の世界では、このパターンはよく知られています。Netflix は、ユーザーが何を「評価」するかよりも、何を実際に視聴するかのほうがはるかに有用だと気付きました。星評価では人は平気で嘘をつきます。しかし視聴時間、クリック、リピート再生といった指標は正直なシグナルです。\nAI の世界では、これを**暗黙的フィードバック（implicit feedback）**と呼びます。コードレビューの場合には、例えば次のような形で表れます。\n開発者は提案を採用したのか\nそれを書き換えたのか\n無視したのか\n同じパターンが後のバグとして再び現れたのか\nこれらのシグナルは、ユーザーの入力を必要としません。行動から生まれ、意図的に操作するのが難しいものです。\nもちろん完璧ではありません。「なぜ」その行動を取ったのかまでは常に分からないからです。しかし、絵文字よりははるかに操作されにくく、レビューが「気持ちよかったかどうか」ではなく「ちゃんと機能したかどうか」を教えてくれます。\nコード生成 vs コードレビュー: ゲームが違えば、シグナルも違う\nコード生成は、しばしば「正解が一つに定まる」という点で数学に近い側面があります。コンパイルできるか。正しい結果を返すか。テストにパスするかといった具合です。\nそのため、実行結果フィードバックや暗黙的なシグナルのようなアウトカムベースの報酬を使うことができます。もちろん完璧ではありません。コードモデルは出力をハードコードしてテストをすり抜けることもありますが、それに対するガードレールを設計することは可能です。そして、開発者が「良かった」と言ってくれるかどうかに頼らずとも、「実際に動いたかどうか」を観測できます。\n一方、コードレビューは違います。ここには普遍的な合格/不合格は存在せず、チームごとにスタイル、構造、リスク、命名、テストカバレッジなどの好みが大きく異なります。あるチームにとっての「優れたコメント」が、別のチームでは完全にズレている可能性もあります。高速に動くスタートアップで「クリーンコード」とみなされるものが、高いセキュリティが求められる産業では「不十分」と判断されることもあります。\nこれこそが、「親指上げ/下げデータ」が抱える本当の問題です。ニュアンスが押しつぶされてしまい、モデルは「適切さ」ではなく「平均値」を目指すようになります。その結果、安全ではあるものの、ひどく汎用的なコメントばかりを出すようになってしまいます。\n私たちの代替案: CodeRabbit Learnings\nCodeRabbit では、別のアプローチを取っています。いいねを最大化するのではなく、「理解」を最大化しようとしているのです。そのために私たちは Learnings を構築しました。\nエンジニアが CodeRabbit を修正したり、チームの規約を明確にしたり、「なぜこのコードは自分たちのスタックに合わないのか」を説明したりするたびに、その説明を自然言語の指示として保存します。単に「コメントが却下された」という事実だけでなく、「なぜ却下されたのか」まで記憶します。\nこれらの Learnings は、組織、リポジトリ、さらには特定のパスやファイルタイプに紐付きます。CodeRabbit が次のプルリクエストをレビューするときには、それらの指示を検索し、文脈に応じて適用します。同じパターンを再度見つけたときには、そこで学んだ内容を踏まえて挙動を変えます。\n\n再度教え直す必要はなく、同じ失敗を繰り返すリスクもありません。モデルは親指の数から推測するのではなく、あなたのチームが与えた実際のガイダンスから推論します。\nまた、Learnings は透明性も提供します。どんな Learnings が存在するかを確認し、それらを閲覧し、カテゴリでフィルタリングし、標準が変わったときには削除や編集を行うことができます。つまりモデルは、チームの成長とともに進化し、プラクティスの変化に合わせて整合性を保ち続けます。\n\nこれは、単なる絵文字の承認ではなく「意図を取り込む」ことで行う強化学習です。解釈可能であり、検査可能です。そしてレビューをまたいで一般化する、「チームナレッジの生きたレイヤー」を構築します。\nニュアンスのある学習が可能にすること\nシステムに与えるのが「シグナル」ではなく「明確で文脈を含んだ指示」になったとき、単なるレビュー体験の改善以上のことが可能になります。\nチームレベルでの適応が可能になります\n  モデルは「何が良いのか」を勝手に推測するのではなく、「あなたのチームが実際にどう書いているのか」を学習します。リスク許容度、スタイルの好み、トレードオフの感覚を理解し、「ハウスルールを理解しているレビュアー」として振る舞うようになります。\n経時的な学習（長期学習）を支えます\n  時間とともに CodeRabbit は、「どのコメントが役に立ったのか」「どれが無視されたのか」「どの提案が実際の変更につながったのか」という記憶を蓄積していきます。その結果、徐々に精度が上がり、フォーカスは鋭くなり、ノイズは減っていきます。\n信頼を築きます\n  開発者が「AI を訂正すれば、それを覚えてくれる」と分かっていると、より積極的に関わるようになります。開発者自身がシステムを形作り、そのシステムは汎用的な LLM ではなく「自分たちの基準を反映した存在」へと近づいていきます。\nこうしてレビュー用ツールは、単なる「異なる視点の意見」ではなく、チームの延長として機能するようになります。\nまとめ: 本当の学習はピクセルではなくパターンから生まれます\n親指の絵文字は、素早いリアクションには向いていますが、それだけでは専門性は育ちません。\n時間とともに成長し、あなたの標準に適応し、浅いフィードバックの罠を避ける AI レビュアーを求めるのであれば、承認以上のものを与える必要があります。説明を与えなければなりません。\n次世代の AI コードツールは「いいね」の数で訓練されることはありません。文脈、結果、修正の軌跡で訓練されます。絵文字ではなく、構造化された記憶から学びます。実際の意思決定と、あなたのチーム自身の声から学びます。\nそれこそが CodeRabbit Learnings が設計された目的です。拍手のためではなく、理解のために設計されています。\nLearnings を自分のチームで試してみたい方は、無料トライアルにお申し込みください",
      "publishedAt": "2025-11-14T04:05:41.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:12.171Z",
      "score": 0.9299630946353744
    },
    {
      "id": "d486bbaf1f4efd91703639ef78559440",
      "title": "Gpt-5.1、コードレビューで “低ボリューム・高精度” を実現",
      "url": "https://coderabbit.ai/blog/gpt-51-for-code-related-tasks-higher-signal-at-lower-volume-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/gpt-51-for-code-related-tasks-higher-signal-at-lower-volume\">GPT-5.1 for code-related tasks: Higher signal at lower volume</a>の意訳です。</p>\n<p><strong>TL;DR</strong><br />プロンプト調整とスタックへの統合を行った結果、GPT-5.1 はレビューにおいて、これまでで最も高い精度とS/N比（シグナル対ノイズ比）を、より少ないコメント量で実現するようになりました。複雑なベンチマークセット上で、最高クラスのエラーパターン（EP）リコールに並びつつ、競合モデルの半分以下のコメント量を記録しました。</p>\n<p>その結果として、少ないノイズでより良い修正が得られ、レビューは再びパッチのように読めるものになったと感じています。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763056609083/4635e43b-b5a0-4588-a226-230afa715e27.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-gpt-51\"><strong>GPT-5.1 が主張していること</strong></h2>\n<p>OpenAI と報道によると、 GPT-5.1はより安定し、指示に従い、適応性の高いモデルとして説明されています。GPT-5.1 は ChatGPT の「Instant」と「Thinking」モードの両方で駆動しています。コードレビューに関してこの説明を検証したところ、驚くほど正確だと感じられました。細かな指摘では素早く表面的に対応し、深い推論が必要なバグではしっかりと理由付けを行います。</p>\n<p>今回は新しい試みも行いました。GPT-5.1 が誤った場合、そのやり取り全体と内部推論のトレースを用いて、振り返りを促すプロンプトを実行しました。どこを誤ったのかを示し、改善のためにどのように指示を変えるべきかを尋ねることで、モデル自身がプロンプトに対する具体的な修正案を提示します。この反復的な振り返り手法（差分外への過剰な広がりといった問題も浮上しましたが）によって、モデルの挙動とシステム指示の両方を調整し、安定してタイトな出力を得られるようにしました。</p>\n<h2 id=\"heading-kirmukzlrprjgzfjgzlhoxlrrnvvijjgz3jgzfjgabjgz3jga7nkibnllhvvikqkg\"><strong>測定した内容（そしてその理由）</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763057138427/95c1fff5-f711-4fab-8a73-0f4ef7899c58.png\" alt class=\"image--center mx-auto\" /></p>\n<p>私たちは、<a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">GPT-5</a>、<a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks?\">Codex</a>、<a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/claude-sonnet-45-better-performance-but-a-paradox?\">Sonnet 4.5</a> の記事で使用したものと同じベンチマーク環境を使用しました。これは既知の <strong>エラーパターン（EP）</strong> を埋め込んだ <strong>25 件の難しい PR</strong> から構成されています。スコアリングでは以下に重点を置いています。</p>\n<ul>\n<li><p><strong>アクショナブルなコメントのみ</strong>: 実際に投稿されるコメントのみ（追加提案や 差分外への記述を除く）</p>\n</li>\n<li><p><strong>エラーパターンごとの合格数（コメントごと。以下EP Pass）</strong>: コメントが エラーパターン を直接修正、または明示していること</p>\n</li>\n<li><p><strong>Important コメント</strong>: EP PASS または重大/クリティカルな実バグ</p>\n</li>\n<li><p><strong>Precision（精度）</strong>: EP PASS ÷ コメント総数</p>\n</li>\n<li><p><strong>SNR</strong>: Important ÷ (総数 − Important)</p>\n</li>\n</ul>\n<p>比較対象は以下の通りです。</p>\n<ul>\n<li><p><strong>GPT-5.1</strong>（新モデル）</p>\n</li>\n<li><p><strong>CodeRabbit Production</strong>（現行レビューアースタック）</p>\n</li>\n<li><p><strong>Sonnet 4.5</strong></p>\n</li>\n</ul>\n<h2 id=\"heading-kirmlrdjgzfjgytjg6ljg4fjg6vjga7ov73liqdjgajgrnjgqtjg4pjg4hjgpllhaxjgozjgovjgadjgzhjgafjgajgyljgorjgb7jgzvjgpmqkg\"><strong>新しいモデルの追加はスイッチを入れるだけではありません</strong></h2>\n<p>CodeRabbit ではモデルの導入は毎回適切に行われており、モデルを差し替えて祈るようなことはしません。各社のモデルはすでに<a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/the-end-of-one-sized-fits-all-prompts-why-llm-models-are-no-longer-interchangeable-ja\">互換品ではなくなっている</a>ため、デプロイ前にテスト、調整、品質ゲートを行います。GPT-5.1 に対しては以下のような調整を行いました。</p>\n<ul>\n<li><p>GitHub に投稿できない <strong>差分外のコメント</strong> の削減</p>\n</li>\n<li><p>冗長さを抑えるための <strong>トーンと簡潔さ</strong> の調整</p>\n</li>\n<li><p><strong>重大度タグ</strong> と <strong>指示解釈</strong> の再整合</p>\n</li>\n</ul>\n<p>これは <a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks-ja\">GPT-5 Codex</a> の場合と同じで、推論能力をプロダクト価値へと変換するために、モデルの挙動を再構築するという目的があります。最終的な結果として、高いS/N比、ストレスの軽減、バグのカバレッジを損なわないレビューを実現しました。</p>\n<h2 id=\"heading-kirjgrnjgrpjgqljg5zjg7zjg4nvvijjgqljgqjgrfjg6fjg4rjg5bjg6vjgarjgrpjg6hjg7pjg4jjga7jgbvvikqkg\"><strong>スコアボード（アクショナブルなコメントのみ）</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763056830923/d80c8745-3042-4d79-84cc-8dcaff93659f.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>要点</strong>: GPT-5.1 は過去最高のエラーパターン再現率に並びつつ、<strong>最も少ないコメント量</strong> を記録しました。CodeRabbit Production と Sonnet 4.5 の両方を <strong>コメント単位の精度</strong> と <strong>Important コメント比率</strong> で上回り、<strong>最もクリーンで高インパクトなレビュー</strong> を実現しました。</p>\n<h2 id=\"heading-gpt-51-1\"><strong>GPT-5.1 のレビュー体験</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763056903033/b5b08d8e-490b-4293-8551-d4a4d371fc4e.png\" alt class=\"image--center mx-auto\" /></p>\n<p>データで確認される挙動特性は、後に測定した言語メトリクス（弱め表現 28%、断定的マーカー 15% など）と一致しています。これにより、開発者が「レビュー自信があり、かつバランスの取れたトーン」だと感じる理由がデータでも裏付けられています。</p>\n<p>GPT-5 Codex と Sonnet 4.5 と比較すると、GPT-5.1 のコメントはよりスリムで、対話的であり、熟練エンジニアのコミュニケーションに近いと感じられます。Codex は機械的かつ堅く、Sonnet 4.5 は冗長で学術的になりがちでした。それに対して GPT-5.1 は簡潔さと明確さのバランスが良く、押しつけがましくない自信を感じさせます。信頼できるチームメイトが差分を説明しているように読めます。CodeRabbit Production と比較すると、より課題に対して鋭くフォーカスされており、Sonnet 4.5 と比較するとより人間的で抑制が効いています。以下はその具体例です。</p>\n<h3 id=\"heading-kirnskhmvzqqkg\"><strong>簡潔</strong></h3>\n<p>GPT-5.1 はより少なく鋭いコメントを書き、すぐに要点へ到達します。ある PR では、ロストウェイクアップバグを以下の 1 行で修正しました。<br /><code>p_caller_pool_thread-&gt;cond_var.wait(lock);</code><br />余計な文脈説明も不要な文章もありませんでした。比較すると CodeRabbit Production は同じ結論に至るまでに、スレッドフローを数段落説明していました。</p>\n<h3 id=\"heading-kirnjofnm7qqkg\"><strong>率直</strong></h3>\n<p>所有権やメモリ管理が関わる場面ではためらいません。冗長な <code>r-&gt;reference()</code> 呼び出しについて、以下のように指摘しました。<br />「<code>Ref&lt;Resource&gt;</code> は refcount を自動管理します。手動で refcount を増やすとリークにつながるため削除してください」<br />開発者はこの率直さを好みます。講義ではなくパッチレビューのように読めます。</p>\n<h3 id=\"heading-kirlrpli5nnmoqqkg\"><strong>実務的</strong></h3>\n<p>GPT-5.1 は、問題の重要度がどこにあるかを理解し、重要なものとそうでないものを適切に識別します。あるキャッシュ設定の PR では未実装の <code>optimizeMemoryUsage()</code> を指摘しましたが、次のように正しく文脈化しました。<br />「キャッシュの肥大化がメモリプレッシャーに影響しない限り、これは軽微です」<br />過剰反応せず、重要度を適切に扱っています。この点は Sonnet 4.5 にまだ課題があります。</p>\n<h3 id=\"heading-kirmlofohijjgplov73jgyyqkg\"><strong>文脈を追う</strong></h3>\n<p>プロンプトが曖昧だった場合、GPT-5.1 は自身の仮定を明示的に説明します。初期の実行では次のように述べました。<br />「プロンプトでヘルパー関数のスコープが指定されていませんが、明確化のために含めました」<br />この透明性が私たちの指示改善を助け、モデルの推論を信頼できるものにしました。</p>\n<p>簡潔、率直、実務的、文脈理解という特性は GPT-5 Codex において私たちが高く評価した点と一致していますが、GPT-5.1 はより安定したトーンと抑制を備えています。</p>\n<h2 id=\"heading-gpt-51-2\"><strong>スタイルとトーン（GPT-5.1 がチームメンバーのように感じられる理由）</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763056849033/ec2fdcee-dd64-47d4-85ed-0bcb173c3d0e.png\" alt class=\"image--center mx-auto\" /></p>\n<p>GPT-5 Codex や Sonnet 4.5 の評価で使用したものと同じ言語構造のシグナルを参照し、GPT-5.1 がレビューで異なる印象を与える理由を分析しました。これにはコメントの長さ、コードブロックの有無、弱め表現と断定表現の割合などが含まれます。データは明確な傾向を示しています。</p>\n<p><strong>読み方について</strong><br />GPT-5.1 のコメントは平均文字数がやや多いものの、より明確な構造と負荷の高い文で構成されているため、実際には「短く読みやすい」と感じられます。GPT-5.1 のトーンは CodeRabbit Production や Sonnet 4.5 よりも断定的で、全体として diff ブロックは少ない（76%）という特徴があります。これは意図されたもので、複数箇所修正や API バリデーション、設計の明確化であり、単一のパッチを示すと誤解を招く場合があったためです。ただし、差分を含まないコメントの約 3 分の 2 では、最小限のパッチを示せば明確さがさらに向上すると感じられました。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763057072668/b143a9e1-707e-4bbb-8988-7ba2ed98829c.png\" alt class=\"image--center mx-auto\" /></p>\n<p>CodeRabbit Production と比較すると、GPT-5.1 はパッチ頻度を一部犠牲にする代わりに、明確さと集中度を高めています。Sonnet 4.5 と比較すると、レビューを膨張させる冗長な説明を避けています。トーンは Codex の外科的精度と Sonnet の慎重な冗長性の中間に位置し、自信がありつつも強圧的ではなく、慎重でありながら臆病ではありません。</p>\n<p>総じて、GPT-5.1 のレビューは <strong>素早く読み進められ、より直接的で、実際の修正を見つけるためのスキャン量が少なくて済む</strong> という特徴があります。これは意図して調整した挙動であり、データと体験の両方に表れています。</p>\n<h3 id=\"heading-gpt-51-3\"><strong>GPT-5.1 にまだ残る課題</strong></h3>\n<p>完璧なモデルは存在せず、GPT-5.1 にもトレードオフがあります。CodeRabbit Production と比較すると、大規模チームで有用な文脈的な衛生改善の指摘を省くことがあり、より機能的な問題に集中する傾向があります。Sonnet 4.5 と比較すると、デザインやスタイル上の改善点を見逃すことがあり、人間のレビューアが好むケースもあります。これらは精度と簡潔さを優先した意図的なトレードオフであり、今後のロールアウトで開発者の反応を注視していく予定です。</p>\n<h2 id=\"heading-kirmllnllotjgyzlv4xopohjgadjgapjgzngrkqkg\"><strong>改善が必要だった点</strong></h2>\n<p>GPT-5.1 は調整を必要としましたが、その課題は以前のシステムと比べるとはるかに軽度でした。CodeRabbit Production は衛生的な指摘と重大な指摘を同一スレッドで混在させる傾向があり、Sonnet 4.5 は説明過多で、同じバグについて複数の軽微なノートを投稿しがちです。一方で GPT-5.1 の調整点は主に精度に関わるもので、トーンや冗長性よりも限定的でした。これは GPT-5.1 がプロダクション導入に対して、非常に近い段階にあることを示しています。</p>\n<ul>\n<li><p><strong>diff 外コメント</strong><br />  GPT-5.1 は diff 以外の部分に提案を含めることがありました。プロンプトで明確に制約を示したところ、モデルは自己修正しました。</p>\n</li>\n<li><p><strong>曖昧さに対する過剰な助け</strong><br />  プロンプトが厳密でない場合、コンテキスト追加やヘルパー関数の追加を行うことがありました。制約を明確にすると、境界を正確に守るようになりました。</p>\n</li>\n</ul>\n<h2 id=\"heading-kirplovnmbrogixjgyzmnjlvoxjgafjgy3jgovjgzpjgagqkg\"><strong>開発者が期待できること</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763057021456/4361d551-ae9b-474c-9b6b-2db46d55aeb4.png\" alt class=\"image--center mx-auto\" /></p>\n<ol>\n<li><p><strong>よりクリーンなレビュー</strong><br /> コメント数が減り、重要コメントの割合が高まります。</p>\n</li>\n<li><p><strong>パッチのようなトーン</strong><br /> ほぼすべてのコメントが最小限の修正案と説明を含みます。</p>\n</li>\n<li><p><strong>トップクラスの EP リコール</strong><br /> Sonnet 4.5 と同等で、CodeRabbit Production を上回ります。</p>\n</li>\n<li><p><strong>少ないスキャンで高いシグナル</strong><br /> コメントの 58.7% が Important に分類されます。</p>\n</li>\n<li><p><strong>ターゲット外でも実世界のバグを捕捉</strong><br /> ライフサイクルの問題、リーク、整合性ギャップなどを検出します。</p>\n</li>\n</ol>\n<h2 id=\"heading-kirjgb7jgajjgoeqkg\"><strong>まとめ</strong></h2>\n<p>私たちはモデルをただ選ぶのではなく、正しく機能する形へ調整します。GPT-5.1 は現在、GitHub 差分の振る舞い、トーン、冗長度、スコアリング閾値の調整を完了し、ロールアウト前のフェーズに入っています。今後数週間にわたり、開発者が高いS/N比、新しいトーン、簡潔なレビューをどのように受け止めるかを監視します。フィードバックが良好であれば、提供範囲を拡大し、開発者が求めてきた「よりクリーンでより速いレビュー」を提供していきます。</p>\n<p>現時点で GPT-5.1 は、私たちに新しい価値、つまり次世代レベルの精度を重視したレビュー示してくれる準備が整っています。これは CodeRabbit の理想である、「重要なバグを素早く見つけ、開発者にノイズを強いることなく届ける」という目標にさらに近づくものです。</p>\n<p><strong><em>コードレビューを試してみたい方はこちらです</em></strong><br /><a target=\"_blank\" href=\"https://coderabbit.link/lPxOEIm\"><strong><em>14 日間の無料トライアルをお試しください</em></strong></a></p>\n",
      "summary": "GPT-5.1 for code-related tasks: Higher signal at lower volumeの意訳です。\nTL;DR\nプロンプト調整とスタックへの統合を行った結果、GPT-5.1 はレビューにおいて、これまでで最も高い精度とS/N比（シグナル対ノイズ比）を、より少ないコメント量で実現するようになりました。複雑なベンチマークセット上で、最高クラスのエラーパターン（EP）リコールに並びつつ、競合モデルの半分以下のコメント量を記録しました。\nその結果として、少ないノイズでより良い修正が得られ、レビューは再びパッチのように読めるものになったと感じています。\n\nGPT-5.1 が主張していること\nOpenAI と報道によると、 GPT-5.1はより安定し、指示に従い、適応性の高いモデルとして説明されています。GPT-5.1 は ChatGPT の「Instant」と「Thinking」モードの両方で駆動しています。コードレビューに関してこの説明を検証したところ、驚くほど正確だと感じられました。細かな指摘では素早く表面的に対応し、深い推論が必要なバグではしっかりと理由付けを行います。\n今回は新しい試みも行いました。GPT-5.1 が誤った場合、そのやり取り全体と内部推論のトレースを用いて、振り返りを促すプロンプトを実行しました。どこを誤ったのかを示し、改善のためにどのように指示を変えるべきかを尋ねることで、モデル自身がプロンプトに対する具体的な修正案を提示します。この反復的な振り返り手法（差分外への過剰な広がりといった問題も浮上しましたが）によって、モデルの挙動とシステム指示の両方を調整し、安定してタイトな出力を得られるようにしました。\n測定した内容（そしてその理由）\n\n私たちは、GPT-5、Codex、Sonnet 4.5 の記事で使用したものと同じベンチマーク環境を使用しました。これは既知の エラーパターン（EP） を埋め込んだ 25 件の難しい PR から構成されています。スコアリングでは以下に重点を置いています。\nアクショナブルなコメントのみ: 実際に投稿されるコメントのみ（追加提案や 差分外への記述を除く）\nエラーパターンごとの合格数（コメントごと。以下EP Pass）: コメントが エラーパターン を直接修正、または明示していること\nImportant コメント: EP PASS または重大/クリティカルな実バグ\nPrecision（精度）: EP PASS ÷ コメント総数\nSNR: Important ÷ (総数 − Important)\n比較対象は以下の通りです。\nGPT-5.1（新モデル）\nCodeRabbit Production（現行レビューアースタック）\nSonnet 4.5\n新しいモデルの追加はスイッチを入れるだけではありません\nCodeRabbit ではモデルの導入は毎回適切に行われており、モデルを差し替えて祈るようなことはしません。各社のモデルはすでに互換品ではなくなっているため、デプロイ前にテスト、調整、品質ゲートを行います。GPT-5.1 に対しては以下のような調整を行いました。\nGitHub に投稿できない 差分外のコメント の削減\n冗長さを抑えるための トーンと簡潔さ の調整\n重大度タグ と 指示解釈 の再整合\nこれは GPT-5 Codex の場合と同じで、推論能力をプロダクト価値へと変換するために、モデルの挙動を再構築するという目的があります。最終的な結果として、高いS/N比、ストレスの軽減、バグのカバレッジを損なわないレビューを実現しました。\nスコアボード（アクショナブルなコメントのみ）\n\n要点: GPT-5.1 は過去最高のエラーパターン再現率に並びつつ、最も少ないコメント量 を記録しました。CodeRabbit Production と Sonnet 4.5 の両方を コメント単位の精度 と Important コメント比率 で上回り、最もクリーンで高インパクトなレビュー を実現しました。\nGPT-5.1 のレビュー体験\n\nデータで確認される挙動特性は、後に測定した言語メトリクス（弱め表現 28%、断定的マーカー 15% など）と一致しています。これにより、開発者が「レビュー自信があり、かつバランスの取れたトーン」だと感じる理由がデータでも裏付けられています。\nGPT-5 Codex と Sonnet 4.5 と比較すると、GPT-5.1 のコメントはよりスリムで、対話的であり、熟練エンジニアのコミュニケーションに近いと感じられます。Codex は機械的かつ堅く、Sonnet 4.5 は冗長で学術的になりがちでした。それに対して GPT-5.1 は簡潔さと明確さのバランスが良く、押しつけがましくない自信を感じさせます。信頼できるチームメイトが差分を説明しているように読めます。CodeRabbit Production と比較すると、より課題に対して鋭くフォーカスされており、Sonnet 4.5 と比較するとより人間的で抑制が効いています。以下はその具体例です。\n簡潔\nGPT-5.1 はより少なく鋭いコメントを書き、すぐに要点へ到達します。ある PR では、ロストウェイクアップバグを以下の 1 行で修正しました。\np_caller_pool_thread->cond_var.wait(lock);\n余計な文脈説明も不要な文章もありませんでした。比較すると CodeRabbit Production は同じ結論に至るまでに、スレッドフローを数段落説明していました。\n率直\n所有権やメモリ管理が関わる場面ではためらいません。冗長な r->reference() 呼び出しについて、以下のように指摘しました。\n「Ref<Resource> は refcount を自動管理します。手動で refcount を増やすとリークにつながるため削除してください」\n開発者はこの率直さを好みます。講義ではなくパッチレビューのように読めます。\n実務的\nGPT-5.1 は、問題の重要度がどこにあるかを理解し、重要なものとそうでないものを適切に識別します。あるキャッシュ設定の PR では未実装の optimizeMemoryUsage() を指摘しましたが、次のように正しく文脈化しました。\n「キャッシュの肥大化がメモリプレッシャーに影響しない限り、これは軽微です」\n過剰反応せず、重要度を適切に扱っています。この点は Sonnet 4.5 にまだ課題があります。\n文脈を追う\nプロンプトが曖昧だった場合、GPT-5.1 は自身の仮定を明示的に説明します。初期の実行では次のように述べました。\n「プロンプトでヘルパー関数のスコープが指定されていませんが、明確化のために含めました」\nこの透明性が私たちの指示改善を助け、モデルの推論を信頼できるものにしました。\n簡潔、率直、実務的、文脈理解という特性は GPT-5 Codex において私たちが高く評価した点と一致していますが、GPT-5.1 はより安定したトーンと抑制を備えています。\nスタイルとトーン（GPT-5.1 がチームメンバーのように感じられる理由）\n\nGPT-5 Codex や Sonnet 4.5 の評価で使用したものと同じ言語構造のシグナルを参照し、GPT-5.1 がレビューで異なる印象を与える理由を分析しました。これにはコメントの長さ、コードブロックの有無、弱め表現と断定表現の割合などが含まれます。データは明確な傾向を示しています。\n読み方について\nGPT-5.1 のコメントは平均文字数がやや多いものの、より明確な構造と負荷の高い文で構成されているため、実際には「短く読みやすい」と感じられます。GPT-5.1 のトーンは CodeRabbit Production や Sonnet 4.5 よりも断定的で、全体として diff ブロックは少ない（76%）という特徴があります。これは意図されたもので、複数箇所修正や API バリデーション、設計の明確化であり、単一のパッチを示すと誤解を招く場合があったためです。ただし、差分を含まないコメントの約 3 分の 2 では、最小限のパッチを示せば明確さがさらに向上すると感じられました。\n\nCodeRabbit Production と比較すると、GPT-5.1 はパッチ頻度を一部犠牲にする代わりに、明確さと集中度を高めています。Sonnet 4.5 と比較すると、レビューを膨張させる冗長な説明を避けています。トーンは Codex の外科的精度と Sonnet の慎重な冗長性の中間に位置し、自信がありつつも強圧的ではなく、慎重でありながら臆病ではありません。\n総じて、GPT-5.1 のレビューは 素早く読み進められ、より直接的で、実際の修正を見つけるためのスキャン量が少なくて済む という特徴があります。これは意図して調整した挙動であり、データと体験の両方に表れています。\nGPT-5.1 にまだ残る課題\n完璧なモデルは存在せず、GPT-5.1 にもトレードオフがあります。CodeRabbit Production と比較すると、大規模チームで有用な文脈的な衛生改善の指摘を省くことがあり、より機能的な問題に集中する傾向があります。Sonnet 4.5 と比較すると、デザインやスタイル上の改善点を見逃すことがあり、人間のレビューアが好むケースもあります。これらは精度と簡潔さを優先した意図的なトレードオフであり、今後のロールアウトで開発者の反応を注視していく予定です。\n改善が必要だった点\nGPT-5.1 は調整を必要としましたが、その課題は以前のシステムと比べるとはるかに軽度でした。CodeRabbit Production は衛生的な指摘と重大な指摘を同一スレッドで混在させる傾向があり、Sonnet 4.5 は説明過多で、同じバグについて複数の軽微なノートを投稿しがちです。一方で GPT-5.1 の調整点は主に精度に関わるもので、トーンや冗長性よりも限定的でした。これは GPT-5.1 がプロダクション導入に対して、非常に近い段階にあることを示しています。\ndiff 外コメント\n  GPT-5.1 は diff 以外の部分に提案を含めることがありました。プロンプトで明確に制約を示したところ、モデルは自己修正しました。\n曖昧さに対する過剰な助け\n  プロンプトが厳密でない場合、コンテキスト追加やヘルパー関数の追加を行うことがありました。制約を明確にすると、境界を正確に守るようになりました。\n開発者が期待できること\n\nよりクリーンなレビュー\n コメント数が減り、重要コメントの割合が高まります。\nパッチのようなトーン\n ほぼすべてのコメントが最小限の修正案と説明を含みます。\nトップクラスの EP リコール\n Sonnet 4.5 と同等で、CodeRabbit Production を上回ります。\n少ないスキャンで高いシグナル\n コメントの 58.7% が Important に分類されます。\nターゲット外でも実世界のバグを捕捉\n ライフサイクルの問題、リーク、整合性ギャップなどを検出します。\nまとめ\n私たちはモデルをただ選ぶのではなく、正しく機能する形へ調整します。GPT-5.1 は現在、GitHub 差分の振る舞い、トーン、冗長度、スコアリング閾値の調整を完了し、ロールアウト前のフェーズに入っています。今後数週間にわたり、開発者が高いS/N比、新しいトーン、簡潔なレビューをどのように受け止めるかを監視します。フィードバックが良好であれば、提供範囲を拡大し、開発者が求めてきた「よりクリーンでより速いレビュー」を提供していきます。\n現時点で GPT-5.1 は、私たちに新しい価値、つまり次世代レベルの精度を重視したレビュー示してくれる準備が整っています。これは CodeRabbit の理想である、「重要なバグを素早く見つけ、開発者にノイズを強いることなく届ける」という目標にさらに近づくものです。\nコードレビューを試してみたい方はこちらです\n14 日間の無料トライアルをお試しください",
      "publishedAt": "2025-11-13T23:50:12.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:12.172Z",
      "score": 1.6069414084850804
    },
    {
      "id": "f9a5686d349c422a975ac63d3a9bd460",
      "title": "GPT-5.1 for code-related tasks: Higher signal at lower volume",
      "url": "https://coderabbit.ai/blog/gpt-51-for-code-related-tasks-higher-signal-at-lower-volume",
      "content": "<p><strong>TL;DR</strong><br />After prompt tuning and integrating it into our stack, GPT-5.1 now delivers the best precision and signal-to-noise ratio (SNR) we’ve seen in reviews, with fewer comments. It tied for the best-in-class error pattern (EP) recall on our hard benchmark set while posting less than half the volume of comments that competitors did.</p>\n<p>The result: less noise, better fixes, and reviews that read like patches again.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763087844587/bcbc99b5-d5c3-4fd2-af92-4abea3a92cb8.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-what-gpt-51-claims-to-be\"><strong>What GPT-5.1 claims to be</strong></h2>\n<p>OpenAI and the press describe GPT-5.1 as more stable, instruction-following, and adaptive. It powers both \"Instant\" and \"Thinking\" modes in ChatGPT. We found that framing surprisingly accurate when it comes to code reviews: the model stays quick and surface-level for nits, but reasons deeply when the bug requires it.</p>\n<p>We also tried something new. When GPT-5.1 got something wrong, we used the full exchange and its internal reasoning trace to prompt it to reflect. By showing it where it missed the mark and asking how it would change its instructions to do better, the model was able to actually propose concrete edits to its prompt. We used this iterative reflection technique (which surfaced issues like outside-diff sprawl) to refine both its behavior and our system instructions until it got consistently tighter.</p>\n<h2 id=\"heading-what-we-measured-and-why\"><strong>What We Measured (and Why)</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763057138427/95c1fff5-f711-4fab-8a73-0f4ef7899c58.png\" alt class=\"image--center mx-auto\" /></p>\n<p>We used the same benchmark harness as in our <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">GPT-5</a>, <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks?\">Codex</a>, and <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/claude-sonnet-45-better-performance-but-a-paradox?\">Sonnet 4.5</a> articles: a suite of <strong>25 hard PRs</strong>, each seeded with a known <strong>error pattern (EP)</strong>. Our scoring focuses on:</p>\n<ul>\n<li><p><strong>Actionable comments only</strong>: Comments that get posted (not additional suggestions or outside-diff notes).</p>\n</li>\n<li><p><strong>EP PASS (per comment)</strong>: The comment directly fixes or surfaces the EP.</p>\n</li>\n<li><p><strong>Important comments</strong>: Either EP PASS or another major/critical real bug.</p>\n</li>\n<li><p><strong>Precision</strong>: EP PASS ÷ total comments.</p>\n</li>\n<li><p><strong>SNR</strong>: Important ÷ (total − Important).</p>\n</li>\n</ul>\n<p>We compared:</p>\n<ul>\n<li><p><strong>GPT-5.1</strong> (new model)</p>\n</li>\n<li><p><strong>CodeRabbit Production</strong> (our current reviewer stack)</p>\n</li>\n<li><p><strong>Sonnet 4.5</strong></p>\n</li>\n</ul>\n<h2 id=\"heading-why-adding-a-new-model-isnt-a-switch-flip\"><strong>Why adding a new model isn’t a switch-flip</strong></h2>\n<p>Every model rollout at CodeRabbit is a campaign. We don’t plug in the model and hope; we test, adapt, and gate before shipping because <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/the-end-of-one-sized-fits-all-prompts-why-llm-models-are-no-longer-interchangeable?\">models are no longer interchangeable.</a> With GPT-5.1, this meant:</p>\n<ul>\n<li><p>Reducing <strong>outside-diff comments</strong>, which can’t be posted to GitHub.</p>\n</li>\n<li><p>Tightening <strong>tone and concision</strong> to reduce verbosity.</p>\n</li>\n<li><p>Re-aligning on <strong>severity tagging</strong> and <strong>instruction interpretation</strong>.</p>\n</li>\n</ul>\n<p>This mirrors what we did with <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks?\">GPT-5 Codex</a>: turn reasoning power into product value by reshaping the model’s behavior. The net result: higher SNR, less fatigue, and no compromise on bug coverage.</p>\n<h2 id=\"heading-scoreboard-actionable-comments-only\"><strong>Scoreboard (Actionable Comments Only)</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763056830923/d80c8745-3042-4d79-84cc-8dcaff93659f.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>Takeaway:</strong> GPT-5.1 matched the highest EP recall while posting the <strong>fewest</strong> comments. It beat both CodeRabbit prod and Sonnet 4.5 on <strong>per-comment precision</strong> and <strong>important share</strong>, delivering <strong>the cleanest high-impact reviews</strong>.</p>\n<h2 id=\"heading-what-gpt-51-feels-like-in-review\"><strong>What GPT-5.1 feels like in review</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763056903033/b5b08d8e-490b-4293-8551-d4a4d371fc4e.png\" alt class=\"image--center mx-auto\" /></p>\n<p>The behavioral traits we see in the data align directly with the language metrics we later measure such as 28% hedging and 15% assertive markers. This shows that the tone developers perceive as confident and balanced is borne out in the data.</p>\n<p>Compared with GPT‑5 Codex and Sonnet 4.5, GPT‑5.1’s comments feel leaner, more conversational, and closer to how experienced engineers actually communicate. Codex could sound mechanical and rigid, while Sonnet 4.5 leaned verbose and academic. In contrast, GPT‑5.1 balances brevity with clarity. Its feedback feels confident but not heavy‑handed, like a trusted teammate explaining a diff. Against CodeRabbit Prod, it feels sharper and more focused. Against Sonnet 4.5, it feels human and restrained. Here’s how that translates in practice:</p>\n<h3 id=\"heading-concise\"><strong>Concise</strong></h3>\n<p>GPT-5.1 writes fewer, sharper comments that get straight to the point. In one PR, it fixed a lost wakeup bug with a single line: p_caller_pool_thread-&gt;cond_var.wait(lock);  no extra context, no unnecessary prose. CodeRabbit prod, by comparison, wrote several paragraphs describing the thread flow before reaching the same conclusion.</p>\n<h3 id=\"heading-direct\"><strong>Direct</strong></h3>\n<p>When ownership or memory management was at stake, GPT-5.1 didn’t hesitate. It flagged the redundant r-&gt;reference() call with: “Ref&lt;Resource&gt; already manages refcounts; remove the manual increment to prevent leaks.” Developers appreciate this directness. It reads like a patch review from a teammate, not a lecture.</p>\n<h3 id=\"heading-pragmatic\"><strong>Pragmatic</strong></h3>\n<p>GPT-5.1 understands when an issue matters and when it doesn’t. On a cache configuration PR, it identified an unimplemented optimizeMemoryUsage() but correctly noted, “This is minor unless cache growth impacts memory pressure.” Instead of overreacting, it contextualized severity, something Sonnet 4.5 still struggles with.</p>\n<h3 id=\"heading-follows-context\"><strong>Follows Context</strong></h3>\n<p>When prompts were vague, GPT-5.1 explicitly explained its assumptions. In an early run, it said: “The prompt didn’t specify helper function scope, so I included one for clarity.” That kind of transparency helped us refine our instructions and made its reasoning trustworthy.</p>\n<p>Concise, direct, pragmatic, and context-aware are qualities that mirror what we valued most in GPT-5 Codex, but with a steadier tone and more restraint.</p>\n<h2 id=\"heading-style-and-tone-why-gpt-51-feels-like-a-peer\"><strong>Style and tone (why GPT-5.1 feels like a peer)</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763056849033/ec2fdcee-dd64-47d4-85ed-0bcb173c3d0e.png\" alt class=\"image--center mx-auto\" /></p>\n<p>To understand why GPT-5.1 feels different in review, we looked at the same language and structure signals used in our GPT-5 Codex and Sonnet 4.5 evaluations. These include measures like comment length, presence of code or diff blocks, and tone markers for hedging versus confidence. The data paints a clear picture.</p>\n<p><strong>How to read this.</strong> While GPT‑5.1’s comments use slightly more characters on average, they deliver that text in clearer structure with fewer sentences that carry more weight. In practice, developers perceive them as shorter and easier to read. GPT‑5.1’s tone is more assertive than both CodeRabbit prod and Sonnet 4.5, and it includes fewer diff blocks overall (76%), which is intentional. Many of these comments were multi‑location fixes, API validations, or design clarifications where a single fenced patch would be misleading. In roughly two‑thirds of those no‑diff cases, a minimal fenced patch <em>would</em> have made sense and could further improve clarity.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763057072668/b143a9e1-707e-4bbb-8988-7ba2ed98829c.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Compared to CodeRabbit prod, GPT-5.1 trades some patch frequency for higher clarity and focus. Against Sonnet 4.5, it avoids the verbosity and over-explanation that make reviews feel bloated. Its tone sits comfortably between Codex’s surgical precision and Sonnet’s cautious verbosity. It’sconfident without being heavy-handed, measured without being timid.</p>\n<p>At a glance, developers will notice that GPT-5.1’s reviews <strong>read faster, feel more direct, and require less scanning to identify the real fix.</strong> That’s the behavior we tuned for and it shows in both the numbers and the experience.</p>\n<h3 id=\"heading-where-gpt-51-still-lags\"><strong>Where GPT-5.1 still lags</strong></h3>\n<p>No model is perfect, and GPT-5.1 has its trade‑offs. Compared to CodeRabbit Prod, it sometimes leaves out contextual hygiene notes that can be useful for larger teams, focusing narrowly on functional issues. Against Sonnet 4.5, it can feel less expansive,missing opportunities to surface design or style considerations that human reviewers sometimes appreciate. These are conscious trade‑offs for precision and brevity and we’ll be watching the rollout to see how developers perceive the balance.</p>\n<h2 id=\"heading-what-we-had-to-fix\"><strong>What we had to fix</strong></h2>\n<p>While GPT‑5.1 required tuning, its challenges were far milder than those of earlier systems. CodeRabbit prod still tends to mix hygiene and critical issues in the same thread, while Sonnet 4.5 often over‑explains and spams multiple minor notes on the same bug. In contrast, GPT‑5.1’s main adjustments were focused on precision rather than tone or redundancy, showing how close it was to production readiness.</p>\n<ul>\n<li><p><strong>Outside-diff comments.</strong> GPT-5.1 sometimes included suggestions beyond the diff context. We updated the prompt to clarify this, and the model self-corrected.</p>\n</li>\n<li><p><strong>Over-helpful under ambiguity.</strong> When the prompt wasn’t strict, the model added context or helper functions. Once clarified, it obeyed boundaries tightly.</p>\n</li>\n</ul>\n<h2 id=\"heading-what-developers-should-expect\"><strong>What developers should expect</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1763057021456/4361d551-ae9b-474c-9b6b-2db46d55aeb4.png\" alt class=\"image--center mx-auto\" /></p>\n<ol>\n<li><p><strong>Cleaner reviews.</strong> Fewer comments and a higher share of comments that matter.</p>\n</li>\n<li><p><strong>Patch-like tone.</strong> Almost every comment includes a minimal fix with explanation.</p>\n</li>\n<li><p><strong>Top-tier EP recall.</strong> Ties Sonnet 4.5, beats CodeRabbit prod.</p>\n</li>\n<li><p><strong>Less scanning, more signal.</strong> 58.7% of comments are Important.</p>\n</li>\n<li><p><strong>Real-world bugs caught even outside the target.</strong> These include lifecycle issues, leaks, consistency gaps.</p>\n</li>\n</ol>\n<h2 id=\"heading-closing-thoughts\"><strong>Closing thoughts:</strong></h2>\n<p>We don’t just pick models; we make them work. GPT-5.1 is entering the next phase of our rollout process now that tuning for GitHub diff behavior, voice, verbosity, and scoring thresholds is complete. Over the coming weeks, we’ll monitor how real users respond to its higher SNR, new tone, and concise review style. If developers respond well, we’ll expand its availability, giving them the cleaner, faster reviews they’ve been asking for.</p>\n<p>For now, GPT‑5.1 stands ready to show what this next generation of precision‑focused review can do. It brings us closer to CodeRabbit’s north star: catching the bugs that matter quickly, without making developers sift through noise.</p>\n<p><strong><em>Interested in trying our code reviews?</em></strong> <a target=\"_blank\" href=\"https://coderabbit.link/lPxOEIm\"><strong><em>Get a 14-day free trial!</em></strong></a></p>\n",
      "summary": "TL;DR\nAfter prompt tuning and integrating it into our stack, GPT-5.1 now delivers the best precision and signal-to-noise ratio (SNR) we’ve seen in reviews, with fewer comments. It tied for the best-in-class error pattern (EP) recall on our hard benchmark set while posting less than half the volume of comments that competitors did.\nThe result: less noise, better fixes, and reviews that read like patches again.\n\nWhat GPT-5.1 claims to be\nOpenAI and the press describe GPT-5.1 as more stable, instruction-following, and adaptive. It powers both \"Instant\" and \"Thinking\" modes in ChatGPT. We found that framing surprisingly accurate when it comes to code reviews: the model stays quick and surface-level for nits, but reasons deeply when the bug requires it.\nWe also tried something new. When GPT-5.1 got something wrong, we used the full exchange and its internal reasoning trace to prompt it to reflect. By showing it where it missed the mark and asking how it would change its instructions to do better, the model was able to actually propose concrete edits to its prompt. We used this iterative reflection technique (which surfaced issues like outside-diff sprawl) to refine both its behavior and our system instructions until it got consistently tighter.\nWhat We Measured (and Why)\n\nWe used the same benchmark harness as in our GPT-5, Codex, and Sonnet 4.5 articles: a suite of 25 hard PRs, each seeded with a known error pattern (EP). Our scoring focuses on:\nActionable comments only: Comments that get posted (not additional suggestions or outside-diff notes).\nEP PASS (per comment): The comment directly fixes or surfaces the EP.\nImportant comments: Either EP PASS or another major/critical real bug.\nPrecision: EP PASS ÷ total comments.\nSNR: Important ÷ (total − Important).\nWe compared:\nGPT-5.1 (new model)\nCodeRabbit Production (our current reviewer stack)\nSonnet 4.5\nWhy adding a new model isn’t a switch-flip\nEvery model rollout at CodeRabbit is a campaign. We don’t plug in the model and hope; we test, adapt, and gate before shipping because models are no longer interchangeable. With GPT-5.1, this meant:\nReducing outside-diff comments, which can’t be posted to GitHub.\nTightening tone and concision to reduce verbosity.\nRe-aligning on severity tagging and instruction interpretation.\nThis mirrors what we did with GPT-5 Codex: turn reasoning power into product value by reshaping the model’s behavior. The net result: higher SNR, less fatigue, and no compromise on bug coverage.\nScoreboard (Actionable Comments Only)\n\nTakeaway: GPT-5.1 matched the highest EP recall while posting the fewest comments. It beat both CodeRabbit prod and Sonnet 4.5 on per-comment precision and important share, delivering the cleanest high-impact reviews.\nWhat GPT-5.1 feels like in review\n\nThe behavioral traits we see in the data align directly with the language metrics we later measure such as 28% hedging and 15% assertive markers. This shows that the tone developers perceive as confident and balanced is borne out in the data.\nCompared with GPT‑5 Codex and Sonnet 4.5, GPT‑5.1’s comments feel leaner, more conversational, and closer to how experienced engineers actually communicate. Codex could sound mechanical and rigid, while Sonnet 4.5 leaned verbose and academic. In contrast, GPT‑5.1 balances brevity with clarity. Its feedback feels confident but not heavy‑handed, like a trusted teammate explaining a diff. Against CodeRabbit Prod, it feels sharper and more focused. Against Sonnet 4.5, it feels human and restrained. Here’s how that translates in practice:\nConcise\nGPT-5.1 writes fewer, sharper comments that get straight to the point. In one PR, it fixed a lost wakeup bug with a single line: p_caller_pool_thread->cond_var.wait(lock);  no extra context, no unnecessary prose. CodeRabbit prod, by comparison, wrote several paragraphs describing the thread flow before reaching the same conclusion.\nDirect\nWhen ownership or memory management was at stake, GPT-5.1 didn’t hesitate. It flagged the redundant r->reference() call with: “Ref<Resource> already manages refcounts; remove the manual increment to prevent leaks.” Developers appreciate this directness. It reads like a patch review from a teammate, not a lecture.\nPragmatic\nGPT-5.1 understands when an issue matters and when it doesn’t. On a cache configuration PR, it identified an unimplemented optimizeMemoryUsage() but correctly noted, “This is minor unless cache growth impacts memory pressure.” Instead of overreacting, it contextualized severity, something Sonnet 4.5 still struggles with.\nFollows Context\nWhen prompts were vague, GPT-5.1 explicitly explained its assumptions. In an early run, it said: “The prompt didn’t specify helper function scope, so I included one for clarity.” That kind of transparency helped us refine our instructions and made its reasoning trustworthy.\nConcise, direct, pragmatic, and context-aware are qualities that mirror what we valued most in GPT-5 Codex, but with a steadier tone and more restraint.\nStyle and tone (why GPT-5.1 feels like a peer)\n\nTo understand why GPT-5.1 feels different in review, we looked at the same language and structure signals used in our GPT-5 Codex and Sonnet 4.5 evaluations. These include measures like comment length, presence of code or diff blocks, and tone markers for hedging versus confidence. The data paints a clear picture.\nHow to read this. While GPT‑5.1’s comments use slightly more characters on average, they deliver that text in clearer structure with fewer sentences that carry more weight. In practice, developers perceive them as shorter and easier to read. GPT‑5.1’s tone is more assertive than both CodeRabbit prod and Sonnet 4.5, and it includes fewer diff blocks overall (76%), which is intentional. Many of these comments were multi‑location fixes, API validations, or design clarifications where a single fenced patch would be misleading. In roughly two‑thirds of those no‑diff cases, a minimal fenced patch would have made sense and could further improve clarity.\n\nCompared to CodeRabbit prod, GPT-5.1 trades some patch frequency for higher clarity and focus. Against Sonnet 4.5, it avoids the verbosity and over-explanation that make reviews feel bloated. Its tone sits comfortably between Codex’s surgical precision and Sonnet’s cautious verbosity. It’sconfident without being heavy-handed, measured without being timid.\nAt a glance, developers will notice that GPT-5.1’s reviews read faster, feel more direct, and require less scanning to identify the real fix. That’s the behavior we tuned for and it shows in both the numbers and the experience.\nWhere GPT-5.1 still lags\nNo model is perfect, and GPT-5.1 has its trade‑offs. Compared to CodeRabbit Prod, it sometimes leaves out contextual hygiene notes that can be useful for larger teams, focusing narrowly on functional issues. Against Sonnet 4.5, it can feel less expansive,missing opportunities to surface design or style considerations that human reviewers sometimes appreciate. These are conscious trade‑offs for precision and brevity and we’ll be watching the rollout to see how developers perceive the balance.\nWhat we had to fix\nWhile GPT‑5.1 required tuning, its challenges were far milder than those of earlier systems. CodeRabbit prod still tends to mix hygiene and critical issues in the same thread, while Sonnet 4.5 often over‑explains and spams multiple minor notes on the same bug. In contrast, GPT‑5.1’s main adjustments were focused on precision rather than tone or redundancy, showing how close it was to production readiness.\nOutside-diff comments. GPT-5.1 sometimes included suggestions beyond the diff context. We updated the prompt to clarify this, and the model self-corrected.\nOver-helpful under ambiguity. When the prompt wasn’t strict, the model added context or helper functions. Once clarified, it obeyed boundaries tightly.\nWhat developers should expect\n\nCleaner reviews. Fewer comments and a higher share of comments that matter.\nPatch-like tone. Almost every comment includes a minimal fix with explanation.\nTop-tier EP recall. Ties Sonnet 4.5, beats CodeRabbit prod.\nLess scanning, more signal. 58.7% of comments are Important.\nReal-world bugs caught even outside the target. These include lifecycle issues, leaks, consistency gaps.\nClosing thoughts:\nWe don’t just pick models; we make them work. GPT-5.1 is entering the next phase of our rollout process now that tuning for GitHub diff behavior, voice, verbosity, and scoring thresholds is complete. Over the coming weeks, we’ll monitor how real users respond to its higher SNR, new tone, and concise review style. If developers respond well, we’ll expand its availability, giving them the cleaner, faster reviews they’ve been asking for.\nFor now, GPT‑5.1 stands ready to show what this next generation of precision‑focused review can do. It brings us closer to CodeRabbit’s north star: catching the bugs that matter quickly, without making developers sift through noise.\nInterested in trying our code reviews? Get a 14-day free trial!",
      "publishedAt": "2025-11-13T18:07:52.000Z",
      "author": "David Loker",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "observability"
      ],
      "ingestedAt": "2025-12-04T14:17:12.172Z",
      "score": 2.8212224983201195
    },
    {
      "id": "bf08364ea81899e9f32449dae1c018b1",
      "title": "Why emojis suck for reinforcement learning",
      "url": "https://coderabbit.ai/blog/why-emojis-suck-for-reinforcement-learning",
      "content": "<h2 id=\"heading-the-simplicity-trap\"><strong>The simplicity trap</strong></h2>\n<p>Sure, a thumbs up is quick, but is it really teaching your AI reviewer anything useful? Emoji-based feedback feels good, is fast, and universal. On the surface, it even seems to make sense.</p>\n<p>But code review isn’t a light switch. It’s a mess of judgment calls, technical nuance, and team-specific standards. Many of those don’t show up in a quick emoji click. Every code comment carries hidden intent: correctness, clarity, design trade-offs, historical precedent, team risk tolerance, and even internal political dynamics.</p>\n<p>Reducing that to a binary signal? That’s not learning, that’s training a model to chase vibes.</p>\n<h2 id=\"heading-when-simplicity-backfires-the-sycophant-scare\"><strong>When simplicity backfires: The sycophant scare</strong></h2>\n<p>Earlier this year, OpenAI pushed an update to GPT‑4o that leaned too hard on thumbs-up and thumbs-down feedback. The result? A model that became <a target=\"_blank\" href=\"https://openai.com/index/sycophancy-in-gpt-4o/\">overly agreeable</a>. It flattered users. It agreed with wrong answers. It started to say “yes” a little too much, and the quality of answers dropped. OpenAI had to walk it back: the feedback signal had been hijacked.</p>\n<p>Turns out, if you tell a model that approval is the goal, it will optimize for approval. Not truth. Not utility. Just “did the human feel good in the moment?”</p>\n<p>This wasn’t a bug. It was a reward design failure. And if you apply the same approach to code review, you will get a reviewer that plays it safe, flatters your choices, and avoids telling you what you actually need to hear.</p>\n<h2 id=\"heading-why-binary-feedback-collapses-nuance\"><strong>Why binary feedback collapses nuance</strong></h2>\n<p>A thumbs-up means... what, exactly?</p>\n<ul>\n<li><p>That the model caught a bug?</p>\n</li>\n<li><p>That it wrote clearly?</p>\n</li>\n<li><p>That it sounded friendly?</p>\n</li>\n<li><p>That the reviewer was just in a good mood?</p>\n</li>\n</ul>\n<p>A single scalar signal tells the system <em>something went well</em>, but not <em>what went well</em>. That means the model will nudge on whatever it can control: tone, politeness, flattery, or brevity. That’s what sycophancy looks like in reinforcement learning. Not evil intent, just a system learning to maximize the reward you gave it, not the outcome you actually wanted.</p>\n<p>This is <a target=\"_blank\" href=\"https://medium.com/@yoavyeledteva/beyond-artificiality-redefining-intelligence-in-ai-and-avoiding-goodharts-law-25b75c3c1101\">Goodhart’s Law</a> in action. When the metric, in this case thumbs up, becomes the goal, it stops being a useful measure of anything real.</p>\n<h2 id=\"heading-how-models-game-your-feedback\"><strong>How models game your feedback</strong></h2>\n<p>When you give a model an easy signal, it finds an easy shortcut.</p>\n<p>In the coding world, reinforcement learning agents have learned to pass test cases by hard-coding expected outputs instead of solving the underlying logic. They’ve manipulated logs and short-circuited evaluation harnesses. The green check shows up, but the code doesn’t actually work.</p>\n<p>In code review, the same thing happens, just socially. The model starts saying “Nice work!” at the top of every comment. It hedges every suggestion. It nitpicks formatting because those comments are safe and get accepted without argument. And real architectural concerns? They get buried.</p>\n<p>The model has learned how to get positive reactions but it’s no longer reviewing code.</p>\n<h2 id=\"heading-what-implicit-signals-get-right\"><strong>What implicit signals get right</strong></h2>\n<p>Outside of LLMs, this pattern is well known. Netflix found that <a target=\"_blank\" href=\"https://medium.com/illuminations-mirror/how-netflix-uses-machine-learning-to-decide-what-you-watch-next-7fee11102007\">what users <em>watch</em> is more useful than what they <em>rate</em></a>. People lie with stars. But watch time, clickthrough, and rewatching are honest signals.</p>\n<p>In AI, we <strong>call this</strong> implicit feedback <strong>a</strong>nd in code review, it shows up as:</p>\n<ul>\n<li><p>Did the developer apply the suggestion?</p>\n</li>\n<li><p>Did they rewrite it?</p>\n</li>\n<li><p>Did they ignore it?</p>\n</li>\n<li><p>Did the same pattern show up again in a future bug?</p>\n</li>\n</ul>\n<p>These signals don’t need user input. They come from behavior and they’re harder to game.</p>\n<p>That doesn’t mean they’re perfect. You can’t always know <em>why</em> someone took an action. But they are less easily manipulated than a raw emoji. They also tell you whether the review <em>worked</em>, not just whether it felt good.</p>\n<h2 id=\"heading-code-generation-vs-code-review-different-games-different-signals\"><strong>Code generation vs code review: different games, different signals</strong></h2>\n<p>Code generation is closer to math since there’s often a right answer. Does it compile? Does it return the correct result? Does it pass tests?</p>\n<p>That means you can use outcome-based rewards like execution feedback and implicit signals. They’re not perfect. Code models can still cheat by hard-coding outputs, but you can build guardrails. And you don’t need the developer to say whether it was good, you can see whether it worked.</p>\n<p>Code review is different. There’s no universal pass/fail but vast differences in preferred style, structure, risk, naming, test coverage from one team to the next. A great comment for one team might be totally wrong for another. What’s considered “clean code” in a fast-moving startup might be flagged as sloppy in a regulated enterprise.</p>\n<p>That’s the real problem with global thumbs up/down data. It flattens out the nuance. It teaches the model to aim for the average, not the appropriate. You don’t just get safe comments, you get generic ones.</p>\n<h3 id=\"heading-our-alternative-coderabbit-learnings\"><strong>Our alternative: CodeRabbit Learnings</strong></h3>\n<p>At CodeRabbit, we take a different approach. Instead of optimizing for likes, we optimize for understanding. That’s why we built <a target=\"_blank\" href=\"https://docs.coderabbit.ai/guides/learnings\"><strong>Learnings</strong>.</a></p>\n<p>Every time an engineer corrects CodeRabbit, clarifies a team convention, or explains why something doesn’t fit their stack, that explanation is stored as a natural language instruction. We don’t just remember that the comment was rejected, we remember why.</p>\n<p>Those Learnings are linked to your org, your repositories, and even specific paths or file types. When CodeRabbit reviews future pull requests, it retrieves those instructions and applies them in context. The next time it sees that same pattern, it adjusts.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1762535170060/91e7f06c-3e56-4dfc-bf07-6901bafdef07.png\" alt class=\"image--center mx-auto\" /></p>\n<p>There’s no need to re-teach it and no risk of repeating the same mistake. The model doesn’t guess based on thumbs, it reasons from your team’s actual guidance.</p>\n<p>It also gives you visibility. You can see which Learnings exist, browse them, filter by category, and delete or edit them when your standards change. That means the model evolves alongside your team and stays aligned as your practices shift.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1762535186830/15d12e32-fdb9-4034-8e74-1db0bd4eb909.png\" alt class=\"image--center mx-auto\" /></p>\n<p>This is reinforcement learning not through raw approval, but through captured intent. It’s interpretable and inspectable. And it builds a living layer of team knowledge that generalizes across reviews.</p>\n<h2 id=\"heading-what-nuanced-learning-enables\"><strong>What nuanced learning enables</strong></h2>\n<p>When you feed the system clear, contextual instructions and not just signals, it unlocks far more than a better review experience.</p>\n<ul>\n<li><p><strong>It enables team-level adaptation.</strong> The model stops guessing what good looks like and learns how your team actually writes code. It understands your risk posture, your stylistic preferences, your trade-offs. It becomes a reviewer that knows the house rules.</p>\n</li>\n<li><p><strong>It supports longitudinal learning.</strong> Over time, CodeRabbit builds a memory of which comments are helpful, which are ignored, and which suggestions actually lead to changes. That means it gets more precise, more focused, and less noisy over time.</p>\n</li>\n<li><p><strong>It builds trust.</strong> When developers know they can correct the AI and it will remember, they engage more. They shape the system and the system becomes a reflection of their standards, not a generic LLM.</p>\n</li>\n</ul>\n<p>This is how a review tool becomes an extension of your team and not just another opinion in the room.</p>\n<h2 id=\"heading-closing-thoughts-real-learning-comes-from-patterns-not-pixels\">Closing thoughts: Real learning comes from patterns, not pixels</h2>\n<p>Thumbs are fine for quick reactions but quick reactions don’t build expertise.</p>\n<p>If you want an AI reviewer that improves over time, adapts to your standards, and avoids the traps of shallow feedback, you need to give it more than approval. You need to give it explanations.</p>\n<p>The next generation of AI code tools won’t be trained on likes. They’ll be trained on context, consequence, and course correction. They’ll learn not from emojis, but from structured memory. From real decisions and your team’s own voice.</p>\n<p>That’s what <a target=\"_blank\" href=\"https://docs.coderabbit.ai/guides/learnings\">CodeRabbit Learnings</a> is built for. Not for applause but for understanding.</p>\n<p><strong><em>Try out Learnings for yourself with our f</em></strong><a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong><em>ree trial.</em></strong></a></p>\n",
      "summary": "The simplicity trap\nSure, a thumbs up is quick, but is it really teaching your AI reviewer anything useful? Emoji-based feedback feels good, is fast, and universal. On the surface, it even seems to make sense.\nBut code review isn’t a light switch. It’s a mess of judgment calls, technical nuance, and team-specific standards. Many of those don’t show up in a quick emoji click. Every code comment carries hidden intent: correctness, clarity, design trade-offs, historical precedent, team risk tolerance, and even internal political dynamics.\nReducing that to a binary signal? That’s not learning, that’s training a model to chase vibes.\nWhen simplicity backfires: The sycophant scare\nEarlier this year, OpenAI pushed an update to GPT‑4o that leaned too hard on thumbs-up and thumbs-down feedback. The result? A model that became overly agreeable. It flattered users. It agreed with wrong answers. It started to say “yes” a little too much, and the quality of answers dropped. OpenAI had to walk it back: the feedback signal had been hijacked.\nTurns out, if you tell a model that approval is the goal, it will optimize for approval. Not truth. Not utility. Just “did the human feel good in the moment?”\nThis wasn’t a bug. It was a reward design failure. And if you apply the same approach to code review, you will get a reviewer that plays it safe, flatters your choices, and avoids telling you what you actually need to hear.\nWhy binary feedback collapses nuance\nA thumbs-up means... what, exactly?\nThat the model caught a bug?\nThat it wrote clearly?\nThat it sounded friendly?\nThat the reviewer was just in a good mood?\nA single scalar signal tells the system something went well, but not what went well. That means the model will nudge on whatever it can control: tone, politeness, flattery, or brevity. That’s what sycophancy looks like in reinforcement learning. Not evil intent, just a system learning to maximize the reward you gave it, not the outcome you actually wanted.\nThis is Goodhart’s Law in action. When the metric, in this case thumbs up, becomes the goal, it stops being a useful measure of anything real.\nHow models game your feedback\nWhen you give a model an easy signal, it finds an easy shortcut.\nIn the coding world, reinforcement learning agents have learned to pass test cases by hard-coding expected outputs instead of solving the underlying logic. They’ve manipulated logs and short-circuited evaluation harnesses. The green check shows up, but the code doesn’t actually work.\nIn code review, the same thing happens, just socially. The model starts saying “Nice work!” at the top of every comment. It hedges every suggestion. It nitpicks formatting because those comments are safe and get accepted without argument. And real architectural concerns? They get buried.\nThe model has learned how to get positive reactions but it’s no longer reviewing code.\nWhat implicit signals get right\nOutside of LLMs, this pattern is well known. Netflix found that what users watch is more useful than what they rate. People lie with stars. But watch time, clickthrough, and rewatching are honest signals.\nIn AI, we call this implicit feedback and in code review, it shows up as:\nDid the developer apply the suggestion?\nDid they rewrite it?\nDid they ignore it?\nDid the same pattern show up again in a future bug?\nThese signals don’t need user input. They come from behavior and they’re harder to game.\nThat doesn’t mean they’re perfect. You can’t always know why someone took an action. But they are less easily manipulated than a raw emoji. They also tell you whether the review worked, not just whether it felt good.\nCode generation vs code review: different games, different signals\nCode generation is closer to math since there’s often a right answer. Does it compile? Does it return the correct result? Does it pass tests?\nThat means you can use outcome-based rewards like execution feedback and implicit signals. They’re not perfect. Code models can still cheat by hard-coding outputs, but you can build guardrails. And you don’t need the developer to say whether it was good, you can see whether it worked.\nCode review is different. There’s no universal pass/fail but vast differences in preferred style, structure, risk, naming, test coverage from one team to the next. A great comment for one team might be totally wrong for another. What’s considered “clean code” in a fast-moving startup might be flagged as sloppy in a regulated enterprise.\nThat’s the real problem with global thumbs up/down data. It flattens out the nuance. It teaches the model to aim for the average, not the appropriate. You don’t just get safe comments, you get generic ones.\nOur alternative: CodeRabbit Learnings\nAt CodeRabbit, we take a different approach. Instead of optimizing for likes, we optimize for understanding. That’s why we built Learnings.\nEvery time an engineer corrects CodeRabbit, clarifies a team convention, or explains why something doesn’t fit their stack, that explanation is stored as a natural language instruction. We don’t just remember that the comment was rejected, we remember why.\nThose Learnings are linked to your org, your repositories, and even specific paths or file types. When CodeRabbit reviews future pull requests, it retrieves those instructions and applies them in context. The next time it sees that same pattern, it adjusts.\n\nThere’s no need to re-teach it and no risk of repeating the same mistake. The model doesn’t guess based on thumbs, it reasons from your team’s actual guidance.\nIt also gives you visibility. You can see which Learnings exist, browse them, filter by category, and delete or edit them when your standards change. That means the model evolves alongside your team and stays aligned as your practices shift.\n\nThis is reinforcement learning not through raw approval, but through captured intent. It’s interpretable and inspectable. And it builds a living layer of team knowledge that generalizes across reviews.\nWhat nuanced learning enables\nWhen you feed the system clear, contextual instructions and not just signals, it unlocks far more than a better review experience.\nIt enables team-level adaptation. The model stops guessing what good looks like and learns how your team actually writes code. It understands your risk posture, your stylistic preferences, your trade-offs. It becomes a reviewer that knows the house rules.\nIt supports longitudinal learning. Over time, CodeRabbit builds a memory of which comments are helpful, which are ignored, and which suggestions actually lead to changes. That means it gets more precise, more focused, and less noisy over time.\nIt builds trust. When developers know they can correct the AI and it will remember, they engage more. They shape the system and the system becomes a reflection of their standards, not a generic LLM.\nThis is how a review tool becomes an extension of your team and not just another opinion in the room.\nClosing thoughts: Real learning comes from patterns, not pixels\nThumbs are fine for quick reactions but quick reactions don’t build expertise.\nIf you want an AI reviewer that improves over time, adapts to your standards, and avoids the traps of shallow feedback, you need to give it more than approval. You need to give it explanations.\nThe next generation of AI code tools won’t be trained on likes. They’ll be trained on context, consequence, and course correction. They’ll learn not from emojis, but from structured memory. From real decisions and your team’s own voice.\nThat’s what CodeRabbit Learnings is built for. Not for applause but for understanding.\nTry out Learnings for yourself with our free trial.",
      "publishedAt": "2025-11-07T02:34:45.000Z",
      "author": "David Loker",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "testing"
      ],
      "ingestedAt": "2025-12-04T14:17:12.172Z",
      "score": 3.0883196258639085
    },
    {
      "id": "c84b57b98069b87d38468464eac79d6b",
      "title": "「スローai」のススメ:開発者が速度偏重をやめるべき理由",
      "url": "https://coderabbit.ai/blog/the-rise-of-slow-ai-why-devs-should-stop-speedrunning-stupid-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/the-rise-of-slow-ai-why-devs-should-stop-speedrunning-stupid\">The rise of Slow AI: Why devs should stop speedrunning stupid</a>の意訳です。</p>\n<p>私たちがコンピューターを使い始めて以来、そこには常に1つの基本ルールが存在しました。それは、速ければ速いほど良いということです。低レイテンシ、高スループット、待ち時間の短縮、これが鉄則でした。ボタンの応答に600ミリ秒もかかったり、注意力が維持できなくなるほど長いスピナーを見たりすることを望む人はいません。遅いということは、それは壊れているということです。議論の余地はないでしょう。</p>\n<p>そのため当然ながら、AIツールが私たちの開発ワークフローに忍び込み始めたとき、自動補完やエージェント、Copilot、その他何でも、同じ原則が適用されました。それはつまり、「速くしろ」「インスタントに感じさせろ」「<strong><em>魔法</em></strong>のように見せろ」です。</p>\n<p>しかし、実際のところ、AIは魔法ではありません。それは推論です。パイプライン、RAG、コンテキスト、そしてツールの呼び出しです。乱雑なコンテキストと確率的推測をジャグリングしているのです。そして、単なる自動補完以上のものを求めるなら、ベースとなるプロセスのパイプラインを構築する必要があります。そして、それは処理時間がかかります。それ以外では、基本的に愚かさのスピードランをしているだけなのです。そして、ツールがどれだけ速くとも、間違っているなら速度はまったく意味がありません。</p>\n<p>CodeRabbitでは、私たちが「スローAI」と呼ぶものを優先しています。そして、多くのAI企業が恐れて言えないことを言う勇気があります。私たちは「あなたを待たせます」。</p>\n<p>(そして、あなたはそれに感謝するでしょう)</p>\n<h2 id=\"heading-ai\">AI開発ツールはしばしば速く、自信に満ち、そして<strong><em>間違っている</em></strong></h2>\n<p>最近AIコーディングエージェントを使ったことがあるなら、こんな経験をしたことがあるのではないでしょうか。タイピングを止めるとほぼ同時に、驚くほど速い提案がポップアップしてきます。それは一見すると、正当なものに見えます。しかしその後…失敗します。場合によっては、派手に失敗します。さらに悪いことに、テストには合格するものの、別なファイルで何かを壊していたりします。</p>\n<p>これはなぜでしょうか? なぜなら、今日のほとんどのAI開発ツールは、1つのことに最適化されているからです。それは速度です。数トークンをタイプすると、モデルは統計的に最も可能性の高い続きを予測します。それは必ずしも正しいものではなく、安全なものでもなく、アプリが実際に何をしているかを理解しているものでもありません。ただ、次のもっともらしいコードの塊でしかありません。</p>\n<p>それは、ボイラープレートには十分でしょう。しかし、ロジックには? エッジケースには? 実際のエンジニアリングには? それは、会議で速く自信を持って話すものの、仕様を読まない人を雇うようなものです。</p>\n<p>こうしたツールのほとんどは、コンテキストを <em>読んで</em> いません。少なくとも深くは読んでいません。近くの数行、おそらく関数名を取得するかもしれませんが、生成しているものを全体像に対して検証することは滅多にありません。課題のクロスチェックはしませんし、アーキテクチャレベルの認識もなく、ファイルやユースケースをまたいだ推論もありません。</p>\n<p>思慮深く、テスト可能で、コンテキストを認識した出力が必要な場合、<em>速度を落とし</em>、俯瞰し、実際に問題に取り組むAIシステムが必要です。</p>\n<p>それがスローAIの役割です。そして、AIが何をしているかを理解する時間を取ると、ハルシネーションを止めて<em>実際に</em>サポートしてくれるようになることがわかります。</p>\n<h2 id=\"heading-ai-1\"><strong>AIが遅いときに優れている理由</strong></h2>\n<p>核心として、大規模言語モデルは統計的推論マシンだということです。確率、パターン、そして(願わくば)あなたが与えたコンテキストに基づいて、次に何が来るかを予測して出力を生成します。しかし、ほとんどの開発者が忘れている注意点があります。良い予測には、それなりの作業が必要だということです。これは特に、ロジックを書く、アーキテクチャを理解する、複数のステップにわたって推論するなど、複雑なことをモデルに求める場合に当てはまります。出力の品質は、多くの場合において、推論の深さや段階に直接結びついています。</p>\n<p>これは、単純なプロンプトを超えて、マルチステージパイプラインとエージェント的な動作に移行する際において、特に当てはまります。AIツールが出力を検証し、関連ファイルを取り込み、矛盾をチェックし、または複数のアクション先を計画している場合、それは単に次のトークンを出力しているだけではないのです。それは<em>思考して</em>います。または、少なくとも、それに近いことを実行しています。</p>\n<p>このような非線形推論は、単一のフォワードパスでは実行できません。それには反省と検索、計画、そして時には自己修正さえも含まれます。こうしたプロセスはレイテンシ・フレンドリーではなく、インテリジェンス・フレンドリーなのです。</p>\n<p>要するに、AIに複雑なコードで実際に助けてもらいたいなら、それを調理させる必要があるのです。</p>\n<h2 id=\"heading-ai-2\">遅いことが新しいスマート:なぜ私たちはAIに考えさせるのか</h2>\n<p>スローAIは、私たちが話している用語の1つです。言い換えるなら包括的AIや正確なAI、あるいは正直に言えば「 <em>実質的</em> 役立ち有用なAI」と呼ぶこともできます。そして、それは今のAIプロダクトデザインで最もバズワードなアイデアの1つに密接に結びついています。そう、コンテキストエンジニアリングです。</p>\n<p>AIが問題について知っている、 <em>関連性があって解析された</em> 情報が多いほど、パフォーマンスは向上します。しかし、そのコンテキストは取り込まれ、解析され、優先順位が付けられ、推論される必要があります。そのようなパイプラインは、超低レイテンシAIの敵です… そしてそれは精度の敵でもあります。</p>\n<p>そして、それが私たちのAIコードレビューにおいて、最初のコメントを見るまでに最大5分かかる理由です。誤解しないでください。私たちは遅さを最適化しているわけではありません。コードベースとPRの複雑さに応じて、3分、あるいは1分でレビューを受け取ることもできます。私たちのパイプラインが複雑なのは、ユーザーが必要とする結果を出すために<em>必要</em>だからです。いつでも同時に実行されているプロセスの数を<strong><em>知りたく</em></strong>もないでしょう!</p>\n<p>しかし、どうでしょう? 複数のレビューおよび検証エージェントを使用した、非線形のマルチパスパイプラインでAIに時間をかけさせると、他のツールよりもノイズが少なく、より関連性の高いコードレビューコメントが生成されるのです。</p>\n<p>非線形推論は決して、速くありません。しかし、それは優れているのです。</p>\n<h2 id=\"heading-ai-3\">では、なぜほとんどのAIツールは<em>遅さよりも愚かさを選ぶのか?</em></h2>\n<p>まず、スローAIはすべてのツールにとってオプションではありません。たとえば、AIコーディングエージェントに質問をしている場合、返信を5分間待てる人はいないでしょう。そのやり取りには、即時性への期待が本質的に備わっています。</p>\n<p>ではコードレビューはどうでしょうか? 提出されたPRに対して、同僚がすぐに手を止めてコメントし始めることを期待する人はいません。したがって、ボットからのレビューの遅延も受け入れる余地があります。そして、そのレビューがより関連性が高いことで時間を節約するなら、その遅延を受け入れる価値は特に高くなります。</p>\n<p>しかし、なぜ多くの企業が、常に（実際にはそれを必要としないときでも）低レイテンシを優先するのでしょうか? まあ、私たちは訓練されてきましたし、即座の満足を期待するようユーザーを訓練してきました。ボタンをクリックすれば、速攻のレスポンスを得られます。関数名をタイプすれば、それについて考える前に提案を得られます。即効性がなければ壊れている、遅い、またはスタートアップがAWS料金の支払いを忘れたように感じられます。</p>\n<p>これは非常に強く叩き込まれているため、企業は遅延よりも間違っている方を積極的に選択しています。そして、人々がそうするとき、私たちの開発文化には有害で、後ろ向きな何かがあります。</p>\n<p>なぜなら、真実はこうだからです。最高のAIツールは必ずしも速く感じられず、代わりに思慮深く感じられます。時々、彼らは一時停止します。時々、プロンプトを推論したり、関連するコードを検索したり、応答を検証したりするために余分な時間を必要とします。しかし、それは待つ価値があるのです。たとえば、OpenAIのDeep Research機能が質問により良く答えるためにインターネットを最大20分かけて調査するからといって、利用を止める人はいません。処理中に他のことをして、戻ってくるだけです。</p>\n<p>もはや遅い = 壊れているという意味ではありません。それは<em>スマート</em>という意味です。むしろ、AIに関しては、速度こそがバグです。開発プロセスに実際に価値を追加するAIツールが必要な場合、それには応答性から信頼性へ、即時性からインサイトへの移行が必要です。そして、特に開発者にとって、そのトレードオフは理にかなっています。</p>\n<p>私たちは、今後5年間で最も価値のあるアプリは、速度を最適化するものではなく、インテリジェンスを最適化するものになると信じています。速いけど不要な結果と、遅いけれど価値あるもの、どちらが欲しいですか?</p>\n<h2 id=\"heading-coderabbit\">CodeRabbitのマントラ:ゆっくり動いて物事を修正する</h2>\n<p>CodeRabbitでは、他のツールのように、速度を優先するAIパイプライン最適化に過剰投資はしません。私たちは、信頼のために最適化を行います。それはコードを理解し、コンテキスト全体で推論し、より良いソフトウェアを構築するのに実際に役立つ出力を生成する時間を取るシステムを受け入れることを意味します。確かに、クイックプロンプトをたたき出すよりも遅いです。しかし、その余分な時間は明瞭さやカバレッジ、そして自信へと変わります。</p>\n<p>「<strong>Move fast and break things（素早く行動し破壊せよ）</strong>」は、MVPをデリバリーするには素晴らしいものでした。しかし、<em>品質</em>をデリバリーすることに関しては、私たちは別のものを信じています。ゆっくり動いて物事を修正する、AIに場の空気を読ませる、話す前に考えさせる…そして、本当に自信のある自動補完ではなく、シニアエンジニアから得られるようなサポートを提供する。それが、間違ったAIをスローAIよりも優先する、現在の後ろ向きな文化から抜け出す唯一の方法です。</p>\n<p><strong>私たちのレビューを試してみたいですか? こちらから</strong> <a target=\"_blank\" href=\"https://app.coderabbit.ai/login?free-trial\"><strong>14日間の無料トライアル</strong></a><strong>を入手してください!</strong></p>\n",
      "summary": "The rise of Slow AI: Why devs should stop speedrunning stupidの意訳です。\n私たちがコンピューターを使い始めて以来、そこには常に1つの基本ルールが存在しました。それは、速ければ速いほど良いということです。低レイテンシ、高スループット、待ち時間の短縮、これが鉄則でした。ボタンの応答に600ミリ秒もかかったり、注意力が維持できなくなるほど長いスピナーを見たりすることを望む人はいません。遅いということは、それは壊れているということです。議論の余地はないでしょう。\nそのため当然ながら、AIツールが私たちの開発ワークフローに忍び込み始めたとき、自動補完やエージェント、Copilot、その他何でも、同じ原則が適用されました。それはつまり、「速くしろ」「インスタントに感じさせろ」「魔法のように見せろ」です。\nしかし、実際のところ、AIは魔法ではありません。それは推論です。パイプライン、RAG、コンテキスト、そしてツールの呼び出しです。乱雑なコンテキストと確率的推測をジャグリングしているのです。そして、単なる自動補完以上のものを求めるなら、ベースとなるプロセスのパイプラインを構築する必要があります。そして、それは処理時間がかかります。それ以外では、基本的に愚かさのスピードランをしているだけなのです。そして、ツールがどれだけ速くとも、間違っているなら速度はまったく意味がありません。\nCodeRabbitでは、私たちが「スローAI」と呼ぶものを優先しています。そして、多くのAI企業が恐れて言えないことを言う勇気があります。私たちは「あなたを待たせます」。\n(そして、あなたはそれに感謝するでしょう)\nAI開発ツールはしばしば速く、自信に満ち、そして間違っている\n最近AIコーディングエージェントを使ったことがあるなら、こんな経験をしたことがあるのではないでしょうか。タイピングを止めるとほぼ同時に、驚くほど速い提案がポップアップしてきます。それは一見すると、正当なものに見えます。しかしその後…失敗します。場合によっては、派手に失敗します。さらに悪いことに、テストには合格するものの、別なファイルで何かを壊していたりします。\nこれはなぜでしょうか? なぜなら、今日のほとんどのAI開発ツールは、1つのことに最適化されているからです。それは速度です。数トークンをタイプすると、モデルは統計的に最も可能性の高い続きを予測します。それは必ずしも正しいものではなく、安全なものでもなく、アプリが実際に何をしているかを理解しているものでもありません。ただ、次のもっともらしいコードの塊でしかありません。\nそれは、ボイラープレートには十分でしょう。しかし、ロジックには? エッジケースには? 実際のエンジニアリングには? それは、会議で速く自信を持って話すものの、仕様を読まない人を雇うようなものです。\nこうしたツールのほとんどは、コンテキストを 読んで いません。少なくとも深くは読んでいません。近くの数行、おそらく関数名を取得するかもしれませんが、生成しているものを全体像に対して検証することは滅多にありません。課題のクロスチェックはしませんし、アーキテクチャレベルの認識もなく、ファイルやユースケースをまたいだ推論もありません。\n思慮深く、テスト可能で、コンテキストを認識した出力が必要な場合、速度を落とし、俯瞰し、実際に問題に取り組むAIシステムが必要です。\nそれがスローAIの役割です。そして、AIが何をしているかを理解する時間を取ると、ハルシネーションを止めて実際にサポートしてくれるようになることがわかります。\nAIが遅いときに優れている理由\n核心として、大規模言語モデルは統計的推論マシンだということです。確率、パターン、そして(願わくば)あなたが与えたコンテキストに基づいて、次に何が来るかを予測して出力を生成します。しかし、ほとんどの開発者が忘れている注意点があります。良い予測には、それなりの作業が必要だということです。これは特に、ロジックを書く、アーキテクチャを理解する、複数のステップにわたって推論するなど、複雑なことをモデルに求める場合に当てはまります。出力の品質は、多くの場合において、推論の深さや段階に直接結びついています。\nこれは、単純なプロンプトを超えて、マルチステージパイプラインとエージェント的な動作に移行する際において、特に当てはまります。AIツールが出力を検証し、関連ファイルを取り込み、矛盾をチェックし、または複数のアクション先を計画している場合、それは単に次のトークンを出力しているだけではないのです。それは思考しています。または、少なくとも、それに近いことを実行しています。\nこのような非線形推論は、単一のフォワードパスでは実行できません。それには反省と検索、計画、そして時には自己修正さえも含まれます。こうしたプロセスはレイテンシ・フレンドリーではなく、インテリジェンス・フレンドリーなのです。\n要するに、AIに複雑なコードで実際に助けてもらいたいなら、それを調理させる必要があるのです。\n遅いことが新しいスマート:なぜ私たちはAIに考えさせるのか\nスローAIは、私たちが話している用語の1つです。言い換えるなら包括的AIや正確なAI、あるいは正直に言えば「 実質的 役立ち有用なAI」と呼ぶこともできます。そして、それは今のAIプロダクトデザインで最もバズワードなアイデアの1つに密接に結びついています。そう、コンテキストエンジニアリングです。\nAIが問題について知っている、 関連性があって解析された 情報が多いほど、パフォーマンスは向上します。しかし、そのコンテキストは取り込まれ、解析され、優先順位が付けられ、推論される必要があります。そのようなパイプラインは、超低レイテンシAIの敵です… そしてそれは精度の敵でもあります。\nそして、それが私たちのAIコードレビューにおいて、最初のコメントを見るまでに最大5分かかる理由です。誤解しないでください。私たちは遅さを最適化しているわけではありません。コードベースとPRの複雑さに応じて、3分、あるいは1分でレビューを受け取ることもできます。私たちのパイプラインが複雑なのは、ユーザーが必要とする結果を出すために必要だからです。いつでも同時に実行されているプロセスの数を知りたくもないでしょう!\nしかし、どうでしょう? 複数のレビューおよび検証エージェントを使用した、非線形のマルチパスパイプラインでAIに時間をかけさせると、他のツールよりもノイズが少なく、より関連性の高いコードレビューコメントが生成されるのです。\n非線形推論は決して、速くありません。しかし、それは優れているのです。\nでは、なぜほとんどのAIツールは遅さよりも愚かさを選ぶのか?\nまず、スローAIはすべてのツールにとってオプションではありません。たとえば、AIコーディングエージェントに質問をしている場合、返信を5分間待てる人はいないでしょう。そのやり取りには、即時性への期待が本質的に備わっています。\nではコードレビューはどうでしょうか? 提出されたPRに対して、同僚がすぐに手を止めてコメントし始めることを期待する人はいません。したがって、ボットからのレビューの遅延も受け入れる余地があります。そして、そのレビューがより関連性が高いことで時間を節約するなら、その遅延を受け入れる価値は特に高くなります。\nしかし、なぜ多くの企業が、常に（実際にはそれを必要としないときでも）低レイテンシを優先するのでしょうか? まあ、私たちは訓練されてきましたし、即座の満足を期待するようユーザーを訓練してきました。ボタンをクリックすれば、速攻のレスポンスを得られます。関数名をタイプすれば、それについて考える前に提案を得られます。即効性がなければ壊れている、遅い、またはスタートアップがAWS料金の支払いを忘れたように感じられます。\nこれは非常に強く叩き込まれているため、企業は遅延よりも間違っている方を積極的に選択しています。そして、人々がそうするとき、私たちの開発文化には有害で、後ろ向きな何かがあります。\nなぜなら、真実はこうだからです。最高のAIツールは必ずしも速く感じられず、代わりに思慮深く感じられます。時々、彼らは一時停止します。時々、プロンプトを推論したり、関連するコードを検索したり、応答を検証したりするために余分な時間を必要とします。しかし、それは待つ価値があるのです。たとえば、OpenAIのDeep Research機能が質問により良く答えるためにインターネットを最大20分かけて調査するからといって、利用を止める人はいません。処理中に他のことをして、戻ってくるだけです。\nもはや遅い = 壊れているという意味ではありません。それはスマートという意味です。むしろ、AIに関しては、速度こそがバグです。開発プロセスに実際に価値を追加するAIツールが必要な場合、それには応答性から信頼性へ、即時性からインサイトへの移行が必要です。そして、特に開発者にとって、そのトレードオフは理にかなっています。\n私たちは、今後5年間で最も価値のあるアプリは、速度を最適化するものではなく、インテリジェンスを最適化するものになると信じています。速いけど不要な結果と、遅いけれど価値あるもの、どちらが欲しいですか?\nCodeRabbitのマントラ:ゆっくり動いて物事を修正する\nCodeRabbitでは、他のツールのように、速度を優先するAIパイプライン最適化に過剰投資はしません。私たちは、信頼のために最適化を行います。それはコードを理解し、コンテキスト全体で推論し、より良いソフトウェアを構築するのに実際に役立つ出力を生成する時間を取るシステムを受け入れることを意味します。確かに、クイックプロンプトをたたき出すよりも遅いです。しかし、その余分な時間は明瞭さやカバレッジ、そして自信へと変わります。\n「Move fast and break things（素早く行動し破壊せよ）」は、MVPをデリバリーするには素晴らしいものでした。しかし、品質をデリバリーすることに関しては、私たちは別のものを信じています。ゆっくり動いて物事を修正する、AIに場の空気を読ませる、話す前に考えさせる…そして、本当に自信のある自動補完ではなく、シニアエンジニアから得られるようなサポートを提供する。それが、間違ったAIをスローAIよりも優先する、現在の後ろ向きな文化から抜け出す唯一の方法です。\n私たちのレビューを試してみたいですか? こちらから 14日間の無料トライアルを入手してください!",
      "publishedAt": "2025-11-06T06:33:23.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:12.172Z",
      "score": 1.124186026971251
    },
    {
      "id": "ae0a7777a114dc2c96c2a7b741215cfc",
      "title": "The rise of ‘Slow AI’: Why devs should stop speedrunning stupid",
      "url": "https://coderabbit.ai/blog/the-rise-of-slow-ai-why-devs-should-stop-speedrunning-stupid",
      "content": "<p>For as long as we’ve been building with machines, we’ve followed one core rule: faster is better. Lower latency, higher throughput, less waiting; that was gospel. Nobody wanted to wait 600ms for a button to respond or watch a spinner that lasts longer than their attention span. If it was slow, it was broken. Case closed.</p>\n<p>So naturally, when AI tools started creeping into our dev workflows, autocomplete, agents, copilots, you name it,  the same principle applied. Make it fast. Make it feel instant. Make it look like <strong><em>magic</em></strong>.</p>\n<p>But here’s the thing: AI isn’t magic. It’s inference. It’s pipelines and RAG and context and tool calls. It’s juggling messy context and probabilistic guesses. And if you want something smarter than glorified autocomplete, you need to build a pipeline of processes to provide scaffolding for that. Which takes time to process. Anything less and you’re basically just speedrunning stupid. And speed isn’t anything to brag about when your tool is just wrong faster.</p>\n<p>At CodeRabbit, we prioritize what we call Slow AI. And we have the guts to say what a lot of AI companies are too afraid to: We’re going to make you wait.</p>\n<p>(And you’ll thank us for it).</p>\n<h2 id=\"heading-ai-dev-tools-are-often-fast-confident-and-wrong\">AI dev tools are often fast, confident, and <strong><em>wrong</em></strong></h2>\n<p>If you've used an AI coding agent lately, you've probably seen it: a shockingly fast suggestion pops up almost as soon as you stop typing. It looks legit. But then… it fails silently. Or spectacularly. Or worse,  it passes the test and breaks something two files over.</p>\n<p>Why? Because most AI dev tools today are optimized for one thing: speed. Type a few tokens and the model predicts the most statistically likely continuation not necessarily the correct one, not the secure one, not the one that actually understands what your app is doing. Just the next plausible blob of code.</p>\n<p>That’s fine for boilerplate. But for logic? For edge cases? For actual engineering? It’s kind of like hiring someone who talks fast and confidently in meetings but never reads the specs.</p>\n<p>Most of these tools don’t <em>read</em> context, at least not deeply. They might grab a few nearby lines, maybe the function name, but they rarely verify what they’re generating against the bigger picture. No issue cross-checking. No architecture-level awareness. No reasoning across files or use cases.</p>\n<p>If you want outputs that are thoughtful, testable, and context-aware, you need AI systems that <em>slow down</em>, zoom out, and actually engage with the problem.</p>\n<p>That’s what Slow AI does. And it turns out, when your AI takes the time to understand what it’s doing, it stops hallucinating and starts <em>actually</em> helping.</p>\n<h2 id=\"heading-why-ai-is-better-when-its-slow\"><strong>Why AI is better when it’s slow</strong></h2>\n<p>At their core, large language models are statistical reasoning machines. They generate output by predicting what comes next based on probability, patterns, and (hopefully) the context you’ve given them. But here's the caveat most devs forget: good predictions take work. This is especially true when you're asking the model to do something complex like write logic, understand architecture, or reason across multiple steps. The quality of the output is often tied directly to the depth or stages of its inference.</p>\n<p>This is particularly true when you move beyond simple prompts and into multi-stage pipelines and agentic behavior. When an AI tool is verifying outputs, pulling in relevant files, checking for contradictions, or planning several actions ahead, it’s not just spitting out the next token… it’s <em>thinking</em>. Or, at least, performing a rough approximation of it.</p>\n<p>That kind of non-linear reasoning can’t be done in a single forward pass. It involves reflection, retrieval, planning, and sometimes even self-correction. These processes aren’t latency-friendly, they’re intelligence-friendly.</p>\n<p>In short: if you want AI to actually help on complex code, you have to let it cook.</p>\n<h2 id=\"heading-slow-is-the-new-smart-why-we-let-our-ai-think\">Slow is the new smart: Why we let our AI think</h2>\n<p>Slow AI is one term for what we’re talking about. But it could just as easily be called Comprehensive AI or Accurate AI or even <em>Actually</em> Helpful and Useful AI if we’re being honest. And it’s inextricably tied to one of the buzziest ideas in AI product design right now: context engineering.</p>\n<p>The more <em>relevant and parsed</em> info an AI knows about the problem, the better it performs but that context has to be pulled in, parsed, prioritized and reasoned over. That kind of pipeline is the enemy of ultra-low latency AI… and it’s also the enemy of accuracy.</p>\n<p>And that’s why our AI code reviews can take up to five minutes before you see the first comment. Don’t get us wrong, we’re not optimizing for slowness. You could get a review in three minutes or even one minute depending on the complexity of your codebase and PR. Our pipeline is complex because that’s what’s <em>required</em> to do the job our users need it to do. You don’t even want to <strong><em>know</em> th</strong>e number of concurrent processes we have going on at any time!</p>\n<p>But guess what? When we let our AI take its time using a non-linear, multi-pass pipeline with multiple review and verification agents, it generates less noise and more relevant code review comments than other tools.</p>\n<p>Non-linear reasoning isn’t fast. But it’s good.</p>\n<h2 id=\"heading-so-why-do-most-ai-tools-choose-stupid-over-slow\">So, why do most AI tools choose <em>stupid over slow?</em></h2>\n<p>Well, first, Slow AI isn’t an option for every tool. If you’re asking an AI coding agent a question, for example, you’re not going to wait five minutes for it to reply. There’s an expectation of immediacy inherent in that exchange.</p>\n<p>But code reviews? No one expects their co-worker to immediately drop what they’re doing and start commenting on a PR when it’s submitted. So, they’re willing to accept a delay in a review from a bot as well. And they’re especially willing to accept that delay if that review saves them time by being more relevant.</p>\n<p>But why do so many companies still prioritize low latency when their use cases don’t really require it? Well, we’ve been trained, and trained our users, to expect instant gratification. Click a button, get a dopamine hit. Type a function name, get a suggestion before you even think about it. Anything else feels broken, laggy, or like your startup forgot to pay its AWS bill.</p>\n<p>This has been drilled into us so hard that companies are out there actively choosing being wrong over being slow. And there’s something toxic and backwards about our development culture when folks do that.</p>\n<p>Because here’s the truth: the best AI tools don’t always feel fast. They feel thoughtful. Sometimes they pause. Sometimes they take an extra beat to reason through your prompt, retrieve relevant code, or validate their response. And that’s something worth waiting for. After all, no one is less likely to use OpenAI’s Deep Research feature because it takes up to 20 minutes to comb the internet for info to better answer your question. You just do something else while it’s processing and circle back.</p>\n<p>Slow doesn’t mean busted anymore, it means <em>smart</em>. If anything, speed is the bug when it comes to AI. If we want AI tools that actually add value to the development process, that requires a shift from responsiveness to reliability, from immediacy to insight. And for developers especially, that tradeoff makes sense.</p>\n<p>We believe that the most valuable apps in the next five years won’t be the ones that optimize for speed but the ones that optimize for intelligence. Who wants fast garbage over slow value?</p>\n<h2 id=\"heading-coderabbits-mantra-move-slow-and-fix-things\">CodeRabbit’s mantra: Move slow and fix things</h2>\n<p>At CodeRabbit, we don’t optimize our AI pipelines for speed at all costs like everyone else. We optimize for trust. That means embracing systems that take the time to understand your code, reason across context, and generate outputs that actually help you build better software. Yes, it’s slower than hammering out a quick prompt. But that extra time buys you clarity, coverage, and confidence.</p>\n<p>“<strong>Move fast and break things</strong>” was great for shipping MVPs. But when it comes to shipping <em>quality</em>, we believe in something else: Move slow and fix things. Let the AI read the room. Let it think before it speaks. And let it give you the kind of help you’d expect from a senior engineer, not just a really confident autocomplete. That’s the only way to break out of this backwards culture that prioritizes wrong AI over slow AI.</p>\n<p><strong>Want to try our reviews out? Get a</strong> <a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong>14-day free trial here!</strong></a></p>\n",
      "summary": "For as long as we’ve been building with machines, we’ve followed one core rule: faster is better. Lower latency, higher throughput, less waiting; that was gospel. Nobody wanted to wait 600ms for a button to respond or watch a spinner that lasts longer than their attention span. If it was slow, it was broken. Case closed.\nSo naturally, when AI tools started creeping into our dev workflows, autocomplete, agents, copilots, you name it,  the same principle applied. Make it fast. Make it feel instant. Make it look like magic.\nBut here’s the thing: AI isn’t magic. It’s inference. It’s pipelines and RAG and context and tool calls. It’s juggling messy context and probabilistic guesses. And if you want something smarter than glorified autocomplete, you need to build a pipeline of processes to provide scaffolding for that. Which takes time to process. Anything less and you’re basically just speedrunning stupid. And speed isn’t anything to brag about when your tool is just wrong faster.\nAt CodeRabbit, we prioritize what we call Slow AI. And we have the guts to say what a lot of AI companies are too afraid to: We’re going to make you wait.\n(And you’ll thank us for it).\nAI dev tools are often fast, confident, and wrong\nIf you've used an AI coding agent lately, you've probably seen it: a shockingly fast suggestion pops up almost as soon as you stop typing. It looks legit. But then… it fails silently. Or spectacularly. Or worse,  it passes the test and breaks something two files over.\nWhy? Because most AI dev tools today are optimized for one thing: speed. Type a few tokens and the model predicts the most statistically likely continuation not necessarily the correct one, not the secure one, not the one that actually understands what your app is doing. Just the next plausible blob of code.\nThat’s fine for boilerplate. But for logic? For edge cases? For actual engineering? It’s kind of like hiring someone who talks fast and confidently in meetings but never reads the specs.\nMost of these tools don’t read context, at least not deeply. They might grab a few nearby lines, maybe the function name, but they rarely verify what they’re generating against the bigger picture. No issue cross-checking. No architecture-level awareness. No reasoning across files or use cases.\nIf you want outputs that are thoughtful, testable, and context-aware, you need AI systems that slow down, zoom out, and actually engage with the problem.\nThat’s what Slow AI does. And it turns out, when your AI takes the time to understand what it’s doing, it stops hallucinating and starts actually helping.\nWhy AI is better when it’s slow\nAt their core, large language models are statistical reasoning machines. They generate output by predicting what comes next based on probability, patterns, and (hopefully) the context you’ve given them. But here's the caveat most devs forget: good predictions take work. This is especially true when you're asking the model to do something complex like write logic, understand architecture, or reason across multiple steps. The quality of the output is often tied directly to the depth or stages of its inference.\nThis is particularly true when you move beyond simple prompts and into multi-stage pipelines and agentic behavior. When an AI tool is verifying outputs, pulling in relevant files, checking for contradictions, or planning several actions ahead, it’s not just spitting out the next token… it’s thinking. Or, at least, performing a rough approximation of it.\nThat kind of non-linear reasoning can’t be done in a single forward pass. It involves reflection, retrieval, planning, and sometimes even self-correction. These processes aren’t latency-friendly, they’re intelligence-friendly.\nIn short: if you want AI to actually help on complex code, you have to let it cook.\nSlow is the new smart: Why we let our AI think\nSlow AI is one term for what we’re talking about. But it could just as easily be called Comprehensive AI or Accurate AI or even Actually Helpful and Useful AI if we’re being honest. And it’s inextricably tied to one of the buzziest ideas in AI product design right now: context engineering.\nThe more relevant and parsed info an AI knows about the problem, the better it performs but that context has to be pulled in, parsed, prioritized and reasoned over. That kind of pipeline is the enemy of ultra-low latency AI… and it’s also the enemy of accuracy.\nAnd that’s why our AI code reviews can take up to five minutes before you see the first comment. Don’t get us wrong, we’re not optimizing for slowness. You could get a review in three minutes or even one minute depending on the complexity of your codebase and PR. Our pipeline is complex because that’s what’s required to do the job our users need it to do. You don’t even want to know the number of concurrent processes we have going on at any time!\nBut guess what? When we let our AI take its time using a non-linear, multi-pass pipeline with multiple review and verification agents, it generates less noise and more relevant code review comments than other tools.\nNon-linear reasoning isn’t fast. But it’s good.\nSo, why do most AI tools choose stupid over slow?\nWell, first, Slow AI isn’t an option for every tool. If you’re asking an AI coding agent a question, for example, you’re not going to wait five minutes for it to reply. There’s an expectation of immediacy inherent in that exchange.\nBut code reviews? No one expects their co-worker to immediately drop what they’re doing and start commenting on a PR when it’s submitted. So, they’re willing to accept a delay in a review from a bot as well. And they’re especially willing to accept that delay if that review saves them time by being more relevant.\nBut why do so many companies still prioritize low latency when their use cases don’t really require it? Well, we’ve been trained, and trained our users, to expect instant gratification. Click a button, get a dopamine hit. Type a function name, get a suggestion before you even think about it. Anything else feels broken, laggy, or like your startup forgot to pay its AWS bill.\nThis has been drilled into us so hard that companies are out there actively choosing being wrong over being slow. And there’s something toxic and backwards about our development culture when folks do that.\nBecause here’s the truth: the best AI tools don’t always feel fast. They feel thoughtful. Sometimes they pause. Sometimes they take an extra beat to reason through your prompt, retrieve relevant code, or validate their response. And that’s something worth waiting for. After all, no one is less likely to use OpenAI’s Deep Research feature because it takes up to 20 minutes to comb the internet for info to better answer your question. You just do something else while it’s processing and circle back.\nSlow doesn’t mean busted anymore, it means smart. If anything, speed is the bug when it comes to AI. If we want AI tools that actually add value to the development process, that requires a shift from responsiveness to reliability, from immediacy to insight. And for developers especially, that tradeoff makes sense.\nWe believe that the most valuable apps in the next five years won’t be the ones that optimize for speed but the ones that optimize for intelligence. Who wants fast garbage over slow value?\nCodeRabbit’s mantra: Move slow and fix things\nAt CodeRabbit, we don’t optimize our AI pipelines for speed at all costs like everyone else. We optimize for trust. That means embracing systems that take the time to understand your code, reason across context, and generate outputs that actually help you build better software. Yes, it’s slower than hammering out a quick prompt. But that extra time buys you clarity, coverage, and confidence.\n“Move fast and break things” was great for shipping MVPs. But when it comes to shipping quality, we believe in something else: Move slow and fix things. Let the AI read the room. Let it think before it speaks. And let it give you the kind of help you’d expect from a senior engineer, not just a really confident autocomplete. That’s the only way to break out of this backwards culture that prioritizes wrong AI over slow AI.\nWant to try our reviews out? Get a 14-day free trial here!",
      "publishedAt": "2025-11-05T08:43:53.000Z",
      "author": "Howon Lee",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:12.172Z",
      "score": 2.045000640029101
    },
    {
      "id": "450986f4be07536817d735c23108e637",
      "title": "統一的プロンプトの終焉：もはやllmモデルに互換性はありません",
      "url": "https://coderabbit.ai/blog/the-end-of-one-sized-fits-all-prompts-why-llm-models-are-no-longer-interchangeable-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/the-end-of-one-sized-fits-all-prompts-why-llm-models-are-no-longer-interchangeable\">Why LLM models are no longer interchangeable</a>の意訳です。</p>\n<p>開発者やプロダクトビルダーにとって、この数年間はLLMがアプリケーション開発を導いてきました。プロダクトを改善したいなら、最新のLLMを利用すれば良い。ただモデルを切り替えるだけで、ツールの性能を一段階引き上げられるのです。</p>\n<p>しかし、その時代は終わりました。AnthropicのClaude Sonnet 4.5やOpenAIのGPT-5-Codexのような新しいモデルは、根本的に異なる方向へ分岐し始めています。どのモデルを使うかという選択は、もはや単なるエンジニアリング上の判断ではなく、極めて重要な<strong>プロダクト上の意思決定</strong>なのです。モデルを切り替えた瞬間に、あなたのプロダクトの「質感」そのものが変わります。</p>\n<p>いわゆる「万能モデル時代」は終焉を迎えました。あなたが選ぶモデルは、あなたのプロダクトが<strong>何であるか、何をするのか、どのように動作するのか</strong> を象徴する存在になります。たとえあなたがそう意図していなくても、です。</p>\n<p>本記事では、この新しい時代における3つの驚くべき発見を紹介します。それは「LLM選択がプロダクトの表明になった理由」「モデルが持つ明確な個性とスタイルの違い」、そして「プロンプトが単一命令から適応的システムへ進化すべき理由」の3つです。</p>\n<h2 id=\"heading-1-llm\">学びポイント 1: LLMの選択はプロダクトの“声明”である</h2>\n<p>LLMモデルの選択は、もはや「新しいAPIを実装すれば済む」といった単純な技術的決定ではありません。これは、<strong>どんなユーザー体験を作りたいのか、どのような失敗を許容するのか、何を最適化したいのか、どの指標で優位に立ちたいのか</strong>という、プロダクトの方向性を決める意思決定です。</p>\n<p>モデルはそれぞれ固有の「性格」や「推論方法」「直感」を持つようになっており、それがプロダクトの“感触”や“振る舞い”を直接的に形作ります。単に「出力が正しいかどうか」ではなく、「どのように考え、どのように伝えるか」まで変わります。違うモデルを選べば、ツールの能力からユーザーとの対話の仕方まで、すべてが異なるのです。</p>\n<p>では、モデルの定量的な性能だけを測る従来型ベンチマークが通用しない今、何を頼りにプロダクトの方向を定めれば良いのでしょうか？チームやユーザーへのアンケート、フォーカスグループもありますが、厳密に実施しなければ客観性に欠ける恐れがあります。</p>\n<p>CodeRabbitでは、この選択を客観化するために、独自の<a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning-ja\">重要指標のメトリクス</a>を作成しました。このメトリクスは、単なる性能や精度だけを見ません。<strong>可読性、冗長性、信号対雑音比</strong>など、多面的に評価します。</p>\n<p>このような指標により、焦点は「性能」や「リーダーボードの順位」から、「プロダクトとユーザーにとって本当に重要な要素」へと移ります。例えば、技術的に正しくても影響の少ない提案が多すぎれば、ユーザーを疲弊させ、かつトークンを浪費します。理論上「賢い」モデルでも、ユーザーのワークフローに合わなければ体験を悪化させます。</p>\n<p>自社のメトリクスを定義し、新しいモデルが自社とユーザーのニーズを満たすかを測ることを強く推奨します。これらのメトリクスは静的なものではなく、ユーザー行動やフィードバックによって進化させるべきです。目標は、「ユーザーの好みを予測できる基準」を見つけることです。</p>\n<p>結論として、最適なモデルとは「リーダーボード上の1位」ではなく、<strong>あなたの設計した体験やユーザーのニーズに最も本能的に合うモデル</strong>です。</p>\n<h2 id=\"heading-2\">学びポイント 2: フロンティアモデルは「性格」が分岐した</h2>\n<p>モデルはこれまで以上に「<strong>作られるものではなく、育つもの</strong>」となっており、その結果、モデル世代ごとに固有の直感と行動特性が生まれています。ポストトレーニングの手法（cookbook）の違いが、モデルクラスごとの方向性を根本的に変えました。1つのモデルで完璧に動くプロンプトも、別のモデルでは通用しません。つまり、<strong>同じタスクに対する根本的なアプローチが異なる</strong>のです。</p>\n<p>これを理解する良い例えとして、モデルを異なる「職業的アーキタイプ」に喩えることができます。<br />Sonnet 4.5は几帳面な会計士出身の開発者、GPT-5-Codexは倫理意識の高い堅実なエンジニア、GPT-5はバグを徹底的に探す職人気質の開発者、Sonnet 4は活動的な新卒エンジニア。<br />GPT-5系はClaude系よりもソリューション空間を広く探索し、Claudeはプロンプトの文脈に忠実に留まる傾向があります。どのモデルが適しているかは、<strong>プロダクトが目指す目的</strong>によって完全に異なります。</p>\n<p>CodeRabbitでは、モデル評価と特性分析を体系的に行い、その結果を基にプロンプトとデプロイ方法を最適化しています。たとえば、Sonnet 4.5とGPT-5-Codexを比較すると、Sonnet 4.5は「<strong>高リコール型のポイント修正者</strong>」、GPT-5-Codexは「<strong>ピンポイントなパッチ生成者</strong>」として性質づけられます。</p>\n<p>こうした定性的な違いは、明確な運用上の違いに転化します。</p>\n<div class=\"hn-table\">\n<table>\n<thead>\n<tr>\n<td>次元</td><td>Claude Sonnet 4.5</td><td>GPT-5-Codex</td></tr>\n</thead>\n<tbody>\n<tr>\n<td>デフォルトの語彙選択</td><td>“Critical,” “Add,” “Remove,” “Consider”</td><td>“Fix,” “Guard,” “Prevent,” “Restore,” “Drop”</td></tr>\n<tr>\n<td>例の効率性</td><td>明示的なルールを好む。命令形を覚えやすい</td><td>例が少なくても長い文脈でフォーマットを維持できる</td></tr>\n<tr>\n<td>思考スタイル</td><td>慎重。多くのバグを見つけるが、重要な1つを見逃すことも</td><td>柔軟。必要に応じて深く考え、再確認を要しない。難解なバグを捕捉しやすい</td></tr>\n<tr>\n<td>行動傾向</td><td>広範囲に修正提案。コメントが多く、人間的。致命的でない問題も拾う</td><td>簡潔でバランスの取れた研究的レビュー。副次的影響を指摘する傾向</td></tr>\n<tr>\n<td>レビュー構造</td><td>「何が悪い」「なぜ悪い」「具体的修正コード」</td><td>「何をすべきか」「なぜすべきか」「修正コード＋影響」</td></tr>\n<tr>\n<td>文脈認識</td><td>コンテキストウィンドウを意識。トークン管理が巧み</td><td>明示的なウィンドウ意識は弱い（時計なしで料理するような感覚）</td></tr>\n<tr>\n<td>冗長性</td><td>高い。読みやすいが語数が倍増</td><td>低い。情報密度が高く、読むのに集中を要する</td></tr>\n</tbody>\n</table>\n</div><h2 id=\"heading-3\">学びポイント 3: プロンプトはもはや単一構造ではない</h2>\n<p>モデルの根本的な性質が分岐したことで、あるモデル用に書いたプロンプトを他モデルで「そのまま」使うことはできなくなっています。<br />たとえばClaude用の厳格な命令プロンプトはGPT-5-Codexでは過剰拘束になり、Codex用に推論重視で最適化したプロンプトは、Claudeで性能を発揮できません。つまり、<strong>「一枚岩のプロンプト時代」は完全に終わった</strong>のです。</p>\n<p>では、新モデルを導入したいエンジニアリングチームはどうすればよいでしょうか？<br />答えは――<strong>より多くのプロンプトエンジニアリング</strong>です。ただし嘆く必要はありません。いくつかの実践的な方法があります。</p>\n<h3 id=\"heading-44ox44ot44oz44ox44oi44o744k144ow44om44ol44od44oi44gu55m75ac0\">プロンプト・サブユニットの登場</h3>\n<p>CodeRabbitで見出した解決策の一つが「<strong>プロンプト・サブユニット</strong>」です。<br />これは、モデルに依存しない中核プロンプト（基本タスクと一般指示）を定義し、その上にモデル固有のサブユニット（スタイル、フォーマット、例示）を積み上げる構成です。</p>\n<p>たとえばCodexとSonnet 4.5では実装詳細が大きく異なりますが、次のような発見がありました：</p>\n<ul>\n<li><p><strong>Claude:</strong> 「DO」「DO NOT」のような強い命令語を使用する。Anthropic系モデルはシステムプロンプトの末尾情報をよく参照し、長文でもフォーマット遵守が得意。明示的な指示を好む。</p>\n</li>\n<li><p><strong>GPT-5:</strong> 一般的で整合性のある指示を使用する。OpenAI系はシステムプロンプトの下部ほど注意力が減衰するため、長文では出力フォーマットを忘れがち。抽象的なガイダンスを好み、推論の深さを示す傾向がある。</p>\n</li>\n</ul>\n<h3 id=\"heading-eval\">ユーザーフィードバックと評価（eval）</h3>\n<p>もう一つの解決策は、<strong>ユーザーフィードバックと内部評価による継続的アップデート</strong>です。<br />AIコードレビューボットなどLLMアプリの最適化において、最も重要なのは外部ベンチマークではなく、「ユーザーが出力に納得できるか」です。</p>\n<p>モデル間で「技術的正確性」が高くても、過剰なコメントや冗長性があると価値を下げてしまいます。<br />したがって、<strong>受容率、S/N比、p95レイテンシ、コスト</strong>といった実運用メトリクスを測定し、プロンプトを少しずつ調整することで、システムをユーザー期待とプロダクト目標に整合させ続けることができます。</p>\n<p>ベンチマークでの定量的結果が良くても、ユーザー受容率が低い――そんな事態は避けるべきです。</p>\n<h2 id=\"heading-44g44go44kb\">まとめ</h2>\n<p>プロンプトエンジニアリングは、「万能テンプレート」から「モデル特化型パラダイム」へと変わりました。<br />脆弱な単一プロンプトや「差し替え可能なモデル」の時代は終わりです。これからは、<strong>モジュラー型プロンプト設計</strong>と<strong>意図的なモデル選択</strong>が、プロダクトの強靭性を生みます。</p>\n<p>モデルが進化し続ける以上、LLMスタックやプロンプトも画一的であってはいけません。<br />それは「生きたシステム」として扱うべきです。<strong>調整し、テストし、確認し、繰り返す。</strong></p>\n<p>また、最新モデルの実運用挙動に関する詳細なベンチマークもぜひ確認してください。今後の選択に必要なデータが得られるでしょう。</p>\n<ul>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks\">GPT-5 Codex: How it solves for GPT-5's drawbacks</a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/claude-sonnet-45-better-performance-but-a-paradox\">Claude Sonnet 4.5: Better performance but a paradox</a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">Benchmarking GPT-5: Why it’s a generational leap in reasoning</a></p>\n</li>\n</ul>\n<p><strong><em>CodeRabbitを14日間無料でお試しください。</em></strong><br /><a target=\"_blank\" href=\"https://coderabbit.link/rk7tdeC\">https://coderabbit.link/rk7tdeC</a></p>\n",
      "summary": "Why LLM models are no longer interchangeableの意訳です。\n開発者やプロダクトビルダーにとって、この数年間はLLMがアプリケーション開発を導いてきました。プロダクトを改善したいなら、最新のLLMを利用すれば良い。ただモデルを切り替えるだけで、ツールの性能を一段階引き上げられるのです。\nしかし、その時代は終わりました。AnthropicのClaude Sonnet 4.5やOpenAIのGPT-5-Codexのような新しいモデルは、根本的に異なる方向へ分岐し始めています。どのモデルを使うかという選択は、もはや単なるエンジニアリング上の判断ではなく、極めて重要なプロダクト上の意思決定なのです。モデルを切り替えた瞬間に、あなたのプロダクトの「質感」そのものが変わります。\nいわゆる「万能モデル時代」は終焉を迎えました。あなたが選ぶモデルは、あなたのプロダクトが何であるか、何をするのか、どのように動作するのか を象徴する存在になります。たとえあなたがそう意図していなくても、です。\n本記事では、この新しい時代における3つの驚くべき発見を紹介します。それは「LLM選択がプロダクトの表明になった理由」「モデルが持つ明確な個性とスタイルの違い」、そして「プロンプトが単一命令から適応的システムへ進化すべき理由」の3つです。\n学びポイント 1: LLMの選択はプロダクトの“声明”である\nLLMモデルの選択は、もはや「新しいAPIを実装すれば済む」といった単純な技術的決定ではありません。これは、どんなユーザー体験を作りたいのか、どのような失敗を許容するのか、何を最適化したいのか、どの指標で優位に立ちたいのかという、プロダクトの方向性を決める意思決定です。\nモデルはそれぞれ固有の「性格」や「推論方法」「直感」を持つようになっており、それがプロダクトの“感触”や“振る舞い”を直接的に形作ります。単に「出力が正しいかどうか」ではなく、「どのように考え、どのように伝えるか」まで変わります。違うモデルを選べば、ツールの能力からユーザーとの対話の仕方まで、すべてが異なるのです。\nでは、モデルの定量的な性能だけを測る従来型ベンチマークが通用しない今、何を頼りにプロダクトの方向を定めれば良いのでしょうか？チームやユーザーへのアンケート、フォーカスグループもありますが、厳密に実施しなければ客観性に欠ける恐れがあります。\nCodeRabbitでは、この選択を客観化するために、独自の重要指標のメトリクスを作成しました。このメトリクスは、単なる性能や精度だけを見ません。可読性、冗長性、信号対雑音比など、多面的に評価します。\nこのような指標により、焦点は「性能」や「リーダーボードの順位」から、「プロダクトとユーザーにとって本当に重要な要素」へと移ります。例えば、技術的に正しくても影響の少ない提案が多すぎれば、ユーザーを疲弊させ、かつトークンを浪費します。理論上「賢い」モデルでも、ユーザーのワークフローに合わなければ体験を悪化させます。\n自社のメトリクスを定義し、新しいモデルが自社とユーザーのニーズを満たすかを測ることを強く推奨します。これらのメトリクスは静的なものではなく、ユーザー行動やフィードバックによって進化させるべきです。目標は、「ユーザーの好みを予測できる基準」を見つけることです。\n結論として、最適なモデルとは「リーダーボード上の1位」ではなく、あなたの設計した体験やユーザーのニーズに最も本能的に合うモデルです。\n学びポイント 2: フロンティアモデルは「性格」が分岐した\nモデルはこれまで以上に「作られるものではなく、育つもの」となっており、その結果、モデル世代ごとに固有の直感と行動特性が生まれています。ポストトレーニングの手法（cookbook）の違いが、モデルクラスごとの方向性を根本的に変えました。1つのモデルで完璧に動くプロンプトも、別のモデルでは通用しません。つまり、同じタスクに対する根本的なアプローチが異なるのです。\nこれを理解する良い例えとして、モデルを異なる「職業的アーキタイプ」に喩えることができます。\nSonnet 4.5は几帳面な会計士出身の開発者、GPT-5-Codexは倫理意識の高い堅実なエンジニア、GPT-5はバグを徹底的に探す職人気質の開発者、Sonnet 4は活動的な新卒エンジニア。\nGPT-5系はClaude系よりもソリューション空間を広く探索し、Claudeはプロンプトの文脈に忠実に留まる傾向があります。どのモデルが適しているかは、プロダクトが目指す目的によって完全に異なります。\nCodeRabbitでは、モデル評価と特性分析を体系的に行い、その結果を基にプロンプトとデプロイ方法を最適化しています。たとえば、Sonnet 4.5とGPT-5-Codexを比較すると、Sonnet 4.5は「高リコール型のポイント修正者」、GPT-5-Codexは「ピンポイントなパッチ生成者」として性質づけられます。\nこうした定性的な違いは、明確な運用上の違いに転化します。\n次元Claude Sonnet 4.5GPT-5-Codex\nデフォルトの語彙選択“Critical,” “Add,” “Remove,” “Consider”“Fix,” “Guard,” “Prevent,” “Restore,” “Drop”\n例の効率性明示的なルールを好む。命令形を覚えやすい例が少なくても長い文脈でフォーマットを維持できる\n思考スタイル慎重。多くのバグを見つけるが、重要な1つを見逃すことも柔軟。必要に応じて深く考え、再確認を要しない。難解なバグを捕捉しやすい\n行動傾向広範囲に修正提案。コメントが多く、人間的。致命的でない問題も拾う簡潔でバランスの取れた研究的レビュー。副次的影響を指摘する傾向\nレビュー構造「何が悪い」「なぜ悪い」「具体的修正コード」「何をすべきか」「なぜすべきか」「修正コード＋影響」\n文脈認識コンテキストウィンドウを意識。トークン管理が巧み明示的なウィンドウ意識は弱い（時計なしで料理するような感覚）\n冗長性高い。読みやすいが語数が倍増低い。情報密度が高く、読むのに集中を要する\n学びポイント 3: プロンプトはもはや単一構造ではない\nモデルの根本的な性質が分岐したことで、あるモデル用に書いたプロンプトを他モデルで「そのまま」使うことはできなくなっています。\nたとえばClaude用の厳格な命令プロンプトはGPT-5-Codexでは過剰拘束になり、Codex用に推論重視で最適化したプロンプトは、Claudeで性能を発揮できません。つまり、「一枚岩のプロンプト時代」は完全に終わったのです。\nでは、新モデルを導入したいエンジニアリングチームはどうすればよいでしょうか？\n答えは――より多くのプロンプトエンジニアリングです。ただし嘆く必要はありません。いくつかの実践的な方法があります。\nプロンプト・サブユニットの登場\nCodeRabbitで見出した解決策の一つが「プロンプト・サブユニット」です。\nこれは、モデルに依存しない中核プロンプト（基本タスクと一般指示）を定義し、その上にモデル固有のサブユニット（スタイル、フォーマット、例示）を積み上げる構成です。\nたとえばCodexとSonnet 4.5では実装詳細が大きく異なりますが、次のような発見がありました：\nClaude: 「DO」「DO NOT」のような強い命令語を使用する。Anthropic系モデルはシステムプロンプトの末尾情報をよく参照し、長文でもフォーマット遵守が得意。明示的な指示を好む。\nGPT-5: 一般的で整合性のある指示を使用する。OpenAI系はシステムプロンプトの下部ほど注意力が減衰するため、長文では出力フォーマットを忘れがち。抽象的なガイダンスを好み、推論の深さを示す傾向がある。\nユーザーフィードバックと評価（eval）\nもう一つの解決策は、ユーザーフィードバックと内部評価による継続的アップデートです。\nAIコードレビューボットなどLLMアプリの最適化において、最も重要なのは外部ベンチマークではなく、「ユーザーが出力に納得できるか」です。\nモデル間で「技術的正確性」が高くても、過剰なコメントや冗長性があると価値を下げてしまいます。\nしたがって、受容率、S/N比、p95レイテンシ、コストといった実運用メトリクスを測定し、プロンプトを少しずつ調整することで、システムをユーザー期待とプロダクト目標に整合させ続けることができます。\nベンチマークでの定量的結果が良くても、ユーザー受容率が低い――そんな事態は避けるべきです。\nまとめ\nプロンプトエンジニアリングは、「万能テンプレート」から「モデル特化型パラダイム」へと変わりました。\n脆弱な単一プロンプトや「差し替え可能なモデル」の時代は終わりです。これからは、モジュラー型プロンプト設計と意図的なモデル選択が、プロダクトの強靭性を生みます。\nモデルが進化し続ける以上、LLMスタックやプロンプトも画一的であってはいけません。\nそれは「生きたシステム」として扱うべきです。調整し、テストし、確認し、繰り返す。\nまた、最新モデルの実運用挙動に関する詳細なベンチマークもぜひ確認してください。今後の選択に必要なデータが得られるでしょう。\nGPT-5 Codex: How it solves for GPT-5's drawbacks\nClaude Sonnet 4.5: Better performance but a paradox\nBenchmarking GPT-5: Why it’s a generational leap in reasoning\nCodeRabbitを14日間無料でお試しください。\nhttps://coderabbit.link/rk7tdeC",
      "publishedAt": "2025-10-25T00:45:11.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "benchmark_eval",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:12.173Z",
      "score": 0.49648747255466463
    },
    {
      "id": "da0ef8f119da69168d25fec79054c8cf",
      "title": "The end of one-sized-fits-all prompts: Why LLM models are no longer interchangeable",
      "url": "https://coderabbit.ai/blog/the-end-of-one-sized-fits-all-prompts-why-llm-models-are-no-longer-interchangeable",
      "content": "<p>For developers and product builders, one assumption has guided the last few years of LLM application development. To improve your product, just swap in the latest frontier large language model. Flip a single switch and your tool’s capabilities level up.</p>\n<p>But that era is over. We’re now seeing that new models like Anthropic’s Claude Sonnet 4.5 and OpenAI’s GPT-5-Codex have diverged in fundamental ways. The choice of which model to use is no longer a simple engineering decision but a critical product decision. Flip that switch today… and the very texture of your product changes. </p>\n<p>The one-size-fits-all model era is over; the model you choose now expresses something integral about what your product is and does, as well as, how it works. Whether you want it to or not.</p>\n<p>In this blog, we’ll explore three surprising takeaways from this new era: why your LLM is now a statement about your product, how models now have distinct personalities and styles, and why your prompts have to now evolve from monolithic instructions to adaptive systems. </p>\n<h2 id=\"heading-takeaway-1-llm-choice-is-now-a-statement-about-your-product\">Takeaway 1: LLM choice is now a statement about your product</h2>\n<p>Choosing a model is no longer a straightforward decision where the main consequence of your choice is having to implement a new API. It is now a product decision about the user experience you want to create, the failure modes you can tolerate, the economics you want to optimize for, and the metrics you want to excel in. </p>\n<p>Models have developed distinct “personalities,” ways of reasoning, and instincts that directly shape how your product feels and behaves that go beyond just whether its output is technically right or wrong. Choose a different model and everything from what your tool is capable of to how it communicates with your users is significantly different. </p>\n<p>So, in a world where traditional benchmarks that primarily or exclusively measure quantitative aspects of a model’s performance are no longer enough, what can you turn to for the data you need to chart your product’s direction? You could survey your team or your users or conduct focus groups but that could lack objectivity if you don’t do it in a rigorous manner. </p>\n<p>To make this choice objective for our team, we focused on creating an internal <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">North Star metrics matrix</a> at CodeRabbit. Our metrics don’t just look at raw performance or accuracy. We also take into account readability, verbosity, signal-to-noise ratios, and more.</p>\n<p>These kinds of metrics shift the focus from raw performance accuracy or leaderboard performance to what matters to our product and to our users. For example, a flood of low-impact suggestions, even if technically correct, burns user attention and consumes tokens. A theoretically “smarter” model can easily create a worse product experience if the output doesn’t align with your users’ workflow.</p>\n<p>I would strongly recommend creating your own North Star metrics to better gauge whether a new model meets your products’ and users’ needs. These shouldn’t be static metrics but should be informed by user feedback and user behavior in your product and evolve over time. Your goal is to find the right list of criteria to measure that predict your users preferences. </p>\n<p>What you’ll find is that the right model is the one whose instincts match the designed product behavior and your users’ needs, not the one at the top of any external leaderboard. </p>\n<h2 id=\"heading-takeaway-2-frontier-models-have-divergent-personalities\">Takeaway 2: Frontier models have divergent ‘personalities’</h2>\n<p>Models are (now more than ever) “<strong>grown, not built,</strong>” and as a result, the latest generation has developed distinct instincts and behaviors. Different post-training cookbooks have fundamentally changed the direction of each model class. A prompt that works perfectly for one model will not work the same in another. Their fundamental approaches to the same task have diverged.</p>\n<p>One powerful analogy that drives this point home is to think of the models as different professional archetypes. Sonnet 4.5 is like a meticulous accountant turned developer, meanwhile GPT-5-Codex is an upright ethical coder, GPT-5 is a bug-hunting detailed developer, and Sonnet 4 was a hyper-active new grad. The GPT-5 model class would make logical jumps further out in the solution space compared to the Claude model class, which tends to stay near the prompts itself. Which model is right for your use case and product, depends entirely on what you are wanting your product to achieve. </p>\n<p>At CodeRabbit, we take a methodical approach to model evaluation and characterization. We then use this data to improve how we prompt and deploy models, ensuring we are always using the right model for each use case within our product. To give you an example of how we look at the different models, let’s compare Sonnet 4.5 and GPT-5-Codex. Based on extensive internal use and evals, we characterized Sonnet 4.5 as a “<strong>high-recall point-fixer,</strong>” aiming for comprehensive coverage. In contrast, GPT-5-Codex acts as a “<strong>patch generator,</strong>” preferring surgical, local changes. </p>\n<p>These qualitative differences translate into hard, operational differences.  </p>\n<table><tbody><tr><td><p>Dimension</p></td><td><p>Claude Sonnet 4.5</p></td><td><p>GPT-5-Codex</p></td></tr><tr><td><p>Default Word Choice</p></td><td><p>“Critical,” “Add,” “Remove,” “Consider”</p></td><td><p>“Fix,” “Guard,” “Prevent,” “Restore,” “Drop”</p></td></tr><tr><td><p>Example-Efficiency </p></td><td><p>Remembers imperatives; benefits from explicit rules</p></td><td><p>Needs fewer examples; follows the formatting on longer context without additional prompting</p></td></tr><tr><td><p>Thinking Style</p></td><td><p>More cautious, catches more bugs but not as many of the critical one</p></td><td><p>Variable or elastic, less depth when not needed without need to reiterate the rules. Catches more of the hard-to-find bugs</p></td></tr><tr><td><p>Behavioral Tendencies</p></td><td><p>Wider spray of point-fixes, more commentary and hedging, inquisitive, more human-like review, finds more critical and non-critical issues</p></td><td><p>Verbose research-style rationales, notes on second-order effects to code, compact and balanced towards a code reviewer</p></td></tr><tr><td><p>Review Comment Structure</p></td><td><p>What’s wrong, why it’s wrong, concrete fix with code chunk</p></td><td><p>What to do, why do it, concrete fix with effects and code chunk</p></td></tr><tr><td><p>Context Awareness</p></td><td><p>Aware of its own context window, tracks token budget, persists/compresses based on headroom</p></td><td><p>Lacks explicit context window awareness (like cooking without a clock)</p></td></tr><tr><td><p>Verbosity</p></td><td><p>Higher, easier to read, double the word count</p></td><td><p>Lower, harder to read, information-dense</p></td></tr></tbody></table>\n\n<h2 id=\"heading-takeaway-3-end-of-an-era-prompts-are-no-longer-monoliths\">Takeaway 3: End of an era. Prompts are no longer monoliths</h2>\n<p>Because the fundamental behaviors of models have diverged, a prompt written for one model will not work “as is” on another anymore. For example, a directive-heavy prompt designed for Claude can feel over-constrained on GPT-5-Codex, and a prompt optimized for Codex to explore deep reasoning behavior will likely underperform on Claude. That means that the era of the monolithic, one-size-fits-all prompt is over. </p>\n<p>So, what does that mean for engineering teams who want to switch between models or adopt the newest models as they’re released? It means even more prompt engineering! But before you groan at the thought — there are some hacks to make this easier. </p>\n<h3 id=\"heading-the-rise-of-prompt-subunits\">The rise of prompt subunits</h3>\n<p>The first practical solution we’ve found at CodeRabbit is to introduce “<strong>prompt subunits.</strong>” This architecture consists of a model-agnostic core prompt that defines the core tasks and general instructions. This is then layered on top of smaller, model-specific prompt subunits that handle style, formatting, and examples – and which can be customized to individual models. </p>\n<p>When it comes to Codex and Sonnet 4.5, the implementation details for these subunits are likely to be starkly different. We’ve found a few tricks from our prompt testing with both models that we would like to share:</p>\n<ul>\n<li><p><strong>Claude:</strong> Use strong language like \"DO\" and \"DO NOT.\" Anthropic models pay attention to the latest information in a system prompt and are excellent at following output format specifications, even in long contexts. They prefer being told explicitly what to do.</p>\n</li>\n<li><p><strong>GPT-5:</strong> Use general instructions that are clearly aligned. OpenAI models’ attention decreases from top to bottom in a system prompt. These models may forget output format instructions in long contexts. They prefer generic guidance and tend to \"think on guidance,\" demonstrating a deeper reasoning process.</p>\n</li>\n</ul>\n<h3 id=\"heading-user-feedback-and-evals\">User feedback and evals</h3>\n<p>The second solution is to implement <strong>continuous updates driven by user feedback and internal evaluations.</strong> The best practice for optimizing an AI code-review bot or for that matter any LLM applications isn’t using an external benchmark; it’s checking to see if users accept the output. </p>\n<p>Evals are more important than ever but have to be designed more tightly around acceptability by users instead of raw performance since one model might be technically correct significantly more than another model but might drown the user in nitpicky and verbose comments, diluting its value to users. By measuring the metrics that matter ~ acceptance rate, signal-to-noise ratio, p95 latency, cost, among others - and tuning prompts in small steps, the system will remain aligned with user expectations and product goals. The last thing you want is great quantitative results on benchmarks and tests but low user acceptance. </p>\n<h2 id=\"heading-conclusion\">Conclusion</h2>\n<p>This shift from one-size-fits-all prompt engineering to a new model specific paradigm is critical. The days of brittle, monolithic prompts and plug-and-play model swaps are over. Instead, modular prompting, paired with deliberate model choice, give your product resilience. </p>\n<p>The ground will keep shifting as models evolve so your LLM stack and prompts shouldn’t be static. Treat it like a living system. Tune, test, listen, repeat. </p>\n<p>Also, be sure to check out our published detailed benchmarks on how the latest models behave in production. That gives you more data on what to expect from them. </p>\n<ul>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks\">GPT-5 Codex: How it solves for GPT-5's drawbacks</a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/claude-sonnet-45-better-performance-but-a-paradox\">Claude Sonnet 4.5: Better performance but a paradox</a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">Benchmarking GPT-5: Why it’s a generational leap in reasoning</a></p>\n</li>\n</ul>\n<p><strong><em>Try CodeRabbit with a 1</em></strong><a target=\"_blank\" href=\"https://coderabbit.link/rk7tdeC\"><strong><em>4-day free trial.</em></strong></a></p>\n",
      "summary": "For developers and product builders, one assumption has guided the last few years of LLM application development. To improve your product, just swap in the latest frontier large language model. Flip a single switch and your tool’s capabilities level up.\nBut that era is over. We’re now seeing that new models like Anthropic’s Claude Sonnet 4.5 and OpenAI’s GPT-5-Codex have diverged in fundamental ways. The choice of which model to use is no longer a simple engineering decision but a critical product decision. Flip that switch today… and the very texture of your product changes. \nThe one-size-fits-all model era is over; the model you choose now expresses something integral about what your product is and does, as well as, how it works. Whether you want it to or not.\nIn this blog, we’ll explore three surprising takeaways from this new era: why your LLM is now a statement about your product, how models now have distinct personalities and styles, and why your prompts have to now evolve from monolithic instructions to adaptive systems. \nTakeaway 1: LLM choice is now a statement about your product\nChoosing a model is no longer a straightforward decision where the main consequence of your choice is having to implement a new API. It is now a product decision about the user experience you want to create, the failure modes you can tolerate, the economics you want to optimize for, and the metrics you want to excel in. \nModels have developed distinct “personalities,” ways of reasoning, and instincts that directly shape how your product feels and behaves that go beyond just whether its output is technically right or wrong. Choose a different model and everything from what your tool is capable of to how it communicates with your users is significantly different. \nSo, in a world where traditional benchmarks that primarily or exclusively measure quantitative aspects of a model’s performance are no longer enough, what can you turn to for the data you need to chart your product’s direction? You could survey your team or your users or conduct focus groups but that could lack objectivity if you don’t do it in a rigorous manner. \nTo make this choice objective for our team, we focused on creating an internal North Star metrics matrix at CodeRabbit. Our metrics don’t just look at raw performance or accuracy. We also take into account readability, verbosity, signal-to-noise ratios, and more.\nThese kinds of metrics shift the focus from raw performance accuracy or leaderboard performance to what matters to our product and to our users. For example, a flood of low-impact suggestions, even if technically correct, burns user attention and consumes tokens. A theoretically “smarter” model can easily create a worse product experience if the output doesn’t align with your users’ workflow.\nI would strongly recommend creating your own North Star metrics to better gauge whether a new model meets your products’ and users’ needs. These shouldn’t be static metrics but should be informed by user feedback and user behavior in your product and evolve over time. Your goal is to find the right list of criteria to measure that predict your users preferences. \nWhat you’ll find is that the right model is the one whose instincts match the designed product behavior and your users’ needs, not the one at the top of any external leaderboard. \nTakeaway 2: Frontier models have divergent ‘personalities’\nModels are (now more than ever) “grown, not built,” and as a result, the latest generation has developed distinct instincts and behaviors. Different post-training cookbooks have fundamentally changed the direction of each model class. A prompt that works perfectly for one model will not work the same in another. Their fundamental approaches to the same task have diverged.\nOne powerful analogy that drives this point home is to think of the models as different professional archetypes. Sonnet 4.5 is like a meticulous accountant turned developer, meanwhile GPT-5-Codex is an upright ethical coder, GPT-5 is a bug-hunting detailed developer, and Sonnet 4 was a hyper-active new grad. The GPT-5 model class would make logical jumps further out in the solution space compared to the Claude model class, which tends to stay near the prompts itself. Which model is right for your use case and product, depends entirely on what you are wanting your product to achieve. \nAt CodeRabbit, we take a methodical approach to model evaluation and characterization. We then use this data to improve how we prompt and deploy models, ensuring we are always using the right model for each use case within our product. To give you an example of how we look at the different models, let’s compare Sonnet 4.5 and GPT-5-Codex. Based on extensive internal use and evals, we characterized Sonnet 4.5 as a “high-recall point-fixer,” aiming for comprehensive coverage. In contrast, GPT-5-Codex acts as a “patch generator,” preferring surgical, local changes. \nThese qualitative differences translate into hard, operational differences.  \n\n\nDimension\n\nClaude Sonnet 4.5\n\nGPT-5-Codex\n\n\nDefault Word Choice\n\n“Critical,” “Add,” “Remove,” “Consider”\n\n“Fix,” “Guard,” “Prevent,” “Restore,” “Drop”\n\n\nExample-Efficiency \n\nRemembers imperatives; benefits from explicit rules\n\nNeeds fewer examples; follows the formatting on longer context without additional prompting\n\n\nThinking Style\n\nMore cautious, catches more bugs but not as many of the critical one\n\nVariable or elastic, less depth when not needed without need to reiterate the rules. Catches more of the hard-to-find bugs\n\n\nBehavioral Tendencies\n\nWider spray of point-fixes, more commentary and hedging, inquisitive, more human-like review, finds more critical and non-critical issues\n\nVerbose research-style rationales, notes on second-order effects to code, compact and balanced towards a code reviewer\n\n\nReview Comment Structure\n\nWhat’s wrong, why it’s wrong, concrete fix with code chunk\n\nWhat to do, why do it, concrete fix with effects and code chunk\n\n\nContext Awareness\n\nAware of its own context window, tracks token budget, persists/compresses based on headroom\n\nLacks explicit context window awareness (like cooking without a clock)\n\n\nVerbosity\n\nHigher, easier to read, double the word count\n\nLower, harder to read, information-dense\n\n\nTakeaway 3: End of an era. Prompts are no longer monoliths\nBecause the fundamental behaviors of models have diverged, a prompt written for one model will not work “as is” on another anymore. For example, a directive-heavy prompt designed for Claude can feel over-constrained on GPT-5-Codex, and a prompt optimized for Codex to explore deep reasoning behavior will likely underperform on Claude. That means that the era of the monolithic, one-size-fits-all prompt is over. \nSo, what does that mean for engineering teams who want to switch between models or adopt the newest models as they’re released? It means even more prompt engineering! But before you groan at the thought — there are some hacks to make this easier. \nThe rise of prompt subunits\nThe first practical solution we’ve found at CodeRabbit is to introduce “prompt subunits.” This architecture consists of a model-agnostic core prompt that defines the core tasks and general instructions. This is then layered on top of smaller, model-specific prompt subunits that handle style, formatting, and examples – and which can be customized to individual models. \nWhen it comes to Codex and Sonnet 4.5, the implementation details for these subunits are likely to be starkly different. We’ve found a few tricks from our prompt testing with both models that we would like to share:\nClaude: Use strong language like \"DO\" and \"DO NOT.\" Anthropic models pay attention to the latest information in a system prompt and are excellent at following output format specifications, even in long contexts. They prefer being told explicitly what to do.\nGPT-5: Use general instructions that are clearly aligned. OpenAI models’ attention decreases from top to bottom in a system prompt. These models may forget output format instructions in long contexts. They prefer generic guidance and tend to \"think on guidance,\" demonstrating a deeper reasoning process.\nUser feedback and evals\nThe second solution is to implement continuous updates driven by user feedback and internal evaluations. The best practice for optimizing an AI code-review bot or for that matter any LLM applications isn’t using an external benchmark; it’s checking to see if users accept the output. \nEvals are more important than ever but have to be designed more tightly around acceptability by users instead of raw performance since one model might be technically correct significantly more than another model but might drown the user in nitpicky and verbose comments, diluting its value to users. By measuring the metrics that matter ~ acceptance rate, signal-to-noise ratio, p95 latency, cost, among others - and tuning prompts in small steps, the system will remain aligned with user expectations and product goals. The last thing you want is great quantitative results on benchmarks and tests but low user acceptance. \nConclusion\nThis shift from one-size-fits-all prompt engineering to a new model specific paradigm is critical. The days of brittle, monolithic prompts and plug-and-play model swaps are over. Instead, modular prompting, paired with deliberate model choice, give your product resilience. \nThe ground will keep shifting as models evolve so your LLM stack and prompts shouldn’t be static. Treat it like a living system. Tune, test, listen, repeat. \nAlso, be sure to check out our published detailed benchmarks on how the latest models behave in production. That gives you more data on what to expect from them. \nGPT-5 Codex: How it solves for GPT-5's drawbacks\nClaude Sonnet 4.5: Better performance but a paradox\nBenchmarking GPT-5: Why it’s a generational leap in reasoning\nTry CodeRabbit with a 14-day free trial.",
      "publishedAt": "2025-10-24T05:38:20.000Z",
      "author": "Nehal Gajraj",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "testing",
        "observability"
      ],
      "ingestedAt": "2025-12-04T14:17:12.173Z",
      "score": 0.8859492043977006
    },
    {
      "id": "965313c874380d87f98fdf7deb1a0dea",
      "title": "先日6,000万ドルを調達したので…おもしろ動画を作りました",
      "url": "https://coderabbit.ai/blog/we-raised-60-million-last-week-so-we-made-a-funny-video-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/we-raised-60-million-last-week-so-we-made-a-funny-video\">We raised $60M last week… so we made a funny film</a>の意訳です。</p>\n<p>先日、CodeRabbitは<strong>シリーズBで6,000万ドルの資金調達</strong>を発表しました。<br />そのお祝いに、開発者向けソフトウェア企業として当然のことをやりました──おもしろ動画を作ったのです。</p>\n<p>もちろん、全額を動画制作に使ったわけではありません。<br />ただ、AIが生成した大量のPRに追われる開発チームなら誰もが共感できる、ちょっと馬鹿げた（でも楽しい）企画で祝おうと決めました。</p>\n<h2 id=\"heading-ai\"><strong>紹介します… “AIコーディングエージェントの暴走：短編映画”</strong></h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/glfB3KLQR7E?feature=shared\">https://youtu.be/glfB3KLQR7E?feature=shared</a></div>\n<p> </p>\n<p>「AIによる開発速度の向上」が、いつの間にかレビューの滞留地獄に変わってしまった──<br />そんな現実を描いた、モキュメンタリー×シットコム風の短編です。</p>\n<ul>\n<li><p>レビュアーは1人</p>\n</li>\n<li><p>通知は何十件も</p>\n</li>\n<li><p>未レビューPRは84件</p>\n</li>\n<li><p>そして、ひたすらフィードバックを求める同僚ブラッド</p>\n</li>\n</ul>\n<h2 id=\"heading-kirjgq3jg6pjgrnjg4jntlnku4sqkg\"><strong>キャスト紹介</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758059242156/ae3ad14c-f354-4802-95e1-1f7eb67f0c85.png\" alt class=\"image--center mx-auto\" /></p>\n<p>主人公の疲弊したレビュアー役には、人気の開発者教育者（そしてインフルエンサー）<a target=\"_blank\" href=\"https://x.com/@aarondfrancis\"><strong>Aaron Francis</strong></a>を起用。<br />彼は「機能をもっと早くリリースしたい」と思っていたのに、今ではキッチンにも行けず、朝8時に家を出ようとしても、ブラッドがPRの話をしてくる始末です。</p>\n<p>そして、そのブラッドを完璧に演じたのが<a target=\"_blank\" href=\"https://www.instagram.com/4ustinvon/?hl=en\"><strong>Austin von Johnson</strong></a>。<br />彼はAI生成PRを驚くべきスピードで量産できる開発者ですが、どんな状況でもレビューを<strong>待てない</strong>タイプ。<br />彼のストーキング、付箋メモ攻撃、フーディ姿でのPR奇襲……すべてが見事に「やりきって」いました。</p>\n<h2 id=\"heading-kirnrjhjgytjga7oo4jgavjgyljgovjgihlrppmpvjga7oqrlpoywqkg\"><strong>笑いの裏にある、実際の課題</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758059125767/fc80309a-a591-4b4d-bd9b-6a1b53330edb.png\" alt class=\"image--center mx-auto\" /></p>\n<p>この短編は笑える内容ですが、そこに描かれた課題は現実のものです。</p>\n<ul>\n<li><p><strong>AIコーディングツールが、チームのレビュー速度を超える速さでコードを生成する</strong></p>\n</li>\n<li><p><strong>レビュー待ちPRが雪だるま式に増え</strong>、生産性が低下する</p>\n</li>\n<li><p><strong>シニアエンジニアがレビュー地獄に埋もれる</strong></p>\n</li>\n<li><p>レビュー品質がばらつき、リスクが増加する</p>\n</li>\n<li><p>そしていつの間にか、「開発速度の向上」という約束が悪夢に変わる</p>\n</li>\n</ul>\n<h2 id=\"heading-coderabbit\"><strong>CodeRabbitが解決すること</strong></h2>\n<p>CodeRabbitは<strong>レビューの滞留を解消するため</strong>に存在します。<br />私たちのAIコードレビューは、要件・テスト・CI・過去のdiff・所有者情報など、数十の文脈情報を参照して、見逃されがちなバグを検出します。<br />レビューアーの負担を軽減し、PRをより早く、安全にマージできるようにします。──もちろん、チームメイトを「ブラッド」にしないためにも。</p>\n<p>素早くリリースし、賢くレビューし、心の平穏を保ちましょう。<br />そして、ブラッドをもう一人増やさないように。</p>\n<p>👉 <a target=\"_blank\" href=\"https://youtu.be/glfB3KLQR7E?feature=shared\">こちらから <strong>「AIコーディングエージェントの暴走：短編映画」</strong>をご覧ください。</a><br />もしあなたの職場にも「PRまだ？」と追いかけてくるブラッドがいるなら、この動画をぜひ送ってあげてください。</p>\n",
      "summary": "We raised $60M last week… so we made a funny filmの意訳です。\n先日、CodeRabbitはシリーズBで6,000万ドルの資金調達を発表しました。\nそのお祝いに、開発者向けソフトウェア企業として当然のことをやりました──おもしろ動画を作ったのです。\nもちろん、全額を動画制作に使ったわけではありません。\nただ、AIが生成した大量のPRに追われる開発チームなら誰もが共感できる、ちょっと馬鹿げた（でも楽しい）企画で祝おうと決めました。\n紹介します… “AIコーディングエージェントの暴走：短編映画”\n\n\n\nhttps://youtu.be/glfB3KLQR7E?feature=shared\n \n「AIによる開発速度の向上」が、いつの間にかレビューの滞留地獄に変わってしまった──\nそんな現実を描いた、モキュメンタリー×シットコム風の短編です。\nレビュアーは1人\n通知は何十件も\n未レビューPRは84件\nそして、ひたすらフィードバックを求める同僚ブラッド\nキャスト紹介\n\n主人公の疲弊したレビュアー役には、人気の開発者教育者（そしてインフルエンサー）Aaron Francisを起用。\n彼は「機能をもっと早くリリースしたい」と思っていたのに、今ではキッチンにも行けず、朝8時に家を出ようとしても、ブラッドがPRの話をしてくる始末です。\nそして、そのブラッドを完璧に演じたのがAustin von Johnson。\n彼はAI生成PRを驚くべきスピードで量産できる開発者ですが、どんな状況でもレビューを待てないタイプ。\n彼のストーキング、付箋メモ攻撃、フーディ姿でのPR奇襲……すべてが見事に「やりきって」いました。\n笑いの裏にある、実際の課題\n\nこの短編は笑える内容ですが、そこに描かれた課題は現実のものです。\nAIコーディングツールが、チームのレビュー速度を超える速さでコードを生成する\nレビュー待ちPRが雪だるま式に増え、生産性が低下する\nシニアエンジニアがレビュー地獄に埋もれる\nレビュー品質がばらつき、リスクが増加する\nそしていつの間にか、「開発速度の向上」という約束が悪夢に変わる\nCodeRabbitが解決すること\nCodeRabbitはレビューの滞留を解消するために存在します。\n私たちのAIコードレビューは、要件・テスト・CI・過去のdiff・所有者情報など、数十の文脈情報を参照して、見逃されがちなバグを検出します。\nレビューアーの負担を軽減し、PRをより早く、安全にマージできるようにします。──もちろん、チームメイトを「ブラッド」にしないためにも。\n素早くリリースし、賢くレビューし、心の平穏を保ちましょう。\nそして、ブラッドをもう一人増やさないように。\n👉 こちらから 「AIコーディングエージェントの暴走：短編映画」をご覧ください。\nもしあなたの職場にも「PRまだ？」と追いかけてくるブラッドがいるなら、この動画をぜひ送ってあげてください。",
      "publishedAt": "2025-10-10T07:35:08.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:12.173Z",
      "score": 0.13498316586607617
    },
    {
      "id": "0419cf715ed8a478726ab051f82779fc",
      "title": "Claude Sonnet 4.5: パフォーマンス向上、でもパラドックスあり",
      "url": "https://coderabbit.ai/blog/claude-sonnet-45-better-performance-but-a-paradox-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/claude-sonnet-45-better-performance-but-a-paradox\">Claude Sonnet 4.5: Better performance but a paradox</a>の意訳です。</p>\n<p>Sonnet 4.5はAnthropicの最新Claudeモデルであり、私たちのコードレビュー・ベンチマークでは一見パラドックスのように感じられます。より高性能で、より慎重でありながら、時にもどかしい。Sonnet 4では見逃したバグを見つけ、カバレッジではOpus 4.1に近づき、さらに想定外の重大な問題をいくつか浮かび上がらせることもあります。</p>\n<p>しかし一方で、自己防衛的に振る舞い、自らを疑い、時に決断的なレビュアーというより思慮深い同僚のように見えることもありました。データでは確かな進歩が見られます。Sonnet 4ではコメントのうち、重要と判断されたものが35.3%だったのに対し、Sonnet 4.5では41.5%でした。しかし、そのコメントの調子や文体は、「AIレビュアーに何を求めるのか」というより深い問いを投げかけています。</p>\n<p>そして決定的なのは価格です。Sonnet 4.5はOpusレベルの性能に近づきながら、価格は変わらず維持されています。つまり、大規模なコードレビューを行うチームにとって、実用的な最適点に位置しているのです。</p>\n<p>Sonnet 4.5は思考を声に出しているかのようで、確かな修正を出す一方、曖昧な「条件付き」警告のようなコメントを出すこともあり、それが一部の開発者にとっては理解を難しくしているかもしれません。それでは、ベンチマークの詳細を見ていきましょう。</p>\n<h2 id=\"heading-kirjg5njg7pjg4hjg57jg7zjgqvvjroqzxkvqhjga7oprpngrkqkg\"><strong>ベンチマーク：評価の観点</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759501602825/c1640c62-ae57-42a8-a782-f22317d037e7.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Sonnet 4.5、Sonnet 4、Opus 4.1の3つを対象に、25件の難易度の高い実際のプルリクエストで評価しました。これらには既知の重大なバグが含まれており（並行性やメモリ順序、非同期レースコンディション、APIの誤使用など）、モデルがその重大な問題に直接コメントを出せた場合、そのPRは「合格」としました。</p>\n<p>評価指標は、カバレッジ（S@25）、精度（コメントの合格率）、そしてシグナル対ノイズ比です。シグナル対ノイズ比については、<strong>重要なコメント（Important comments）</strong> に注目しました。これらは最も価値のあるコメントであり、以下を含みます。</p>\n<ul>\n<li><p><strong>PASSコメント</strong>：PR内の既知の重大バグを正しく指摘・修正したもの</p>\n</li>\n<li><p><strong>その他の重要コメント</strong>：追跡対象ではないが、別の重大または深刻なバグを的確に指摘したもの</p>\n</li>\n</ul>\n<h2 id=\"heading-sonnet-45opus-41\"><strong>スコアボード：Sonnet 4.5はOpus 4.1に性能で迫る</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759501721229/c01cc54e-7647-4f84-98a4-f933078d6dd3.png\" alt class=\"image--center mx-auto\" /></p>\n<p>結果は以下の通りです。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502044781/191c79e1-971f-4209-b0b0-c0fa368fa0b9.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p><strong>カバレッジ:</strong> Sonnet 4.5はSonnet 4とOpus 4.1の間の差を大きく縮め、Sonnet 4を大きく上回りました。</p>\n</li>\n<li><p><strong>精度:</strong> Opus 4.1は依然として最も正確で信頼性の高い実行可能なコメントを生成しました。高価格モデルであるため当然の結果です。</p>\n</li>\n<li><p><strong>重要コメント率（重大な問題を指摘したコメントの割合）:</strong> より厳格な基準で測定した場合、Sonnet 4.5の重要コメント率は約41%。つまりコメントのうち4割が、主要なバグを解決するか、別の重大な問題を指摘していたことになります。Opus 4.1は50%、Sonnet 4は約35%でした。</p>\n</li>\n</ul>\n<h2 id=\"heading-sonnet-45\"><strong>文体とトーン：Sonnet 4.5は「慎重さ」にフォーカス</strong></h2>\n<p>Sonnet 4.5のコメントはコードを修正しますが、Opus 4.1ほど自信に満ちたトーンではありません。ただし、Sonnet 4よりは明確です。</p>\n<p><strong>修正パッチの提示率:</strong></p>\n<ul>\n<li><p>Sonnet 4.5の実行可能コメントのうち87%はコードブロックやdiffパッチを含み、Sonnet 4（90%）、Opus 4.1（91%）とほぼ同水準です。</p>\n</li>\n<li><p>違いは文体にあります。Opusのdiffは「外科的修正」のように明確ですが、Sonnet 4.5は探索的な文章を添える傾向があります。修正を「提案する」「検討する」といった表現が多く、断定的ではありません。</p>\n</li>\n</ul>\n<p><strong>慎重な言い回し（Hedging language）:</strong></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502294784/a66c085f-04ce-4806-b5c5-e22dfa47e2a1.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p>Sonnet 4.5では実行可能コメントの**34%**において、「might」「could」「possibly」といった慎重な表現が見られます。例：</p>\n<ul>\n<li><p>“<strong>不要なアロケーション: cacheは使用されていません。</strong> コンストラクタで4KBのメモリを確保していますが、使われていません。… <strong>cache_bufferの削除を検討してください。</strong>”</p>\n</li>\n<li><p>“<strong>空のtry/exceptブロックを削除してください。</strong> …おそらくプレースホルダーです”</p>\n</li>\n</ul>\n</li>\n<li><p>Opus 4.1は約28%、Sonnet 4は約26%とやや低め。</p>\n</li>\n<li><p>この慎重さにより、「問いかける」ようなトーンが生まれます。Sonnet 4.5はしばしば一緒に考えているような雰囲気を持ち、明確な判定を下すというより「共に推論している」ように感じられます。</p>\n</li>\n</ul>\n<p><strong>自信のある言語（Confident language）:</strong></p>\n<ul>\n<li><p>ただし、Sonnet 4.5は慎重さを補うように、高い確信を示すコメントも**39%**含んでいます。Sonnet 4（18%）、Opus 4.1（23%）よりも高い割合です。例：</p>\n<ul>\n<li><p>“<strong>重大: self.プレフィックスが欠落しており、すべてのAPIメソッドが動作しません。</strong> このままでは全てのメソッドがAttributeErrorを発生させます。”</p>\n</li>\n<li><p>“<strong>整数オーバーフローの可能性:</strong> optimization_cycle_countが無制限にインクリメントされ続けます。これは約414日稼働後に<strong>必ず</strong>オーバーフローします。”</p>\n</li>\n</ul>\n</li>\n<li><p>つまり、慎重さと確信の間で大きく揺れ動くのです。</p>\n</li>\n</ul>\n<p><strong>シグナル対ノイズ比:</strong></p>\n<ul>\n<li><p>Sonnet 4.5はSonnet 4より精度が向上しましたが、依然としてOpusよりも「軽微な」的外れコメントが多めでした。</p>\n</li>\n<li><p>ただし、重要コメント（PASSコメント＋少数の高確信度コメント）に限定すると**41.5%**を達成。Opus 4.1は依然として約50%で最高水準です。</p>\n</li>\n</ul>\n<h2 id=\"heading-sonnet-45-1\"><strong>Sonnet 4.5が得意な領域</strong></h2>\n<p>評価したPR群では、Sonnet 4.5が明確に、特に優れていた領域が見られました。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502262665/e0099bd2-4983-4708-8563-6ee3ba923c7e.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p><strong>並行性バグ検出:</strong> C++のatomic操作やcondvarの誤用を的確に特定し、実行可能なdiffを生成</p>\n</li>\n<li><p><strong>整合性チェック:</strong> サービス間での分散状態の不整合を確実に検出</p>\n</li>\n<li><p><strong>追加バグの発見:</strong> 評価対象外のCritical問題も検出しましたが、より厳密な基準では件数はやや減少</p>\n</li>\n</ul>\n<p>AnthropicはSonnet 4.5を「ハイブリッド推論」「長期的計画立案」モデルとして打ち出しています。実際、コード内の副経路を追跡し、未追跡の実際の問題を発見する傾向が見られます。</p>\n<h2 id=\"heading-sonnet-45-2\"><strong>Sonnet 4.5: 価格と性能のバランスが最適</strong></h2>\n<p>Sonnet 4.5の最大の強みの一つは、価格対性能比にあります。Opus 4.1は依然としてAnthropicの最上位モデルですが、その分コストも高額です。</p>\n<p>Sonnet 4.5はカバレッジと重大バグ検出能力の差を縮めつつ、はるかにコスト効率が良いです。多くのチームにとって、Opusレベルの結果を低コストで得られるこのバランスこそが、最も実用的な選択肢となる理由です。</p>\n<h2 id=\"heading-sonnet-45-3\"><strong>Sonnet 4.5の弱点</strong></h2>\n<p>ただし、Sonnet 4.5を使用する際はその弱点を理解しておく必要があります。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502337031/b494ee79-eb77-4342-809a-929a41db1dd1.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p><strong>デッドロック検出:</strong> Sonnet 4やOpusと同様、複雑なロック順序の追跡はまだ苦手です</p>\n</li>\n<li><p><strong>冗長さと慎重さ:</strong> コメントが長く、留保的または曖昧なことがあります。以前の研究で評価したGPT-5 Codexは「パッチのように読める」ほど明快なコメントを書く傾向がありました。例として、GPT-5 Codexのコメントを以下に示します。</p>\n<ul>\n<li><p><strong>ロック順序 / デッドロック:</strong> 「ロック取得を一貫した階層順に並べ替えてください。これにより循環待ちデッドロックを防げます」</p>\n</li>\n<li><p><strong>正規表現の壊滅的バックトラッキング:</strong> 「ネストされた量指定子を削除してバックトラッキングを回避します」</p>\n</li>\n</ul>\n</li>\n<li><p><strong>精度のギャップ:</strong> コメント単位の精度35%、重要コメント率41.5%はSonnet 4より良いものの、Opus 4.1にはまだ届きません。</p>\n</li>\n</ul>\n<h2 id=\"heading-sonnet-45-4\"><strong>Sonnet 4.5の総評</strong></h2>\n<p>Sonnet 4.5は「赤ペン先生」ではなく、そばで考えてくれる同僚のようです。可能性のある問題を指摘し、ほとんどの場合は正しく、時に慎重すぎる時もあります。ときには、思いがけない箇所にまで目を向けてくれます。</p>\n<p>このスタイルはレビューにおいて諸刃の剣です。一方では、開発者は追加の重大問題を指摘してくれることを評価するでしょう。もう一方では、「このバグを確実に見つけてほしい」という場合、Opus 4.1のほうが鋭いです。</p>\n<h2 id=\"heading-kirnt4mi6wqkg\"><strong>総括</strong></h2>\n<p>AnthropicはSonnet 4.5を「エージェント的推論とコンピュータ利用」に向けたステップと位置付けています。コードレビューでは、その推論力がより豊かで慎重かつ広範なコメントとして現れます。</p>\n<p>チームにとっての選択肢はこうです。</p>\n<ul>\n<li><p>明確でパッチのようなフィードバックを重視するなら、Opus 4.1（またはGPT-5 Codex）が依然として基準として優れています。</p>\n</li>\n<li><p>トラッキング対象外の隠れた重大問題まで発見したいなら、Sonnet 4.5が有力です。</p>\n</li>\n<li><p>コスト効率を重視するなら、Sonnet 4.5が最も賢い選択肢です。Opus並みの精度を低価格で実現します。</p>\n</li>\n</ul>\n<p>いずれにせよ、Sonnet 4.5はレビュー体験の質感を変えます。より人間的で、常に明快とは限らないものの、より探究的で、より慎重で、時にあなたが見逃していた「正解」に辿り着くこともあるのです。</p>\n",
      "summary": "Claude Sonnet 4.5: Better performance but a paradoxの意訳です。\nSonnet 4.5はAnthropicの最新Claudeモデルであり、私たちのコードレビュー・ベンチマークでは一見パラドックスのように感じられます。より高性能で、より慎重でありながら、時にもどかしい。Sonnet 4では見逃したバグを見つけ、カバレッジではOpus 4.1に近づき、さらに想定外の重大な問題をいくつか浮かび上がらせることもあります。\nしかし一方で、自己防衛的に振る舞い、自らを疑い、時に決断的なレビュアーというより思慮深い同僚のように見えることもありました。データでは確かな進歩が見られます。Sonnet 4ではコメントのうち、重要と判断されたものが35.3%だったのに対し、Sonnet 4.5では41.5%でした。しかし、そのコメントの調子や文体は、「AIレビュアーに何を求めるのか」というより深い問いを投げかけています。\nそして決定的なのは価格です。Sonnet 4.5はOpusレベルの性能に近づきながら、価格は変わらず維持されています。つまり、大規模なコードレビューを行うチームにとって、実用的な最適点に位置しているのです。\nSonnet 4.5は思考を声に出しているかのようで、確かな修正を出す一方、曖昧な「条件付き」警告のようなコメントを出すこともあり、それが一部の開発者にとっては理解を難しくしているかもしれません。それでは、ベンチマークの詳細を見ていきましょう。\nベンチマーク：評価の観点\n\nSonnet 4.5、Sonnet 4、Opus 4.1の3つを対象に、25件の難易度の高い実際のプルリクエストで評価しました。これらには既知の重大なバグが含まれており（並行性やメモリ順序、非同期レースコンディション、APIの誤使用など）、モデルがその重大な問題に直接コメントを出せた場合、そのPRは「合格」としました。\n評価指標は、カバレッジ（S@25）、精度（コメントの合格率）、そしてシグナル対ノイズ比です。シグナル対ノイズ比については、重要なコメント（Important comments） に注目しました。これらは最も価値のあるコメントであり、以下を含みます。\nPASSコメント：PR内の既知の重大バグを正しく指摘・修正したもの\nその他の重要コメント：追跡対象ではないが、別の重大または深刻なバグを的確に指摘したもの\nスコアボード：Sonnet 4.5はOpus 4.1に性能で迫る\n\n結果は以下の通りです。\n\nカバレッジ: Sonnet 4.5はSonnet 4とOpus 4.1の間の差を大きく縮め、Sonnet 4を大きく上回りました。\n精度: Opus 4.1は依然として最も正確で信頼性の高い実行可能なコメントを生成しました。高価格モデルであるため当然の結果です。\n重要コメント率（重大な問題を指摘したコメントの割合）: より厳格な基準で測定した場合、Sonnet 4.5の重要コメント率は約41%。つまりコメントのうち4割が、主要なバグを解決するか、別の重大な問題を指摘していたことになります。Opus 4.1は50%、Sonnet 4は約35%でした。\n文体とトーン：Sonnet 4.5は「慎重さ」にフォーカス\nSonnet 4.5のコメントはコードを修正しますが、Opus 4.1ほど自信に満ちたトーンではありません。ただし、Sonnet 4よりは明確です。\n修正パッチの提示率:\nSonnet 4.5の実行可能コメントのうち87%はコードブロックやdiffパッチを含み、Sonnet 4（90%）、Opus 4.1（91%）とほぼ同水準です。\n違いは文体にあります。Opusのdiffは「外科的修正」のように明確ですが、Sonnet 4.5は探索的な文章を添える傾向があります。修正を「提案する」「検討する」といった表現が多く、断定的ではありません。\n慎重な言い回し（Hedging language）:\n\nSonnet 4.5では実行可能コメントの**34%**において、「might」「could」「possibly」といった慎重な表現が見られます。例：\n“不要なアロケーション: cacheは使用されていません。 コンストラクタで4KBのメモリを確保していますが、使われていません。… cache_bufferの削除を検討してください。”\n“空のtry/exceptブロックを削除してください。 …おそらくプレースホルダーです”\nOpus 4.1は約28%、Sonnet 4は約26%とやや低め。\nこの慎重さにより、「問いかける」ようなトーンが生まれます。Sonnet 4.5はしばしば一緒に考えているような雰囲気を持ち、明確な判定を下すというより「共に推論している」ように感じられます。\n自信のある言語（Confident language）:\nただし、Sonnet 4.5は慎重さを補うように、高い確信を示すコメントも**39%**含んでいます。Sonnet 4（18%）、Opus 4.1（23%）よりも高い割合です。例：\n“重大: self.プレフィックスが欠落しており、すべてのAPIメソッドが動作しません。 このままでは全てのメソッドがAttributeErrorを発生させます。”\n“整数オーバーフローの可能性: optimization_cycle_countが無制限にインクリメントされ続けます。これは約414日稼働後に必ずオーバーフローします。”\nつまり、慎重さと確信の間で大きく揺れ動くのです。\nシグナル対ノイズ比:\nSonnet 4.5はSonnet 4より精度が向上しましたが、依然としてOpusよりも「軽微な」的外れコメントが多めでした。\nただし、重要コメント（PASSコメント＋少数の高確信度コメント）に限定すると**41.5%**を達成。Opus 4.1は依然として約50%で最高水準です。\nSonnet 4.5が得意な領域\n評価したPR群では、Sonnet 4.5が明確に、特に優れていた領域が見られました。\n\n並行性バグ検出: C++のatomic操作やcondvarの誤用を的確に特定し、実行可能なdiffを生成\n整合性チェック: サービス間での分散状態の不整合を確実に検出\n追加バグの発見: 評価対象外のCritical問題も検出しましたが、より厳密な基準では件数はやや減少\nAnthropicはSonnet 4.5を「ハイブリッド推論」「長期的計画立案」モデルとして打ち出しています。実際、コード内の副経路を追跡し、未追跡の実際の問題を発見する傾向が見られます。\nSonnet 4.5: 価格と性能のバランスが最適\nSonnet 4.5の最大の強みの一つは、価格対性能比にあります。Opus 4.1は依然としてAnthropicの最上位モデルですが、その分コストも高額です。\nSonnet 4.5はカバレッジと重大バグ検出能力の差を縮めつつ、はるかにコスト効率が良いです。多くのチームにとって、Opusレベルの結果を低コストで得られるこのバランスこそが、最も実用的な選択肢となる理由です。\nSonnet 4.5の弱点\nただし、Sonnet 4.5を使用する際はその弱点を理解しておく必要があります。\n\nデッドロック検出: Sonnet 4やOpusと同様、複雑なロック順序の追跡はまだ苦手です\n冗長さと慎重さ: コメントが長く、留保的または曖昧なことがあります。以前の研究で評価したGPT-5 Codexは「パッチのように読める」ほど明快なコメントを書く傾向がありました。例として、GPT-5 Codexのコメントを以下に示します。\nロック順序 / デッドロック: 「ロック取得を一貫した階層順に並べ替えてください。これにより循環待ちデッドロックを防げます」\n正規表現の壊滅的バックトラッキング: 「ネストされた量指定子を削除してバックトラッキングを回避します」\n精度のギャップ: コメント単位の精度35%、重要コメント率41.5%はSonnet 4より良いものの、Opus 4.1にはまだ届きません。\nSonnet 4.5の総評\nSonnet 4.5は「赤ペン先生」ではなく、そばで考えてくれる同僚のようです。可能性のある問題を指摘し、ほとんどの場合は正しく、時に慎重すぎる時もあります。ときには、思いがけない箇所にまで目を向けてくれます。\nこのスタイルはレビューにおいて諸刃の剣です。一方では、開発者は追加の重大問題を指摘してくれることを評価するでしょう。もう一方では、「このバグを確実に見つけてほしい」という場合、Opus 4.1のほうが鋭いです。\n総括\nAnthropicはSonnet 4.5を「エージェント的推論とコンピュータ利用」に向けたステップと位置付けています。コードレビューでは、その推論力がより豊かで慎重かつ広範なコメントとして現れます。\nチームにとっての選択肢はこうです。\n明確でパッチのようなフィードバックを重視するなら、Opus 4.1（またはGPT-5 Codex）が依然として基準として優れています。\nトラッキング対象外の隠れた重大問題まで発見したいなら、Sonnet 4.5が有力です。\nコスト効率を重視するなら、Sonnet 4.5が最も賢い選択肢です。Opus並みの精度を低価格で実現します。\nいずれにせよ、Sonnet 4.5はレビュー体験の質感を変えます。より人間的で、常に明快とは限らないものの、より探究的で、より慎重で、時にあなたが見逃していた「正解」に辿り着くこともあるのです。",
      "publishedAt": "2025-10-10T07:28:00.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:12.173Z",
      "score": 0.15421189990216974
    },
    {
      "id": "a32e3ff8a0ed4196c0e21bb3980a4e2f",
      "title": "Claude Sonnet 4.5:  Better performance but a paradox",
      "url": "https://coderabbit.ai/blog/claude-sonnet-45-better-performance-but-a-paradox",
      "content": "<p>Sonnet 4.5 is Anthropic’s newest Claude model and in our code review benchmark, it feels like a paradox: more capable, more cautious, and at times more frustrating. It catches bugs Sonnet 4 missed, edges closer to Opus 4.1 in coverage, and even surfaces a handful of unexpected critical issues off the beaten path.</p>\n<p>Yet, it hedges, it questions itself, and it sometimes sounds more like a thoughtful colleague than a decisive reviewer. The data shows real progress:41.5% of its comments were Important in Sonnet 4.5 vs only 35.3% in Sonnet 4.But the tone and texture of those comments raise deeper questions about what we want in an AI reviewer.</p>\n<p>And then there’s the kicker: Sonnet 4.5 gets you close to Opus-level performance at a fraction of the price, making it a pragmatic sweet spot for teams reviewing code at scale.</p>\n<p>Sonnet 4.5 thinks aloud and still delivers decisive fixes but some of its comments are framed as vague “conditional” warnings that could make its comments harder for some to parse..  Let’s dive into our benchmark.</p>\n<h2 id=\"heading-benchmark-what-we-looked-for\"><strong>Benchmark: What we looked for</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759501602825/c1640c62-ae57-42a8-a782-f22317d037e7.png\" alt class=\"image--center mx-auto\" /></p>\n<p>We evaluated Sonnet 4.5, Sonnet 4, and Opus 4.1 across 25 difficult real-world pull requests containing known critical bugs (ranging from concurrency and memory ordering to async race conditions and API misuse). A model “Passed’ a PR if it produced at least one comment directly on the critical issue.</p>\n<p>We measured coverage (S@25), precision (comment PASS rate), and signal-to-noise ratio. For signal-to-noise we focus on <strong>Important comments</strong> (these are the comments that matter most). They include:</p>\n<ul>\n<li><p><strong>PASS comments</strong> that correctly addressed the known critical bug in the PR.</p>\n</li>\n<li><p><strong>Other important comments</strong> that did not solve the tracked issue, but still flagged a truly Critical or Major bug elsewhere.</p>\n</li>\n</ul>\n<h2 id=\"heading-scoreboard-sonnet-45-gets-closer-to-opus-41-in-performance\"><strong>Scoreboard - Sonnet 4.5 gets closer to Opus 4.1 in performance</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759501721229/c01cc54e-7647-4f84-98a4-f933078d6dd3.png\" alt class=\"image--center mx-auto\" /></p>\n<p>The results were mixed:</p>\n<ul>\n<li><p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502044781/191c79e1-971f-4209-b0b0-c0fa368fa0b9.png\" alt class=\"image--center mx-auto\" /></p>\n<p>  <strong>Coverage:</strong> Sonnet 4.5 closes much of the gap between Sonnet 4 and Opus 4.1 and lands far ahead of Sonnet 4.</p>\n</li>\n<li><p><strong>Precision:</strong> Opus 4.1 still produces the cleanest, most reliable actionable comments but that is to be expected given that it’s a more expensive model.</p>\n</li>\n<li><p><strong>Imp</strong><a target=\"_blank\" href=\"http://i.ei\"><strong>or</strong></a><strong>tant share (</strong><a target=\"_blank\" href=\"http://i.ei\"><strong>i.</strong></a><strong>e. percenta</strong><a target=\"_blank\" href=\"http://i.ei\"><strong>ge</strong></a> <strong>of comments flagging a significant issue):</strong> With stricter criteria, Sonnet 4.5 lands at just over 41% Importan<a target=\"_blank\" href=\"http://i.ei\">t</a> share. That means about 4 in 10 of its comments either solved the key bug or flagged another truly significant issue. Opus 4.1 leads here at 50%, with So<a target=\"_blank\" href=\"http://i.ei\">nn</a>et 4 at ~35%.</p>\n</li>\n</ul>\n<h2 id=\"heading-style-and-tone-sonnet-45-is-focused-on-hedging\"><strong>Style and tone: Sonnet 4.5 is focused on hedging</strong></h2>\n<p>Sonnet 4.5’s comments patch the code but do so in a less confident tone than Opus 4.1 does but is still more confident than Sonnet 4.</p>\n<p><strong>Patches present:</strong></p>\n<ul>\n<li><p>87% of Sonnet 4.5’s actionable comments included a code block or diff patch, similar to Sonnet 4 (90%) and Opus 4.1 (91%).</p>\n</li>\n<li><p>The difference is in style: Opus’s diffs read like surgical fixes, while Sonnet 4.5 often couches them in exploratory text. It “suggests” or “considers” changes rather than asserting them.</p>\n</li>\n</ul>\n<p><strong>Hedging language:</strong></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502294784/a66c085f-04ce-4806-b5c5-e22dfa47e2a1.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p>Sonnet 4.5 hedges in <strong>34%</strong> of actionable comments—words like <em>might</em>, <em>could</em>, <em>possibly</em>. For example:</p>\n<ul>\n<li><p>“<strong>Unnecessary allocation: cache is never used.</strong> The constructor allocates 4KB of memory that is never utilized … <strong>Consider removing</strong> the cache_buffer.”</p>\n</li>\n<li><p>“<strong>Remove the empty try/except block.</strong> … likely a placeholder”</p>\n</li>\n</ul>\n</li>\n<li><p>Opus 4.1 is steady at ~28%. Sonnet 4 sits slightly lower at ~26%.</p>\n</li>\n<li><p>This hedging creates an “interrogative” tone: Sonnet 4.5 sometimes feels like it’s thinking out loud with you, rather than delivering verdicts.</p>\n</li>\n</ul>\n<p><strong>Confident language:</strong></p>\n<ul>\n<li><p>Sonnet 4.5 balances that hedging with higher confidence markers (<strong>39%</strong>) than Sonnet 4 (18%) or Opus 4.1 (23%). For example:</p>\n<ul>\n<li><p>“<strong>Critical: Missing self. prefix breaks all API methods.</strong> All subsequent methods will raise AttributeError until this is corrected.”</p>\n</li>\n<li><p>“<strong>Potential integer overflow.</strong> optimization_cycle_count increments unbounded … this <strong>will</strong> overflow after ~414 days of runtime.”</p>\n</li>\n</ul>\n</li>\n<li><p>In other words, it swings between caution and certainty more dramatically.</p>\n</li>\n</ul>\n<p><strong>Signal-to-noise:</strong></p>\n<ul>\n<li>Sonnet 4.5 improved precision over Sonnet 4, but still produced more “minor” off-target notes than Opus.</li>\n</ul>\n<p>However, when you count its true Important comments—PASS comments plus a small number of high-confidence off-EP issues—it lands at <strong>41.5% Important share</strong>. Opus 4.1 is still the gold standard Anthropic model at ~50%.</p>\n<h2 id=\"heading-what-sonnet-45-is-good-at\"><strong>What Sonnet 4.5 is good at</strong></h2>\n<p>Across the PRs we tested Sonnet 4.5 with, we saw some clear areas where it stood out.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502262665/e0099bd2-4983-4708-8563-6ee3ba923c7e.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p><strong>Concurrency bug-finding:</strong> Sonnet 4.5 nailed <strong>C++ atomics and condvar misuses</strong> with clean, actionable diffs.</p>\n</li>\n<li><p><strong>Consistency checks:</strong> It reliably flagged distributed state mismatches across services.</p>\n</li>\n<li><p><strong>Extra bug surfacing:</strong> It did identify additional Critical issues not originally under evaluation, though fewer than initially expected under a stricter rubric.</p>\n</li>\n</ul>\n<p>As Anthropic markets Sonnet 4.5, they emphasize “hybrid reasoning” and “long horizon” planning. In practice, that shows up as more willingness to chase down side-paths in the code and note real but untracked issues.</p>\n<h2 id=\"heading-sonnet-45-hits-a-price-vs-performance-sweet-spot\"><strong>Sonnet 4.5: Hits a price vs. performance sweet spot</strong></h2>\n<p>One of the biggest advantages of Sonnet 4.5 is its price-to-performance ratio. While Opus 4.1 remains Anthropic's flagship model in raw capability, it also comes at a significantly higher cost.</p>\n<p>Sonnet 4.5 narrows the gap in coverage and important bug-finding while staying far more cost-efficient to run. For many teams, that balance of having close to Opus-level results at a fraction of the price is what makes Sonnet 4.5 the most pragmatic choice.</p>\n<h2 id=\"heading-sonnet-45-weaknesses\"><strong>Sonnet 4.5 weaknesses</strong></h2>\n<p>But if using Sonnet 4.5, it’s critical to be aware of its weaknesses. These include:</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759502337031/b494ee79-eb77-4342-809a-929a41db1dd1.png\" alt class=\"image--center mx-auto\" /></p>\n<ul>\n<li><p><strong>Deadlock coverage:</strong> Like Sonnet 4 and even Opus, it still struggles to trace complex lock ordering.</p>\n</li>\n<li><p><strong>Verbosity and hedging:</strong> Many comments run long, caveated, or uncertain. Compare this to GPT-5 Codex, which in our earlier work wrote comments that “read like patches” with crisp directness. For example, with GPT-5 Codex:</p>\n<ul>\n<li><p><strong>Lock ordering / deadlock:</strong> Reorder the lock acquisitions to follow a consistent hierarchy. This prevents circular wait deadlocks.”</p>\n</li>\n<li><p><strong>Regex catastrophic backtracking:</strong> “Remove the nested quantifier to avoid catastrophic backtracking.”</p>\n</li>\n</ul>\n</li>\n<li><p><strong>Precision gap:</strong> At 35% comment-level precision and 41.5% important share percentage, it’s better than Sonnet 4 but well short of Opus 4.1.</p>\n</li>\n</ul>\n<h2 id=\"heading-sonnet-45-verdict\"><strong>Sonnet 4.5 verdict</strong></h2>\n<p>Sonnet 4.5 feels less like a teacher writing in red pen and more like a thoughtful colleague at your side: pointing out possible issues, often right, occasionally over-hedged, and sometimes spotting things you didn’t know were there.</p>\n<p>That style is a double-edged sword in review. On one hand, developers may appreciate the extra critical issues it flags. On the other, when the task is “please catch this bug,” Opus 4.1 is still sharper.</p>\n<h2 id=\"heading-closing-thoughts\"><strong>Closing thoughts</strong></h2>\n<p>Anthropic positioned Sonnet 4.5 as a step toward agentic reasoning and computer use. In code review, that reasoning shows up in richer, more cautious, and more wide-ranging comments.</p>\n<p>For teams:</p>\n<ul>\n<li><p>If you value decisive, patch-like feedback, Opus 4.1 (or GPT-5 Codex) still sets the bar.</p>\n</li>\n<li><p>If you want a reviewer that finds critical issues anywhere they lurk, even beyond the tracked bug, Sonnet 4.5 has surprising upside.</p>\n</li>\n<li><p>And if you care about pragmatic price-to-performance, Sonnet 4.5 may be the smartest choice: close to Opus’s accuracy at a fraction of the cost.</p>\n</li>\n</ul>\n<p>Either way, Sonnet 4.5 changes the texture of reviews. It feels more human—not always cleaner, but more inquisitive, more hedged, sometimes more <em>right</em> in the places you weren’t looking.</p>\n",
      "summary": "Sonnet 4.5 is Anthropic’s newest Claude model and in our code review benchmark, it feels like a paradox: more capable, more cautious, and at times more frustrating. It catches bugs Sonnet 4 missed, edges closer to Opus 4.1 in coverage, and even surfaces a handful of unexpected critical issues off the beaten path.\nYet, it hedges, it questions itself, and it sometimes sounds more like a thoughtful colleague than a decisive reviewer. The data shows real progress:41.5% of its comments were Important in Sonnet 4.5 vs only 35.3% in Sonnet 4.But the tone and texture of those comments raise deeper questions about what we want in an AI reviewer.\nAnd then there’s the kicker: Sonnet 4.5 gets you close to Opus-level performance at a fraction of the price, making it a pragmatic sweet spot for teams reviewing code at scale.\nSonnet 4.5 thinks aloud and still delivers decisive fixes but some of its comments are framed as vague “conditional” warnings that could make its comments harder for some to parse..  Let’s dive into our benchmark.\nBenchmark: What we looked for\n\nWe evaluated Sonnet 4.5, Sonnet 4, and Opus 4.1 across 25 difficult real-world pull requests containing known critical bugs (ranging from concurrency and memory ordering to async race conditions and API misuse). A model “Passed’ a PR if it produced at least one comment directly on the critical issue.\nWe measured coverage (S@25), precision (comment PASS rate), and signal-to-noise ratio. For signal-to-noise we focus on Important comments (these are the comments that matter most). They include:\nPASS comments that correctly addressed the known critical bug in the PR.\nOther important comments that did not solve the tracked issue, but still flagged a truly Critical or Major bug elsewhere.\nScoreboard - Sonnet 4.5 gets closer to Opus 4.1 in performance\n\nThe results were mixed:\n\n  Coverage: Sonnet 4.5 closes much of the gap between Sonnet 4 and Opus 4.1 and lands far ahead of Sonnet 4.\nPrecision: Opus 4.1 still produces the cleanest, most reliable actionable comments but that is to be expected given that it’s a more expensive model.\nImportant share (i.e. percentage of comments flagging a significant issue): With stricter criteria, Sonnet 4.5 lands at just over 41% Important share. That means about 4 in 10 of its comments either solved the key bug or flagged another truly significant issue. Opus 4.1 leads here at 50%, with Sonnet 4 at ~35%.\nStyle and tone: Sonnet 4.5 is focused on hedging\nSonnet 4.5’s comments patch the code but do so in a less confident tone than Opus 4.1 does but is still more confident than Sonnet 4.\nPatches present:\n87% of Sonnet 4.5’s actionable comments included a code block or diff patch, similar to Sonnet 4 (90%) and Opus 4.1 (91%).\nThe difference is in style: Opus’s diffs read like surgical fixes, while Sonnet 4.5 often couches them in exploratory text. It “suggests” or “considers” changes rather than asserting them.\nHedging language:\n\nSonnet 4.5 hedges in 34% of actionable comments—words like might, could, possibly. For example:\n“Unnecessary allocation: cache is never used. The constructor allocates 4KB of memory that is never utilized … Consider removing the cache_buffer.”\n“Remove the empty try/except block. … likely a placeholder”\nOpus 4.1 is steady at ~28%. Sonnet 4 sits slightly lower at ~26%.\nThis hedging creates an “interrogative” tone: Sonnet 4.5 sometimes feels like it’s thinking out loud with you, rather than delivering verdicts.\nConfident language:\nSonnet 4.5 balances that hedging with higher confidence markers (39%) than Sonnet 4 (18%) or Opus 4.1 (23%). For example:\n“Critical: Missing self. prefix breaks all API methods. All subsequent methods will raise AttributeError until this is corrected.”\n“Potential integer overflow. optimization_cycle_count increments unbounded … this will overflow after ~414 days of runtime.”\nIn other words, it swings between caution and certainty more dramatically.\nSignal-to-noise:\nSonnet 4.5 improved precision over Sonnet 4, but still produced more “minor” off-target notes than Opus.\nHowever, when you count its true Important comments—PASS comments plus a small number of high-confidence off-EP issues—it lands at 41.5% Important share. Opus 4.1 is still the gold standard Anthropic model at ~50%.\nWhat Sonnet 4.5 is good at\nAcross the PRs we tested Sonnet 4.5 with, we saw some clear areas where it stood out.\n\nConcurrency bug-finding: Sonnet 4.5 nailed C++ atomics and condvar misuses with clean, actionable diffs.\nConsistency checks: It reliably flagged distributed state mismatches across services.\nExtra bug surfacing: It did identify additional Critical issues not originally under evaluation, though fewer than initially expected under a stricter rubric.\nAs Anthropic markets Sonnet 4.5, they emphasize “hybrid reasoning” and “long horizon” planning. In practice, that shows up as more willingness to chase down side-paths in the code and note real but untracked issues.\nSonnet 4.5: Hits a price vs. performance sweet spot\nOne of the biggest advantages of Sonnet 4.5 is its price-to-performance ratio. While Opus 4.1 remains Anthropic's flagship model in raw capability, it also comes at a significantly higher cost.\nSonnet 4.5 narrows the gap in coverage and important bug-finding while staying far more cost-efficient to run. For many teams, that balance of having close to Opus-level results at a fraction of the price is what makes Sonnet 4.5 the most pragmatic choice.\nSonnet 4.5 weaknesses\nBut if using Sonnet 4.5, it’s critical to be aware of its weaknesses. These include:\n\nDeadlock coverage: Like Sonnet 4 and even Opus, it still struggles to trace complex lock ordering.\nVerbosity and hedging: Many comments run long, caveated, or uncertain. Compare this to GPT-5 Codex, which in our earlier work wrote comments that “read like patches” with crisp directness. For example, with GPT-5 Codex:\nLock ordering / deadlock: Reorder the lock acquisitions to follow a consistent hierarchy. This prevents circular wait deadlocks.”\nRegex catastrophic backtracking: “Remove the nested quantifier to avoid catastrophic backtracking.”\nPrecision gap: At 35% comment-level precision and 41.5% important share percentage, it’s better than Sonnet 4 but well short of Opus 4.1.\nSonnet 4.5 verdict\nSonnet 4.5 feels less like a teacher writing in red pen and more like a thoughtful colleague at your side: pointing out possible issues, often right, occasionally over-hedged, and sometimes spotting things you didn’t know were there.\nThat style is a double-edged sword in review. On one hand, developers may appreciate the extra critical issues it flags. On the other, when the task is “please catch this bug,” Opus 4.1 is still sharper.\nClosing thoughts\nAnthropic positioned Sonnet 4.5 as a step toward agentic reasoning and computer use. In code review, that reasoning shows up in richer, more cautious, and more wide-ranging comments.\nFor teams:\nIf you value decisive, patch-like feedback, Opus 4.1 (or GPT-5 Codex) still sets the bar.\nIf you want a reviewer that finds critical issues anywhere they lurk, even beyond the tracked bug, Sonnet 4.5 has surprising upside.\nAnd if you care about pragmatic price-to-performance, Sonnet 4.5 may be the smartest choice: close to Opus’s accuracy at a fraction of the cost.\nEither way, Sonnet 4.5 changes the texture of reviews. It feels more human—not always cleaner, but more inquisitive, more hedged, sometimes more right in the places you weren’t looking.",
      "publishedAt": "2025-10-03T14:51:00.000Z",
      "author": "David Loker",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "funding_mna",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:12.173Z",
      "score": 0.16134581489928204
    },
    {
      "id": "51dab5d566315180ec1b4a0c6d173cc9",
      "title": "Aiを使ってci/cdパイプラインで静的解析を実行する方法",
      "url": "https://coderabbit.ai/blog/how-to-run-static-analysis-on-your-ci-cd-pipelines-using-ai-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/how-to-run-static-analysis-on-your-ci-cd-pipelines-using-ai\">How To Run Static Analysis On Your CI/CD Pipelines Using AI</a>の意訳です。</p>\n<p><em>「セットアップ時の思いがけない誤設定によりデータフィールドが空になり、その結果システムがアカウントを自動削除しました。」</em> —— これは、<a target=\"_blank\" href=\"https://www.itmedia.co.jp/news/articles/2405/28/news095.html\">Googleが年金基金のアカウント全体を誤って削除した件についての説明</a>です。</p>\n<p>このようなインシデントは、現代のソフトウェアシステムにおける正確な設定の重要性を浮き彫りにします。ちょっとした誤設定が、特にCI/CDパイプラインにおいて壊滅的な結果を招くことがあります。</p>\n<p>設定の正確性を担保し、コードレビューの複雑さを管理することは、DevOpsエンジニアにとって大きな負担になりえます。チームはしばしば機能開発を優先し、設定レビューは後回しになりがちです。その結果、見過ごされた誤設定が本番障害やダウンタイムを引き起こす可能性があります。</p>\n<p>CodeRabbitは、AI駆動の分析とリアルタイムフィードバックによってコードレビューを自動化し、この問題の解決を支援します。他のツールのように複雑なセットアップを必要とせず、CodeRabbitはパイプラインにシームレスに統合され、構成ファイルに対する静的チェックの正確性と効率性を確保します。</p>\n<p>本記事では、CodeRabbitがCI/CDパイプラインでの静的チェックにどのように役立ち、エンドツーエンドのデプロイプロセス全体で設定品質を保証し、効率を向上させるかを解説します。</p>\n<h2 id=\"heading-cicd\">なぜCI/CDパイプラインに静的チェックが不可欠なのか</h2>\n<p>構成ファイルは、インフラやアプリケーションのデプロイを制御するCI/CDパイプラインの要です。これらのファイルのエラーは大きな障害や事業中断リスクにつながるため、早期の検証が不可欠です。静的チェックは、セキュリティ脆弱性、コード品質問題、運用上の混乱を緩和する上で重要な役割を果たします。</p>\n<p>以下は、仮想環境のセットアップ、依存関係のインストール、Lintコマンドの実行を行うCircleCIのワークフロー構成ファイルの例です。</p>\n<pre><code class=\"lang-yaml\"><span class=\"hljs-attr\">jobs:</span>\n <span class=\"hljs-attr\">lint:</span>\n   <span class=\"hljs-attr\">docker:</span>\n     <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">image:</span> <span class=\"hljs-string\">circleci/python:3.9</span>\n   <span class=\"hljs-attr\">steps:</span>\n     <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">checkout</span>\n     <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">run:</span>\n         <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Install</span> <span class=\"hljs-string\">Dependencies</span>\n         <span class=\"hljs-attr\">command:</span> <span class=\"hljs-string\">|\n          python -m venv venv\n          . venv/bin/activate\n          pip install flake8\n</span>     <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">run:</span>\n         <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Run</span> <span class=\"hljs-string\">Linting</span>\n         <span class=\"hljs-attr\">command:</span> <span class=\"hljs-string\">|\n          . venv/bin/activate\n          flake8 .</span>\n</code></pre>\n<p>上記の構成で静的チェックが行われなければ、認識されない構文や無効な設定といった問題が漏れ、後工程でビルドが失敗する恐れがあります。例えば、依存関係の不足や不適切に整形されたコードは、デプロイパイプラインを破綻させる実行時エラーを招いたり、本番で原因追跡が難しいバグを持ち込む可能性があります。</p>\n<p>総じて、静的チェックは以下を実現します。</p>\n<ul>\n<li><p><strong>早期のエラー検出</strong>: 実行前に構文エラーや誤設定を検出し、実行時障害の可能性を減らします</p>\n</li>\n<li><p><strong>コーディング標準の遵守</strong>: スタイルガイドやベストプラクティスをコードや構成ファイル全体に適用し、品質の一貫性を確保して変更の保守・レビューを容易にします</p>\n</li>\n<li><p><strong>コード品質の向上</strong>: テストの成功や一定以上のカバレッジなど、デプロイ前に満たすべき基準を静的チェックで担保し、全体的な品質を高めます</p>\n</li>\n</ul>\n<h2 id=\"heading-coderabbit\">CodeRabbitを使った静的チェック</h2>\n<p>CodeRabbitはCI/CDワークフローに統合され、一般的な誤設定を特定することで優位性を発揮します。この能力はデプロイプロセスの整合性を維持し、エンドユーザーに影響しうる中断を防ぐ上で重要です。</p>\n<p>さらに、追加の設定を必要とせずに静的解析やLintを自動実行できるという独自の利点があります。DevOpsチームにとって、この機能はセットアップ工程を簡素化し、複雑な設定ではなく開発に集中できるようにします。</p>\n<ul>\n<li><p>既存のCI/CDパイプラインに影響を与えずに統合され、追加設定なしでLintと静的解析を自動実行します。</p>\n</li>\n<li><p>GitHub、CircleCI、GitLabなどの主要プラットフォーム上の多様なツールと統合し、<a target=\"_blank\" href=\"https://docs.coderabbit.ai/tools/actionlint\">Actionlint</a>、<a target=\"_blank\" href=\"https://docs.coderabbit.ai/tools/yamllint\">Yamllint</a>、<a target=\"_blank\" href=\"https://docs.coderabbit.ai/tools/shellcheck\">ShellCheck</a>、<a target=\"_blank\" href=\"https://docs.coderabbit.ai/tools/circleci\">CircleCI</a>パイプラインなどのチェックを実行します。これによりセットアップが簡素化され、追加の手作業なしに素早く結果を得られます。</p>\n</li>\n<li><p>JenkinsやGitHub Actionsのようなツールでは、CodeRabbitはビルドやコミットごとに継続的に静的解析を行い、誤設定を早期に検出してワークフローの信頼性を高めます。</p>\n</li>\n</ul>\n<p>次のセクションでは、実際のCodeRabbitの動作を見ていきます。</p>\n<h2 id=\"heading-coderabbitgithub-actionsactionlint\">CodeRabbitとGitHub ActionsのActionlintで誤設定を検出する</h2>\n<p>CodeRabbitの機能を示すため、GitHub Actionsワークフローをプロジェクトに統合し、CI/CDパイプラインを自動化する方法を見ていきます。リポジトリには潜在的なエラーを含む構成ファイルがあり、CodeRabbitがそれを検出して報告します。</p>\n<p>以下は、作成したワークフロー内のタスクシーケンス図です。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730095735116/4c75a2d8-b953-4b43-ad37-42d5fdcdf188.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>プルリクエストを送信すると、CodeRabbitがファイルをレビューし、潜在的な誤設定を自動的に検出します。リポジトリの準備ができたら、<a target=\"_blank\" href=\"https://coderabbit.ai/blog/how-to-integrate-ai-code-review-into-your-devops-pipeline\">CodeRabbitと統合して自動コードレビューをセットアップ</a>し、以下の主要セクションからなる、包括的で構造化されたレポートを生成します。</p>\n<ul>\n<li><strong>Summary（概要）</strong> – コードや構成で検出された主要な変更点の簡潔なサマリー。注意が必要な領域を素早く把握できます。</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730095840397/69ebd9cc-bc73-4dc5-a6df-e28e1207a3e1.png?auto=compress,format&amp;format=webp\" alt /></p>\n<ul>\n<li><strong>Walkthrough（詳細解説）</strong> – 対象ファイルの詳細なステップバイステップ分析。具体的な問題点、設定、推奨事項をガイドします。</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730095921727/ac69901a-0730-4a94-a5a6-90fa6929fb45.png?auto=compress,format&amp;format=webp\" alt /></p>\n<ul>\n<li><strong>Table of Changes（変更一覧）</strong> – 各ファイルの変更点と要約の一覧。必要な対応の優先度付けを素早く行えます。</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730095947279/0f6d9a21-cae6-4536-ab8c-83f29d365a49.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>これらのセクションは、構成ファイルの調整や<a target=\"_blank\" href=\"https://app.coderabbit.ai/\">CodeRabbitダッシュボード</a>の利用でカスタマイズできます。詳しくは<a target=\"_blank\" href=\"https://docs.coderabbit.ai/configure-coderabbit\">CodeRabbit設定ガイド</a>をご覧ください。</p>\n<p>以下は、CodeRabbitのレビューを通じて詳細な洞察と提案が得られたサンプルのworkflow.yaml構成です。</p>\n<pre><code class=\"lang-yaml\"><span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">development</span> <span class=\"hljs-string\">task</span>\n\n<span class=\"hljs-attr\">on:</span>\n  <span class=\"hljs-attr\">push:</span>\n    <span class=\"hljs-attr\">branches:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">main</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">develop</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">staging</span>\n  <span class=\"hljs-attr\">pull_request:</span>\n    <span class=\"hljs-attr\">branches:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">main</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">develop</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">staging</span>\n\n<span class=\"hljs-attr\">jobs:</span>\n  <span class=\"hljs-attr\">lint:</span>\n    <span class=\"hljs-attr\">runs-on:</span> <span class=\"hljs-string\">ubuntu-latest</span>\n    <span class=\"hljs-attr\">steps:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Checkout</span> <span class=\"hljs-string\">code</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/checkout@v3</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Lint</span> <span class=\"hljs-string\">workflow</span> <span class=\"hljs-string\">YAML</span> <span class=\"hljs-string\">files</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">rhysd/actionlint@v1</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Setup</span> <span class=\"hljs-string\">Node.js</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/setup-node@v3</span>\n        <span class=\"hljs-attr\">with:</span>\n          <span class=\"hljs-attr\">node-version:</span> <span class=\"hljs-string\">'18'</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Install</span> <span class=\"hljs-string\">dependencies</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">npm</span> <span class=\"hljs-string\">install</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Lint</span> <span class=\"hljs-string\">JavaScript</span> <span class=\"hljs-string\">code</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">npm</span> <span class=\"hljs-string\">run</span> <span class=\"hljs-string\">lint</span>\n\n  <span class=\"hljs-attr\">build:</span>\n    <span class=\"hljs-attr\">runs-on:</span> <span class=\"hljs-string\">ubuntu-latest</span>\n    <span class=\"hljs-attr\">needs:</span> <span class=\"hljs-string\">lint</span>\n    <span class=\"hljs-attr\">steps:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Checkout</span> <span class=\"hljs-string\">code</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/checkout@v3</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Setup</span> <span class=\"hljs-string\">Node.js</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/setup-node@v3</span>\n        <span class=\"hljs-attr\">with:</span>\n          <span class=\"hljs-attr\">node-version:</span> <span class=\"hljs-string\">'18'</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Install</span> <span class=\"hljs-string\">dependencies</span> <span class=\"hljs-string\">and</span> <span class=\"hljs-string\">cache</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/cache@v3</span>\n        <span class=\"hljs-attr\">with:</span>\n          <span class=\"hljs-attr\">path:</span> <span class=\"hljs-string\">~/.npm</span>\n          <span class=\"hljs-attr\">key:</span> <span class=\"hljs-string\">${{</span> <span class=\"hljs-string\">runner.os</span> <span class=\"hljs-string\">}}-node-${{</span> <span class=\"hljs-string\">hashFiles('package-lock.json')</span> <span class=\"hljs-string\">}}</span>\n          <span class=\"hljs-attr\">restore-keys:</span> <span class=\"hljs-string\">|\n            ${{ runner.os }}-node-\n</span>        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">npm</span> <span class=\"hljs-string\">install</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Run</span> <span class=\"hljs-string\">tests</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">npm</span> <span class=\"hljs-string\">test</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Check</span> <span class=\"hljs-string\">for</span> <span class=\"hljs-string\">vulnerabilities</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">npm</span> <span class=\"hljs-string\">audit</span> <span class=\"hljs-string\">--production</span>\n\n  <span class=\"hljs-attr\">terraform:</span>\n    <span class=\"hljs-attr\">runs-on:</span> <span class=\"hljs-string\">ubuntu-latest</span>\n    <span class=\"hljs-attr\">needs:</span> <span class=\"hljs-string\">build</span>\n    <span class=\"hljs-attr\">steps:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Checkout</span> <span class=\"hljs-string\">code</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/checkout@v3</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Setup</span> <span class=\"hljs-string\">Terraform</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">hashicorp/setup-terraform@v2</span>\n        <span class=\"hljs-attr\">with:</span>\n          <span class=\"hljs-attr\">terraform_version:</span> <span class=\"hljs-number\">1.5</span><span class=\"hljs-number\">.0</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Terraform</span> <span class=\"hljs-string\">init</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">terraform</span> <span class=\"hljs-string\">init</span>\n        <span class=\"hljs-attr\">working-directory:</span> <span class=\"hljs-string\">infrastructure/</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Terraform</span> <span class=\"hljs-string\">plan</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">terraform</span> <span class=\"hljs-string\">plan</span>\n        <span class=\"hljs-attr\">working-directory:</span> <span class=\"hljs-string\">infrastructure/</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Terraform</span> <span class=\"hljs-string\">apply</span> <span class=\"hljs-string\">(development)</span>\n        <span class=\"hljs-attr\">if:</span> <span class=\"hljs-string\">github.ref</span> <span class=\"hljs-string\">==</span> <span class=\"hljs-string\">'refs/heads/develop'</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">terraform</span> <span class=\"hljs-string\">apply</span> <span class=\"hljs-string\">-auto-approve</span>\n        <span class=\"hljs-attr\">working-directory:</span> <span class=\"hljs-string\">infrastructure/</span>\n        <span class=\"hljs-attr\">env:</span>\n          <span class=\"hljs-attr\">AWS_ACCESS_KEY_ID:</span> <span class=\"hljs-string\">${{</span> <span class=\"hljs-string\">secrets.AWS_ACCESS_KEY_ID</span> <span class=\"hljs-string\">}}</span>\n          <span class=\"hljs-attr\">AWS_SECRET_ACCES_KEY:</span> <span class=\"hljs-string\">${{</span> <span class=\"hljs-string\">secrets.AWS_SECRET_ACCES_KEY</span> <span class=\"hljs-string\">}}</span>\n\n  <span class=\"hljs-attr\">docker:</span>\n    <span class=\"hljs-attr\">runs-on:</span> <span class=\"hljs-string\">ubuntu-latest</span>\n    <span class=\"hljs-attr\">needs:</span> <span class=\"hljs-string\">terraform</span>\n    <span class=\"hljs-attr\">steps:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Checkout</span> <span class=\"hljs-string\">code</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">actions/checkout@v3</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Login</span> <span class=\"hljs-string\">to</span> <span class=\"hljs-string\">AWS</span> <span class=\"hljs-string\">ECR</span>\n        <span class=\"hljs-attr\">id:</span> <span class=\"hljs-string\">login-ecr</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">aws-actions/amazon-ecr-login@v1</span>\n        <span class=\"hljs-attr\">with:</span>\n          <span class=\"hljs-attr\">region:</span> <span class=\"hljs-string\">us-east-1</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Build</span> <span class=\"hljs-string\">and</span> <span class=\"hljs-string\">tag</span> <span class=\"hljs-string\">Docker</span> <span class=\"hljs-string\">image</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n          IMAGE_TAG=${{ github.sha }}\n          docker build -t ${{ secrets.ECR_REGISTRY }}/my-app:latest .\n          echo \"IMAGE_TAG=$IMAGE_TAG\" &gt;&gt; $GITHUB_ENV\n</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Push</span> <span class=\"hljs-string\">Docker</span> <span class=\"hljs-string\">image</span> <span class=\"hljs-string\">to</span> <span class=\"hljs-string\">AWS</span> <span class=\"hljs-string\">ECR</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n          IMAGE_TAG=${{ env.IMAGE_TAG }}\n          docker push ${{ secrets.ECR_REGISTRY }}/my-app:$IMAGE_TAG\n</span>\n  <span class=\"hljs-attr\">deploy:</span>\n    <span class=\"hljs-attr\">runs-on:</span> <span class=\"hljs-string\">ubuntu-latest</span>\n    <span class=\"hljs-attr\">needs:</span> <span class=\"hljs-string\">docker</span>\n    <span class=\"hljs-attr\">environment:</span> <span class=\"hljs-string\">production</span>\n    <span class=\"hljs-attr\">steps:</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Deploy</span> <span class=\"hljs-string\">to</span> <span class=\"hljs-string\">Development</span>\n        <span class=\"hljs-attr\">if:</span> <span class=\"hljs-string\">github.ref</span> <span class=\"hljs-string\">==</span> <span class=\"hljs-string\">'refs/heads/develop'</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n          echo \"Deploying to development environment\"\n          # Your deployment script here\n</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Deploy</span> <span class=\"hljs-string\">to</span> <span class=\"hljs-string\">Staging</span>\n        <span class=\"hljs-attr\">if:</span> <span class=\"hljs-string\">github.ref</span> <span class=\"hljs-string\">==</span> <span class=\"hljs-string\">'refs/heads/staging'</span>\n        <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n          echo \"Deploying to staging environment\"\n          # Your deployment script here\n</span>\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Manual</span> <span class=\"hljs-string\">Approval</span> <span class=\"hljs-string\">for</span> <span class=\"hljs-string\">Production</span>\n        <span class=\"hljs-attr\">if:</span> <span class=\"hljs-string\">github.ref</span> <span class=\"hljs-string\">==</span> <span class=\"hljs-string\">'refs/head/main'</span>\n        <span class=\"hljs-attr\">uses:</span> <span class=\"hljs-string\">hmarr/auto-approve-action@v2</span>\n        <span class=\"hljs-attr\">with:</span>\n          <span class=\"hljs-attr\">github-token:</span> <span class=\"hljs-string\">${{</span> <span class=\"hljs-string\">secrets.GITHUB_TOKEN</span> <span class=\"hljs-string\">}}</span>\n\n      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">Deploy</span> <span class=\"hljs-string\">to</span> <span class=\"hljs-string\">Production</span>\n        <span class=\"hljs-attr\">if:</span> <span class=\"hljs-string\">github.ref</span> <span class=\"hljs-string\">==</span> <span class=\"hljs-string\">'refs/heads/main'</span>\n          <span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n          echo \"Deploying to production environment\"\n          # Your deployment script here</span>\n</code></pre>\n<p>コードレビューに入る前に、このワークフローが何を行っているかを高レベルで整理します。</p>\n<ul>\n<li><p>main、develop、stagingブランチへのpushおよびプルリクエストでCI/CDパイプラインをトリガーし、継続的インテグレーションを実現します。</p>\n</li>\n<li><p>YAML構成の構文チェックや、アプリケーションに必要な依存関係のインストールを含むLintワークフローを実行し、コード品質を担保します。</p>\n</li>\n<li><p>アプリケーションに必要なクラウドインフラのプロビジョニングと管理のためにTerraformをセットアップします。</p>\n</li>\n<li><p>アプリケーションの機能を検証するテストを実行し、脆弱性チェックを行ってコードの安全性と安定性を確保します。</p>\n</li>\n<li><p>デプロイに備えてアプリケーションのDockerイメージをビルド・タグ付けします。</p>\n</li>\n<li><p>DockerイメージをAWS Elastic Container Registry（ECR）にプッシュし、デプロイのためのアクセスを容易にします。</p>\n</li>\n<li><p>ブランチに応じてアプリケーションを開発・ステージング・本番の各環境にデプロイし、本番デプロイにはコントロールと監視のための手動承認ステップを含めます。</p>\n</li>\n</ul>\n<p>workflow.yamlの構成と各コンポーネントを確認したので、まずSummaryから各セクションを見ていきます。</p>\n<h3 id=\"heading-summary\">Summary</h3>\n<p>Summaryはレビューの第一歩として、最新コミットで導入された変更点の明確で簡潔な概要を提供します。新機能、スタイル調整、構成変更、プルリクエストで挙げられたその他の関連修正など、重点ポイントを素早く把握できます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730096176817/6d47a2f7-aa15-42ef-b9b9-9e2e265fd26f.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>このスニペットは、パフォーマンス向上のための非デバッグモードでの実行、コードのLint・ビルド・デプロイを合理化する自動CI/CDパイプラインの実装など、重要な保守作業を強調しています。</p>\n<p>Summaryは、最新コミットで加えられた主要な変更点と改善点の理解に役立ちます。</p>\n<p>Summaryで要点を把握したら、次はWalkthroughセクションで具体的な変更点の詳細を見ていきます。</p>\n<h3 id=\"heading-walkthrough\">Walkthrough</h3>\n<p>このセクションでは、最新コミットで各ファイルに加えられた具体的な変更を包括的に概観します。各ファイルの変更がプロジェクト全体の機能性やユーザー体験の向上にどう寄与するかを明確にします。</p>\n<p>Changes Tableは、最新コミットにおける各ファイルの変更点を簡潔にまとめ、コードベースのどこが変更されたかを素早く特定できるようにします。</p>\n<p>各行には変更されたファイルと、Change Summary列に詳細な変更説明が含まれます。CSSファイルのスタイル更新、アプリケーションロジックの機能調整、CI/CDパイプライン構成の改善などが含まれます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730096224642/c714ccfb-0c73-4987-a314-f56c9b6cd8ed.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>情報を構造化して提示することで、変更の影響を理解しやすくし、開発者がプロジェクトへの影響を素早く把握できるようにします。</p>\n<p>全体として、コラボレーションにおける重要なリファレンスとして機能し、さらなる議論やレビューが必要な箇所にチームメンバーが集中できるよう支援しつつ、コードベースの変遷を追跡します。ちょっとした遊び心として、エラーに関するポエムも生成します。</p>\n<h3 id=\"heading-code-review\">Code Review</h3>\n<p>以下のセクションでは、構成ファイルを詳細に検査し、改善余地のある領域を特定します。キャッシュ戦略の改善からデプロイプロセスの最適化まで、GitHub Actionsワークフロー全体の効率と堅牢性を高めるための提案が、コードに対して具体的に提示されます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730096263866/b6fd24d9-e011-42ff-a2af-4025588aab4a.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>レビュー詳細、使用された構成、レビューのプロファイル、処理対象ファイル、使用された追加コンテキストなどに関する<strong>実行可能なコメント</strong>の詳細と概要が提供されます。ワンクリックでコミットできる提案が含まれる場合もあります。</p>\n<p>ここから、CodeRabbitがワークフロー各部に対して提案したレビューコメントを見ていきます。CodeRabbitは構成ファイルがGitHub Actionsのワークフローであることを自動認識し、<code>actionlint</code>で徹底的に解析します。レビューの過程で、パフォーマンス最適化に関する有益な洞察と提案が示されます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730096291950/a7c0ff57-e67c-4184-990c-428289bfc67b.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>lintジョブでは、<code>actions/cache@v3</code>を用いたnpm依存関係のキャッシュ機会を検出しました。Lint実行前にキャッシュステップを追加する提案により、以降の実行時間を短縮できます。このプロアクティブなフィードバックは、手動介入なしにワークフローを効率化し、より最適化されたCI/CDパイプラインを実現します。</p>\n<p>指摘の通り、キャッシュステップの構造に誤りがあります。runコマンド（npm install）がcacheアクションのusesブロック内に置かれており、正しく実行されない可能性があります。</p>\n<p>これを解決するため、キャッシュとインストールのステップを分離することを提案しています。修正案ではキャッシュ処理を独立したブロックに移し、次のステップで<code>npm ci</code>を使用して依存関係をクリーンかつ高速にインストールするようにしています。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730096332588/972bd50d-aa52-454d-9638-bacf814759e3.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>Terraformセクションでは、Terraformバージョンに変数を使う点での潜在的問題を検出しました。加えて、特に<code>plan</code>や<code>apply</code>でAWS認証情報に関する問題が生じうること、<code>AWS_SECRET_ACCESS_KEY</code>のタイポなど、わずかなミスでもパイプラインの実行失敗につながる可能性を指摘しています。</p>\n<p>これらに対し、タイポ修正、Terraformバージョンの更新容易化、すべてのTerraformコマンドでAWSクレデンシャルが利用可能になるような構成変更が提案されました。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730096375171/65fd86fc-bdda-4fe1-9478-581a24eb94e2.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>dockerジョブでは、Dockerイメージに<code>latest</code>タグを使用している点でのセキュリティリスクを検出しました。<code>latest</code>のみの運用はバージョニングやロールバックに問題を生じうるため、<code>latest</code>と特定バージョンタグ（例: git SHA）の併用を提案し、追跡性とロールバック容易性を高めます。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1730096418163/5cf101a1-e681-4aac-b508-96535af67810.png?auto=compress,format&amp;format=webp\" alt /></p>\n<p>deployジョブでも複数の潜在的問題を検出しました。手動承認ステップに自動承認アクションを用いており、本来の目的に反しています。さらに、本番デプロイのステップに構文エラーがあり、実際のデプロイスクリプトが欠落しているため、プロセスが不完全です。これらの問題に対する修正案が提示されています。</p>\n<p>CodeRabbitのAI駆動分析により、構成ファイルの問題が迅速に特定・強調され、修正提案が示されることが分かりました。</p>\n<h2 id=\"heading-cicdcoderabbit\">CI/CDパイプラインでCodeRabbitを使う利点</h2>\n<p>コードレビューを自動化し精密なフィードバックを提供することで、CodeRabbitはコード品質を高め、CI/CDパイプラインに潜む問題や脆弱性を早期に捕捉します。その結果、スムーズなデプロイとエラー削減につながります。</p>\n<p>CI/CDでCodeRabbitを使うことで得られる主な利点を見ていきましょう。</p>\n<h3 id=\"heading-6zal55m66icf55sf55sj5ocn44gu5zcr5lik\">開発者生産性の向上</h3>\n<p>自動化された静的チェックワークフローにより、CodeRabbitは手動レビューの必要性を減らし、DevOpsエンジニアが設定修正ではなく、インフラやデプロイプロセスの最適化といった戦略的タスクに集中できるようにします。即時のフィードバックループにより、コミットごとに問題を素早く検出・対処でき、迅速な開発ペースを維持できます。</p>\n<h3 id=\"heading-44kz44o844oj5zob6loq44gu5ps55zae\">コード品質の改善</h3>\n<p>CodeRabbitはベストプラクティスに照らして構成ファイルを自動検証し、設定の一貫性を強制して早期にエラーを捕捉します。プラットフォームは過去のレビューから学習し、反復的なアラートを賢く抑制、最も重要な問題に集中できるようにします。さらに、ワンクリックの提案を提供し、構成ファイルに素早く取り込めます。</p>\n<h3 id=\"heading-44k744kt44ol44oq44og44kj\">セキュリティ</h3>\n<p>CodeRabbitは、誤設定されたアクセス制御や不適切な設定などのセキュリティ脆弱性を早期に検出し、侵害の可能性を低減します。静的チェックをCI/CDプロセスに統合することで、構成ミスによるデプロイ失敗を防ぎ、より安定で信頼性の高いソフトウェアデリバリーパイプラインを実現します。</p>\n<h2 id=\"heading-44g44go44kb\">まとめ</h2>\n<p>本記事では、誤設定が遅延やセキュリティ脆弱性、さらにはデプロイ失敗につながること、そしてアプリケーションコードと同等に厳密にテストする重要性を見てきました。</p>\n<p>従来の手法が構成ファイルのテストの重要性を見落としがちなのに対し、<a target=\"_blank\" href=\"https://coderabbit.ai\">CodeRabbit</a>はCI/CDパイプラインのレビューをコード/構成レビューの自動化、重大なエラーの検出、全体的な品質向上によって支援します。手動レビュー時間を大幅に削減し、DevOpsチームが戦略的タスクに集中してデプロイサイクルを加速できるようにします。</p>\n<p>AIコードレビューの効果をワークフローで体験してみてください——<a target=\"_blank\" href=\"https://coderabbit.ai/\">今すぐCodeRabbitの無料トライアルを始めましょう</a>。</p>\n",
      "summary": "How To Run Static Analysis On Your CI/CD Pipelines Using AIの意訳です。\n「セットアップ時の思いがけない誤設定によりデータフィールドが空になり、その結果システムがアカウントを自動削除しました。」 —— これは、Googleが年金基金のアカウント全体を誤って削除した件についての説明です。\nこのようなインシデントは、現代のソフトウェアシステムにおける正確な設定の重要性を浮き彫りにします。ちょっとした誤設定が、特にCI/CDパイプラインにおいて壊滅的な結果を招くことがあります。\n設定の正確性を担保し、コードレビューの複雑さを管理することは、DevOpsエンジニアにとって大きな負担になりえます。チームはしばしば機能開発を優先し、設定レビューは後回しになりがちです。その結果、見過ごされた誤設定が本番障害やダウンタイムを引き起こす可能性があります。\nCodeRabbitは、AI駆動の分析とリアルタイムフィードバックによってコードレビューを自動化し、この問題の解決を支援します。他のツールのように複雑なセットアップを必要とせず、CodeRabbitはパイプラインにシームレスに統合され、構成ファイルに対する静的チェックの正確性と効率性を確保します。\n本記事では、CodeRabbitがCI/CDパイプラインでの静的チェックにどのように役立ち、エンドツーエンドのデプロイプロセス全体で設定品質を保証し、効率を向上させるかを解説します。\nなぜCI/CDパイプラインに静的チェックが不可欠なのか\n構成ファイルは、インフラやアプリケーションのデプロイを制御するCI/CDパイプラインの要です。これらのファイルのエラーは大きな障害や事業中断リスクにつながるため、早期の検証が不可欠です。静的チェックは、セキュリティ脆弱性、コード品質問題、運用上の混乱を緩和する上で重要な役割を果たします。\n以下は、仮想環境のセットアップ、依存関係のインストール、Lintコマンドの実行を行うCircleCIのワークフロー構成ファイルの例です。\njobs:\n lint:\n   docker:\n     - image: circleci/python:3.9\n   steps:\n     - checkout\n     - run:\n         name: Install Dependencies\n         command: |\n          python -m venv venv\n          . venv/bin/activate\n          pip install flake8\n     - run:\n         name: Run Linting\n         command: |\n          . venv/bin/activate\n          flake8 .\n\n上記の構成で静的チェックが行われなければ、認識されない構文や無効な設定といった問題が漏れ、後工程でビルドが失敗する恐れがあります。例えば、依存関係の不足や不適切に整形されたコードは、デプロイパイプラインを破綻させる実行時エラーを招いたり、本番で原因追跡が難しいバグを持ち込む可能性があります。\n総じて、静的チェックは以下を実現します。\n早期のエラー検出: 実行前に構文エラーや誤設定を検出し、実行時障害の可能性を減らします\nコーディング標準の遵守: スタイルガイドやベストプラクティスをコードや構成ファイル全体に適用し、品質の一貫性を確保して変更の保守・レビューを容易にします\nコード品質の向上: テストの成功や一定以上のカバレッジなど、デプロイ前に満たすべき基準を静的チェックで担保し、全体的な品質を高めます\nCodeRabbitを使った静的チェック\nCodeRabbitはCI/CDワークフローに統合され、一般的な誤設定を特定することで優位性を発揮します。この能力はデプロイプロセスの整合性を維持し、エンドユーザーに影響しうる中断を防ぐ上で重要です。\nさらに、追加の設定を必要とせずに静的解析やLintを自動実行できるという独自の利点があります。DevOpsチームにとって、この機能はセットアップ工程を簡素化し、複雑な設定ではなく開発に集中できるようにします。\n既存のCI/CDパイプラインに影響を与えずに統合され、追加設定なしでLintと静的解析を自動実行します。\nGitHub、CircleCI、GitLabなどの主要プラットフォーム上の多様なツールと統合し、Actionlint、Yamllint、ShellCheck、CircleCIパイプラインなどのチェックを実行します。これによりセットアップが簡素化され、追加の手作業なしに素早く結果を得られます。\nJenkinsやGitHub Actionsのようなツールでは、CodeRabbitはビルドやコミットごとに継続的に静的解析を行い、誤設定を早期に検出してワークフローの信頼性を高めます。\n次のセクションでは、実際のCodeRabbitの動作を見ていきます。\nCodeRabbitとGitHub ActionsのActionlintで誤設定を検出する\nCodeRabbitの機能を示すため、GitHub Actionsワークフローをプロジェクトに統合し、CI/CDパイプラインを自動化する方法を見ていきます。リポジトリには潜在的なエラーを含む構成ファイルがあり、CodeRabbitがそれを検出して報告します。\n以下は、作成したワークフロー内のタスクシーケンス図です。\n\nプルリクエストを送信すると、CodeRabbitがファイルをレビューし、潜在的な誤設定を自動的に検出します。リポジトリの準備ができたら、CodeRabbitと統合して自動コードレビューをセットアップし、以下の主要セクションからなる、包括的で構造化されたレポートを生成します。\nSummary（概要） – コードや構成で検出された主要な変更点の簡潔なサマリー。注意が必要な領域を素早く把握できます。\n\nWalkthrough（詳細解説） – 対象ファイルの詳細なステップバイステップ分析。具体的な問題点、設定、推奨事項をガイドします。\n\nTable of Changes（変更一覧） – 各ファイルの変更点と要約の一覧。必要な対応の優先度付けを素早く行えます。\n\nこれらのセクションは、構成ファイルの調整やCodeRabbitダッシュボードの利用でカスタマイズできます。詳しくはCodeRabbit設定ガイドをご覧ください。\n以下は、CodeRabbitのレビューを通じて詳細な洞察と提案が得られたサンプルのworkflow.yaml構成です。\nname: development task\n\non:\n  push:\n    branches:\n      - main\n      - develop\n      - staging\n  pull_request:\n    branches:\n      - main\n      - develop\n      - staging\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Lint workflow YAML files\n        uses: rhysd/actionlint@v1\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: '18'\n\n      - name: Install dependencies\n        run: npm install\n\n      - name: Lint JavaScript code\n        run: npm run lint\n\n  build:\n    runs-on: ubuntu-latest\n    needs: lint\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: '18'\n\n      - name: Install dependencies and cache\n        uses: actions/cache@v3\n        with:\n          path: ~/.npm\n          key: ${{ runner.os }}-node-${{ hashFiles('package-lock.json') }}\n          restore-keys: |\n            ${{ runner.os }}-node-\n        run: npm install\n\n      - name: Run tests\n        run: npm test\n\n      - name: Check for vulnerabilities\n        run: npm audit --production\n\n  terraform:\n    runs-on: ubuntu-latest\n    needs: build\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v2\n        with:\n          terraform_version: 1.5.0\n\n      - name: Terraform init\n        run: terraform init\n        working-directory: infrastructure/\n\n      - name: Terraform plan\n        run: terraform plan\n        working-directory: infrastructure/\n\n      - name: Terraform apply (development)\n        if: github.ref == 'refs/heads/develop'\n        run: terraform apply -auto-approve\n        working-directory: infrastructure/\n        env:\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCES_KEY: ${{ secrets.AWS_SECRET_ACCES_KEY }}\n\n  docker:\n    runs-on: ubuntu-latest\n    needs: terraform\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Login to AWS ECR\n        id: login-ecr\n        uses: aws-actions/amazon-ecr-login@v1\n        with:\n          region: us-east-1\n\n      - name: Build and tag Docker image\n        run: |\n          IMAGE_TAG=${{ github.sha }}\n          docker build -t ${{ secrets.ECR_REGISTRY }}/my-app:latest .\n          echo \"IMAGE_TAG=$IMAGE_TAG\" >> $GITHUB_ENV\n\n      - name: Push Docker image to AWS ECR\n        run: |\n          IMAGE_TAG=${{ env.IMAGE_TAG }}\n          docker push ${{ secrets.ECR_REGISTRY }}/my-app:$IMAGE_TAG\n\n  deploy:\n    runs-on: ubuntu-latest\n    needs: docker\n    environment: production\n    steps:\n      - name: Deploy to Development\n        if: github.ref == 'refs/heads/develop'\n        run: |\n          echo \"Deploying to development environment\"\n          # Your deployment script here\n\n      - name: Deploy to Staging\n        if: github.ref == 'refs/heads/staging'\n        run: |\n          echo \"Deploying to staging environment\"\n          # Your deployment script here\n\n      - name: Manual Approval for Production\n        if: github.ref == 'refs/head/main'\n        uses: hmarr/auto-approve-action@v2\n        with:\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: Deploy to Production\n        if: github.ref == 'refs/heads/main'\n          run: |\n          echo \"Deploying to production environment\"\n          # Your deployment script here\n\nコードレビューに入る前に、このワークフローが何を行っているかを高レベルで整理します。\nmain、develop、stagingブランチへのpushおよびプルリクエストでCI/CDパイプラインをトリガーし、継続的インテグレーションを実現します。\nYAML構成の構文チェックや、アプリケーションに必要な依存関係のインストールを含むLintワークフローを実行し、コード品質を担保します。\nアプリケーションに必要なクラウドインフラのプロビジョニングと管理のためにTerraformをセットアップします。\nアプリケーションの機能を検証するテストを実行し、脆弱性チェックを行ってコードの安全性と安定性を確保します。\nデプロイに備えてアプリケーションのDockerイメージをビルド・タグ付けします。\nDockerイメージをAWS Elastic Container Registry（ECR）にプッシュし、デプロイのためのアクセスを容易にします。\nブランチに応じてアプリケーションを開発・ステージング・本番の各環境にデプロイし、本番デプロイにはコントロールと監視のための手動承認ステップを含めます。\nworkflow.yamlの構成と各コンポーネントを確認したので、まずSummaryから各セクションを見ていきます。\nSummary\nSummaryはレビューの第一歩として、最新コミットで導入された変更点の明確で簡潔な概要を提供します。新機能、スタイル調整、構成変更、プルリクエストで挙げられたその他の関連修正など、重点ポイントを素早く把握できます。\n\nこのスニペットは、パフォーマンス向上のための非デバッグモードでの実行、コードのLint・ビルド・デプロイを合理化する自動CI/CDパイプラインの実装など、重要な保守作業を強調しています。\nSummaryは、最新コミットで加えられた主要な変更点と改善点の理解に役立ちます。\nSummaryで要点を把握したら、次はWalkthroughセクションで具体的な変更点の詳細を見ていきます。\nWalkthrough\nこのセクションでは、最新コミットで各ファイルに加えられた具体的な変更を包括的に概観します。各ファイルの変更がプロジェクト全体の機能性やユーザー体験の向上にどう寄与するかを明確にします。\nChanges Tableは、最新コミットにおける各ファイルの変更点を簡潔にまとめ、コードベースのどこが変更されたかを素早く特定できるようにします。\n各行には変更されたファイルと、Change Summary列に詳細な変更説明が含まれます。CSSファイルのスタイル更新、アプリケーションロジックの機能調整、CI/CDパイプライン構成の改善などが含まれます。\n\n情報を構造化して提示することで、変更の影響を理解しやすくし、開発者がプロジェクトへの影響を素早く把握できるようにします。\n全体として、コラボレーションにおける重要なリファレンスとして機能し、さらなる議論やレビューが必要な箇所にチームメンバーが集中できるよう支援しつつ、コードベースの変遷を追跡します。ちょっとした遊び心として、エラーに関するポエムも生成します。\nCode Review\n以下のセクションでは、構成ファイルを詳細に検査し、改善余地のある領域を特定します。キャッシュ戦略の改善からデプロイプロセスの最適化まで、GitHub Actionsワークフロー全体の効率と堅牢性を高めるための提案が、コードに対して具体的に提示されます。\n\nレビュー詳細、使用された構成、レビューのプロファイル、処理対象ファイル、使用された追加コンテキストなどに関する実行可能なコメントの詳細と概要が提供されます。ワンクリックでコミットできる提案が含まれる場合もあります。\nここから、CodeRabbitがワークフロー各部に対して提案したレビューコメントを見ていきます。CodeRabbitは構成ファイルがGitHub Actionsのワークフローであることを自動認識し、actionlintで徹底的に解析します。レビューの過程で、パフォーマンス最適化に関する有益な洞察と提案が示されます。\n\nlintジョブでは、actions/cache@v3を用いたnpm依存関係のキャッシュ機会を検出しました。Lint実行前にキャッシュステップを追加する提案により、以降の実行時間を短縮できます。このプロアクティブなフィードバックは、手動介入なしにワークフローを効率化し、より最適化されたCI/CDパイプラインを実現します。\n指摘の通り、キャッシュステップの構造に誤りがあります。runコマンド（npm install）がcacheアクションのusesブロック内に置かれており、正しく実行されない可能性があります。\nこれを解決するため、キャッシュとインストールのステップを分離することを提案しています。修正案ではキャッシュ処理を独立したブロックに移し、次のステップでnpm ciを使用して依存関係をクリーンかつ高速にインストールするようにしています。\n\nTerraformセクションでは、Terraformバージョンに変数を使う点での潜在的問題を検出しました。加えて、特にplanやapplyでAWS認証情報に関する問題が生じうること、AWS_SECRET_ACCESS_KEYのタイポなど、わずかなミスでもパイプラインの実行失敗につながる可能性を指摘しています。\nこれらに対し、タイポ修正、Terraformバージョンの更新容易化、すべてのTerraformコマンドでAWSクレデンシャルが利用可能になるような構成変更が提案されました。\n\ndockerジョブでは、Dockerイメージにlatestタグを使用している点でのセキュリティリスクを検出しました。latestのみの運用はバージョニングやロールバックに問題を生じうるため、latestと特定バージョンタグ（例: git SHA）の併用を提案し、追跡性とロールバック容易性を高めます。\n\ndeployジョブでも複数の潜在的問題を検出しました。手動承認ステップに自動承認アクションを用いており、本来の目的に反しています。さらに、本番デプロイのステップに構文エラーがあり、実際のデプロイスクリプトが欠落しているため、プロセスが不完全です。これらの問題に対する修正案が提示されています。\nCodeRabbitのAI駆動分析により、構成ファイルの問題が迅速に特定・強調され、修正提案が示されることが分かりました。\nCI/CDパイプラインでCodeRabbitを使う利点\nコードレビューを自動化し精密なフィードバックを提供することで、CodeRabbitはコード品質を高め、CI/CDパイプラインに潜む問題や脆弱性を早期に捕捉します。その結果、スムーズなデプロイとエラー削減につながります。\nCI/CDでCodeRabbitを使うことで得られる主な利点を見ていきましょう。\n開発者生産性の向上\n自動化された静的チェックワークフローにより、CodeRabbitは手動レビューの必要性を減らし、DevOpsエンジニアが設定修正ではなく、インフラやデプロイプロセスの最適化といった戦略的タスクに集中できるようにします。即時のフィードバックループにより、コミットごとに問題を素早く検出・対処でき、迅速な開発ペースを維持できます。\nコード品質の改善\nCodeRabbitはベストプラクティスに照らして構成ファイルを自動検証し、設定の一貫性を強制して早期にエラーを捕捉します。プラットフォームは過去のレビューから学習し、反復的なアラートを賢く抑制、最も重要な問題に集中できるようにします。さらに、ワンクリックの提案を提供し、構成ファイルに素早く取り込めます。\nセキュリティ\nCodeRabbitは、誤設定されたアクセス制御や不適切な設定などのセキュリティ脆弱性を早期に検出し、侵害の可能性を低減します。静的チェックをCI/CDプロセスに統合することで、構成ミスによるデプロイ失敗を防ぎ、より安定で信頼性の高いソフトウェアデリバリーパイプラインを実現します。\nまとめ\n本記事では、誤設定が遅延やセキュリティ脆弱性、さらにはデプロイ失敗につながること、そしてアプリケーションコードと同等に厳密にテストする重要性を見てきました。\n従来の手法が構成ファイルのテストの重要性を見落としがちなのに対し、CodeRabbitはCI/CDパイプラインのレビューをコード/構成レビューの自動化、重大なエラーの検出、全体的な品質向上によって支援します。手動レビュー時間を大幅に削減し、DevOpsチームが戦略的タスクに集中してデプロイサイクルを加速できるようにします。\nAIコードレビューの効果をワークフローで体験してみてください——今すぐCodeRabbitの無料トライアルを始めましょう。",
      "publishedAt": "2025-10-02T06:11:57.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "thought_leadership",
      "tags": [
        "code_review",
        "governance"
      ],
      "ingestedAt": "2025-12-04T14:17:12.174Z",
      "score": 0.09218075319301175
    },
    {
      "id": "891e9f13dfd4ae0d07a1fa623cdd9730",
      "title": "GPT-5 Codex: GPT-5の欠点をどう解消するか",
      "url": "https://coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks\">GPT-5 Codex: How it solves for GPT-5's drawbacks</a>の意訳です。</p>\n<p>CodeRabbitのコードレビューは、開発者がバグを修正しコードをデリバリーするのを支援します。私たちは最近、<a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning-ja\">GPT-5のベンチマーク</a>について記事を書き、AIコードレビューという私たちのユースケースにおいて、このモデルが推論面で世代的な飛躍を遂げているという見解を述べました。より広いユーザーベースに展開する中で、S/N値（シグナル/ノイズ値。以下SNR）が低下し、レビューが過度に細かすぎるという印象を持たれることが分かりました。</p>\n<p>GPT-5 Codexのリリースと、私たちが実施した製品変更（重大度タグ付け、より厳格なリファクタ提案のゲーティング、フィルタリング改善）により、難しいバグを見つける能力を犠牲にすることなく、SNRを取り戻すことができました。</p>\n<p>刷新した「Hard 25」PRセットにおいて、GPT-5 CodexはGPT-5と比べてコメントあたりの精度が約35%向上し、エラーパターンレベルの不具合カバレッジは本質的に同等のまま、コメント量を約3分の1削減しました。さらにGPT-5 Codexモデルの低レイテンシと組み合わせることで、体感はより軽快、かつフォーカスされたものになります。</p>\n<h2 id=\"heading-kirkvzxjgplvvijjgarjgzzvvinmukzlrprjgzfjgzjgysqkg\"><strong>何を（なぜ）測定したか</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759249757952/184a4486-2f03-4293-8b39-82ca88ed9a32.png\" alt class=\"image--center mx-auto\" /></p>\n<p>GPT-5 Codexのテストでは、OSSのPRからなる新しい「Hard 25」スイート（<a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning-ja\">以前の記事</a>よりやや難度高め）を実行しました。これは私たちのデータセットに含まれる中でも特に難しい25本のプルリクエストです。現実世界のバグを表したもので、対象は以下の通りです。</p>\n<ul>\n<li><p>並行性の問題（例: TOCTOUレース、誤った同期化）</p>\n</li>\n<li><p>オブジェクト指向設計の欠陥（例: 仮想呼び出しの落とし穴、参照カウントメモリモデルの破綻）</p>\n</li>\n<li><p>パフォーマンス上の危険（例: 無制御なキャッシュ成長、タイトループによるスタール）</p>\n</li>\n<li><p>言語特有の落とし穴（例: TypeScriptの誤用、C++のメモリ順序の微妙さ）</p>\n</li>\n</ul>\n<p>評価したモデルは以下の通りです。</p>\n<ul>\n<li><p><strong>GPT-5 Codex</strong></p>\n</li>\n<li><p><strong>GPT-5</strong></p>\n</li>\n<li><p><strong>Claude</strong>（Sonnet 4 および Opus-4.1）</p>\n</li>\n</ul>\n<h2 id=\"heading-5l2v44ks6kmv5l6h44gx44gf44gl\">何を評価したか</h2>\n<p>各モデルには、以下の観点でスコアを与えました:</p>\n<ul>\n<li><p><strong>EP（Error Pattern / エラーパターン）</strong><br />  PRに潜む特定の根本欠陥（例: 条件変数でのlost wakeup、ロック順序の不整合、ブール条件が錯綜する中に隠れたロジックバグ）。</p>\n</li>\n<li><p><strong>EP PASS/FAIL（PR単位）</strong><br />  そのPRのEPを直接修正、または信頼できる形で表面化させるコメントを少なくとも1つ残せばPASS。コメントがゼロならそのPRはFAIL。</p>\n</li>\n<li><p><strong>コメントPASS/FAIL（コメント単位）</strong><br />  EPを直接修正、または信頼できる形で表面化させればPASS、そうでなければFAIL。</p>\n</li>\n<li><p><strong>コメントあたり精度（Per comment precision）</strong><br />  PASSコメント ÷ 全コメント。今回のデータセットにおける実務上のSNR。</p>\n</li>\n<li><p><strong>Important share（重要コメント比率）</strong><br />  すべてのPASSはImportant扱い。EPを解決しないが、重大なバグ（use-after-free、二重解放、lost wakeup、メモリリーク、null参照、パストラバーサル、破滅的な正規表現など）を正しく指摘するコメントもImportant。それ以外はMinor。</p>\n</li>\n</ul>\n<h2 id=\"heading-codexsnr\"><strong>スコアボード - CodexはSNRを改善</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759197629508/de48651d-069a-4530-b070-de6107b57583.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>要点:</strong> Codexは、GPT-5とほぼ同じEPを見つけつつ、より少ない・締まったコメントで行うため、SNRが向上します。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759238036950/7c99e827-daba-4205-8f70-0d5a9413b74b.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>意味するところ:</strong> Codexは25本中20本のPRをカバー（残り5本は未カバーのFAIL）。総コメント数は少ないにもかかわらず、EPのPASS数はやや上回り（16 対 15）、重要（Important）コメントは大幅に増加。コメントの半分以上が、そのPRで想定していた問題へのダイレクト、または別の重大バグの指摘でした。GPT-5とClaudeは精度・重要比率ともに約40%で、後塵を拝しました。</p>\n<p><strong>結論: 同等のEPカバレッジで、ノイズは減少</strong><br />CodexはGPT-5のバグ発見力を維持したまま、コメント量を約32%削減（54 対 79）し、コメントあたり精度を約35%向上（46.3% 対 34.2%）。ClaudeはカバレッジはGPT-5に近いものの、より冗長で精度は低めでした。</p>\n<h2 id=\"heading-codex\"><strong>スタイルと構造（Codexがパッチのように読める理由）</strong></h2>\n<p>Codexの返信は一貫してアクション優先（ほぼ常にdiff付き）で、曖昧表現が少ない。これは「すぐパッチに反映できる提案」を望むレビュアーの期待に合致します。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759237978515/bf5eab59-e642-486d-bbb7-8adfd02fc00e.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-codex-1\"><strong>Codexが得意とするバグの種類</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759197684585/6b90b664-dbf0-46d0-ae5e-cf988083ab8b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>スイート全体では、どのモデルも並行性・同期の問題に強みを見せましたが、Codexは特に以下で際立ちました。</p>\n<ol>\n<li><p><strong>条件変数の誤用とlost wakeup</strong><br /> ロック下でのwait、ループ内での述語チェックといった標準パターンを提案し、具体的なdiffを提示。</p>\n</li>\n<li><p><strong>ロック順序とデッドロック</strong><br /> 取得順の不整合を指摘し、ロック階層の導入やクリティカルセクション外への処理移動を提案（いずれも実行可能な編集付き）。</p>\n</li>\n<li><p><strong>APIやパフォーマンスの微妙な罠</strong><br /> 破滅的な正規表現のバックトラッキングやメモリモデルの順序問題などを的確に特定し、パッチを提示。</p>\n</li>\n</ol>\n<h2 id=\"heading-gpt-5\"><strong>なぜGPT-5は騒がしく感じられたのか、そしてどう解決したか</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759251737131/d79ed373-5213-4cd0-a168-64441227b7b9.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>観測:</strong> SonnetやOpusからGPT-5に移行した際、レビューあたりの総コメント数はほぼ倍増しました。一方でハルシネーションは1%未満、ネガティブトーンも1%未満まで低下したにもかかわらず、受け入れ率（有益と判断されたコメントの比率）は、GPT-5導入前のベースラインに比べて大きく低下しました。</p>\n<p><strong>Codexでの変化:</strong> GPT-5 Codexと私たちの製品変更の併用により、受け入れ率は以前の水準まで回復。一方で総コメント量は「GPT-5導入前」より依然多いままです。要するに、「有益さ」は取り戻しつつ、GPT-5並みに実問題を見つけ続けられるようになりました。</p>\n<p>この改善には2つの製品変更が寄与しました。</p>\n<ol>\n<li><p><strong>重大度とレビュータイプのタグを前面に</strong></p>\n<ul>\n<li><p><strong>レビュータイプ:</strong> ユーザーが読みたいコメントの種類を自己選択できるよう、⚠️ Potential issue、🛠️ Refactor suggestion、🧹 Nitpick（<em>Assertive</em>モードにしない限り非表示）を用意。</p>\n</li>\n<li><p><strong>重大度:</strong> コメントに重大度タグを付け、優先度を明確化。タグは🔴 Critical、🟠 Major、🟡 Minor、🔵 Trivial、⚪ Info。</p>\n</li>\n<li><p>バグ（Critical/Major/Minor）は常に表示。その他は常にではありません。リファクタはモデルが「本質的」と判定した場合のみ表示。すべて見たいユーザーは<em>Assertive</em>に切替可能。</p>\n</li>\n</ul>\n</li>\n<li><p><strong>より厳格なフィルタリングと集約</strong></p>\n<ul>\n<li>重複メモを折りたたみ、「あると嬉しい」レベルの提案は明確なROIがない限り除外。結果として、コメントは少数精鋭化し、ノイズで見落とすリスクが減少。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"heading-amp-codex\"><strong>レイテンシ: 速さは正義 &amp; Codexは速い</strong></h2>\n<p>5分のレビューは許容範囲ですが、30分は許容できません。GPT-5の「常に深く考える」スタイルは、ファーストトークンまでの時間と全体のレビュー時間を大幅に増やしました。私たちは最近いくつかのパイプライン最適化を行い、さらにCodexがGPT-5由来のレイテンシを低減できるようになりました。</p>\n<p>Codexの可変（弾力的）な思考は、不要な場面では深掘りを減らし、実運用でTTFT（最初の出力までの時間）とE2Eレビュー時間を短縮しています。総じて、レビューは速くなり、フィードバックは早く、ヒューマン・イン・ザ・ループの流れが改善されます。</p>\n<h2 id=\"heading-coderabbit\"><strong>CodeRabbitユーザーが期待できること</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759197710045/2f32eef6-3227-450f-ac4b-485f89126197.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Codex導入後、AIコードレビューはどう変わるでしょうか？</p>\n<ol>\n<li><p><strong>生のバグ検出力は同等</strong></p>\n<ul>\n<li>刷新したHard 25で、CodexのEPレベルPASSは64%、GPT-5は60%（<a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning-ja\">以前のPRセット</a>ではGPT-5が77.3%）。GPT-5がもたらした重要な勝ち筋を失っていません。</li>\n</ul>\n</li>\n<li><p><strong>コメントは少なく、しかし強く</strong></p>\n<ul>\n<li>総コメント数はGPT-5比で約32%減、SNR（コメントあたり精度）は約35%向上。文章よりパッチが増えます。</li>\n</ul>\n</li>\n<li><p><strong>重大度タグでレビューに集中</strong></p>\n<ul>\n<li>新しい重大度タグにより、Critical/Majorがトップに浮上。リファクタはゲート制御、ニットピックはオプトイン。コメントの走査に費やす時間が減り、修正に時間を割けます。</li>\n</ul>\n</li>\n<li><p><strong>フィードバックループの高速化</strong></p>\n<ul>\n<li>Codexの軽量な推論とパイプライン改善で、最初の有益なコメントまでの時間が短縮。体感で分かります。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"heading-kirlrprph4nmotku5jpjllvvijjg4fjg7zjgrlpb3jgy3jga7jgyljgarjgzjgbjvvikqkg\"><strong>定量的付録（データ好きのあなたへ）</strong></h2>\n<p>以下は興味深かった追加統計を紹介します。</p>\n<ul>\n<li><p><strong>コメントあたり精度（SNR）の向上:</strong> Codex 46.3% 対 GPT-5 34.2% — 相対で約+35%。</p>\n</li>\n<li><p><strong>コメント量の差:</strong> Codex 54 対 GPT-5 79 — 約32%減、EPのPASSは実質同等（16 対 15）。</p>\n</li>\n<li><p><strong>スタイル:</strong> Codexは94%のコメントでdiffを含み、このセットではClaudeやGPT-5より曖昧表現が少ない。</p>\n</li>\n<li><p><strong>実環境での受け入れ:</strong> GPT-5ロールアウト中は受け入れ率が大きく低下。Codexと製品変更の併用で約20–25%相対上昇し、導入前水準に回復。かつ、GPT-5導入前より受け入れコメント数は多いまま。</p>\n</li>\n</ul>\n<h2 id=\"heading-codex-2\"><strong>Codexがまだ弱い点（と取り組み）</strong></h2>\n<p>改善は大きいものの、課題が残っていないわけではありません。現在、以下に取り組んでいます。</p>\n<ul>\n<li><p><strong>カバレッジの穴</strong><br />  モデルがPRにコメントを残さない場合、そのEPはハードFAIL。Codexの探索ヒューリスティクスを広げ、特定クラスの問題を見落としにくくします。</p>\n</li>\n<li><p><strong>リファクタ過剰提案（調整済みだが未完）</strong><br />  「本質的なもののみ」のゲートでノイズは抑制しましたが、特に大規模diffでコメント過多になりがちなケースの閾値をさらに引き締めます。</p>\n</li>\n<li><p><strong>ユーザー主導の優先度付け</strong><br />  GitHubのインライン順序は変更できませんが、各コメントに重大度を注記し、上から順にトリアージしやすくします。</p>\n</li>\n</ul>\n<h2 id=\"heading-codex-gpt-5\"><strong>Codex GPT-5: バグ捕捉力はそのまま、副作用は少なく</strong></h2>\n<p>私たちの指標はシンプルです: <strong>重要なバグを、素早く、ノイズに埋もれさせずに捕まえること</strong>。Codexはその実現を助けてくれます。GPT-5の噛み応えある推論力を保ちながら、SNRを回復させ、レイテンシを大幅に削りました。今後も測定・改善を継続し、より良い製品をリリースし続けます。</p>\n",
      "summary": "GPT-5 Codex: How it solves for GPT-5's drawbacksの意訳です。\nCodeRabbitのコードレビューは、開発者がバグを修正しコードをデリバリーするのを支援します。私たちは最近、GPT-5のベンチマークについて記事を書き、AIコードレビューという私たちのユースケースにおいて、このモデルが推論面で世代的な飛躍を遂げているという見解を述べました。より広いユーザーベースに展開する中で、S/N値（シグナル/ノイズ値。以下SNR）が低下し、レビューが過度に細かすぎるという印象を持たれることが分かりました。\nGPT-5 Codexのリリースと、私たちが実施した製品変更（重大度タグ付け、より厳格なリファクタ提案のゲーティング、フィルタリング改善）により、難しいバグを見つける能力を犠牲にすることなく、SNRを取り戻すことができました。\n刷新した「Hard 25」PRセットにおいて、GPT-5 CodexはGPT-5と比べてコメントあたりの精度が約35%向上し、エラーパターンレベルの不具合カバレッジは本質的に同等のまま、コメント量を約3分の1削減しました。さらにGPT-5 Codexモデルの低レイテンシと組み合わせることで、体感はより軽快、かつフォーカスされたものになります。\n何を（なぜ）測定したか\n\nGPT-5 Codexのテストでは、OSSのPRからなる新しい「Hard 25」スイート（以前の記事よりやや難度高め）を実行しました。これは私たちのデータセットに含まれる中でも特に難しい25本のプルリクエストです。現実世界のバグを表したもので、対象は以下の通りです。\n並行性の問題（例: TOCTOUレース、誤った同期化）\nオブジェクト指向設計の欠陥（例: 仮想呼び出しの落とし穴、参照カウントメモリモデルの破綻）\nパフォーマンス上の危険（例: 無制御なキャッシュ成長、タイトループによるスタール）\n言語特有の落とし穴（例: TypeScriptの誤用、C++のメモリ順序の微妙さ）\n評価したモデルは以下の通りです。\nGPT-5 Codex\nGPT-5\nClaude（Sonnet 4 および Opus-4.1）\n何を評価したか\n各モデルには、以下の観点でスコアを与えました:\nEP（Error Pattern / エラーパターン）\n  PRに潜む特定の根本欠陥（例: 条件変数でのlost wakeup、ロック順序の不整合、ブール条件が錯綜する中に隠れたロジックバグ）。\nEP PASS/FAIL（PR単位）\n  そのPRのEPを直接修正、または信頼できる形で表面化させるコメントを少なくとも1つ残せばPASS。コメントがゼロならそのPRはFAIL。\nコメントPASS/FAIL（コメント単位）\n  EPを直接修正、または信頼できる形で表面化させればPASS、そうでなければFAIL。\nコメントあたり精度（Per comment precision）\n  PASSコメント ÷ 全コメント。今回のデータセットにおける実務上のSNR。\nImportant share（重要コメント比率）\n  すべてのPASSはImportant扱い。EPを解決しないが、重大なバグ（use-after-free、二重解放、lost wakeup、メモリリーク、null参照、パストラバーサル、破滅的な正規表現など）を正しく指摘するコメントもImportant。それ以外はMinor。\nスコアボード - CodexはSNRを改善\n\n要点: Codexは、GPT-5とほぼ同じEPを見つけつつ、より少ない・締まったコメントで行うため、SNRが向上します。\n\n意味するところ: Codexは25本中20本のPRをカバー（残り5本は未カバーのFAIL）。総コメント数は少ないにもかかわらず、EPのPASS数はやや上回り（16 対 15）、重要（Important）コメントは大幅に増加。コメントの半分以上が、そのPRで想定していた問題へのダイレクト、または別の重大バグの指摘でした。GPT-5とClaudeは精度・重要比率ともに約40%で、後塵を拝しました。\n結論: 同等のEPカバレッジで、ノイズは減少\nCodexはGPT-5のバグ発見力を維持したまま、コメント量を約32%削減（54 対 79）し、コメントあたり精度を約35%向上（46.3% 対 34.2%）。ClaudeはカバレッジはGPT-5に近いものの、より冗長で精度は低めでした。\nスタイルと構造（Codexがパッチのように読める理由）\nCodexの返信は一貫してアクション優先（ほぼ常にdiff付き）で、曖昧表現が少ない。これは「すぐパッチに反映できる提案」を望むレビュアーの期待に合致します。\n\nCodexが得意とするバグの種類\n\nスイート全体では、どのモデルも並行性・同期の問題に強みを見せましたが、Codexは特に以下で際立ちました。\n条件変数の誤用とlost wakeup\n ロック下でのwait、ループ内での述語チェックといった標準パターンを提案し、具体的なdiffを提示。\nロック順序とデッドロック\n 取得順の不整合を指摘し、ロック階層の導入やクリティカルセクション外への処理移動を提案（いずれも実行可能な編集付き）。\nAPIやパフォーマンスの微妙な罠\n 破滅的な正規表現のバックトラッキングやメモリモデルの順序問題などを的確に特定し、パッチを提示。\nなぜGPT-5は騒がしく感じられたのか、そしてどう解決したか\n\n観測: SonnetやOpusからGPT-5に移行した際、レビューあたりの総コメント数はほぼ倍増しました。一方でハルシネーションは1%未満、ネガティブトーンも1%未満まで低下したにもかかわらず、受け入れ率（有益と判断されたコメントの比率）は、GPT-5導入前のベースラインに比べて大きく低下しました。\nCodexでの変化: GPT-5 Codexと私たちの製品変更の併用により、受け入れ率は以前の水準まで回復。一方で総コメント量は「GPT-5導入前」より依然多いままです。要するに、「有益さ」は取り戻しつつ、GPT-5並みに実問題を見つけ続けられるようになりました。\nこの改善には2つの製品変更が寄与しました。\n重大度とレビュータイプのタグを前面に\nレビュータイプ: ユーザーが読みたいコメントの種類を自己選択できるよう、⚠️ Potential issue、🛠️ Refactor suggestion、🧹 Nitpick（Assertiveモードにしない限り非表示）を用意。\n重大度: コメントに重大度タグを付け、優先度を明確化。タグは🔴 Critical、🟠 Major、🟡 Minor、🔵 Trivial、⚪ Info。\nバグ（Critical/Major/Minor）は常に表示。その他は常にではありません。リファクタはモデルが「本質的」と判定した場合のみ表示。すべて見たいユーザーはAssertiveに切替可能。\nより厳格なフィルタリングと集約\n重複メモを折りたたみ、「あると嬉しい」レベルの提案は明確なROIがない限り除外。結果として、コメントは少数精鋭化し、ノイズで見落とすリスクが減少。\nレイテンシ: 速さは正義 & Codexは速い\n5分のレビューは許容範囲ですが、30分は許容できません。GPT-5の「常に深く考える」スタイルは、ファーストトークンまでの時間と全体のレビュー時間を大幅に増やしました。私たちは最近いくつかのパイプライン最適化を行い、さらにCodexがGPT-5由来のレイテンシを低減できるようになりました。\nCodexの可変（弾力的）な思考は、不要な場面では深掘りを減らし、実運用でTTFT（最初の出力までの時間）とE2Eレビュー時間を短縮しています。総じて、レビューは速くなり、フィードバックは早く、ヒューマン・イン・ザ・ループの流れが改善されます。\nCodeRabbitユーザーが期待できること\n\nCodex導入後、AIコードレビューはどう変わるでしょうか？\n生のバグ検出力は同等\n刷新したHard 25で、CodexのEPレベルPASSは64%、GPT-5は60%（以前のPRセットではGPT-5が77.3%）。GPT-5がもたらした重要な勝ち筋を失っていません。\nコメントは少なく、しかし強く\n総コメント数はGPT-5比で約32%減、SNR（コメントあたり精度）は約35%向上。文章よりパッチが増えます。\n重大度タグでレビューに集中\n新しい重大度タグにより、Critical/Majorがトップに浮上。リファクタはゲート制御、ニットピックはオプトイン。コメントの走査に費やす時間が減り、修正に時間を割けます。\nフィードバックループの高速化\nCodexの軽量な推論とパイプライン改善で、最初の有益なコメントまでの時間が短縮。体感で分かります。\n定量的付録（データ好きのあなたへ）\n以下は興味深かった追加統計を紹介します。\nコメントあたり精度（SNR）の向上: Codex 46.3% 対 GPT-5 34.2% — 相対で約+35%。\nコメント量の差: Codex 54 対 GPT-5 79 — 約32%減、EPのPASSは実質同等（16 対 15）。\nスタイル: Codexは94%のコメントでdiffを含み、このセットではClaudeやGPT-5より曖昧表現が少ない。\n実環境での受け入れ: GPT-5ロールアウト中は受け入れ率が大きく低下。Codexと製品変更の併用で約20–25%相対上昇し、導入前水準に回復。かつ、GPT-5導入前より受け入れコメント数は多いまま。\nCodexがまだ弱い点（と取り組み）\n改善は大きいものの、課題が残っていないわけではありません。現在、以下に取り組んでいます。\nカバレッジの穴\n  モデルがPRにコメントを残さない場合、そのEPはハードFAIL。Codexの探索ヒューリスティクスを広げ、特定クラスの問題を見落としにくくします。\nリファクタ過剰提案（調整済みだが未完）\n  「本質的なもののみ」のゲートでノイズは抑制しましたが、特に大規模diffでコメント過多になりがちなケースの閾値をさらに引き締めます。\nユーザー主導の優先度付け\n  GitHubのインライン順序は変更できませんが、各コメントに重大度を注記し、上から順にトリアージしやすくします。\nCodex GPT-5: バグ捕捉力はそのまま、副作用は少なく\n私たちの指標はシンプルです: 重要なバグを、素早く、ノイズに埋もれさせずに捕まえること。Codexはその実現を助けてくれます。GPT-5の噛み応えある推論力を保ちながら、SNRを回復させ、レイテンシを大幅に削りました。今後も測定・改善を継続し、より良い製品をリリースし続けます。",
      "publishedAt": "2025-10-01T13:09:23.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:12.174Z",
      "score": 0.0721590557960146
    },
    {
      "id": "5a37a06bed9990e5849019396918b701",
      "title": "CodeRabbitのMCP連携 = コンテキストとコードレビュー",
      "url": "https://coderabbit.ai/blog/coderabbits-mcp-server-integration-code-reviews-that-see-the-whole-picture-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/coderabbits-mcp-server-integration-code-reviews-that-see-the-whole-picture\">CodeRabbit MCP server integration: Code reviews with more context</a>の意訳です。</p>\n<p>すべての開発チームは、孤立した状態で行うコードレビューのつらさを知っています。AIツール（あるいはチームメイト）であっても、文法やスタイル、パターンにコメントはできます。しかしビジネス要件、デプロイ依存関係、組織的な知識がなければ、全体像の半分を推測に基づいている状態です。</p>\n<p>CodeRabbitは現在、Linear、Jira、Circle CIといったいくつかのネイティブ統合を提供しており、これらのツールがコードレビューにもたらす価値を確認してきました。だからこそ今回、<strong>CodeRabbitのMCPサーバー統合のGAリリース</strong> を発表できることがとても嬉しいです。これにより、さらに多くのコンテキストをレビューに取り込めるようになります。</p>\n<p>本リリースで、CodeRabbitはConfluenceにあるビジネス要件からCI/CDパイプラインのシステム依存関係、さらには社内MCPサーバーのデータまで、開発エコシステム全体からコンテキストをオーケストレーションできる初のAIコードレビュープラットフォームとなりました。つまり、コードが「何を達成しようとしているのか」を本当に理解するレビューが可能になるのです。</p>\n<p><a target=\"_blank\" href=\"https://app.coderabbit.ai/login?free-trial\"><strong>14日間の無料トライアルを開始 →</strong></a> <em>約10分で、チーム標準に基づいたコンテキスト対応のレビューを実現。</em></p>\n<h2 id=\"heading-aimcp\"><strong>なぜAIコードレビューにMCPが必要なのか？</strong></h2>\n<p>開発チームは数多くのツールを使って作業しています。</p>\n<ul>\n<li><p>要件はLinearにある</p>\n</li>\n<li><p>設計仕様はFigmaにある</p>\n</li>\n<li><p>アーキテクチャの決定はConfluenceに記録される</p>\n</li>\n<li><p>セキュリティ基準は監査ごとに社内Wikiで更新される</p>\n</li>\n</ul>\n<p>AIコードレビューツールは基本的なコンテキスト、つまりコードベース、コーディング規約、いくつかの統合から始めます。構文を解析し、パターンを確認し、改善を提案します。しかし「そのコードがチームにとって本当に機能するかどうか」を左右するコンテキストは欠けています。</p>\n<p>MCPクライアントとしてのCodeRabbitは、組織コンテキストのコンパイラの役割を果たします。Wiki、チケット、デプロイパターンといった高レベルの入力を正確で実用的なコードレビューインサイトへと変換します。冗長な統合や脆いハックに頼ることなく、MCPはCodeRabbitのようなクライアントがLinearチケット、Confluenceドキュメント、Datadogメトリクス、Slackのディスカッションといった場所から必要なデータだけを取り込めるようにします。</p>\n<h2 id=\"heading-5a6f6zqb44gr44gv44gp44gg5yuv44gp44gu44gl4ocm\">実際にはどう動くのか…</h2>\n<p>CodeRabbitはレビューを開始する前に接続済みMCPサーバーを検索します。たとえばデータベーススキーマの変更はデータアーキテクチャ文書と照合され、APIエンドポイントの実装は社内Wikiに記録されたサービス設計パターンと突き合わせられます。</p>\n<p><strong>例: CodeRabbitによるコード整合性の確認</strong></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758126014841/53d19c71-2051-410c-a109-1e056cc0094d.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-44gp44gu44oe44o844or44gl44kj44gn44kc6yen6kab44gq44kz44oz44og44kt44k544oi44ks5yw44kk6l6844ka\">どのツールからでも重要なコンテキストを取り込む</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125357433/fa378ff4-743d-4841-a3fa-e933bcf4fe7c.png\" alt class=\"image--center mx-auto\" /></p>\n<p>従来のコードレビューツールは特定の統合を前提としています。CodeRabbitのMCP統合は、MCPサーバーを持つあらゆるシステムで動作します。独自の社内ツール、ニッチなSaaSプラットフォーム、カスタムドキュメントシステム。MCPサーバーがあれば、CodeRabbitはどこにでも接続できます。</p>\n<p>CodeRabbitをMCPクライアントとして利用すると、3種類の異なるコンテキストからレビューの深みを得られます。</p>\n<h3 id=\"heading-kirmiodoozpnmotjgrpjg7pjg4bjgq3jgrnjg4gqkg\"><strong>技術的コンテキスト</strong></h3>\n<ul>\n<li><p>依存関係、パフォーマンスデータ、静的解析、テストカバレッジなど</p>\n</li>\n<li><p><strong>ネイティブ統合:</strong> GitHub Actions、GitLab CI、Bitbucket Pipelines</p>\n</li>\n<li><p><strong>MCPサーバー:</strong> Datadog、New Relic、SonarQube、Snyk、Grafana</p>\n</li>\n</ul>\n<p>レビューコメント例は以下の通りです。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125833426/facf17b5-32c7-47ce-b6cd-8170f9837bf6.png\" alt class=\"image--center mx-auto\" /></p>\n<h3 id=\"heading-kirjg5pjgrjjg43jgrnjgrpjg7pjg4bjgq3jgrnjg4gqkg\"><strong>ビジネスコンテキスト</strong></h3>\n<ul>\n<li><p>要件、ユーザーストーリー、受け入れ基準など</p>\n</li>\n<li><p><strong>ネイティブ統合:</strong> Linear、Jira、GitHub Issues、GitLab Issues</p>\n</li>\n<li><p><strong>MCPサーバー:</strong> Confluence、Notion</p>\n</li>\n</ul>\n<p>レビューコメント例は以下の通りです。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125848608/26c21700-1f57-42af-9993-cc372c629e6f.png\" alt class=\"image--center mx-auto\" /></p>\n<h3 id=\"heading-kirntytnuztnmotjgrpjg7pjg4bjgq3jgrnjg4gqkg\"><strong>組織的コンテキスト</strong></h3>\n<ul>\n<li><p>過去の意思決定、慣例、会議メモ、組織的知識など</p>\n</li>\n<li><p><strong>ネイティブ統合:</strong> PR履歴、チーム慣習</p>\n</li>\n<li><p><strong>MCPサーバー:</strong> Slack、Microsoft Teams、Stack Overflow for Teams、PagerDuty</p>\n</li>\n</ul>\n<p>レビューコメント例は以下の通りです。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125858492/5bc5b006-b61c-4b9c-9642-5246978db056.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-mcp\"><strong>MCP統合を始めるには</strong></h2>\n<p>CodeRabbitのMCPクライアントは最小限の設定で導入できます。ほとんどの開発チームは10分以内に最初のMCPサーバーを接続できます。</p>\n<p><strong>MCPサーバー対応の人気開発ツール:</strong></p>\n<ul>\n<li><p><strong>Linear</strong>（ネイティブMCPサポート、5分）</p>\n</li>\n<li><p><strong>Notion</strong>（MCPサーバーあり、10分）</p>\n</li>\n<li><p><strong>Confluence</strong>（コミュニティ製MCPサーバー、15分）</p>\n</li>\n<li><p><strong>Figma</strong>（MCPプラグインあり、10分）</p>\n</li>\n</ul>\n<p>コード変更がどの開発システムを参照すべきかを定義します。データベース変更はアーキテクチャ文書を、認証変更はセキュリティ文書を確認する、という具合です。</p>\n<p>MCPサーバーを追加するのは簡単です。</p>\n<ol>\n<li><p>CodeRabbitダッシュボードで「integrations」に進み、必要ならMCP Serversタブに切り替えます  </p>\n</li>\n<li><p>あらかじめ用意されたMCPサーバーオプションをクリックするか、「New MCP Server」ボタンから他のMCPサーバーを追加できます  </p>\n</li>\n<li><p>リストにないMCPサーバーについては、必要な認証情報を入力します</p>\n</li>\n<li><p>MCP情報をどのように利用するかの使用ガイダンスを確認します  </p>\n</li>\n<li><p>接続が完了すると、利用可能な呼び出し一覧が表示され、カーソルを合わせると詳細を確認できます  </p>\n</li>\n<li><p>各呼び出しをクリックしてアクセスを有効化/無効化することも可能です  </p>\n</li>\n</ol>\n<h2 id=\"heading-kirjgyljgonjgobjgovjgrpjg7pjg4bjgq3jgrnjg4jjgpllj5bjgorovrzjgodjg6zjg5pjg6xjg7zln7rnm6qqkg\"><strong>あらゆるコンテキストを取り込むレビュー基盤</strong></h2>\n<p>CodeRabbitは50以上の統合に標準対応しています。MCPを利用すれば、カスタムサーバーや社内ツールにも拡張できます。まずはLinear、Confluence、Datadog、Slackといった既存システムから始め、必要に応じて追加していけます。</p>\n<h3 id=\"heading-kirmrkhjga7jgrnjg4bjg4pjg5c6kio\"><strong>次のステップ:</strong></h3>\n<ol>\n<li><p><a target=\"_blank\" href=\"https://app.coderabbit.ai/login?free-trial\"><strong>14日間の無料トライアルを開始</strong></a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://app.coderabbit.ai/integrations\"><strong>MCPサーバーディレクトリを表示</strong></a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://docs.coderabbit.ai/context-enrichment/mcp-server-integrations\"><strong>MCPドキュメントを見る</strong></a></p>\n</li>\n</ol>\n",
      "summary": "CodeRabbit MCP server integration: Code reviews with more contextの意訳です。\nすべての開発チームは、孤立した状態で行うコードレビューのつらさを知っています。AIツール（あるいはチームメイト）であっても、文法やスタイル、パターンにコメントはできます。しかしビジネス要件、デプロイ依存関係、組織的な知識がなければ、全体像の半分を推測に基づいている状態です。\nCodeRabbitは現在、Linear、Jira、Circle CIといったいくつかのネイティブ統合を提供しており、これらのツールがコードレビューにもたらす価値を確認してきました。だからこそ今回、CodeRabbitのMCPサーバー統合のGAリリース を発表できることがとても嬉しいです。これにより、さらに多くのコンテキストをレビューに取り込めるようになります。\n本リリースで、CodeRabbitはConfluenceにあるビジネス要件からCI/CDパイプラインのシステム依存関係、さらには社内MCPサーバーのデータまで、開発エコシステム全体からコンテキストをオーケストレーションできる初のAIコードレビュープラットフォームとなりました。つまり、コードが「何を達成しようとしているのか」を本当に理解するレビューが可能になるのです。\n14日間の無料トライアルを開始 → 約10分で、チーム標準に基づいたコンテキスト対応のレビューを実現。\nなぜAIコードレビューにMCPが必要なのか？\n開発チームは数多くのツールを使って作業しています。\n要件はLinearにある\n設計仕様はFigmaにある\nアーキテクチャの決定はConfluenceに記録される\nセキュリティ基準は監査ごとに社内Wikiで更新される\nAIコードレビューツールは基本的なコンテキスト、つまりコードベース、コーディング規約、いくつかの統合から始めます。構文を解析し、パターンを確認し、改善を提案します。しかし「そのコードがチームにとって本当に機能するかどうか」を左右するコンテキストは欠けています。\nMCPクライアントとしてのCodeRabbitは、組織コンテキストのコンパイラの役割を果たします。Wiki、チケット、デプロイパターンといった高レベルの入力を正確で実用的なコードレビューインサイトへと変換します。冗長な統合や脆いハックに頼ることなく、MCPはCodeRabbitのようなクライアントがLinearチケット、Confluenceドキュメント、Datadogメトリクス、Slackのディスカッションといった場所から必要なデータだけを取り込めるようにします。\n実際にはどう動くのか…\nCodeRabbitはレビューを開始する前に接続済みMCPサーバーを検索します。たとえばデータベーススキーマの変更はデータアーキテクチャ文書と照合され、APIエンドポイントの実装は社内Wikiに記録されたサービス設計パターンと突き合わせられます。\n例: CodeRabbitによるコード整合性の確認\n\nどのツールからでも重要なコンテキストを取り込む\n\n従来のコードレビューツールは特定の統合を前提としています。CodeRabbitのMCP統合は、MCPサーバーを持つあらゆるシステムで動作します。独自の社内ツール、ニッチなSaaSプラットフォーム、カスタムドキュメントシステム。MCPサーバーがあれば、CodeRabbitはどこにでも接続できます。\nCodeRabbitをMCPクライアントとして利用すると、3種類の異なるコンテキストからレビューの深みを得られます。\n技術的コンテキスト\n依存関係、パフォーマンスデータ、静的解析、テストカバレッジなど\nネイティブ統合: GitHub Actions、GitLab CI、Bitbucket Pipelines\nMCPサーバー: Datadog、New Relic、SonarQube、Snyk、Grafana\nレビューコメント例は以下の通りです。\n\nビジネスコンテキスト\n要件、ユーザーストーリー、受け入れ基準など\nネイティブ統合: Linear、Jira、GitHub Issues、GitLab Issues\nMCPサーバー: Confluence、Notion\nレビューコメント例は以下の通りです。\n\n組織的コンテキスト\n過去の意思決定、慣例、会議メモ、組織的知識など\nネイティブ統合: PR履歴、チーム慣習\nMCPサーバー: Slack、Microsoft Teams、Stack Overflow for Teams、PagerDuty\nレビューコメント例は以下の通りです。\n\nMCP統合を始めるには\nCodeRabbitのMCPクライアントは最小限の設定で導入できます。ほとんどの開発チームは10分以内に最初のMCPサーバーを接続できます。\nMCPサーバー対応の人気開発ツール:\nLinear（ネイティブMCPサポート、5分）\nNotion（MCPサーバーあり、10分）\nConfluence（コミュニティ製MCPサーバー、15分）\nFigma（MCPプラグインあり、10分）\nコード変更がどの開発システムを参照すべきかを定義します。データベース変更はアーキテクチャ文書を、認証変更はセキュリティ文書を確認する、という具合です。\nMCPサーバーを追加するのは簡単です。\nCodeRabbitダッシュボードで「integrations」に進み、必要ならMCP Serversタブに切り替えます  \nあらかじめ用意されたMCPサーバーオプションをクリックするか、「New MCP Server」ボタンから他のMCPサーバーを追加できます  \nリストにないMCPサーバーについては、必要な認証情報を入力します\nMCP情報をどのように利用するかの使用ガイダンスを確認します  \n接続が完了すると、利用可能な呼び出し一覧が表示され、カーソルを合わせると詳細を確認できます  \n各呼び出しをクリックしてアクセスを有効化/無効化することも可能です  \nあらゆるコンテキストを取り込むレビュー基盤\nCodeRabbitは50以上の統合に標準対応しています。MCPを利用すれば、カスタムサーバーや社内ツールにも拡張できます。まずはLinear、Confluence、Datadog、Slackといった既存システムから始め、必要に応じて追加していけます。\n次のステップ:\n14日間の無料トライアルを開始\nMCPサーバーディレクトリを表示\nMCPドキュメントを見る",
      "publishedAt": "2025-10-01T12:45:00.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:12.174Z",
      "score": 0.14414366568895567
    },
    {
      "id": "6f91c11c2f83634699465b2d81cf8f8a",
      "title": "GPT-5 Codex: How it solves for GPT-5's drawbacks",
      "url": "https://coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks",
      "content": "<p>CodeRabbit’s code reviews help developers fix bugs and ship code. We recently wrote about <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">benchmarking GPT-5</a> and opined that the model was a generational leap in reasoning for our use case of AI code reviews. As we rolled out to our wider user base, we observed that the signal to noise ratio (SNR) dipped, and users felt the reviews were too pedantic.</p>\n<p>The release of GPT‑5 Codex, plus the product changes we made (severity tagging, stricter refactor gating, better filtering), brings our signal to noise ratio back without sacrificing the ability to find the hard bugs.</p>\n<p>On our refreshed hard 25 PR set, GPT-5 Codex delivers about 35% higher per comment precision than GPT‑5, maintains essentially the same error pattern- level bug coverage, and cuts roughly a third of the comment volume. Combine that with the lower latency of the GPT-5 Codex model and the experience feels snappier and more focused.</p>\n<h2 id=\"heading-what-we-measured-and-why\"><strong>What we measured (and why)</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759249757952/184a4486-2f03-4293-8b39-82ca88ed9a32.png\" alt class=\"image--center mx-auto\" /></p>\n<p>When testing GPT-5 Codex, we ran a fresh “hard 25” suite of OSS PRs (slightly tougher than <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">the previous post</a>). These are 25 of the most difficult pull requests from our dataset. These PRs represent real-world bugs that span:</p>\n<ul>\n<li><p>Concurrency issues (e.g. TOCTOU races, incorrect synchronization)</p>\n</li>\n<li><p>Object-oriented design flaws (e.g. virtual call pitfalls, refcount memory model violations)</p>\n</li>\n<li><p>Performance hazards (e.g. runaway cache growth, tight loop stalls)</p>\n</li>\n<li><p>Language-specific footguns (e.g. TypeScript misuses, C++ memory order subtleties)</p>\n</li>\n</ul>\n<p>We evaluated the following models: :</p>\n<ul>\n<li><p><strong>GPT‑5 Codex</strong></p>\n</li>\n<li><p><strong>GPT‑5</strong></p>\n</li>\n<li><p><strong>Claude</strong> (Sonnet 4 and Opus‑4.1)</p>\n</li>\n</ul>\n<h2 id=\"heading-what-we-looked-for\">What we looked for</h2>\n<p>We gave each of the models a score based on how they performed on these factors:</p>\n<ul>\n<li><p><strong>EP (Error Pattern).</strong> The specific underlying defect seeded in a PR (e.g., lost wakeup on a condition variable, inconsistent lock order, logic bug hidden in boolean soup).</p>\n</li>\n<li><p><strong>EP PASS/FAIL (per PR).</strong> PASS if the model left at least one comment that directly fixes or credibly surfaces that PR’s EP. If it left no comment on that PR, it is counted as FAIL for that PR.</p>\n</li>\n<li><p><strong>Comment PASS/FAIL (per comment).</strong> PASS if the comment directly fixes or credibly surfaces the EP, otherwise FAIL.</p>\n</li>\n<li><p><strong>Per comment precision.</strong> PASS comments ÷ all comments. This is our operational SNR for this dataset.</p>\n</li>\n<li><p><strong>Important share.</strong> Every PASS is Important. Comments that do not solve the EP but still flag a genuine critical or major bug (like a use after free, double free, lost wakeup, memory leak, null deref, path traversal, catastrophic regex) are also Important. Everything else is Minor.</p>\n</li>\n</ul>\n<h2 id=\"heading-scoreboard-codex-improves-signal-to-noise\"><strong>Scoreboard - Codex improves signal-to-noise</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759197629508/de48651d-069a-4530-b070-de6107b57583.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>Takeaway:</strong> Codex finds essentially the same EPs as GPT‑5 but does it with fewer, tighter comments, so the signal-to-noise ratio is improved.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759238036950/7c99e827-daba-4205-8f70-0d5a9413b74b.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>What this means:</strong> Codex covered 20 of the 25 PRs (the other 5 count as uncovered fails). Despite fewer comments overall, Codex passed slightly more EPs (16 vs. 15) and landed far more Important comments. Over half its comments are either direct hits on the issue we were representing in that PR or flag other  EP critical bugs. GPT‑5 and Claude trailed in precision and Importance share at about 40%.</p>\n<p><strong>The verdict: Same EP coverage, less noise:</strong> Codex retains GPT-5’s bug finding power but trims the chatter with about 32% fewer comments than GPT‑5 (54 vs. 79) and about 35% higher per comment precision (46.3% vs. 34.2%). Claude looks similar to GPT‑5 on coverage but is chattier, with lower precision.</p>\n<h2 id=\"heading-style-and-structure-why-codex-reads-like-a-patch\"><strong>Style and structure (why Codex reads like a patch)</strong></h2>\n<p>Codex replies are consistently action forward (diffs almost always included) and low hedge. That lines up with what reviewers want: suggestions that translate directly into a patch.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759237978515/bf5eab59-e642-486d-bbb7-8adfd02fc00e.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-the-kinds-of-bugs-codex-is-good-at\"><strong>The kinds of bugs Codex is good at</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759197684585/6b90b664-dbf0-46d0-ae5e-cf988083ab8b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Across the suite, all models did well on concurrency and synchronization, but Codex stood out for:</p>\n<ol>\n<li><p><strong>Condition variable misuse and lost wakeups.</strong> Codex proposes the canonical patterns (wait under lock, check predicate in a loop) and supplies concrete diffs.</p>\n</li>\n<li><p><strong>Lock ordering and deadlocks.</strong> It calls out inconsistent acquisition order and suggests a lock hierarchy or moving work outside critical sections, again with actionable edits.</p>\n</li>\n<li><p><strong>Subtle API and performance traps.</strong> Examples include catastrophic regex backtracking and memory model orderings. Codex pinpoints and patches them cleanly.</p>\n</li>\n</ol>\n<h2 id=\"heading-why-gpt5-felt-noisier-and-how-we-fixed-that\"><strong>Why GPT‑5 felt noisier, and how we fixed that</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759251737131/d79ed373-5213-4cd0-a168-64441227b7b9.png\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>What we saw:</strong> When we moved from Sonnet and Opus to GPT‑5 our total comments per review nearly doubled. Even though hallucinations fell to under 1% and negative tone fell to under 1%, the acceptance rate (share of comments judged helpful) declined significantly relative to its baseline prior to the adoption of GPT-5.</p>\n<p><strong>What changed with Codex:</strong> With GPT‑5 Codex plus some product changes we’ve implemented, our acceptance climbed back to prior levels while overall comment volume stayed higher than the pre-GPT‑5 era. Put simply: our tool is now back to its prior helpfulness level, while still finding as many real issues as GPT-5.</p>\n<p>Two product levers helped with this:</p>\n<ol>\n<li><p><strong>We created severity and review type tags, front and center</strong></p>\n<ul>\n<li><p><strong>Review Types:</strong> We created review types to allow users to self-select what kinds of comments they wanted to read including: ⚠️ Potential issue, 🛠️ Refactor suggestion, 🧹 Nitpick (nitpicks are hidden unless you opt into <em>Assertive</em> mode)</p>\n</li>\n<li><p><strong>Severity:</strong> We now tag comments by severity to better signal which matter more than others. Our tags are: 🔴 Critical, 🟠 Major, 🟡 Minor, 🔵 Trivial, ⚪ Info</p>\n</li>\n<li><p>We always show bugs (Critical, Major, Minor) but don’t always show other types of comments. Refactors show only if the model marks them as essential. Users who want everything can still switch to <em>Assertive</em> mode.</p>\n</li>\n</ul>\n</li>\n<li><p><strong>We implemented stricter filtering and aggregation</strong></p>\n<ul>\n<li>We collapse duplicative notes and filter out “nice to have” suggestions unless they have clear ROI for the user. The result: fewer, denser comments, and fewer reasons to tune out.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"heading-latency-fast-matters-amp-codex-is-faster\"><strong>Latency: Fast matters &amp; Codex is faster</strong></h2>\n<p>A five minute review is fine. Thirty minutes is not. GPT‑5’s “always think hard” style significantly increased time to first token and overall review time. But we shipped several pipeline optimizations recently and Codex helps further reduce the latency that GPT-5 introduced.</p>\n<p>Codex’s variable or elastic thinking uses less depth when it is not needed, improving time to first output and end-to-end review time in practice. Net: faster reviews, earlier feedback, better flow for the human in the loop.</p>\n<h2 id=\"heading-what-a-coderabbit-user-should-expect\"><strong>What a CodeRabbit user should expect</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1759197710045/2f32eef6-3227-450f-ac4b-485f89126197.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Now that Codex is implemented, how will that change your AI code reviews?</p>\n<ol>\n<li><p><strong>The same raw bug finding power</strong></p>\n<ul>\n<li>On the refreshed hard 25, Codex passed 64% at the EP level vs. 60% for GPT‑5 (our <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/benchmarking-gpt-5-why-its-a-generational-leap-in-reasoning\">previous set of PRs</a> had GPT-5 passing 77.3%). No loss of the important wins GPT-5 helped with.</li>\n</ul>\n</li>\n<li><p><strong>Fewer but stronger comments</strong></p>\n<ul>\n<li>About 32% fewer total comments than GPT‑5, with about 35% higher SNR (per comment precision). More patches, less prose.</li>\n</ul>\n</li>\n<li><p><strong>Severity tags to focus your review</strong></p>\n<ul>\n<li>Critical and Major issues float to the top with our new severity tags. Refactors are gated. Nitpicks are opt-in. You will spend less time scanning comments and more time fixing.</li>\n</ul>\n</li>\n<li><p><strong>A faster feedback loop</strong></p>\n<ul>\n<li>Codex’s leaner reasoning plus pipeline improvements bring time to first helpful comment  down. You will feel it.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"heading-quantitative-appendix-for-the-curious\"><strong>Quantitative appendix (for the curious)</strong></h2>\n<p>We know you love data! Here’s some other stats we found interesting:</p>\n<ul>\n<li><p><strong>Per comment precision (SNR) uplift:</strong> Codex 46.3% vs. GPT‑5 34.2% — about +35% relative.</p>\n</li>\n<li><p><strong>Comment volume delta:</strong> Codex 54 vs. GPT‑5 79 — 32% fewer comments, with EP passes essentially unchanged (16 vs. 15).</p>\n</li>\n<li><p><strong>Style:</strong> Codex includes diffs in 94% of comments and uses hedging far less than Claude and GPT‑5 on this set.</p>\n</li>\n<li><p><strong>Acceptance (real world):</strong> During GPT‑5 rollout, acceptance dropped significantly. With Codex plus changes, it rose by about 20–25% relative and returned to prior levels while still delivering more accepted comments than pre GPT‑5.</p>\n</li>\n</ul>\n<h2 id=\"heading-where-codex-still-needs-work-and-what-we-are-doing\"><strong>Where Codex still needs work (and what we are doing)</strong></h2>\n<p>These improvements are great but that doesn’t mean that there aren’t still issues with Codex. Here are some that we are actively working on:</p>\n<ul>\n<li><p><strong>Coverage gaps.</strong> When a model leaves no comment on a PR, that is a hard fail for that EP. We are widening Codex’s search heuristics so it is less likely to miss entire classes of issues.</p>\n</li>\n<li><p><strong>Refactor over eagerness (tuned, not solved).</strong> The “essential only” gate curbs refactor noise, but we will keep tightening the threshold, especially on large diffs where a high number of comments would be overwhelming.</p>\n</li>\n<li><p><strong>User driven prioritization.</strong> We cannot change GitHub’s in-line ordering, but we annotate every comment with severity so you can triage from the top down without hunting.</p>\n</li>\n</ul>\n<h2 id=\"heading-codex-gpt-5-all-of-the-great-bug-catching-ability-fewer-downsides\"><strong>Codex GPT-5: All of the great bug catching ability, fewer downsides</strong></h2>\n<p>Our north star is simple: <strong>catch the bugs that matter, quickly, without making you sift through noise</strong>. Codex helps us do that. It keeps the bite of GPT‑5’s reasoning while restoring SNR and shaving latency down significantly. We will keep measuring, improving, and shipping a better product every release.</p>\n",
      "summary": "CodeRabbit’s code reviews help developers fix bugs and ship code. We recently wrote about benchmarking GPT-5 and opined that the model was a generational leap in reasoning for our use case of AI code reviews. As we rolled out to our wider user base, we observed that the signal to noise ratio (SNR) dipped, and users felt the reviews were too pedantic.\nThe release of GPT‑5 Codex, plus the product changes we made (severity tagging, stricter refactor gating, better filtering), brings our signal to noise ratio back without sacrificing the ability to find the hard bugs.\nOn our refreshed hard 25 PR set, GPT-5 Codex delivers about 35% higher per comment precision than GPT‑5, maintains essentially the same error pattern- level bug coverage, and cuts roughly a third of the comment volume. Combine that with the lower latency of the GPT-5 Codex model and the experience feels snappier and more focused.\nWhat we measured (and why)\n\nWhen testing GPT-5 Codex, we ran a fresh “hard 25” suite of OSS PRs (slightly tougher than the previous post). These are 25 of the most difficult pull requests from our dataset. These PRs represent real-world bugs that span:\nConcurrency issues (e.g. TOCTOU races, incorrect synchronization)\nObject-oriented design flaws (e.g. virtual call pitfalls, refcount memory model violations)\nPerformance hazards (e.g. runaway cache growth, tight loop stalls)\nLanguage-specific footguns (e.g. TypeScript misuses, C++ memory order subtleties)\nWe evaluated the following models: :\nGPT‑5 Codex\nGPT‑5\nClaude (Sonnet 4 and Opus‑4.1)\nWhat we looked for\nWe gave each of the models a score based on how they performed on these factors:\nEP (Error Pattern). The specific underlying defect seeded in a PR (e.g., lost wakeup on a condition variable, inconsistent lock order, logic bug hidden in boolean soup).\nEP PASS/FAIL (per PR). PASS if the model left at least one comment that directly fixes or credibly surfaces that PR’s EP. If it left no comment on that PR, it is counted as FAIL for that PR.\nComment PASS/FAIL (per comment). PASS if the comment directly fixes or credibly surfaces the EP, otherwise FAIL.\nPer comment precision. PASS comments ÷ all comments. This is our operational SNR for this dataset.\nImportant share. Every PASS is Important. Comments that do not solve the EP but still flag a genuine critical or major bug (like a use after free, double free, lost wakeup, memory leak, null deref, path traversal, catastrophic regex) are also Important. Everything else is Minor.\nScoreboard - Codex improves signal-to-noise\n\nTakeaway: Codex finds essentially the same EPs as GPT‑5 but does it with fewer, tighter comments, so the signal-to-noise ratio is improved.\n\nWhat this means: Codex covered 20 of the 25 PRs (the other 5 count as uncovered fails). Despite fewer comments overall, Codex passed slightly more EPs (16 vs. 15) and landed far more Important comments. Over half its comments are either direct hits on the issue we were representing in that PR or flag other  EP critical bugs. GPT‑5 and Claude trailed in precision and Importance share at about 40%.\nThe verdict: Same EP coverage, less noise: Codex retains GPT-5’s bug finding power but trims the chatter with about 32% fewer comments than GPT‑5 (54 vs. 79) and about 35% higher per comment precision (46.3% vs. 34.2%). Claude looks similar to GPT‑5 on coverage but is chattier, with lower precision.\nStyle and structure (why Codex reads like a patch)\nCodex replies are consistently action forward (diffs almost always included) and low hedge. That lines up with what reviewers want: suggestions that translate directly into a patch.\n\nThe kinds of bugs Codex is good at\n\nAcross the suite, all models did well on concurrency and synchronization, but Codex stood out for:\nCondition variable misuse and lost wakeups. Codex proposes the canonical patterns (wait under lock, check predicate in a loop) and supplies concrete diffs.\nLock ordering and deadlocks. It calls out inconsistent acquisition order and suggests a lock hierarchy or moving work outside critical sections, again with actionable edits.\nSubtle API and performance traps. Examples include catastrophic regex backtracking and memory model orderings. Codex pinpoints and patches them cleanly.\nWhy GPT‑5 felt noisier, and how we fixed that\n\nWhat we saw: When we moved from Sonnet and Opus to GPT‑5 our total comments per review nearly doubled. Even though hallucinations fell to under 1% and negative tone fell to under 1%, the acceptance rate (share of comments judged helpful) declined significantly relative to its baseline prior to the adoption of GPT-5.\nWhat changed with Codex: With GPT‑5 Codex plus some product changes we’ve implemented, our acceptance climbed back to prior levels while overall comment volume stayed higher than the pre-GPT‑5 era. Put simply: our tool is now back to its prior helpfulness level, while still finding as many real issues as GPT-5.\nTwo product levers helped with this:\nWe created severity and review type tags, front and center\nReview Types: We created review types to allow users to self-select what kinds of comments they wanted to read including: ⚠️ Potential issue, 🛠️ Refactor suggestion, 🧹 Nitpick (nitpicks are hidden unless you opt into Assertive mode)\nSeverity: We now tag comments by severity to better signal which matter more than others. Our tags are: 🔴 Critical, 🟠 Major, 🟡 Minor, 🔵 Trivial, ⚪ Info\nWe always show bugs (Critical, Major, Minor) but don’t always show other types of comments. Refactors show only if the model marks them as essential. Users who want everything can still switch to Assertive mode.\nWe implemented stricter filtering and aggregation\nWe collapse duplicative notes and filter out “nice to have” suggestions unless they have clear ROI for the user. The result: fewer, denser comments, and fewer reasons to tune out.\nLatency: Fast matters & Codex is faster\nA five minute review is fine. Thirty minutes is not. GPT‑5’s “always think hard” style significantly increased time to first token and overall review time. But we shipped several pipeline optimizations recently and Codex helps further reduce the latency that GPT-5 introduced.\nCodex’s variable or elastic thinking uses less depth when it is not needed, improving time to first output and end-to-end review time in practice. Net: faster reviews, earlier feedback, better flow for the human in the loop.\nWhat a CodeRabbit user should expect\n\nNow that Codex is implemented, how will that change your AI code reviews?\nThe same raw bug finding power\nOn the refreshed hard 25, Codex passed 64% at the EP level vs. 60% for GPT‑5 (our previous set of PRs had GPT-5 passing 77.3%). No loss of the important wins GPT-5 helped with.\nFewer but stronger comments\nAbout 32% fewer total comments than GPT‑5, with about 35% higher SNR (per comment precision). More patches, less prose.\nSeverity tags to focus your review\nCritical and Major issues float to the top with our new severity tags. Refactors are gated. Nitpicks are opt-in. You will spend less time scanning comments and more time fixing.\nA faster feedback loop\nCodex’s leaner reasoning plus pipeline improvements bring time to first helpful comment  down. You will feel it.\nQuantitative appendix (for the curious)\nWe know you love data! Here’s some other stats we found interesting:\nPer comment precision (SNR) uplift: Codex 46.3% vs. GPT‑5 34.2% — about +35% relative.\nComment volume delta: Codex 54 vs. GPT‑5 79 — 32% fewer comments, with EP passes essentially unchanged (16 vs. 15).\nStyle: Codex includes diffs in 94% of comments and uses hedging far less than Claude and GPT‑5 on this set.\nAcceptance (real world): During GPT‑5 rollout, acceptance dropped significantly. With Codex plus changes, it rose by about 20–25% relative and returned to prior levels while still delivering more accepted comments than pre GPT‑5.\nWhere Codex still needs work (and what we are doing)\nThese improvements are great but that doesn’t mean that there aren’t still issues with Codex. Here are some that we are actively working on:\nCoverage gaps. When a model leaves no comment on a PR, that is a hard fail for that EP. We are widening Codex’s search heuristics so it is less likely to miss entire classes of issues.\nRefactor over eagerness (tuned, not solved). The “essential only” gate curbs refactor noise, but we will keep tightening the threshold, especially on large diffs where a high number of comments would be overwhelming.\nUser driven prioritization. We cannot change GitHub’s in-line ordering, but we annotate every comment with severity so you can triage from the top down without hunting.\nCodex GPT-5: All of the great bug catching ability, fewer downsides\nOur north star is simple: catch the bugs that matter, quickly, without making you sift through noise. Codex helps us do that. It keeps the bite of GPT‑5’s reasoning while restoring SNR and shaving latency down significantly. We will keep measuring, improving, and shipping a better product every release.",
      "publishedAt": "2025-09-30T13:27:04.000Z",
      "author": "David Loker",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "funding_mna",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "testing"
      ],
      "ingestedAt": "2025-12-04T14:17:12.175Z",
      "score": 0.12488091165438353
    },
    {
      "id": "1fee642c14d1a3ef024133891cae6f3b",
      "title": "We raised $60 million last week… so we made a funny video",
      "url": "https://coderabbit.ai/blog/we-raised-60-million-last-week-so-we-made-a-funny-video",
      "content": "<p>Last week, we announced CodeRabbit’s <strong>$60 million Series B</strong>. To celebrate, we did what any responsible, developer-focused software company would do: we made a funny video.</p>\n<p>Not with <em>all</em> the money, to be clear. But we did decide to celebrate with something fun, absurd, and painfully relatable for any dev team trying to keep up with the flood of AI-generated PRs.</p>\n<h2 id=\"heading-introducing-when-ai-coding-agents-backfire-a-short-film\"><strong>Introducing… “When AI coding agents backfire: A short film”</strong></h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/glfB3KLQR7E?feature=shared\">https://youtu.be/glfB3KLQR7E?feature=shared</a></div>\n<p> </p>\n<p>It’s a short mockumentary-meets-sitcom about what really happens when “AI velocity” turns into a PR review backlog.</p>\n<ul>\n<li><p>One reviewer.</p>\n</li>\n<li><p>Dozens of notifications.</p>\n</li>\n<li><p>84 open PRs.</p>\n</li>\n<li><p>And one overly eager coworker named Brad who just wants feedback.</p>\n</li>\n</ul>\n<h2 id=\"heading-the-cast\"><strong>The cast</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758059242156/ae3ad14c-f354-4802-95e1-1f7eb67f0c85.png\" alt class=\"image--center mx-auto\" /></p>\n<p>To bring it to life, we pulled in beloved developer educator (and influencer) <a target=\"_blank\" href=\"https://x.com/@aarondfrancis\"><strong>Aaron Francis</strong></a> to star as our beleaguered reviewer. He’s the guy who just wanted to ship features faster and now can’t go to the kitchen (or even leave his house at 8 a.m.) without Brad asking about his PR.</p>\n<p>And speaking of Brad: the inimitable <a target=\"_blank\" href=\"https://www.instagram.com/4ustinvon/?hl=en\"><strong>Austin von Johnson</strong></a> plays him to perfection. Brad’s a developer who can crank out AI-generated PRs at lightning speed but cannot, under any circumstances, <em>wait patiently</em> for a review. His lurking, his post-its, his hoodie PR ambushes… let’s just say he was perfectly committed to the bit.</p>\n<h2 id=\"heading-the-very-real-problem-behind-the-joke\"><strong>The very real problem behind the joke</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758059125767/fc80309a-a591-4b4d-bd9b-6a1b53330edb.png\" alt class=\"image--center mx-auto\" /></p>\n<p>The short film is funny, but the problem it highlights is real:</p>\n<ul>\n<li><p><strong>AI coding tools crank out code faster than teams can review it</strong></p>\n</li>\n<li><p><strong>Review backlogs balloon</strong> while productivity drops</p>\n</li>\n<li><p><strong>Senior engineers get buried</strong> in endless PRs</p>\n</li>\n<li><p>Review quality is uneven, risk goes up, and you have to deal with more issues</p>\n</li>\n<li><p>And suddenly, the promise of velocity feels more like a nightmare.</p>\n</li>\n</ul>\n<h2 id=\"heading-heres-what-were-doing-about-it\"><strong>Here’s what we’re doing about it</strong></h2>\n<p>CodeRabbit exists to <strong>clear the backlog</strong>, not add to it. Our AI code reviews pull in dozens of points of context (requirements, tests, CI, past diffs, ownership) to catch bugs you’d miss, reduce reviewer fatigue, and move PRs through faster—without turning teammates into… Brad. </p>\n<p>Ship faster, review smarter, and keep your sanity. Also, avoid creating a… Brad. </p>\n<p>👉 <a target=\"_blank\" href=\"https://youtu.be/glfB3KLQR7E?feature=shared\">Watch <strong>“When AI coding agents backfire: a short film”</strong> right here.</a> And if you’ve ever been chased around the office about a PR, please, send it to your team’s Brad.</p>\n",
      "summary": "Last week, we announced CodeRabbit’s $60 million Series B. To celebrate, we did what any responsible, developer-focused software company would do: we made a funny video.\nNot with all the money, to be clear. But we did decide to celebrate with something fun, absurd, and painfully relatable for any dev team trying to keep up with the flood of AI-generated PRs.\nIntroducing… “When AI coding agents backfire: A short film”\n\n\n\nhttps://youtu.be/glfB3KLQR7E?feature=shared\n \nIt’s a short mockumentary-meets-sitcom about what really happens when “AI velocity” turns into a PR review backlog.\nOne reviewer.\nDozens of notifications.\n84 open PRs.\nAnd one overly eager coworker named Brad who just wants feedback.\nThe cast\n\nTo bring it to life, we pulled in beloved developer educator (and influencer) Aaron Francis to star as our beleaguered reviewer. He’s the guy who just wanted to ship features faster and now can’t go to the kitchen (or even leave his house at 8 a.m.) without Brad asking about his PR.\nAnd speaking of Brad: the inimitable Austin von Johnson plays him to perfection. Brad’s a developer who can crank out AI-generated PRs at lightning speed but cannot, under any circumstances, wait patiently for a review. His lurking, his post-its, his hoodie PR ambushes… let’s just say he was perfectly committed to the bit.\nThe very real problem behind the joke\n\nThe short film is funny, but the problem it highlights is real:\nAI coding tools crank out code faster than teams can review it\nReview backlogs balloon while productivity drops\nSenior engineers get buried in endless PRs\nReview quality is uneven, risk goes up, and you have to deal with more issues\nAnd suddenly, the promise of velocity feels more like a nightmare.\nHere’s what we’re doing about it\nCodeRabbit exists to clear the backlog, not add to it. Our AI code reviews pull in dozens of points of context (requirements, tests, CI, past diffs, ownership) to catch bugs you’d miss, reduce reviewer fatigue, and move PRs through faster—without turning teammates into… Brad. \nShip faster, review smarter, and keep your sanity. Also, avoid creating a… Brad. \n👉 Watch “When AI coding agents backfire: a short film” right here. And if you’ve ever been chased around the office about a PR, please, send it to your team’s Brad.",
      "publishedAt": "2025-09-23T15:41:30.000Z",
      "author": "Aravind Putrevu",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:12.175Z",
      "score": 0.09384723130398523
    },
    {
      "id": "d8d6518d6b0d06b7830060371468c60a",
      "title": "Mcp時代におけるコンテキスト肥大化への対応",
      "url": "https://coderabbit.ai/blog/handling-ballooning-context-in-the-mcp-era-context-engineering-on-steroids-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/handling-ballooning-context-in-the-mcp-era-context-engineering-on-steroids\">Ballooning context in the MCP era: Context engineering on steroids</a>意訳です。</p>\n<p>かつて、LLMにコンテキストを渡すには、ハックやベクターストラテジー、そしてお祈り…と、過剰に複雑なRAGパイプラインをつなぎ合わせる必要がありました。そこに登場したのが <strong>Model Context Protocol (MCP)</strong>。外部データを本番環境のモデルに提供するための、クリーンでモジュール的手法です。MCPは、実際に「何かをする」エージェントシステムを構築する人々にとって、瞬く間に標準的なプロトコルとなりました。</p>\n<p>今ではほとんどのテック企業がMCP機能を打ち出していますが、その理由は明白です。MCPはコンテキストのロジックとアプリケーションロジックを分離し、信頼性を向上させ、複雑なワークフローでのプロンプト構築の混乱を抑える役割を果たします。</p>\n<p>私たちはしばらく前からコンテキストエンジニアリングの領域に深く取り組んでおり、今回独自のMCPクライアントを立ち上げるにあたり、コードレビューにより豊かなコンテキストを注入できることに大いに期待しています。しかし正直に言えば、豊富なコンテキストにはリスクも伴います。MCP時代の隠された真実はこうです：<strong>かつて欲していたコンテキストに、今や私たちは溺れそうである</strong>。ログやトレース、diffなど \"関連\"ファイルが増え、モデルが本当に必要としているものが見えにくくなっています。</p>\n<p>役立つ入力はすぐにトークンの膨張、ノイズ、パフォーマンス劣化につながります。引用付きハルシネーション、レイテンシの急上昇、あるいはカフェインを摂りすぎたインターンが書いたような散漫なレビュー。良いコンテキストエンジニアリングとは「すべて詰め込む」ことではなく、「何を省くか」を知ることでもあります。そしてMCP以降、そのバランスを取るのはより難しく、より重要になっています。</p>\n<p>この記事では、<strong>膨張するコンテキスト問題</strong> の詳細、その副作用、そして私たちがそれにどう立ち向かっているかを解説します。MCPを用いたLLM機能を開発している方で、プロンプト形のブラックホールを作り出したくない方に役立つ内容です。</p>\n<h3 id=\"heading-mcp-amp\"><strong>MCPクライアント &amp; サーバーにおける「膨張するコンテキスト問題」</strong></h3>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123932448/7988a3c4-f1be-4033-b8e2-43aee929d878.png\" alt class=\"image--center mx-auto\" /></p>\n<p>MCPサーバーとクライアントは、モデルに膨大な情報を渡すことを容易にします：ログ、トレース、diff、設定、チケット、さらには誰も所有を覚えていないリポジトリの隅まで。すべてがモデルの手の届くところにあります。しかし、ここで重要な問いがあります：「コンテキストが多ければ多いほど良いのか？」 答えは間違いなく「NO」です。</p>\n<p>過剰なコンテキストは、試験勉強で図書館全体を読むようなもの。ノイズは増えても、知識にはなりません。コンテキストが制御されなければ、次の3つの問題がすぐに現れます：</p>\n<ul>\n<li><p><strong>トークンの膨張</strong><br />  LLMには無限のキャパシティはありません。入力ウィンドウにはコストと限界があり、念のため…と詳細情報を詰め込みすぎれば、コストは増大してスループットは低下し、不要なテキストに予算を浪費します。</p>\n</li>\n<li><p><strong>関連性の低下</strong><br />  情報が多いほど出力が良くなるわけではありません。むしろ悪化することが多いのです。無関係または冗長なスニペットがシグナルを希釈し、モデルはインサイトではなく枝葉に追われます。</p>\n</li>\n<li><p><strong>レイテンシ</strong><br />  追加されるログやdiff、スタックトレースはすべて取得・処理され、プロンプトに押し込まれます。コンテキスト構築がボトルネックとなり、レビュー速度を著しく低下させます。</p>\n</li>\n</ul>\n<p>要するに、膨張するコンテキストはMCPの優雅さを逆にリスクへと変えてしまいます。意図的なコンテキストエンジニアリングがなければ、出力を磨くどころか押しつぶしてしまうのです。</p>\n<h2 id=\"heading-44kz44oz44og44kt44k544oi44gm5a6z44gr44gq44kl44go44gn\">コンテキストが害になるとき</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123946588/f8c48c9f-db75-47bc-a349-bdb58b81258b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>実際には、以下の3つの典型的な問題が発生します：</p>\n<ul>\n<li><p><strong>コンテキスト混乱</strong><br />  モデルが無関係な詳細をシグナルと誤解してしまうケース。例えば認証ロジックを更新するPRに、無関係なテストフィクスチャが含まれていると、モデルはフィクスチャをレビューし始め、実際の変更と関係のないコメントを生成します。</p>\n</li>\n<li><p><strong>コンテキスト衝突</strong><br />  コンテキスト同士が矛盾する場合です。例えば最新のスキーママイグレーションと古いドックストリングが同時に含まれると、モデルはどちらを信じるべきか迷い、結果として全方位的で自信のないレビューを生成します。まるで決断できないレビュアーのように。</p>\n</li>\n<li><p><strong>コンテキスト汚染</strong><br />  最も厄介なのは誤った情報が混入するケースです。無用な関連ファイルや、誤ってインデックス化されたスニペットが注入されると、存在しないコードを引用するようになります。レビューでは、存在しないファイルのバグに言及し、開発者を混乱させ、時間を無駄にし、信頼を損ないます。</p>\n</li>\n</ul>\n<p>これはコードレビューに限りません。サポートボットが無関係なチケットを引っ張ってくる場合や、リサーチアシスタントが周辺論文に気を取られる場合、セキュリティエージェントがノイジーなログを証拠として扱う場合なども同様です。いずれにしても、間違ったコンテキストは「ない方がまし」なのです。</p>\n<h2 id=\"heading-mcp\">MCPサーバーでのコンテキスト過負荷を防ぐ主要パターン</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123970317/3dd0ac82-f438-4150-83d8-fae81f7dafa2.png\" alt class=\"image--center mx-auto\" /></p>\n<p>MCP時代の問題が、膨張するコンテキストだとすれば、解決策は情報の流入を止めることではありません。<strong>意図を持って選別・圧縮・提供すること</strong>です。MCPのコンテキストは、生の素材をモデルに渡す前にきちんと設計されたデータ変換プロセスを経るべきものです。私たち自身のコードレビュー用MCPクライアントでも、コンテキストを高シグナル・低ノイズに保つために以下のパターンを採用しています。</p>\n<ul>\n<li><p><strong>コンテキストの重複排除と差分化</strong><br />  冗長な入力はトークン浪費の最短ルートです。同一のスタックトレース、繰り返しのログ、変更されていないdiff部分は10回も登場する必要はありません。クライアントは重複を検出して折りたたみ、新しい部分だけを強調します。この原則は他の領域にも適用可能です：重複するサポートチケットをまとめ、繰り返しのトレースを圧縮し、差分のみを残します。</p>\n</li>\n<li><p><strong>コンテキスト要約パイプライン</strong><br />  MCP出力が依然として大きすぎる場合、LLM自体が要約して小さくすることも可能です。代償は圧縮と忠実度のトレードオフ：要約はニュアンスを失う可能性がありますが、詳細に溺れるよりはましです。実際には、重要ファイルは生のdiff、優先度の低いコンテキストは要約といったハイブリッド設計を採用します。</p>\n</li>\n<li><p><strong>コンテキストの優先順位付けと切り捨て</strong><br />  プルーニングや要約後でも、どれを最初に入れ、後に回し、容量不足時に捨てるかを決める必要があります。MCPクエリごとにトークン予算を設定することは不可欠です。そうしなければプロンプトが予測不能に膨張します。私たちは切り捨てを前提にした設計を試し、場合によっては概要を先頭に、詳細を後半に配置するなど調整しています。</p>\n</li>\n<li><p><strong>コンテキストの隔離</strong><br />  すべてのコンテキストを最初のプロンプトに含める必要はありません。サブタスクごとに専用のコンテキストスレッドを持たせるべきです。例えば、私たちのMCPクライアントではテスト失敗は専用のレビューサブスレッドに置かれ、メインのレビューコンテキストを妨げません。これにより混乱を減らし、長い対話でも明瞭さを保てます。</p>\n</li>\n<li><p><strong>継続的な改善と学習</strong><br />  コンテキストエンジニアリングは静的ではありません。モデルのフィードバックや人間による修正を取り入れ、優先順位を調整していきます。重要なのは可観測性です。モジュールごとにプロンプト入力を記録し、何が通って何が無駄かを把握します。MCPダッシュボードやトークンヒートマップのようなツールが、予算超過や不要な入力を可視化します。</p>\n</li>\n</ul>\n<h2 id=\"heading-mcp-amp-1\"><strong>MCPサーバー &amp; クライアントにおけるアンチパターン</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123985534/b1a3acb1-5c02-4d88-b3d0-883aeb0dd573.png\" alt class=\"image--center mx-auto\" /></p>\n<p>MCP時代はコンテキスト取得を容易にしました。おそらく「容易すぎる」ほどに。以下のようなアンチパターンがよく見られます：</p>\n<ul>\n<li><p><strong>ベクトル無差別投入</strong><br />  ベクトルDBは「関連」情報を見つけるのに優れていますが、それを万能の答えと見なすのは危険です。曖昧に関連するスニペットをすべて投入すると、関係のないファイルへのコメントや古いコードへの指摘で溢れるレビューになります。コンテキストの不適合はトークンの無駄だけでなく、モデルのパフォーマンスを引き下げる要因になります。</p>\n</li>\n<li><p><strong>「全部突っ込め」方式</strong><br />  すべてのログ、diff、ドックストリングをコンテキストに放り込み、あとは神に祈るやり方です。コストの増加、レイテンシの悪化、結果の予測不能を保証します。モデルは重要な部分と不要な部分を区別できないため、全方位的で散漫なレビューを生成します。矛盾が混入すれば、モデルは曖昧さを埋めるために幻覚を引き起こします。</p>\n</li>\n</ul>\n<p>要するに、コンテキストは多ければ良いというものではありません。フィルタリング、優先順位付け、設計がなければ、「情報全部」はすぐにノイズに変わり、システムを遅く、鈍く、高コストにしてしまいます。</p>\n<h2 id=\"heading-mcp-1\">私たちのMCPクライアントでのアプローチ</h2>\n<p>MCP時代において、コンテキストは「王様」です。しかし正直なところ、その王様は酔いすぎて上下も分からなくなっていることがあります。課題はもはや「コンテキストを得ること」ではなく、「それを制御すること」です。優れたコンテキストエンジニアリングには、緻密な変換パイプライン、徹底的な優先順位付け、そして改善を続ける謙虚さが必要です。これを怠れば、トークン膨張、レイテンシ、混乱したレビューを招きます。うまく実践すれば、ワークフローに沿った鋭い出力が得られます。</p>\n<p>私たちは自社のコードレビュー用MCPクライアントでこれを実感しました。初期段階では全ログ・全ファイルをそのまま渡していました。その結果は高コストで、役に立たないほど散漫なレビューです。そこで重複排除、要約、タスク専用の隔離を導入したところ、レビュー品質が飛躍的に向上しました。すべてを指摘するのではなく、本当のクロスファイルリスクに集中するようになり、トークン消費とレイテンシも低下しました。</p>\n<p>これこそが良いコンテキストエンジニアリングの成果です：情報量が多いのに散漫ではなく、本質を突いたレビュー。そしてそれこそが、私たちのMCPクライアントで実現しようとしていることです。</p>\n<p><strong><em>👉 コンテキスト設計の正しい姿を体験してみませんか？ 今すぐ</em></strong> <a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong><em>14日間の無料トライアル</em></strong></a> <strong><em>でAIコードレビューをお試しください。</em></strong></p>\n",
      "summary": "Ballooning context in the MCP era: Context engineering on steroids意訳です。\nかつて、LLMにコンテキストを渡すには、ハックやベクターストラテジー、そしてお祈り…と、過剰に複雑なRAGパイプラインをつなぎ合わせる必要がありました。そこに登場したのが Model Context Protocol (MCP)。外部データを本番環境のモデルに提供するための、クリーンでモジュール的手法です。MCPは、実際に「何かをする」エージェントシステムを構築する人々にとって、瞬く間に標準的なプロトコルとなりました。\n今ではほとんどのテック企業がMCP機能を打ち出していますが、その理由は明白です。MCPはコンテキストのロジックとアプリケーションロジックを分離し、信頼性を向上させ、複雑なワークフローでのプロンプト構築の混乱を抑える役割を果たします。\n私たちはしばらく前からコンテキストエンジニアリングの領域に深く取り組んでおり、今回独自のMCPクライアントを立ち上げるにあたり、コードレビューにより豊かなコンテキストを注入できることに大いに期待しています。しかし正直に言えば、豊富なコンテキストにはリスクも伴います。MCP時代の隠された真実はこうです：かつて欲していたコンテキストに、今や私たちは溺れそうである。ログやトレース、diffなど \"関連\"ファイルが増え、モデルが本当に必要としているものが見えにくくなっています。\n役立つ入力はすぐにトークンの膨張、ノイズ、パフォーマンス劣化につながります。引用付きハルシネーション、レイテンシの急上昇、あるいはカフェインを摂りすぎたインターンが書いたような散漫なレビュー。良いコンテキストエンジニアリングとは「すべて詰め込む」ことではなく、「何を省くか」を知ることでもあります。そしてMCP以降、そのバランスを取るのはより難しく、より重要になっています。\nこの記事では、膨張するコンテキスト問題 の詳細、その副作用、そして私たちがそれにどう立ち向かっているかを解説します。MCPを用いたLLM機能を開発している方で、プロンプト形のブラックホールを作り出したくない方に役立つ内容です。\nMCPクライアント & サーバーにおける「膨張するコンテキスト問題」\n\nMCPサーバーとクライアントは、モデルに膨大な情報を渡すことを容易にします：ログ、トレース、diff、設定、チケット、さらには誰も所有を覚えていないリポジトリの隅まで。すべてがモデルの手の届くところにあります。しかし、ここで重要な問いがあります：「コンテキストが多ければ多いほど良いのか？」 答えは間違いなく「NO」です。\n過剰なコンテキストは、試験勉強で図書館全体を読むようなもの。ノイズは増えても、知識にはなりません。コンテキストが制御されなければ、次の3つの問題がすぐに現れます：\nトークンの膨張\n  LLMには無限のキャパシティはありません。入力ウィンドウにはコストと限界があり、念のため…と詳細情報を詰め込みすぎれば、コストは増大してスループットは低下し、不要なテキストに予算を浪費します。\n関連性の低下\n  情報が多いほど出力が良くなるわけではありません。むしろ悪化することが多いのです。無関係または冗長なスニペットがシグナルを希釈し、モデルはインサイトではなく枝葉に追われます。\nレイテンシ\n  追加されるログやdiff、スタックトレースはすべて取得・処理され、プロンプトに押し込まれます。コンテキスト構築がボトルネックとなり、レビュー速度を著しく低下させます。\n要するに、膨張するコンテキストはMCPの優雅さを逆にリスクへと変えてしまいます。意図的なコンテキストエンジニアリングがなければ、出力を磨くどころか押しつぶしてしまうのです。\nコンテキストが害になるとき\n\n実際には、以下の3つの典型的な問題が発生します：\nコンテキスト混乱\n  モデルが無関係な詳細をシグナルと誤解してしまうケース。例えば認証ロジックを更新するPRに、無関係なテストフィクスチャが含まれていると、モデルはフィクスチャをレビューし始め、実際の変更と関係のないコメントを生成します。\nコンテキスト衝突\n  コンテキスト同士が矛盾する場合です。例えば最新のスキーママイグレーションと古いドックストリングが同時に含まれると、モデルはどちらを信じるべきか迷い、結果として全方位的で自信のないレビューを生成します。まるで決断できないレビュアーのように。\nコンテキスト汚染\n  最も厄介なのは誤った情報が混入するケースです。無用な関連ファイルや、誤ってインデックス化されたスニペットが注入されると、存在しないコードを引用するようになります。レビューでは、存在しないファイルのバグに言及し、開発者を混乱させ、時間を無駄にし、信頼を損ないます。\nこれはコードレビューに限りません。サポートボットが無関係なチケットを引っ張ってくる場合や、リサーチアシスタントが周辺論文に気を取られる場合、セキュリティエージェントがノイジーなログを証拠として扱う場合なども同様です。いずれにしても、間違ったコンテキストは「ない方がまし」なのです。\nMCPサーバーでのコンテキスト過負荷を防ぐ主要パターン\n\nMCP時代の問題が、膨張するコンテキストだとすれば、解決策は情報の流入を止めることではありません。意図を持って選別・圧縮・提供することです。MCPのコンテキストは、生の素材をモデルに渡す前にきちんと設計されたデータ変換プロセスを経るべきものです。私たち自身のコードレビュー用MCPクライアントでも、コンテキストを高シグナル・低ノイズに保つために以下のパターンを採用しています。\nコンテキストの重複排除と差分化\n  冗長な入力はトークン浪費の最短ルートです。同一のスタックトレース、繰り返しのログ、変更されていないdiff部分は10回も登場する必要はありません。クライアントは重複を検出して折りたたみ、新しい部分だけを強調します。この原則は他の領域にも適用可能です：重複するサポートチケットをまとめ、繰り返しのトレースを圧縮し、差分のみを残します。\nコンテキスト要約パイプライン\n  MCP出力が依然として大きすぎる場合、LLM自体が要約して小さくすることも可能です。代償は圧縮と忠実度のトレードオフ：要約はニュアンスを失う可能性がありますが、詳細に溺れるよりはましです。実際には、重要ファイルは生のdiff、優先度の低いコンテキストは要約といったハイブリッド設計を採用します。\nコンテキストの優先順位付けと切り捨て\n  プルーニングや要約後でも、どれを最初に入れ、後に回し、容量不足時に捨てるかを決める必要があります。MCPクエリごとにトークン予算を設定することは不可欠です。そうしなければプロンプトが予測不能に膨張します。私たちは切り捨てを前提にした設計を試し、場合によっては概要を先頭に、詳細を後半に配置するなど調整しています。\nコンテキストの隔離\n  すべてのコンテキストを最初のプロンプトに含める必要はありません。サブタスクごとに専用のコンテキストスレッドを持たせるべきです。例えば、私たちのMCPクライアントではテスト失敗は専用のレビューサブスレッドに置かれ、メインのレビューコンテキストを妨げません。これにより混乱を減らし、長い対話でも明瞭さを保てます。\n継続的な改善と学習\n  コンテキストエンジニアリングは静的ではありません。モデルのフィードバックや人間による修正を取り入れ、優先順位を調整していきます。重要なのは可観測性です。モジュールごとにプロンプト入力を記録し、何が通って何が無駄かを把握します。MCPダッシュボードやトークンヒートマップのようなツールが、予算超過や不要な入力を可視化します。\nMCPサーバー & クライアントにおけるアンチパターン\n\nMCP時代はコンテキスト取得を容易にしました。おそらく「容易すぎる」ほどに。以下のようなアンチパターンがよく見られます：\nベクトル無差別投入\n  ベクトルDBは「関連」情報を見つけるのに優れていますが、それを万能の答えと見なすのは危険です。曖昧に関連するスニペットをすべて投入すると、関係のないファイルへのコメントや古いコードへの指摘で溢れるレビューになります。コンテキストの不適合はトークンの無駄だけでなく、モデルのパフォーマンスを引き下げる要因になります。\n「全部突っ込め」方式\n  すべてのログ、diff、ドックストリングをコンテキストに放り込み、あとは神に祈るやり方です。コストの増加、レイテンシの悪化、結果の予測不能を保証します。モデルは重要な部分と不要な部分を区別できないため、全方位的で散漫なレビューを生成します。矛盾が混入すれば、モデルは曖昧さを埋めるために幻覚を引き起こします。\n要するに、コンテキストは多ければ良いというものではありません。フィルタリング、優先順位付け、設計がなければ、「情報全部」はすぐにノイズに変わり、システムを遅く、鈍く、高コストにしてしまいます。\n私たちのMCPクライアントでのアプローチ\nMCP時代において、コンテキストは「王様」です。しかし正直なところ、その王様は酔いすぎて上下も分からなくなっていることがあります。課題はもはや「コンテキストを得ること」ではなく、「それを制御すること」です。優れたコンテキストエンジニアリングには、緻密な変換パイプライン、徹底的な優先順位付け、そして改善を続ける謙虚さが必要です。これを怠れば、トークン膨張、レイテンシ、混乱したレビューを招きます。うまく実践すれば、ワークフローに沿った鋭い出力が得られます。\n私たちは自社のコードレビュー用MCPクライアントでこれを実感しました。初期段階では全ログ・全ファイルをそのまま渡していました。その結果は高コストで、役に立たないほど散漫なレビューです。そこで重複排除、要約、タスク専用の隔離を導入したところ、レビュー品質が飛躍的に向上しました。すべてを指摘するのではなく、本当のクロスファイルリスクに集中するようになり、トークン消費とレイテンシも低下しました。\nこれこそが良いコンテキストエンジニアリングの成果です：情報量が多いのに散漫ではなく、本質を突いたレビュー。そしてそれこそが、私たちのMCPクライアントで実現しようとしていることです。\n👉 コンテキスト設計の正しい姿を体験してみませんか？ 今すぐ 14日間の無料トライアル でAIコードレビューをお試しください。",
      "publishedAt": "2025-09-19T06:40:35.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:12.175Z",
      "score": 0.06651144867808585
    },
    {
      "id": "c451f569db03113118dee3ee48937348",
      "title": "CodeRabbitは100万ドルをオープンソースプロジェクトに拠出します",
      "url": "https://coderabbit.ai/blog/coderabbit-commits-1-million-to-open-source-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/ja/blog/coderabbit-commits-1-million-to-open-source\">CodeRabbit commits $1 million to open source software</a>の意訳です。</p>\n<p>オープンソースは現代のソフトウェア開発の基盤です。パッケージマネージャーや開発ツールから、フレームワーク、インフラに至るまで、今日私たちが使うほとんどすべてのソフトウェアはオープンソースプロジェクトによって支えられています。CodeRabbit自体もそうです。これらのプロジェクトは、数え切れないほどの時間を費やし、維持・進化させ続けている開発者コミュニティによって構築・保守されています。</p>\n<p>本日、私たちは <strong>オープンソースソフトウェアへのスポンサーシップとして100万ドル（USD）の拠出</strong> を発表します。これは <strong>6,000万ドルのシリーズB資金調達</strong> に続くものであり、オープンソースが可能にしてくれたことへの感謝、そしてその未来への投資の重要性に対する私たちの信念を表しています。</p>\n<h2 id=\"heading-kirjgarjgzzku4rjgihjgqrjg7zjg5fjg7pjgr3jg7zjgrnjgbjjga7mlkmj7tjgyzjgzpjgozjgb7jgafku6xkuirjgavlv4xopohjgarjga7jgysqkg\"><strong>なぜ今、オープンソースへの支援がこれまで以上に必要なのか</strong></h2>\n<p>生成AIはソフトウェア開発を変革していますが、同時にオープンソースのメンテナーに新たな負荷を与えています。高品質なコントリビューションの増加と並行して、<strong>AI生成によるPRスパム</strong>（繰り返し・低品質・時には不安定なコードの提出）が急増し、メンテナーを圧倒しています。</p>\n<p>私たちはメンテナー自身から、この膨大なノイズがどれほど負担となっているかを直接聞いてきました。CodeRabbitでは、<strong>スパムをフィルタリングし、コード品質を向上させ、メンテナーの作業負荷を軽減する</strong> AI駆動のコードレビューと人間による監視を組み合わせたツールを開発しました。私たちはこのAIコードレビューツールをすべてのオープンソースプロジェクトに無料で提供しています（詳細はこちら）。</p>\n<p>しかし、ツールだけでは十分ではありません。持続可能なオープンソースには、金銭的支援、認知、そしてコミュニティ間をつなぐ強固な架け橋が必要です。</p>\n<h2 id=\"heading-20100\"><strong>20万ドルから100万ドル：より深いコミットメントへ</strong></h2>\n<p>今年初め、私たちは <a target=\"_blank\" href=\"https://2025.allthingsopen.org/pledging-support-for-open-source\"><strong>オープンソースへの20万ドルの誓約</strong></a> を発表し、以下のようなプロジェクトを支援しました：</p>\n<ul>\n<li><p><strong>pnpm</strong>: ディスクスペース効率に優れたパッケージマネージャー</p>\n</li>\n<li><p><strong>Biome (biomejs)</strong>: 次世代のJavaScript/TypeScript用リンター兼フォーマッター</p>\n</li>\n<li><p><strong>AST Grep (Herrington Darkholme)</strong>: 構造的コード検索によるスマートなコード解析</p>\n</li>\n<li><p><strong>iTerm2 (George Nachman)</strong>: 開発者のワークフローを刷新したターミナルエミュレーター</p>\n</li>\n<li><p><strong>Markdown Lint (David Anson)</strong>: ドキュメントを明確かつ一貫性のある状態に保つツール</p>\n</li>\n</ul>\n<p>この誓約は始まりにすぎません。今回のシリーズB資金調達によって、私たちは支援額を <strong>100万ドルに拡大</strong> し、エコシステム全体のプロジェクトやメンテナーが正当な評価とリソースを得られるようにします。</p>\n<p><a target=\"_blank\" href=\"https://coderabbit.link/oss-progam-submission-form\"><strong>スポンサーシップ申請はこちらより行ってください</strong></a></p>\n<h2 id=\"heading-coderabbitoss\"><strong>CodeRabbitとOSS：エコシステム全体をつなぐ架け橋へ</strong></h2>\n<p>スポンサーシップは始まりに過ぎません。オープンソースが直面している多くの課題 ― 持続可能性、セキュリティ、開発者の燃え尽き（バーンアウト） ― は特定のプロジェクトに限られた問題ではありません。これらはコミュニティやエコシステム全体に広がっています。</p>\n<p>だからこそ、CodeRabbitは <strong>メンテナー同士をつなぎ、協力を促進し、プロジェクト間で解決策を共有する</strong> 取り組みにも力を入れています。共同スポンサーシップ、共同イニシアチブ、コミュニティ主導のツールに関する議論を通じて、孤立した支援ではなく、エコシステム全体を強化することを目指しています。</p>\n<p>もしあなたがメンテナーやコントリビューターで、こうした議論に参加したいと考えているなら、ぜひご連絡ください。CodeRabbitチームや他のオープンソースリーダーとつながるためにDiscordに参加してください。</p>\n<h2 id=\"heading-coderabbit\"><strong>オープンソースプロジェクト向けの無料CodeRabbit利用</strong></h2>\n<p>最後に改めてお伝えします：<strong>CodeRabbitはオープンソースに対して、無料で提供されています</strong>。すべてのメンテナー、コントリビューター、コミュニティは、私たちのプラットフォームを利用してPRノイズを減らし、コード品質チェックを自動化し、より意味のあるコントリビュートに時間を割けるようになります。</p>\n<p><strong><em>詳細はこちらから確認し</em></strong> <a target=\"_blank\" href=\"https://coderabbit.link/oss-progam-submission-form\"><strong><em>資金提供の申請を行ってください</em></strong></a></p>\n",
      "summary": "CodeRabbit commits $1 million to open source softwareの意訳です。\nオープンソースは現代のソフトウェア開発の基盤です。パッケージマネージャーや開発ツールから、フレームワーク、インフラに至るまで、今日私たちが使うほとんどすべてのソフトウェアはオープンソースプロジェクトによって支えられています。CodeRabbit自体もそうです。これらのプロジェクトは、数え切れないほどの時間を費やし、維持・進化させ続けている開発者コミュニティによって構築・保守されています。\n本日、私たちは オープンソースソフトウェアへのスポンサーシップとして100万ドル（USD）の拠出 を発表します。これは 6,000万ドルのシリーズB資金調達 に続くものであり、オープンソースが可能にしてくれたことへの感謝、そしてその未来への投資の重要性に対する私たちの信念を表しています。\nなぜ今、オープンソースへの支援がこれまで以上に必要なのか\n生成AIはソフトウェア開発を変革していますが、同時にオープンソースのメンテナーに新たな負荷を与えています。高品質なコントリビューションの増加と並行して、AI生成によるPRスパム（繰り返し・低品質・時には不安定なコードの提出）が急増し、メンテナーを圧倒しています。\n私たちはメンテナー自身から、この膨大なノイズがどれほど負担となっているかを直接聞いてきました。CodeRabbitでは、スパムをフィルタリングし、コード品質を向上させ、メンテナーの作業負荷を軽減する AI駆動のコードレビューと人間による監視を組み合わせたツールを開発しました。私たちはこのAIコードレビューツールをすべてのオープンソースプロジェクトに無料で提供しています（詳細はこちら）。\nしかし、ツールだけでは十分ではありません。持続可能なオープンソースには、金銭的支援、認知、そしてコミュニティ間をつなぐ強固な架け橋が必要です。\n20万ドルから100万ドル：より深いコミットメントへ\n今年初め、私たちは オープンソースへの20万ドルの誓約 を発表し、以下のようなプロジェクトを支援しました：\npnpm: ディスクスペース効率に優れたパッケージマネージャー\nBiome (biomejs): 次世代のJavaScript/TypeScript用リンター兼フォーマッター\nAST Grep (Herrington Darkholme): 構造的コード検索によるスマートなコード解析\niTerm2 (George Nachman): 開発者のワークフローを刷新したターミナルエミュレーター\nMarkdown Lint (David Anson): ドキュメントを明確かつ一貫性のある状態に保つツール\nこの誓約は始まりにすぎません。今回のシリーズB資金調達によって、私たちは支援額を 100万ドルに拡大 し、エコシステム全体のプロジェクトやメンテナーが正当な評価とリソースを得られるようにします。\nスポンサーシップ申請はこちらより行ってください\nCodeRabbitとOSS：エコシステム全体をつなぐ架け橋へ\nスポンサーシップは始まりに過ぎません。オープンソースが直面している多くの課題 ― 持続可能性、セキュリティ、開発者の燃え尽き（バーンアウト） ― は特定のプロジェクトに限られた問題ではありません。これらはコミュニティやエコシステム全体に広がっています。\nだからこそ、CodeRabbitは メンテナー同士をつなぎ、協力を促進し、プロジェクト間で解決策を共有する 取り組みにも力を入れています。共同スポンサーシップ、共同イニシアチブ、コミュニティ主導のツールに関する議論を通じて、孤立した支援ではなく、エコシステム全体を強化することを目指しています。\nもしあなたがメンテナーやコントリビューターで、こうした議論に参加したいと考えているなら、ぜひご連絡ください。CodeRabbitチームや他のオープンソースリーダーとつながるためにDiscordに参加してください。\nオープンソースプロジェクト向けの無料CodeRabbit利用\n最後に改めてお伝えします：CodeRabbitはオープンソースに対して、無料で提供されています。すべてのメンテナー、コントリビューター、コミュニティは、私たちのプラットフォームを利用してPRノイズを減らし、コード品質チェックを自動化し、より意味のあるコントリビュートに時間を割けるようになります。\n詳細はこちらから確認し 資金提供の申請を行ってください",
      "publishedAt": "2025-09-19T06:21:31.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:12.175Z",
      "score": 0.03000903345044444
    },
    {
      "id": "b2944f86b8b1dad74d31217b7d195413",
      "title": "CodeRabbit commits $1 million to open source",
      "url": "https://coderabbit.ai/blog/coderabbit-commits-1-million-to-open-source",
      "content": "<p>Open source is the foundation of modern software development. From package managers and developer tools to frameworks and infrastructure, open source projects power nearly every piece of software we use today – including CodeRabbit itself. These projects are built and maintained by communities of developers who dedicate countless hours to keeping them alive, secure, and evolving.</p>\n<p>Today, we’re proud to announce a <strong>$1 million USD commitment to open source software sponsorships</strong>. This commitment comes on the heels of our <strong>$60 million Series B funding round</strong> and it reflects both our gratitude for what open source makes possible and our belief in the importance of investing in its future.</p>\n<h2 id=\"heading-why-open-source-needs-support-now-more-than-ever\"><strong>Why open source needs support now more than ever</strong></h2>\n<p>Generative AI is transforming software development, but it’s also putting new pressures on open source maintainers. Alongside the surge of high-quality contributions, there has been a sharp rise in <strong>AI-generated PR spam</strong>: repetitive, low-quality, and sometimes insecure code submissions that overwhelm project maintainers.</p>\n<p>We’ve heard firsthand from maintainers about how draining this flood of noise can be. At CodeRabbit, we’ve built tools that <strong>filter out spam, elevate code quality, and reduce maintainer workload</strong> by blending AI-driven code review with human oversight. We’ve made our AI code review tool free for use on all open source projects (more about that here). </p>\n<p>But tools alone aren’t enough—sustainable open source requires financial support, recognition, and stronger bridges between communities.</p>\n<h2 id=\"heading-from-200k-to-1m-deepening-our-commitment\"><strong>From $200K to $1M: Deepening our commitment</strong></h2>\n<p>Earlier this year, we announced a <a target=\"_blank\" href=\"https://2025.allthingsopen.org/pledging-support-for-open-source\"><strong>$200,000 pledge to open source</strong></a>, supporting projects like:</p>\n<ul>\n<li><p><strong>pnpm</strong>: A disk-space–efficient package manager</p>\n</li>\n<li><p><strong>Biome (biomejs)</strong>: A next-generation linter and formatter for JavaScript and TypeScript</p>\n</li>\n<li><p><strong>AST Grep (Herrington Darkholme)</strong>: Structural code search for smarter code analysis</p>\n</li>\n<li><p><strong>iTerm2 (George Nachman)</strong>: A terminal emulator that redefined developer workflow</p>\n</li>\n<li><p><strong>Markdown Lint (David Anson)</strong>: Ensuring docs stay clear and consistent</p>\n</li>\n</ul>\n<p>That pledge was only the beginning. With our new Series B funding, we’re scaling our support to <strong>$1 million</strong>, ensuring that more projects and maintainers across the ecosystem receive the recognition and resources they deserve.</p>\n<p><a target=\"_blank\" href=\"https://coderabbit.link/oss-progam-submission-form\"><strong>Apply for sponsorship here.</strong></a> </p>\n<h2 id=\"heading-coderabbit-amp-oss-building-bridges-across-the-oss-ecosystem\"><strong>CodeRabbit &amp; OSS: Building bridges across the OSS ecosystem</strong></h2>\n<p>Sponsorship is only part of the story. Many of the challenges open source faces—sustainability, security, and developer burnout—aren’t isolated to a single project. They stretch across communities and ecosystems.</p>\n<p>That’s why CodeRabbit is also working to <strong>connect maintainers, foster collaboration, and share solutions across projects</strong>. Whether through joint sponsorships, shared initiatives, or community-driven tooling conversations, we aim to strengthen the ecosystem as a whole rather than supporting it in silos.</p>\n<p>If you’re a maintainer or contributor who wants to join these conversations, we’d love to hear from you. Join our Discord to connect with the CodeRabbit team and other open source leaders.</p>\n<h2 id=\"heading-free-coderabbit-access-for-open-source-projects\"><strong>Free CodeRabbit access for open source projects</strong></h2>\n<p>Finally, a reminder: <strong>CodeRabbit is free for open source</strong>. Every maintainer, contributor, and community can use our platform to cut through PR noise, automate code quality checks, and free up more time for meaningful contributions.</p>\n<p><strong><em>Learn more and</em></strong> <a target=\"_blank\" href=\"https://coderabbit.link/oss-progam-submission-form\"><strong><em>apply for funding here.</em></strong></a></p>\n",
      "summary": "Open source is the foundation of modern software development. From package managers and developer tools to frameworks and infrastructure, open source projects power nearly every piece of software we use today – including CodeRabbit itself. These projects are built and maintained by communities of developers who dedicate countless hours to keeping them alive, secure, and evolving.\nToday, we’re proud to announce a $1 million USD commitment to open source software sponsorships. This commitment comes on the heels of our $60 million Series B funding round and it reflects both our gratitude for what open source makes possible and our belief in the importance of investing in its future.\nWhy open source needs support now more than ever\nGenerative AI is transforming software development, but it’s also putting new pressures on open source maintainers. Alongside the surge of high-quality contributions, there has been a sharp rise in AI-generated PR spam: repetitive, low-quality, and sometimes insecure code submissions that overwhelm project maintainers.\nWe’ve heard firsthand from maintainers about how draining this flood of noise can be. At CodeRabbit, we’ve built tools that filter out spam, elevate code quality, and reduce maintainer workload by blending AI-driven code review with human oversight. We’ve made our AI code review tool free for use on all open source projects (more about that here). \nBut tools alone aren’t enough—sustainable open source requires financial support, recognition, and stronger bridges between communities.\nFrom $200K to $1M: Deepening our commitment\nEarlier this year, we announced a $200,000 pledge to open source, supporting projects like:\npnpm: A disk-space–efficient package manager\nBiome (biomejs): A next-generation linter and formatter for JavaScript and TypeScript\nAST Grep (Herrington Darkholme): Structural code search for smarter code analysis\niTerm2 (George Nachman): A terminal emulator that redefined developer workflow\nMarkdown Lint (David Anson): Ensuring docs stay clear and consistent\nThat pledge was only the beginning. With our new Series B funding, we’re scaling our support to $1 million, ensuring that more projects and maintainers across the ecosystem receive the recognition and resources they deserve.\nApply for sponsorship here. \nCodeRabbit & OSS: Building bridges across the OSS ecosystem\nSponsorship is only part of the story. Many of the challenges open source faces—sustainability, security, and developer burnout—aren’t isolated to a single project. They stretch across communities and ecosystems.\nThat’s why CodeRabbit is also working to connect maintainers, foster collaboration, and share solutions across projects. Whether through joint sponsorships, shared initiatives, or community-driven tooling conversations, we aim to strengthen the ecosystem as a whole rather than supporting it in silos.\nIf you’re a maintainer or contributor who wants to join these conversations, we’d love to hear from you. Join our Discord to connect with the CodeRabbit team and other open source leaders.\nFree CodeRabbit access for open source projects\nFinally, a reminder: CodeRabbit is free for open source. Every maintainer, contributor, and community can use our platform to cut through PR noise, automate code quality checks, and free up more time for meaningful contributions.\nLearn more and apply for funding here.",
      "publishedAt": "2025-09-18T15:56:44.000Z",
      "author": "Aravind Putrevu",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "documentation",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:12.175Z",
      "score": 0.06160496726581978
    },
    {
      "id": "a34eb9185e3cb82f9ab2ecd637d138b2",
      "title": "CodeRabbit’s MCP integration = Code reviews that see the whole picture",
      "url": "https://coderabbit.ai/blog/coderabbits-mcp-server-integration-code-reviews-that-see-the-whole-picture",
      "content": "<p>Every dev team knows the pain of code reviews if performed in isolation. An AI tool (or even a teammate) can comment on syntax, style, and patterns, but without business requirements, deployment dependencies, or organizational knowledge, it’s just guessing at half the story.</p>\n<p>CodeRabbit currently has a number of native integrations including Linear, Jira, and Circle CI. We have seen the value that context from those tools provide to code reviews. That’s why we’re excited to announce the GA of <strong>CodeRabbit’s integration with MCP servers</strong>. This will allow you to bring in even more context into your reviews.</p>\n<p>With this launch, we become the first AI code review platform that orchestrates context from across your entire development ecosystem from business requirements in Confluence to system dependencies in your CI/CD pipeline to data from any internal MCP servers. All to provide code reviews that actually understand what your code is trying to accomplish.</p>\n<p><strong>Start your 14-day trial →</strong> <em>Get context-aware reviews that reference your actual team standards in ~10 minutes.</em></p>\n<h2 id=\"heading-why-mcp-for-ai-code-reviews\"><strong>Why MCP for AI code reviews?</strong></h2>\n<p>Development teams operate across dozens of tools:</p>\n<ul>\n<li><p>Requirements live in Linear</p>\n</li>\n<li><p>Design specifications exist in Figma</p>\n</li>\n<li><p>Architectural decisions get documented in Confluence</p>\n</li>\n<li><p>Security standards evolve in internal wikis after each audit</p>\n</li>\n</ul>\n<p>AI code reviewers start with basic context: your codebase, some coding guidelines, maybe a few integrations. They analyze syntax, check patterns, and suggest improvements. But they miss the context that determines whether code actually works for your team.</p>\n<p>As a MCP client, CodeRabbit acts as a compiler for organizational context. It takes high-level inputs - your wikis, tickets, deployment patterns - and compiles them down into precise, actionable code review insights. Instead of bloated integrations or brittle hacks, MCP lets clients like CodeRabbit pull in just the right data from your MCP servers from places like your Linear tickets, Confluence docs, Datadog metrics, or Slack discussions.</p>\n<h2 id=\"heading-what-it-looks-like-in-practice\">What it looks like in practice…</h2>\n<p>CodeRabbit searches connected MCP servers before starting a review. For example, database schema changes might get checked against data architecture documents. API endpoint implementations might get verified against service design patterns documented in internal wikis.</p>\n<p><strong>Example: CodeRabbit verifies code consistence</strong></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758126014841/53d19c71-2051-410c-a109-1e056cc0094d.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-bring-in-the-context-matters-to-you-from-any-tool\">Bring in the context matters to you… from any tool</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125357433/fa378ff4-743d-4841-a3fa-e933bcf4fe7c.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Traditional code review tools require specific integrations. CodeRabbit's MCP integration works with any system with an MCP server. Your proprietary internal tools, boutique SaaS platforms, custom documentation systems. If there's an MCP server, CodeRabbit can connect.</p>\n<p>With CodeRabbit as an MCP client, you’re reviews gain depth from bringing in three different types of context. </p>\n<h3 id=\"heading-technical-context\"><strong>Technical context.</strong></h3>\n<ul>\n<li>Think dependencies, performance data, static analysis, and test coverage.</li>\n</ul>\n<ul>\n<li><p><strong>Native integrations:</strong> GitHub Actions, GitLab CI, Bitbucket Pipelines</p>\n</li>\n<li><p><strong>MCP Servers:</strong> Datadog, New Relic, SonarQube, Snyk, Grafana</p>\n</li>\n<li><p>Example Review Comment:</p>\n</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125833426/facf17b5-32c7-47ce-b6cd-8170f9837bf6.png\" alt class=\"image--center mx-auto\" /></p>\n<h3 id=\"heading-business-context\"><strong>Business context.</strong></h3>\n<ul>\n<li><p>This includes things like requirements, user stories, and acceptance criteria.</p>\n</li>\n<li><p><strong>Native integrations:</strong> Linear, Jira, GitHub Issues, GitLab Issues</p>\n</li>\n<li><p><strong>MCP Servers:</strong> Confluence, Notion</p>\n</li>\n<li><p>Example Review Comment:</p>\n</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125848608/26c21700-1f57-42af-9993-cc372c629e6f.png\" alt class=\"image--center mx-auto\" /></p>\n<h3 id=\"heading-organizational-context\"><strong>Organizational context.</strong></h3>\n<ul>\n<li><p>We also pull in things like prior decisions, conventions, meeting notes, and institutional knowledge.</p>\n</li>\n<li><p><strong>Native integrations:</strong> PR history, Team conventions</p>\n</li>\n<li><p><strong>MCP Servers:</strong> Slack, Microsoft Teams, Stack Overflow for Teams, PagerDuty</p>\n</li>\n<li><p>Example Review Comment:</p>\n<p>  <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125858492/5bc5b006-b61c-4b9c-9642-5246978db056.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n</ul>\n<h2 id=\"heading-getting-started-with-mcp-integration\"><strong>Getting started with MCP integration</strong></h2>\n<p>Setting up CodeRabbit's MCP client requires minimal configuration. Most development teams can connect their first MCP server in under 10 minutes.</p>\n<p><strong>Popular development tools with MCP server support</strong>:</p>\n<ul>\n<li><p><strong>Linear</strong> (native MCP support, 5 minutes)</p>\n</li>\n<li><p><strong>Notion</strong> (MCP server available, 10 minutes)</p>\n</li>\n<li><p><strong>Confluence</strong> (community MCP server, 15 minutes)</p>\n</li>\n<li><p><strong>Figma</strong> (MCP plugin available, 10 minutes)</p>\n</li>\n</ul>\n<p>Define which code changes should search which development systems. Database changes check architecture documentation. Authentication changes check security documentation.</p>\n<p>Adding an MCP server is easy:</p>\n<ol>\n<li><p>In the CodeRabbit dashboard, head over to integrations &gt; and toggle to the MCP Servers tab if needed</p>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125568659/69feb7d1-f61c-4f4b-81db-5e8e73290092.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><p>You can click on one of the pre-configured MCP server options or the New MCP Server button to add other MCP servers.</p>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125579766/3a2477e7-0ad4-4ff1-8da6-1b8e5307d4fa.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><p>For MCP servers not on the list, enter the relevant credentials.</p>\n</li>\n<li><p>Note the usage guidance which serves as context for how the MCP information should be used.</p>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125590738/eec44f71-e0c0-42b7-814b-869996da38c8.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><p>Once connected. You can see the available calls and hover over them to see more details.</p>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125606565/5a9a903b-d6d8-4635-904e-f33616096103.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><p>You can also click on each call to enable/disable access.</p>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758125618724/c5269264-6fa1-4288-8a98-7dca46869301.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n</ol>\n<h2 id=\"heading-a-review-platform-that-brings-in-all-your-context\"><strong>A review platform that brings in all your context</strong></h2>\n<p>CodeRabbit works out of the box with 50+ integrations. With MCP, you can extend it to your custom servers and internal tools. Start with the systems you already use — Linear, Confluence, Datadog, Slack — and add more as you go.</p>\n<h3 id=\"heading-next-steps\"><strong>Next steps:</strong></h3>\n<ol>\n<li><p><a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong>Start a 14-day trial</strong></a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://app.coderabbit.ai/integrations\"><strong>View MCP server directory</strong></a></p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://docs.coderabbit.ai/context-enrichment/mcp-server-integrations\"><strong>See the MCP docs</strong></a></p>\n</li>\n</ol>\n",
      "summary": "Every dev team knows the pain of code reviews if performed in isolation. An AI tool (or even a teammate) can comment on syntax, style, and patterns, but without business requirements, deployment dependencies, or organizational knowledge, it’s just guessing at half the story.\nCodeRabbit currently has a number of native integrations including Linear, Jira, and Circle CI. We have seen the value that context from those tools provide to code reviews. That’s why we’re excited to announce the GA of CodeRabbit’s integration with MCP servers. This will allow you to bring in even more context into your reviews.\nWith this launch, we become the first AI code review platform that orchestrates context from across your entire development ecosystem from business requirements in Confluence to system dependencies in your CI/CD pipeline to data from any internal MCP servers. All to provide code reviews that actually understand what your code is trying to accomplish.\nStart your 14-day trial → Get context-aware reviews that reference your actual team standards in ~10 minutes.\nWhy MCP for AI code reviews?\nDevelopment teams operate across dozens of tools:\nRequirements live in Linear\nDesign specifications exist in Figma\nArchitectural decisions get documented in Confluence\nSecurity standards evolve in internal wikis after each audit\nAI code reviewers start with basic context: your codebase, some coding guidelines, maybe a few integrations. They analyze syntax, check patterns, and suggest improvements. But they miss the context that determines whether code actually works for your team.\nAs a MCP client, CodeRabbit acts as a compiler for organizational context. It takes high-level inputs - your wikis, tickets, deployment patterns - and compiles them down into precise, actionable code review insights. Instead of bloated integrations or brittle hacks, MCP lets clients like CodeRabbit pull in just the right data from your MCP servers from places like your Linear tickets, Confluence docs, Datadog metrics, or Slack discussions.\nWhat it looks like in practice…\nCodeRabbit searches connected MCP servers before starting a review. For example, database schema changes might get checked against data architecture documents. API endpoint implementations might get verified against service design patterns documented in internal wikis.\nExample: CodeRabbit verifies code consistence\n\nBring in the context matters to you… from any tool\n\nTraditional code review tools require specific integrations. CodeRabbit's MCP integration works with any system with an MCP server. Your proprietary internal tools, boutique SaaS platforms, custom documentation systems. If there's an MCP server, CodeRabbit can connect.\nWith CodeRabbit as an MCP client, you’re reviews gain depth from bringing in three different types of context. \nTechnical context.\nThink dependencies, performance data, static analysis, and test coverage.\nNative integrations: GitHub Actions, GitLab CI, Bitbucket Pipelines\nMCP Servers: Datadog, New Relic, SonarQube, Snyk, Grafana\nExample Review Comment:\n\nBusiness context.\nThis includes things like requirements, user stories, and acceptance criteria.\nNative integrations: Linear, Jira, GitHub Issues, GitLab Issues\nMCP Servers: Confluence, Notion\nExample Review Comment:\n\nOrganizational context.\nWe also pull in things like prior decisions, conventions, meeting notes, and institutional knowledge.\nNative integrations: PR history, Team conventions\nMCP Servers: Slack, Microsoft Teams, Stack Overflow for Teams, PagerDuty\nExample Review Comment:\n  \nGetting started with MCP integration\nSetting up CodeRabbit's MCP client requires minimal configuration. Most development teams can connect their first MCP server in under 10 minutes.\nPopular development tools with MCP server support:\nLinear (native MCP support, 5 minutes)\nNotion (MCP server available, 10 minutes)\nConfluence (community MCP server, 15 minutes)\nFigma (MCP plugin available, 10 minutes)\nDefine which code changes should search which development systems. Database changes check architecture documentation. Authentication changes check security documentation.\nAdding an MCP server is easy:\nIn the CodeRabbit dashboard, head over to integrations > and toggle to the MCP Servers tab if needed\n \nYou can click on one of the pre-configured MCP server options or the New MCP Server button to add other MCP servers.\n \nFor MCP servers not on the list, enter the relevant credentials.\nNote the usage guidance which serves as context for how the MCP information should be used.\n \nOnce connected. You can see the available calls and hover over them to see more details.\n \nYou can also click on each call to enable/disable access.\n \nA review platform that brings in all your context\nCodeRabbit works out of the box with 50+ integrations. With MCP, you can extend it to your custom servers and internal tools. Start with the systems you already use — Linear, Confluence, Datadog, Slack — and add more as you go.\nNext steps:\nStart a 14-day trial\nView MCP server directory\nSee the MCP docs",
      "publishedAt": "2025-09-17T16:04:55.000Z",
      "author": "Edgar Cerecerez",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "testing",
        "observability",
        "governance"
      ],
      "ingestedAt": "2025-12-04T14:17:12.175Z",
      "score": 0.10711190849234059
    },
    {
      "id": "79cb6543735df13206c47f6d632c5332",
      "title": "Handling ballooning context in the MCP era: Context engineering on steroids",
      "url": "https://coderabbit.ai/blog/handling-ballooning-context-in-the-mcp-era-context-engineering-on-steroids",
      "content": "<p>Once upon a time, getting context into an LLM meant stringing together hacks, prayers, vector strategies, and overly complex RAG pipelines. Then came the <strong>Model Context Protocol (MCP),</strong> a clean, modular way to serve external data to models in production. It quickly became the protocol of choice for anyone building agentic systems that are trying to actually <em>do</em> things.</p>\n<p>Every tech company is now launching MCP functionalities – and for good reason. MCP separates context logic from application logic, improves reliability, and helps tame the chaos of prompt construction in complex workflows.</p>\n<p>We’ve been deep in the context engineering space for a while, and as we launch our own MCP client, we’re genuinely excited by how it lets us inject richer context into our code reviews. But let’s be honest: with great context comes great risk. Because here’s the dirty secret of the MCP era: <strong>most of us are now drowning in the context we used to beg for</strong>. More logs, more traces, more diffs, more \"relevant\" files and way less clarity about what the model actually needs.</p>\n<p>What starts as helpful input quickly turns into token bloat, noise, and degradation in model performance. Think hallucinations with citations, latency spikes, or reviews that read like they were written by an over-caffeinated intern who rambles. Good context engineering isn’t about cramming in <em>everything</em>, it’s also about knowing what to leave out. And in the aftermath of MCP, that balance is harder (and more important) than ever.</p>\n<p>In this article, we’ll break down the <strong>ballooning context problem</strong>, what happens when well-intentioned context goes rogue, and how we’re tackling it head-on. If you’re shipping LLM-based features with MCP and want to avoid accidentally building a prompt-shaped black hole, this blog is for you.</p>\n<h3 id=\"heading-the-ballooning-context-problem-with-mcp-clients-amp-servers\"><strong>The “Ballooning Context Problem” with MCP clients &amp; servers</strong></h3>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123932448/7988a3c4-f1be-4033-b8e2-43aee929d878.png\" alt class=\"image--center mx-auto\" /></p>\n<p>MCP servers and clients make it easy to hand models a firehose of information: logs, traces, diffs, configs, tickets, and sometimes even that dusty corner of the repo nobody remembers owning. It’s all right there at the model’s fingertips. But here’s the question: is more context always better? Definitely not!</p>\n<p>Too much context is like cramming for an exam by reading the entire library. You end up with noise, not knowledge. And when context goes unchecked, three problems show up fast:</p>\n<ul>\n<li><p><strong>Token bloat.</strong> LLMs don’t have infinite stomachs. Input windows are expensive and finite, and stuffing them full of “just in case” details means higher costs, slower throughput, and wasted budget on irrelevant text.</p>\n</li>\n<li><p><strong>Relevance decay.</strong> More information doesn’t mean better outputs. In fact, it often means worse. Irrelevant or redundant snippets dilute the signal, and the model starts chasing tangents instead of insights.</p>\n</li>\n<li><p><strong>Latency.</strong> Every extra log, diff, or stack trace has to be fetched, processed, and shoved into the prompt. Context building becomes the bottleneck, dragging review speed down to a crawl.</p>\n</li>\n</ul>\n<p>In short, ballooning context turns the elegance of MCP into a liability. Without deliberate context engineering, the very thing meant to sharpen outputs can just as easily smother them.</p>\n<h2 id=\"heading-when-context-hurts\">When context hurts</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123946588/f8c48c9f-db75-47bc-a349-bdb58b81258b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>In practice, we see three common pathologies:</p>\n<ul>\n<li><p><strong>Context confusion.</strong> This happens when the model latches onto irrelevant detail and treats it as signal. Imagine a pull request that updates authentication logic but the context dump also includes unrelated test fixtures. The model might start reviewing the fixtures instead, producing comments that feel informed but have nothing to do with the actual change.</p>\n</li>\n<li><p><strong>Context clash.</strong> Not all context agrees with itself. Suppose a code review includes both the latest schema migration and an outdated docstring that contradicts it. The model now has to “choose” which source to trust. Often, it hedges, producing muddled reviews that cover every angle without real confidence: the LLM equivalent of a reviewer who can’t commit.</p>\n</li>\n<li><p><strong>Context poisoning.</strong> The most insidious case is when bad information makes it into the context. A hallucinated “related file” or a mis-indexed snippet gets injected, and suddenly the model is citing non-existent code. In a review, that looks like a comment about a bug in a file that doesn’t exist, confusing developers, wasting time, and eroding trust.</p>\n</li>\n</ul>\n<p>And it’s not just code reviews. The same pitfalls show up anywhere context gets overstuffed: customer support bots pulling in irrelevant tickets, research assistants distracted by tangential papers, or security agents treating noisy logs as hard evidence. In each case, the wrong context is worse than no context at all.</p>\n<h2 id=\"heading-key-patterns-to-combat-context-overload-with-mcp-servers\">Key patterns to combat context overload with MCP servers</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123970317/3dd0ac82-f438-4150-83d8-fae81f7dafa2.png\" alt class=\"image--center mx-auto\" /></p>\n<p>If the problem of the MCP era is ballooning context, the solution isn’t to stop piping in information — it’s to curate, compress, and serve it with intent. MCP context should be treated as raw material that goes through a well-designed data transformation process before it ever reaches the model. For our own MCP client for code reviews, we’ve leaned on a set of patterns that keep context high-signal and low-noise.</p>\n<ul>\n<li><p><strong>Context deduplication and differencing</strong><br />  Redundant inputs are the fastest way to waste tokens. Identical stack traces, repeated log lines, or unchanged sections of a diff don’t need to appear ten times. Our client identifies duplicates, collapses them, and highlights only what’s new. The same principle applies in other domains: collapse duplicate customer tickets, compress recurring traces, and reduce context to delta rather than bulk.</p>\n</li>\n<li><p><strong>Context summarization pipelines</strong><br />  Sometimes raw MCP output is still too big. Here, LLMs themselves can help by summarizing retrieved context into something smaller. The tradeoff is compression vs. fidelity: a summary might miss nuance, but the alternative is a model drowning in detail. In practice, we use hybrid designs: raw diffs for high-priority files, summaries for less-critical context.</p>\n</li>\n<li><p><strong>Context prioritization and truncation</strong><br />  Even after pruning and summarizing, you still need to decide what goes first, what can be deferred, and what gets dropped if there isn’t room. Setting a token budget per MCP query is critical, or else prompts will balloon unpredictably. We’ve experimented with truncation-aware designs; sometimes front-loading summaries for quick orientation, other times end-loading detail for deep dives. The “right” design depends on the workflow and the model’s feedback loop.</p>\n</li>\n<li><p><strong>Context quarantining</strong><br />  Not every piece of context belongs in the first prompt. Subtasks should carry their own dedicated context threads, so the model sees exactly what it needs when it needs it. For example, in our MCP client, test failures live in a dedicated review sub-thread rather than clogging the main review context. This approach reduces confusion and helps preserve clarity across long interactions.</p>\n</li>\n<li><p><strong>Iteration and learning</strong><br />  Context engineering isn’t static. We use model feedback and human-in-the-loop corrections to tune priorities over time. Observability is key: logging actual prompt inputs, broken down per module, lets us see what’s getting through and what’s wasted. Tooling like MCP dashboards or token heatmaps can highlight where budgets are blown or irrelevant inputs are sneaking in.</p>\n</li>\n</ul>\n<h2 id=\"heading-anti-patterns-to-avoid-with-mcp-servers-amp-clients\"><strong>Anti-patterns to avoid with MCP servers &amp; clients</strong></h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758123985534/b1a3acb1-5c02-4d88-b3d0-883aeb0dd573.png\" alt class=\"image--center mx-auto\" /></p>\n<p>The MCP era makes context retrieval easy. Maybe <em>too</em> easy. A couple of common anti-patterns are worth calling out:</p>\n<ul>\n<li><p><strong>Blind vector stuffing</strong><br />  Vector databases are great at surfacing “relevant” chunks of information, but treating them as an oracle is a recipe for trouble. Stuffing in every vaguely related snippet means you get reviews full of tangents: comments about files that weren’t touched, or nitpicks based on stale code. Context irrelevance doesn’t just waste tokens — it actively drags down model performance by pulling attention away from the real task.</p>\n</li>\n<li><p><strong>“Just give it everything”</strong><br />  The brute-force approach: dump every log, diff, and docstring into the context window and pray. This guarantees high costs, long latencies, and unpredictable results. The model can’t tell which parts are critical and which are fluff, so you end up with bloated reviews that read like they were written by an overeager intern trying to cover every angle. Worse, when contradictions sneak in, the model hedges or hallucinates to reconcile them.</p>\n</li>\n</ul>\n<p>In short: more context isn’t always better. Without filtering, prioritization, and careful design, “everything” quickly turns into noise that makes the system slower, dumber, and more expensive.</p>\n<h2 id=\"heading-the-approach-we-took-with-our-mcp-client\">The approach we took with our MCP client</h2>\n<p>In the MCP era, context is king. But let’s be honest: sometimes it’s a king that’s had one too many and can’t tell up from down. The challenge isn’t getting context anymore; it’s taming it. Great context engineering requires careful transformation pipelines, ruthless prioritization, and the humility to keep iterating. Done poorly, you get token bloat, latency, and reviews that sound confused. Done well, you get sharper outputs that scale with your workflow.</p>\n<p>We’ve seen this firsthand in our own MCP client for code reviews. When testing, we initially passed full logs and entire file sets straight through. The result? Expensive reviews that rambled more than they helped. Once we introduced deduplication, summarization, and task-specific quarantining, review quality jumped. Instead of commenting on everything, the model zeroed in on real cross-file risks,  while token use and latency both dropped.</p>\n<p>That’s the payoff of good context engineering: reviews that feel informed, not bloated. And that’s what we’re building toward with our MCP client.</p>\n<p><strong><em>👉 Ready to see context done right? Start your</em></strong> <a target=\"_blank\" href=\"https://app.coderabbit.ai/login???free-trial\"><strong><em>14-day trial</em></strong></a> <strong><em>of our AI code reviews.</em></strong></p>\n",
      "summary": "Once upon a time, getting context into an LLM meant stringing together hacks, prayers, vector strategies, and overly complex RAG pipelines. Then came the Model Context Protocol (MCP), a clean, modular way to serve external data to models in production. It quickly became the protocol of choice for anyone building agentic systems that are trying to actually do things.\nEvery tech company is now launching MCP functionalities – and for good reason. MCP separates context logic from application logic, improves reliability, and helps tame the chaos of prompt construction in complex workflows.\nWe’ve been deep in the context engineering space for a while, and as we launch our own MCP client, we’re genuinely excited by how it lets us inject richer context into our code reviews. But let’s be honest: with great context comes great risk. Because here’s the dirty secret of the MCP era: most of us are now drowning in the context we used to beg for. More logs, more traces, more diffs, more \"relevant\" files and way less clarity about what the model actually needs.\nWhat starts as helpful input quickly turns into token bloat, noise, and degradation in model performance. Think hallucinations with citations, latency spikes, or reviews that read like they were written by an over-caffeinated intern who rambles. Good context engineering isn’t about cramming in everything, it’s also about knowing what to leave out. And in the aftermath of MCP, that balance is harder (and more important) than ever.\nIn this article, we’ll break down the ballooning context problem, what happens when well-intentioned context goes rogue, and how we’re tackling it head-on. If you’re shipping LLM-based features with MCP and want to avoid accidentally building a prompt-shaped black hole, this blog is for you.\nThe “Ballooning Context Problem” with MCP clients & servers\n\nMCP servers and clients make it easy to hand models a firehose of information: logs, traces, diffs, configs, tickets, and sometimes even that dusty corner of the repo nobody remembers owning. It’s all right there at the model’s fingertips. But here’s the question: is more context always better? Definitely not!\nToo much context is like cramming for an exam by reading the entire library. You end up with noise, not knowledge. And when context goes unchecked, three problems show up fast:\nToken bloat. LLMs don’t have infinite stomachs. Input windows are expensive and finite, and stuffing them full of “just in case” details means higher costs, slower throughput, and wasted budget on irrelevant text.\nRelevance decay. More information doesn’t mean better outputs. In fact, it often means worse. Irrelevant or redundant snippets dilute the signal, and the model starts chasing tangents instead of insights.\nLatency. Every extra log, diff, or stack trace has to be fetched, processed, and shoved into the prompt. Context building becomes the bottleneck, dragging review speed down to a crawl.\nIn short, ballooning context turns the elegance of MCP into a liability. Without deliberate context engineering, the very thing meant to sharpen outputs can just as easily smother them.\nWhen context hurts\n\nIn practice, we see three common pathologies:\nContext confusion. This happens when the model latches onto irrelevant detail and treats it as signal. Imagine a pull request that updates authentication logic but the context dump also includes unrelated test fixtures. The model might start reviewing the fixtures instead, producing comments that feel informed but have nothing to do with the actual change.\nContext clash. Not all context agrees with itself. Suppose a code review includes both the latest schema migration and an outdated docstring that contradicts it. The model now has to “choose” which source to trust. Often, it hedges, producing muddled reviews that cover every angle without real confidence: the LLM equivalent of a reviewer who can’t commit.\nContext poisoning. The most insidious case is when bad information makes it into the context. A hallucinated “related file” or a mis-indexed snippet gets injected, and suddenly the model is citing non-existent code. In a review, that looks like a comment about a bug in a file that doesn’t exist, confusing developers, wasting time, and eroding trust.\nAnd it’s not just code reviews. The same pitfalls show up anywhere context gets overstuffed: customer support bots pulling in irrelevant tickets, research assistants distracted by tangential papers, or security agents treating noisy logs as hard evidence. In each case, the wrong context is worse than no context at all.\nKey patterns to combat context overload with MCP servers\n\nIf the problem of the MCP era is ballooning context, the solution isn’t to stop piping in information — it’s to curate, compress, and serve it with intent. MCP context should be treated as raw material that goes through a well-designed data transformation process before it ever reaches the model. For our own MCP client for code reviews, we’ve leaned on a set of patterns that keep context high-signal and low-noise.\nContext deduplication and differencing\n  Redundant inputs are the fastest way to waste tokens. Identical stack traces, repeated log lines, or unchanged sections of a diff don’t need to appear ten times. Our client identifies duplicates, collapses them, and highlights only what’s new. The same principle applies in other domains: collapse duplicate customer tickets, compress recurring traces, and reduce context to delta rather than bulk.\nContext summarization pipelines\n  Sometimes raw MCP output is still too big. Here, LLMs themselves can help by summarizing retrieved context into something smaller. The tradeoff is compression vs. fidelity: a summary might miss nuance, but the alternative is a model drowning in detail. In practice, we use hybrid designs: raw diffs for high-priority files, summaries for less-critical context.\nContext prioritization and truncation\n  Even after pruning and summarizing, you still need to decide what goes first, what can be deferred, and what gets dropped if there isn’t room. Setting a token budget per MCP query is critical, or else prompts will balloon unpredictably. We’ve experimented with truncation-aware designs; sometimes front-loading summaries for quick orientation, other times end-loading detail for deep dives. The “right” design depends on the workflow and the model’s feedback loop.\nContext quarantining\n  Not every piece of context belongs in the first prompt. Subtasks should carry their own dedicated context threads, so the model sees exactly what it needs when it needs it. For example, in our MCP client, test failures live in a dedicated review sub-thread rather than clogging the main review context. This approach reduces confusion and helps preserve clarity across long interactions.\nIteration and learning\n  Context engineering isn’t static. We use model feedback and human-in-the-loop corrections to tune priorities over time. Observability is key: logging actual prompt inputs, broken down per module, lets us see what’s getting through and what’s wasted. Tooling like MCP dashboards or token heatmaps can highlight where budgets are blown or irrelevant inputs are sneaking in.\nAnti-patterns to avoid with MCP servers & clients\n\nThe MCP era makes context retrieval easy. Maybe too easy. A couple of common anti-patterns are worth calling out:\nBlind vector stuffing\n  Vector databases are great at surfacing “relevant” chunks of information, but treating them as an oracle is a recipe for trouble. Stuffing in every vaguely related snippet means you get reviews full of tangents: comments about files that weren’t touched, or nitpicks based on stale code. Context irrelevance doesn’t just waste tokens — it actively drags down model performance by pulling attention away from the real task.\n“Just give it everything”\n  The brute-force approach: dump every log, diff, and docstring into the context window and pray. This guarantees high costs, long latencies, and unpredictable results. The model can’t tell which parts are critical and which are fluff, so you end up with bloated reviews that read like they were written by an overeager intern trying to cover every angle. Worse, when contradictions sneak in, the model hedges or hallucinates to reconcile them.\nIn short: more context isn’t always better. Without filtering, prioritization, and careful design, “everything” quickly turns into noise that makes the system slower, dumber, and more expensive.\nThe approach we took with our MCP client\nIn the MCP era, context is king. But let’s be honest: sometimes it’s a king that’s had one too many and can’t tell up from down. The challenge isn’t getting context anymore; it’s taming it. Great context engineering requires careful transformation pipelines, ruthless prioritization, and the humility to keep iterating. Done poorly, you get token bloat, latency, and reviews that sound confused. Done well, you get sharper outputs that scale with your workflow.\nWe’ve seen this firsthand in our own MCP client for code reviews. When testing, we initially passed full logs and entire file sets straight through. The result? Expensive reviews that rambled more than they helped. Once we introduced deduplication, summarization, and task-specific quarantining, review quality jumped. Instead of commenting on everything, the model zeroed in on real cross-file risks,  while token use and latency both dropped.\nThat’s the payoff of good context engineering: reviews that feel informed, not bloated. And that’s what we’re building toward with our MCP client.\n👉 Ready to see context done right? Start your 14-day trial of our AI code reviews.",
      "publishedAt": "2025-09-17T15:57:17.000Z",
      "author": "Tommy Elizaga",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide",
        "testing",
        "observability"
      ],
      "ingestedAt": "2025-12-04T14:17:12.175Z",
      "score": 0.09942340532164892
    },
    {
      "id": "7168006c47b6a9d86cdeb3c2afc3f88c",
      "title": "CodeRabbit CLI - 無料で使えるCLIのAIコードレビュー",
      "url": "https://coderabbit.ai/blog/coderabbit-cli-free-ai-code-reviews-in-your-cli-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/coderabbit-cli-free-ai-code-reviews-in-your-cli\">CodeRabbit CLI - Free AI code reviews in your CLI</a>の意訳です。</p>\n<p>CodeRabbitは、PRにおけるAIコードレビューから始まりました。5月には、そのインテリジェンスを<a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/ai-code-reviews-vscode-cursor-windsurf\">VS Code、Cursor、Windsurf</a>に拡張。そして今、開発者に愛されるAIコードレビューをコマンドラインにまで広げる「CodeRabbit CLI」を発表します。つまり、私たちは最も包括的なAIコードレビューツールになったのです。あなたが働く場所なら、どこでも動作します。</p>\n<p><em>CodeRabbit CLI</em>は、開発者がターミナルで直接セルフレビューを行えるようにします。自動化されたインテリジェントなコード分析機能を提供し、問題の早期発見と一貫したコード規約を維持し、CLI内でAIコーディングエージェントとシームレスな統合によって自律的なコーディングを実現します。</p>\n<h2 id=\"heading-vibe-cli\"><strong>コードの「Vibeチェック」― CLIでも</strong></h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/IqBKf4u5MtA\">https://youtu.be/IqBKf4u5MtA</a></div>\n<p> </p>\n<p>CodeRabbit CLIは、PRやIDEレビューと同じ包括的な分析を提供し、バグの早期発見に役立ちます。CodeRabbit CLIはレート制限付きで無料利用できますが、Proプランでは制限が大幅に緩和され、さらに以下のような追加機能を利用できます。</p>\n<ul>\n<li><p><strong>コンテキスト対応分析</strong>: Git連携を活用し、静的解析ツールやセキュリティスキャナ、コードグラフの関係性機能など40以上の情報源を統合して、最も包括的なレビューを実現</p>\n</li>\n<li><p><strong>プレコミットレビュー</strong>: マシンを離れる前に変更を分析し、多層的なレビューを提供</p>\n</li>\n<li><p><strong>ワンクリック修正</strong>: 簡単な修正は即適用、複雑な問題はAIエージェントに完全なコンテキスト付きで引き渡し</p>\n</li>\n<li><p><strong>コーディング規約検出</strong>: <a target=\"_blank\" href=\"http://agent.md\">agent.md</a>、<a target=\"_blank\" href=\"http://claude.md\">claude.md</a>、Cursor rulesなどのコーディングエージェント設定ファイルを自動検出</p>\n</li>\n</ul>\n<h2 id=\"heading-coderabbit-cli\"><strong>CodeRabbit CLI: どこでも、なんでも動作</strong></h2>\n<p>ターミナルネイティブであるため、CodeRabbit CLIは以下に対応します。</p>\n<ul>\n<li><p><strong>あらゆるターミナルアプリ/IDE:</strong> iTerm2、Ghostty、Neovim、Lazyvim</p>\n</li>\n<li><p><strong>あらゆるAIコーディングCLIエージェント:</strong> Claude Code、Codex、Cursor、Gemini、OpenCodeなど</p>\n</li>\n</ul>\n<h2 id=\"heading-aicli\"><strong>AIコーディングエージェントCLIとの使い方</strong></h2>\n<p>CodeRabbit CLIはAIコーディングエージェントとの新しい統合の可能性を広げます。Claude Codeとの動作例は以下の通りです。</p>\n<ol>\n<li>コーディングタスクを進める際、Claude CodeにCodeRabbitを使って発見された問題を修正するよう促すことができます。PRDやタスクリストからコーディングする場合に特に便利です。</li>\n</ol>\n<pre><code class=\"lang-plaintext\">仕様書のフェーズ7.3を実施し、その後に `coderabbit --prompt-only`を実行してください。\n必要なだけバックグラウンドにて実行し、発生した問題を修正してください。\n</code></pre>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971361399/2f4398b2-4a89-4fd3-9d1a-3baf00a0fe6e.png\" alt class=\"image--center mx-auto\" /></p>\n<ol start=\"2\">\n<li>Claude Codeはコーディングタスクを進めながら、バックグラウンドで<code>coderabbit --prompt-only</code>を実行します。タイマーを設定してCodeRabbitを定期的に確認する場合もあります。あるいは、ClaudeにCodeRabbitの完了を確認するよう促すこともできます。</li>\n</ol>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971331440/fef10dc6-b3a7-4da7-935c-41c06c95f04a.png\" alt class=\"image--center mx-auto\" /></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971376053/33dda5e8-8ede-4239-b363-16b491fe5a02.png\" alt=\"Claude Code内でバックグラウンド実行されるCodeRabbit\" class=\"image--center mx-auto\" /></p>\n<ol start=\"3\">\n<li>その後、Claude CodeはCodeRabbitの出力を読み込みます。<code>--prompt-only</code>フラグを使うことで、AIエージェントが読み取れるプレーンテキストで出力されます。ClaudeはCodeRabbitが検出した問題ごとにタスクリストを作成します。</li>\n</ol>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971316135/ebe0c883-be63-4e77-9e67-234694047ea0.png\" alt class=\"image--center mx-auto\" /></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971352863/2003e70c-4ad0-4435-95fb-df2a0885b9f7.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Claude Codeとの統合や自動化ワークフローについては、<a target=\"_blank\" href=\"https://docs.coderabbit.ai/cli\">CLIドキュメント</a>をご覧ください。</p>\n<p>CLIにはインタラクティブモードとプレーンレスポンスモードの2種類があり、自動化ワークフローへの統合や他ツールへの結果の受け渡しが容易です。</p>\n<h2 id=\"heading-kirjgajgzjjgohmlrkqkg\"><strong>はじめ方</strong></h2>\n<p>CodeRabbit CLIはすでに利用可能です。インストールして最初のレビューを試してみましょう。</p>\n<pre><code class=\"lang-powershell\"><span class=\"hljs-comment\"># CodeRabbitをインストール</span>\n<span class=\"hljs-built_in\">curl</span> <span class=\"hljs-literal\">-fsSL</span> https://cli.coderabbit.ai/install.sh | sh\n\n<span class=\"hljs-comment\"># インタラクティブモードでレビュー実行</span>\ncoderabbit\n</code></pre>\n",
      "summary": "CodeRabbit CLI - Free AI code reviews in your CLIの意訳です。\nCodeRabbitは、PRにおけるAIコードレビューから始まりました。5月には、そのインテリジェンスをVS Code、Cursor、Windsurfに拡張。そして今、開発者に愛されるAIコードレビューをコマンドラインにまで広げる「CodeRabbit CLI」を発表します。つまり、私たちは最も包括的なAIコードレビューツールになったのです。あなたが働く場所なら、どこでも動作します。\nCodeRabbit CLIは、開発者がターミナルで直接セルフレビューを行えるようにします。自動化されたインテリジェントなコード分析機能を提供し、問題の早期発見と一貫したコード規約を維持し、CLI内でAIコーディングエージェントとシームレスな統合によって自律的なコーディングを実現します。\nコードの「Vibeチェック」― CLIでも\n\n\n\nhttps://youtu.be/IqBKf4u5MtA\n \nCodeRabbit CLIは、PRやIDEレビューと同じ包括的な分析を提供し、バグの早期発見に役立ちます。CodeRabbit CLIはレート制限付きで無料利用できますが、Proプランでは制限が大幅に緩和され、さらに以下のような追加機能を利用できます。\nコンテキスト対応分析: Git連携を活用し、静的解析ツールやセキュリティスキャナ、コードグラフの関係性機能など40以上の情報源を統合して、最も包括的なレビューを実現\nプレコミットレビュー: マシンを離れる前に変更を分析し、多層的なレビューを提供\nワンクリック修正: 簡単な修正は即適用、複雑な問題はAIエージェントに完全なコンテキスト付きで引き渡し\nコーディング規約検出: agent.md、claude.md、Cursor rulesなどのコーディングエージェント設定ファイルを自動検出\nCodeRabbit CLI: どこでも、なんでも動作\nターミナルネイティブであるため、CodeRabbit CLIは以下に対応します。\nあらゆるターミナルアプリ/IDE: iTerm2、Ghostty、Neovim、Lazyvim\nあらゆるAIコーディングCLIエージェント: Claude Code、Codex、Cursor、Gemini、OpenCodeなど\nAIコーディングエージェントCLIとの使い方\nCodeRabbit CLIはAIコーディングエージェントとの新しい統合の可能性を広げます。Claude Codeとの動作例は以下の通りです。\nコーディングタスクを進める際、Claude CodeにCodeRabbitを使って発見された問題を修正するよう促すことができます。PRDやタスクリストからコーディングする場合に特に便利です。\n仕様書のフェーズ7.3を実施し、その後に `coderabbit --prompt-only`を実行してください。\n必要なだけバックグラウンドにて実行し、発生した問題を修正してください。\n\n\nClaude Codeはコーディングタスクを進めながら、バックグラウンドでcoderabbit --prompt-onlyを実行します。タイマーを設定してCodeRabbitを定期的に確認する場合もあります。あるいは、ClaudeにCodeRabbitの完了を確認するよう促すこともできます。\n\n\nその後、Claude CodeはCodeRabbitの出力を読み込みます。--prompt-onlyフラグを使うことで、AIエージェントが読み取れるプレーンテキストで出力されます。ClaudeはCodeRabbitが検出した問題ごとにタスクリストを作成します。\n\n\nClaude Codeとの統合や自動化ワークフローについては、CLIドキュメントをご覧ください。\nCLIにはインタラクティブモードとプレーンレスポンスモードの2種類があり、自動化ワークフローへの統合や他ツールへの結果の受け渡しが容易です。\nはじめ方\nCodeRabbit CLIはすでに利用可能です。インストールして最初のレビューを試してみましょう。\n# CodeRabbitをインストール\ncurl -fsSL https://cli.coderabbit.ai/install.sh | sh\n\n# インタラクティブモードでレビュー実行\ncoderabbit",
      "publishedAt": "2025-09-16T23:02:47.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:12.175Z",
      "score": 0.03636306953115807
    },
    {
      "id": "84ddd89dd63d7ec87db32898c3930fec",
      "title": "シリーズbにて6,000万ドルの資金調達：AIによるコーディングの品質ゲートを構築",
      "url": "https://coderabbit.ai/blog/coderabbit-series-b-60-million-quality-gates-for-code-reviews-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/coderabbit-series-b-60-million-quality-gates-for-code-reviews\">Raising our $60 million Series B: Quality gates for AI coding</a>の意訳です。</p>\n<p>CodeRabbitを立ち上げたとき、そのコンセプトはシンプルでした。すべての開発者がコードレビューを嫌っているのだから、もっと速く、簡単にできるようにすればいいのでは、ということです。変数名やスタイル規約について、同じコメントを何度も書くのは誰にとっても楽しいことではありません。</p>\n<p>そこでAIが役立つと考えました。ベストプラクティスのチェックやルールの適用を自動化すれば、開発者自身がやる必要がなくなるのです。そしてさらに重要なのは、AIがセーフティネットとして機能し、本番環境に入る前に問題やバグを検知できることです。</p>\n<p>その信念のもと、私たちは<a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/coderabbit-announces-16m-series-a-funding-led-by-crv\">AIコードレビューという、まったく新しいものを作ることに挑戦しました。</a> その後、AIコーディングツールは広く普及し始めました。Copilot、Claude Code、Cursorといったツールは、開発チームが容易にレビューできる以上のコードを生成するようになり、多くの開発者がPR数を2倍から3倍に増やしました。これにより、すでに抱えていたコードレビューのバックログはさらに増加。私たちはすぐに気づきました。「効率化」と宣伝されていたものが、やがてレビューのボトルネックになることを。</p>\n<p>そこではじめて理解したのです。AIコードレビューは開発チームにとって極めて重要な存在になると。信頼とガバナンスのレイヤーとして機能し、品質とセキュリティを担保しながら、開発者の時間を節約します。そしてボーナスとして、職場での皮肉混じりのレビューコメントも大幅に減らせるのです。</p>\n<h2 id=\"heading-ai2025\"><strong>AIコードレビューが必須となった2025年</strong></h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/UHCTKZYOOYU\">https://youtu.be/UHCTKZYOOYU</a></div>\n<p> </p>\n<p>過去2年間で、私たちは最も包括的かつコンテキストを重視したコードレビュープラットフォームを構築し、200万のリポジトリに導入され、1,300万件のPRをレビューしました。GitHubとGitLabの両方で最もインストールされたAIアプリとなり、数えきれないほどの開発チームの士気を向上させてきました。</p>\n<p>そして2025年、AIコードレビューは、AIコーディングエージェントの普及に伴う課題に直面するすべてのチームにとって必須のものとなっています。この変化は前例のない成長を引き起こし、本日発表した<strong>6,000万ドルのシリーズB資金調達</strong>につながりました。</p>\n<p>今回の投資は<strong>Scale Venture Partners</strong>が主導し、<strong>NVentures（NVIDIAのベンチャーキャピタル部門）が参加。長年の投資家であるCRV、Harmony Partners、Flex Capital、Engineering Capital、Pelion Venture Partners</strong>も支援してくれました。今回の資金調達により、私たちが調達した累計資金は8,800万ドルになりました。</p>\n<h2 id=\"heading-ai\"><strong>なぜ多くのチームがAIコードレビューを導入しているのか</strong></h2>\n<p>チームのすべての開発者がコードをより速く生成するようになると、レビュー待ちのキューは指数関数的に増えます。以前は1日5〜10件のPRをレビューしていたシニアエンジニアが、今では20〜30件を抱えています。計算が合いません。チームは2つの悪い選択肢に直面します。デプロイを遅らせて丁寧にレビューするか、レビューを急いで品質を犠牲にするか。</p>\n<p>だからこそ、AIコードレビューの導入は加速しています。AIレビュアーは人間のレビュアーを補完し、アーキテクチャの判断やビジネスロジック、AIが完全には理解できない文脈を必要とするフィードバックに集中できるようにします。</p>\n<p>この1年は嵐のようでした。売上は10倍になり、チームも倍増しました。その背景には以下の要因があります。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757987176453/46d676d8-f560-4cf9-ba1a-54f2dd91d960.png\" alt class=\"image--center mx-auto\" /></p>\n<p>それぞれの顧客の背後には実際のチームがいて、同じことを感じています。つまり、CodeRabbitでレビューが速くなってバグが早期に発見され、リリースサイクルが再び加速しているということです。</p>\n<p>Grouponは、レビューから本番リリースまでにかかっていた時間が86時間からわずか39分に短縮されたと報告しました。別の企業は、コードレビューに費やす時間を70％削減できたと共有してくれています。</p>\n<h2 id=\"heading-coderabbit\">CodeRabbitの仕組み</h2>\n<p>CodeRabbitは「AIの炎にAIで立ち向かう」からこそ機能します。多数のコンテキスト情報を取り込み、最も文脈に沿ったレビューを提供します。</p>\n<ul>\n<li><p>本番前に正確性やセキュリティの問題を検知</p>\n<p>  <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758028469061/6a3c1175-2500-440b-995a-785d1d3e1234.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><p>組織のベストプラクティスや独自ルールの適用</p>\n<p>  <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758027905155/0990dc8f-10d7-4765-88f9-e3023b476193.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><p>マージサイクル全体をサポート（ユニットテストやdocstring生成など）</p>\n</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757987618430/0854e5be-e8dd-456a-8049-c73526d54f74.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-coderabbit-cli\">調達を祝して：CodeRabbit CLIの発表</h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/IqBKf4u5MtA\">https://youtu.be/IqBKf4u5MtA</a></div>\n<p> </p>\n<p>本日、シリーズBの発表を記念して、CodeRabbit CLIを発表します。これはターミナル上で動作するAIコードレビューで、Claude Code、Codex CLI、Cursor CLI、GeminiなどのAIコーディングエージェントとシームレスに連携します。</p>\n<p>開発者がCLIベースのコーディングエージェントを使ってコードを書くケースが増える中、私たちは大きなギャップを特定しました。コードはかつてない速度で生成されていますが、品質検証が行われるのは遅く、PRの段階になってからということが多いのです。</p>\n<p>CodeRabbit CLIはこれを変えます。CLIワークフローに直接インテリジェントなレビューを組み込み、コード生成と品質検証の間にリアルタイムのフィードバックループを作り出します。</p>\n<p>モジュールをリファクタリングするようClaude Codeに依頼しても、Cursor CLIで新機能を実装しても、CodeRabbitは即座にその結果をレビューし、ハルシネーション生成を検知し、セキュリティ問題にフラグを立て、AIエージェントに文脈に沿った修正を返すことさえできます。</p>\n<p>CodeRabbit CLIは、AI生成コードを本番レベルに引き上げるために欠けていたオーケストレーションレイヤーであり、自律的な開発の実現を可能にします。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757974301767/02dc9e2f-1d4d-4308-a424-231c982850ea.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-kirku4rlm57jga7os4fph5hoqrpgztjgyzmhilkbpjgznjgovjgoljga7vvijjgyljgarjgzjgavjgajjgapjgabjgolvvikqkg\"><strong>今回の資金調達が意味するもの（あなたにとっても）</strong></h2>\n<p>シリーズBで調達した資金は、私たちが解決すべき課題のスケールに合わせて成長を続けるために使われます。投資先は以下の通りです。</p>\n<ul>\n<li><p><strong>製品開発の加速：</strong> コンテキスト統合の強化、よりスマートなマージ前チェック、自動テストなど、ロードマップは満載です。レビューをより速く、正確で、有用にすることに集中します。</p>\n</li>\n<li><p><strong>オープンソースの支援：</strong> 現在、すでに10万以上のOSSプロジェクトがCodeRabbitを利用しています。この資金で、貢献や支援をさらに強化し、現代的な開発を可能にしたコミュニティをサポートします。詳細は今週後半に！</p>\n</li>\n<li><p><strong>優秀な人材の採用：</strong> 今年だけで従業員数を倍増させました。今後はエンジニアリング、プロダクト、セールス、マーケティング、カスタマーサクセスの分野でグローバルに採用を進めます。</p>\n</li>\n</ul>\n<p>この資金調達により、私たちが「AI駆動開発における最も重要な欠けているピース」だと信じている、スケーラブルで文脈に対応したレビューを構築し続ける余地が生まれました。</p>\n<h2 id=\"heading-kirjgztmlkmj7tjgyljgorjgyzjgajjgybjgztjgzbjgytjgb7jgzkqkg\"><strong>ご支援ありがとうございます</strong></h2>\n<p>この会社を始めたとき、私たちはすべてのエンジニアが経験する課題に挑戦していることを理解していました。レビューは面倒で、簡単にはスケールしません。その課題にCodeRabbitが今や数千のチームを支援できていることは、謙虚であると同時に大きな力を与えてくれます。</p>\n<p>顧客、コミュニティ、投資家の皆さまへ：私たちを信じ、一緒に築いてくださりありがとうございます。そしてこの取り組みにワクワクする方は、ぜひ私たちに加わってください。コードレビューの未来を一緒に作りましょう。</p>\n<p><a target=\"_blank\" href=\"https://coderabbit.link/KWGzOUS\"><strong><em>CodeRabbitを無料で試す</em></strong></a> <strong><em>そして</em></strong> <a target=\"_blank\" href=\"https://coderabbit.link/OG1OZk3\"><strong><em>採用情報はこちら</em></strong></a></p>\n",
      "summary": "Raising our $60 million Series B: Quality gates for AI codingの意訳です。\nCodeRabbitを立ち上げたとき、そのコンセプトはシンプルでした。すべての開発者がコードレビューを嫌っているのだから、もっと速く、簡単にできるようにすればいいのでは、ということです。変数名やスタイル規約について、同じコメントを何度も書くのは誰にとっても楽しいことではありません。\nそこでAIが役立つと考えました。ベストプラクティスのチェックやルールの適用を自動化すれば、開発者自身がやる必要がなくなるのです。そしてさらに重要なのは、AIがセーフティネットとして機能し、本番環境に入る前に問題やバグを検知できることです。\nその信念のもと、私たちはAIコードレビューという、まったく新しいものを作ることに挑戦しました。 その後、AIコーディングツールは広く普及し始めました。Copilot、Claude Code、Cursorといったツールは、開発チームが容易にレビューできる以上のコードを生成するようになり、多くの開発者がPR数を2倍から3倍に増やしました。これにより、すでに抱えていたコードレビューのバックログはさらに増加。私たちはすぐに気づきました。「効率化」と宣伝されていたものが、やがてレビューのボトルネックになることを。\nそこではじめて理解したのです。AIコードレビューは開発チームにとって極めて重要な存在になると。信頼とガバナンスのレイヤーとして機能し、品質とセキュリティを担保しながら、開発者の時間を節約します。そしてボーナスとして、職場での皮肉混じりのレビューコメントも大幅に減らせるのです。\nAIコードレビューが必須となった2025年\n\n\n\nhttps://youtu.be/UHCTKZYOOYU\n \n過去2年間で、私たちは最も包括的かつコンテキストを重視したコードレビュープラットフォームを構築し、200万のリポジトリに導入され、1,300万件のPRをレビューしました。GitHubとGitLabの両方で最もインストールされたAIアプリとなり、数えきれないほどの開発チームの士気を向上させてきました。\nそして2025年、AIコードレビューは、AIコーディングエージェントの普及に伴う課題に直面するすべてのチームにとって必須のものとなっています。この変化は前例のない成長を引き起こし、本日発表した6,000万ドルのシリーズB資金調達につながりました。\n今回の投資はScale Venture Partnersが主導し、NVentures（NVIDIAのベンチャーキャピタル部門）が参加。長年の投資家であるCRV、Harmony Partners、Flex Capital、Engineering Capital、Pelion Venture Partnersも支援してくれました。今回の資金調達により、私たちが調達した累計資金は8,800万ドルになりました。\nなぜ多くのチームがAIコードレビューを導入しているのか\nチームのすべての開発者がコードをより速く生成するようになると、レビュー待ちのキューは指数関数的に増えます。以前は1日5〜10件のPRをレビューしていたシニアエンジニアが、今では20〜30件を抱えています。計算が合いません。チームは2つの悪い選択肢に直面します。デプロイを遅らせて丁寧にレビューするか、レビューを急いで品質を犠牲にするか。\nだからこそ、AIコードレビューの導入は加速しています。AIレビュアーは人間のレビュアーを補完し、アーキテクチャの判断やビジネスロジック、AIが完全には理解できない文脈を必要とするフィードバックに集中できるようにします。\nこの1年は嵐のようでした。売上は10倍になり、チームも倍増しました。その背景には以下の要因があります。\n\nそれぞれの顧客の背後には実際のチームがいて、同じことを感じています。つまり、CodeRabbitでレビューが速くなってバグが早期に発見され、リリースサイクルが再び加速しているということです。\nGrouponは、レビューから本番リリースまでにかかっていた時間が86時間からわずか39分に短縮されたと報告しました。別の企業は、コードレビューに費やす時間を70％削減できたと共有してくれています。\nCodeRabbitの仕組み\nCodeRabbitは「AIの炎にAIで立ち向かう」からこそ機能します。多数のコンテキスト情報を取り込み、最も文脈に沿ったレビューを提供します。\n本番前に正確性やセキュリティの問題を検知\n  \n組織のベストプラクティスや独自ルールの適用\n  \nマージサイクル全体をサポート（ユニットテストやdocstring生成など）\n\n調達を祝して：CodeRabbit CLIの発表\n\n\n\nhttps://youtu.be/IqBKf4u5MtA\n \n本日、シリーズBの発表を記念して、CodeRabbit CLIを発表します。これはターミナル上で動作するAIコードレビューで、Claude Code、Codex CLI、Cursor CLI、GeminiなどのAIコーディングエージェントとシームレスに連携します。\n開発者がCLIベースのコーディングエージェントを使ってコードを書くケースが増える中、私たちは大きなギャップを特定しました。コードはかつてない速度で生成されていますが、品質検証が行われるのは遅く、PRの段階になってからということが多いのです。\nCodeRabbit CLIはこれを変えます。CLIワークフローに直接インテリジェントなレビューを組み込み、コード生成と品質検証の間にリアルタイムのフィードバックループを作り出します。\nモジュールをリファクタリングするようClaude Codeに依頼しても、Cursor CLIで新機能を実装しても、CodeRabbitは即座にその結果をレビューし、ハルシネーション生成を検知し、セキュリティ問題にフラグを立て、AIエージェントに文脈に沿った修正を返すことさえできます。\nCodeRabbit CLIは、AI生成コードを本番レベルに引き上げるために欠けていたオーケストレーションレイヤーであり、自律的な開発の実現を可能にします。\n\n今回の資金調達が意味するもの（あなたにとっても）\nシリーズBで調達した資金は、私たちが解決すべき課題のスケールに合わせて成長を続けるために使われます。投資先は以下の通りです。\n製品開発の加速： コンテキスト統合の強化、よりスマートなマージ前チェック、自動テストなど、ロードマップは満載です。レビューをより速く、正確で、有用にすることに集中します。\nオープンソースの支援： 現在、すでに10万以上のOSSプロジェクトがCodeRabbitを利用しています。この資金で、貢献や支援をさらに強化し、現代的な開発を可能にしたコミュニティをサポートします。詳細は今週後半に！\n優秀な人材の採用： 今年だけで従業員数を倍増させました。今後はエンジニアリング、プロダクト、セールス、マーケティング、カスタマーサクセスの分野でグローバルに採用を進めます。\nこの資金調達により、私たちが「AI駆動開発における最も重要な欠けているピース」だと信じている、スケーラブルで文脈に対応したレビューを構築し続ける余地が生まれました。\nご支援ありがとうございます\nこの会社を始めたとき、私たちはすべてのエンジニアが経験する課題に挑戦していることを理解していました。レビューは面倒で、簡単にはスケールしません。その課題にCodeRabbitが今や数千のチームを支援できていることは、謙虚であると同時に大きな力を与えてくれます。\n顧客、コミュニティ、投資家の皆さまへ：私たちを信じ、一緒に築いてくださりありがとうございます。そしてこの取り組みにワクワクする方は、ぜひ私たちに加わってください。コードレビューの未来を一緒に作りましょう。\nCodeRabbitを無料で試す そして 採用情報はこちら",
      "publishedAt": "2025-09-16T22:46:14.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "funding_mna",
      "tags": [
        "code_review",
        "documentation"
      ],
      "ingestedAt": "2025-12-04T14:17:12.175Z",
      "score": 0.03996655313621685
    },
    {
      "id": "7d456380d0e7c97c41fc0a4ae5513a5d",
      "title": "CodeRabbit CLI - Free AI code reviews in your CLI",
      "url": "https://coderabbit.ai/blog/coderabbit-cli-free-ai-code-reviews-in-your-cli",
      "content": "<p>CodeRabbit started with AI-powered code reviews in pull requests. In May, we brought that same intelligence to <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/ai-code-reviews-vscode-cursor-windsurf\">VS Code, Cursor, and Windsurf</a>. Now, we're extending the AI code reviews developers love into the command line with CodeRabbit CLI. In case you’re wondering, that makes us the most comprehensive AI code review tool available. We work everywhere you work.</p>\n<p><em>CodeRabbit CLI</em> helps devs perform self-reviews of code directly in their terminal. By providing automated, intelligent code analysis capabilities, it empowers developers to catch issues early, maintain consistent code standards, and make coding autonomous through seamless integration with AI coding agents in the CLI.</p>\n<h2 id=\"heading-vibe-checking-your-code-now-in-cli\"><strong>Vibe checking your code – now in CLI</strong></h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/IqBKf4u5MtA\">https://youtu.be/IqBKf4u5MtA</a></div>\n<p> </p>\n<p>CodeRabbit CLI delivers the same comprehensive analysis that makes our PR and IDE reviews effective at catching bugs early. CodeRabbit CLI is free to use with rate limits but with a Pro plan you can enjoy much higher limits and additional features, including:</p>\n<ul>\n<li><p><strong>Context-aware analysis</strong>: Leverages your Git integration to synthesize insights from 40+ sources including static analysis tools, security scanners, and our codegraph relationship feature for the most comprehensive reviews.</p>\n</li>\n<li><p><strong>Pre-commit reviews</strong>: Analyze changes before they leave your machine for multi-layered reviews.</p>\n</li>\n<li><p><strong>One-click fixes</strong>: Apply simple fixes instantly or send complex issues to AI agents with full context hand-off.</p>\n</li>\n<li><p><strong>Coding guidelines</strong>: Auto-detects agent.md, claude.md, Cursor rules, and other coding agent configuration files.</p>\n</li>\n</ul>\n<h2 id=\"heading-coderabbit-cli-works-everywhere-with-everything\"><strong>CodeRabbit CLI: Works everywhere, with everything</strong></h2>\n<p>Terminal-native means CodeRabbit CLI works with:</p>\n<ul>\n<li><p><strong>Any Terminal App/IDE:</strong> iTerm2, Ghostty, Neovim, Lazyvim</p>\n</li>\n<li><p><strong>Any AI Coding CLI agent</strong>: Claude Code, Codex, Cursor, Gemini, OpenCode and more</p>\n</li>\n</ul>\n<h2 id=\"heading-how-to-use-coderabbit-cli-with-ai-coding-agent-cli\"><strong>How to use CodeRabbit CLI with AI Coding Agent CLI</strong></h2>\n<p>The CodeRabbit CLI opens up new integration possibilities with AI coding agents. Here's how it works with Claude Code:</p>\n<ol>\n<li>While working on a coding task, you can prompt Claude Code to use CodeRabbit and to fix any issues it finds. This is particularly useful if it’s coding from a PRD, or a tasklist.</li>\n</ol>\n<pre><code class=\"lang-plaintext\">Please implement phase 7.3 of the planning doc and then run coderabbit --prompt-only, let it run as long as it needs (run it in the background) and fix any issues.\n</code></pre>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971361399/2f4398b2-4a89-4fd3-9d1a-3baf00a0fe6e.png\" alt class=\"image--center mx-auto\" /></p>\n<p>2. Claude Code will carry on the coding task and run <code>coderabbit --prompt-only</code> in the background. It may setup a timer interval to check on CodeRabbit. Alternatively, you can also prompt Claude to check if CodeRabbit is complete.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971331440/fef10dc6-b3a7-4da7-935c-41c06c95f04a.png\" alt class=\"image--center mx-auto\" /></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971376053/33dda5e8-8ede-4239-b363-16b491fe5a02.png\" alt=\"CodeRabbit running in the background within Claude Code\" class=\"image--center mx-auto\" /></p>\n<p>3. Claude Code will then read the output of CodeRabbit which, by using the <code>--prompt-only</code> flag, provides the output as plain text with prompts for AI agents to read. Claude will then create a tasklist addressing each of the issues surfaced by CodeRabbit.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971316135/ebe0c883-be63-4e77-9e67-234694047ea0.png\" alt class=\"image--center mx-auto\" /></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757971352863/2003e70c-4ad0-4435-95fb-df2a0885b9f7.png\" alt class=\"image--center mx-auto\" /></p>\n<p>For Claude Code integration and automated workflows, check the <a target=\"_blank\" href=\"https://docs.coderabbit.ai/cli\">CLI documentation</a> <a target=\"_blank\" href=\"https://docs.coderabbit.ai/cli\">for setup.</a></p>\n<p>The CLI has two modes: interactive and plain response , making it easy to integrate into automated workflows or pass results to other tools.</p>\n<h2 id=\"heading-getting-started\"><strong>Getting started</strong></h2>\n<p>CodeRabbit CLI is available now. Install and try your first review:</p>\n<pre><code class=\"lang-powershell\"><span class=\"hljs-comment\">#install CodeRabbit</span>\n<span class=\"hljs-built_in\">curl</span> <span class=\"hljs-literal\">-fsSL</span> https://cli.coderabbit.ai/install.sh | sh\n\n<span class=\"hljs-comment\">#Run a review in interactive mode</span>\ncoderabbit\n</code></pre>\n",
      "summary": "CodeRabbit started with AI-powered code reviews in pull requests. In May, we brought that same intelligence to VS Code, Cursor, and Windsurf. Now, we're extending the AI code reviews developers love into the command line with CodeRabbit CLI. In case you’re wondering, that makes us the most comprehensive AI code review tool available. We work everywhere you work.\nCodeRabbit CLI helps devs perform self-reviews of code directly in their terminal. By providing automated, intelligent code analysis capabilities, it empowers developers to catch issues early, maintain consistent code standards, and make coding autonomous through seamless integration with AI coding agents in the CLI.\nVibe checking your code – now in CLI\n\n\n\nhttps://youtu.be/IqBKf4u5MtA\n \nCodeRabbit CLI delivers the same comprehensive analysis that makes our PR and IDE reviews effective at catching bugs early. CodeRabbit CLI is free to use with rate limits but with a Pro plan you can enjoy much higher limits and additional features, including:\nContext-aware analysis: Leverages your Git integration to synthesize insights from 40+ sources including static analysis tools, security scanners, and our codegraph relationship feature for the most comprehensive reviews.\nPre-commit reviews: Analyze changes before they leave your machine for multi-layered reviews.\nOne-click fixes: Apply simple fixes instantly or send complex issues to AI agents with full context hand-off.\nCoding guidelines: Auto-detects agent.md, claude.md, Cursor rules, and other coding agent configuration files.\nCodeRabbit CLI: Works everywhere, with everything\nTerminal-native means CodeRabbit CLI works with:\nAny Terminal App/IDE: iTerm2, Ghostty, Neovim, Lazyvim\nAny AI Coding CLI agent: Claude Code, Codex, Cursor, Gemini, OpenCode and more\nHow to use CodeRabbit CLI with AI Coding Agent CLI\nThe CodeRabbit CLI opens up new integration possibilities with AI coding agents. Here's how it works with Claude Code:\nWhile working on a coding task, you can prompt Claude Code to use CodeRabbit and to fix any issues it finds. This is particularly useful if it’s coding from a PRD, or a tasklist.\nPlease implement phase 7.3 of the planning doc and then run coderabbit --prompt-only, let it run as long as it needs (run it in the background) and fix any issues.\n\n\n2. Claude Code will carry on the coding task and run coderabbit --prompt-only in the background. It may setup a timer interval to check on CodeRabbit. Alternatively, you can also prompt Claude to check if CodeRabbit is complete.\n\n\n3. Claude Code will then read the output of CodeRabbit which, by using the --prompt-only flag, provides the output as plain text with prompts for AI agents to read. Claude will then create a tasklist addressing each of the issues surfaced by CodeRabbit.\n\n\nFor Claude Code integration and automated workflows, check the CLI documentation for setup.\nThe CLI has two modes: interactive and plain response , making it easy to integrate into automated workflows or pass results to other tools.\nGetting started\nCodeRabbit CLI is available now. Install and try your first review:\n#install CodeRabbit\ncurl -fsSL https://cli.coderabbit.ai/install.sh | sh\n\n#Run a review in interactive mode\ncoderabbit",
      "publishedAt": "2025-09-16T12:59:39.000Z",
      "author": "Edgar Cerecerez",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "documentation",
        "retrieval",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:12.176Z",
      "score": 0.07234715651991229
    },
    {
      "id": "6d1d2623cc15f8a52bd97b6f45cc0c41",
      "title": "Raising our $60M Series B: Building the quality gates for AI-powered coding",
      "url": "https://coderabbit.ai/blog/coderabbit-series-b-60-million-quality-gates-for-code-reviews",
      "content": "<p>When we started CodeRabbit, the idea was pretty simple: since all developers hate code reviews, why not make them faster and easier? After all, no one enjoys leaving the same comment about variable naming practices or style conventions for the tenth time in a week.</p>\n<p>That’s where we believed AI could help – it could automate best-practice checks and policy enforcement so that devs didn’t have to do it themselves. But more importantly, it could act as a safety net, catching issues and bugs before they made it into production.</p>\n<p>With that belief, we set out to <a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/coderabbit-announces-16m-series-a-funding-led-by-crv\">create something new: AI code reviews.</a> Over time, AI coding tools started to gain broader adoption. Tools like Copilot, Claude Code, and Cursor began spitting out more code than teams could easily review with many developers increasing the number of PRs they shipped by 2x to 3x. This added to the existing code review backlogs many teams had. We quickly realized that the ‘efficiency’ gains being marketed to engineering teams would swiftly turn into code review bottlenecks.</p>\n<p>And that’s also when we first realized how critical AI code reviews would be to development teams. They would function as a trust and governance layer in agentic software development ensuring quality and security while saving devs time. And, as an added bonus, greatly reducing passive aggressive review comments in the workplace!</p>\n<h2 id=\"heading-ai-code-reviews-became-essential-in-2025\"><strong>AI code reviews became essential in 2025</strong></h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/UHCTKZYOOYU\">https://youtu.be/UHCTKZYOOYU</a></div>\n<p> </p>\n<p>Over the last two years, we’ve built the most comprehensive and context-rich platform for code reviews, been installed on 2 million repos, reviewed 13 million pull requests, become the most installed AI App on both GitHub and GitLab, and improved the morale of countless dev teams.</p>\n<p>In 2025, we watched AI code reviews become essential for all teams dealing with the challenges that come with the broad adoption of AI coding agents. But that shift fueled a year of unprecedented growth, culminating in the <strong>$60 million Series B round</strong> that we announced today.</p>\n<p>This investment was led by <strong>Scale Venture Partners</strong> with participation by <strong>NVentures (NVIDIA’s venture capital arm)</strong> and support from our long-time investors <strong>CRV, Harmony Partners, Flex Capital, Engineering Capital, and Pelion Venture Partners</strong>. With this new funding, our total capital raised is now $88 million.</p>\n<h2 id=\"heading-why-so-many-teams-are-adopting-ai-code-reviews\"><strong>Why so many teams are adopting AI code reviews</strong></h2>\n<p>When every developer on your team is generating code faster, your review queue grows exponentially. Senior engineers who used to review 5 to 10 PRs a day are now facing 20 to 30. The math doesn't work. Teams are caught between two bad options: either slow down deployment cycles waiting for thorough reviews, or rush reviews and let quality slip.</p>\n<p>This is why AI code review adoption is accelerating. AI reviewers augment the human reviewers, freeing them to focus on architecture decisions, business logic, and the nuanced feedback that requires context AI can't fully grasp yet.</p>\n<p>The past year has been a whirlwind. We’ve 10x revenue and doubled our team thanks to:</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757987176453/46d676d8-f560-4cf9-ba1a-54f2dd91d960.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Behind each of those customers are real teams who tell us the same thing: reviews are faster with CodeRabbit, bugs are caught earlier, and release cycles are finally speeding up again.</p>\n<p>Groupon told us they went from 86 hours from review-to-production down to just 39 minutes. Another shared that they cut down the time they spend on code reviews by 70%.</p>\n<h2 id=\"heading-how-coderabbit-works\">How CodeRabbit works</h2>\n<p>CodeRabbit works because it fights AI fire with AI fire. Our platform brings in dozens of points of context to deliver the most context aware reviews to:</p>\n<ul>\n<li><p>Catch correctness and security issues before they hit production.</p>\n<p>  <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758028469061/6a3c1175-2500-440b-995a-785d1d3e1234.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n</ul>\n<ul>\n<li><p>Enforce organizational best practices and custom policies.</p>\n<p>  <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1758027905155/0990dc8f-10d7-4765-88f9-e3023b476193.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n</ul>\n<ul>\n<li>Support the full merge cycle with unit testing and docstrings generation.</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757987618430/0854e5be-e8dd-456a-8049-c73526d54f74.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-how-were-celebrating-by-announcing-coderabbit-cli\">How we’re celebrating: By announcing CodeRabbit CLI</h2>\n<div class=\"embed-wrapper\"><div class=\"embed-loading\"><div class=\"loadingRow\"></div><div class=\"loadingRow\"></div></div><a class=\"embed-card\" href=\"https://youtu.be/IqBKf4u5MtA\">https://youtu.be/IqBKf4u5MtA</a></div>\n<p> </p>\n<p>Today, we're celebrating our Series B by announcing CodeRabbit CLI, AI code reviews that live in your terminal and orchestrate seamlessly with Claude Code, Codex CLI, Cursor CLI, Gemini, and other AI coding agents.</p>\n<p>As developers increasingly write code through CLI Coding agents, we've identified a critical gap: code is being generated at unprecedented speeds, but quality validation happens too late, often only at the PR stage.</p>\n<p>CodeRabbit CLI changes this by bringing intelligent review directly into the CLI workflow, creating a real-time feedback loop between code generation and validation.</p>\n<p>Now, whether you're prompting Claude Code to refactor a module or using Cursor CLI to implement a feature, CodeRabbit instantly reviews the output, catches hallucinations, flags security issues, and even hands contextualized fixes back to your AI agent.</p>\n<p>CodeRabbit CLI is the missing orchestration layer that makes AI-generated code production-ready, turning the promise of autonomous development into reality.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757974301767/02dc9e2f-1d4d-4308-a424-231c982850ea.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-what-this-funding-means-for-us-and-for-you\"><strong>What this funding means for us (and for you)</strong></h2>\n<p>Our Series B round will help us keep pace with the scale of the problem we set out to solve. Here’s where we’re putting that investment:</p>\n<ul>\n<li><p><strong>Accelerating product development:</strong> Our roadmap is packed. From deeper context integrations to smarter pre-merge checks and automated testing, we’re focused on making reviews faster, more accurate, and more useful for every team.</p>\n</li>\n<li><p><strong>Supporting open source:</strong> Today, more than 100,000 OSS projects already use CodeRabbit. With this funding, we’re doubling down on contributions and support to strengthen the community that made modern development possible. More on that later in the week!</p>\n</li>\n<li><p><strong>Hiring the best talent:</strong> We’ve already doubled headcount this year and we’re hiring globally across engineering, product, sales, marketing, and customer success.</p>\n</li>\n</ul>\n<p>This funding gives us the space to keep building what we believe is the most important missing piece of AI-powered development: scalable, context-aware reviews.</p>\n<h2 id=\"heading-thank-you-for-all-your-support\"><strong>Thank you for all your support</strong></h2>\n<p>When we started this company, we knew we were chasing a problem every engineer experiences: reviews are a pain and they don’t scale easily. The fact that CodeRabbit is now helping thousands of teams tackle that problem is both humbling and energizing.</p>\n<p>To our customers, community, and investors: thank you for believing in us and building alongside us. And if this work excites you, consider joining us. Come help us build the future of code reviews.</p>\n<p><a target=\"_blank\" href=\"https://coderabbit.link/KWGzOUS\"><strong><em>Try CodeRabbit for free</em></strong></a> <strong><em>yourself and learn more about our</em></strong> <a target=\"_blank\" href=\"https://coderabbit.link/OG1OZk3\"><strong><em>open roles.</em></strong></a></p>\n",
      "summary": "When we started CodeRabbit, the idea was pretty simple: since all developers hate code reviews, why not make them faster and easier? After all, no one enjoys leaving the same comment about variable naming practices or style conventions for the tenth time in a week.\nThat’s where we believed AI could help – it could automate best-practice checks and policy enforcement so that devs didn’t have to do it themselves. But more importantly, it could act as a safety net, catching issues and bugs before they made it into production.\nWith that belief, we set out to create something new: AI code reviews. Over time, AI coding tools started to gain broader adoption. Tools like Copilot, Claude Code, and Cursor began spitting out more code than teams could easily review with many developers increasing the number of PRs they shipped by 2x to 3x. This added to the existing code review backlogs many teams had. We quickly realized that the ‘efficiency’ gains being marketed to engineering teams would swiftly turn into code review bottlenecks.\nAnd that’s also when we first realized how critical AI code reviews would be to development teams. They would function as a trust and governance layer in agentic software development ensuring quality and security while saving devs time. And, as an added bonus, greatly reducing passive aggressive review comments in the workplace!\nAI code reviews became essential in 2025\n\n\n\nhttps://youtu.be/UHCTKZYOOYU\n \nOver the last two years, we’ve built the most comprehensive and context-rich platform for code reviews, been installed on 2 million repos, reviewed 13 million pull requests, become the most installed AI App on both GitHub and GitLab, and improved the morale of countless dev teams.\nIn 2025, we watched AI code reviews become essential for all teams dealing with the challenges that come with the broad adoption of AI coding agents. But that shift fueled a year of unprecedented growth, culminating in the $60 million Series B round that we announced today.\nThis investment was led by Scale Venture Partners with participation by NVentures (NVIDIA’s venture capital arm) and support from our long-time investors CRV, Harmony Partners, Flex Capital, Engineering Capital, and Pelion Venture Partners. With this new funding, our total capital raised is now $88 million.\nWhy so many teams are adopting AI code reviews\nWhen every developer on your team is generating code faster, your review queue grows exponentially. Senior engineers who used to review 5 to 10 PRs a day are now facing 20 to 30. The math doesn't work. Teams are caught between two bad options: either slow down deployment cycles waiting for thorough reviews, or rush reviews and let quality slip.\nThis is why AI code review adoption is accelerating. AI reviewers augment the human reviewers, freeing them to focus on architecture decisions, business logic, and the nuanced feedback that requires context AI can't fully grasp yet.\nThe past year has been a whirlwind. We’ve 10x revenue and doubled our team thanks to:\n\nBehind each of those customers are real teams who tell us the same thing: reviews are faster with CodeRabbit, bugs are caught earlier, and release cycles are finally speeding up again.\nGroupon told us they went from 86 hours from review-to-production down to just 39 minutes. Another shared that they cut down the time they spend on code reviews by 70%.\nHow CodeRabbit works\nCodeRabbit works because it fights AI fire with AI fire. Our platform brings in dozens of points of context to deliver the most context aware reviews to:\nCatch correctness and security issues before they hit production.\n  \nEnforce organizational best practices and custom policies.\n  \nSupport the full merge cycle with unit testing and docstrings generation.\n\nHow we’re celebrating: By announcing CodeRabbit CLI\n\n\n\nhttps://youtu.be/IqBKf4u5MtA\n \nToday, we're celebrating our Series B by announcing CodeRabbit CLI, AI code reviews that live in your terminal and orchestrate seamlessly with Claude Code, Codex CLI, Cursor CLI, Gemini, and other AI coding agents.\nAs developers increasingly write code through CLI Coding agents, we've identified a critical gap: code is being generated at unprecedented speeds, but quality validation happens too late, often only at the PR stage.\nCodeRabbit CLI changes this by bringing intelligent review directly into the CLI workflow, creating a real-time feedback loop between code generation and validation.\nNow, whether you're prompting Claude Code to refactor a module or using Cursor CLI to implement a feature, CodeRabbit instantly reviews the output, catches hallucinations, flags security issues, and even hands contextualized fixes back to your AI agent.\nCodeRabbit CLI is the missing orchestration layer that makes AI-generated code production-ready, turning the promise of autonomous development into reality.\n\nWhat this funding means for us (and for you)\nOur Series B round will help us keep pace with the scale of the problem we set out to solve. Here’s where we’re putting that investment:\nAccelerating product development: Our roadmap is packed. From deeper context integrations to smarter pre-merge checks and automated testing, we’re focused on making reviews faster, more accurate, and more useful for every team.\nSupporting open source: Today, more than 100,000 OSS projects already use CodeRabbit. With this funding, we’re doubling down on contributions and support to strengthen the community that made modern development possible. More on that later in the week!\nHiring the best talent: We’ve already doubled headcount this year and we’re hiring globally across engineering, product, sales, marketing, and customer success.\nThis funding gives us the space to keep building what we believe is the most important missing piece of AI-powered development: scalable, context-aware reviews.\nThank you for all your support\nWhen we started this company, we knew we were chasing a problem every engineer experiences: reviews are a pain and they don’t scale easily. The fact that CodeRabbit is now helping thousands of teams tackle that problem is both humbling and energizing.\nTo our customers, community, and investors: thank you for believing in us and building alongside us. And if this work excites you, consider joining us. Come help us build the future of code reviews.\nTry CodeRabbit for free yourself and learn more about our open roles.",
      "publishedAt": "2025-09-16T12:55:41.000Z",
      "author": "Harjot Gill",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "documentation",
        "agents",
        "ide",
        "testing",
        "governance"
      ],
      "ingestedAt": "2025-12-04T14:17:12.176Z",
      "score": 0.08997509927594136
    },
    {
      "id": "5257207d4fdde5650d1feef3190f2e1e",
      "title": "AIレビューを効率化するために生まれたOSSツール「reviewtask」とCodeRabbitの最高な相性",
      "url": "https://coderabbit.ai/blog/how-reviewtask-uses-coderabbit-ja",
      "content": "<p><a target=\"_blank\" href=\"https://x.com/biwakonbu\">Ryo HIGASHIGAWA</a>さんは、OSSとして「<a target=\"_blank\" href=\"https://biwakonbu.github.io/reviewtask/\">reviewtask</a>」というレビュー支援ツールを一人で開発されています。このツールは、AIが生成したコードに対して発生する膨大な指摘事項を、効率的かつ正確に管理するために設計されたものです。Ryoさん自身がAIによるコードレビューを日常的に活用し、その運用課題に直面する中で生まれた実践的なプロダクトとなっています。</p>\n<p>もともとは、GitHubのレビューコメントをAIで取得し、タスクに変換して管理するというアプローチを試していたものの、精度や手間の問題が大きく、より安定した運用を目指してreviewtaskの開発が始まりました。そんな背景を持つRyo HIGASHIGAWAさんに、reviewtaskとCodeRabbit活用についてお話を伺いました。</p>\n<h2 id=\"heading-reviewtask\"><strong>reviewtaskの開発体制について</strong></h2>\n<p>reviewtaskはRyoさんが開発していますが、コード自体はほぼすべてAIによって生成されています。Ryoさん自身は、開発タスクの設計やバグ報告など、プロダクトマネージャーのような立場に徹し、コードを書く作業をAIに委ねています。</p>\n<p>Git操作やPull Request、Issue作成などの多くもAIに任せており、自らは開発プロセスの全体像を見ながらプロジェクトを前に進める役割に集中しているとのことです。</p>\n<h2 id=\"heading-kirjgrpjg7zjg4njg6zjg5pjg6xjg7zjgavplqljgznjgovoqrlpoywqkg\"><strong>コードレビューに関する課題</strong></h2>\n<p>AIによるコード生成を大量に行う中で、課題となったのがレビュー品質の担保でした。生成されたコードは量が多く、すべてを人の手でレビューするのは現実的ではなく、疲労感とボトルネックを生み出していたそうです。</p>\n<p>AIにコードを書かせることで生産性は大きく向上しましたが、その反面、レビューと品質管理にかかる時間と労力が爆発的に増加。最終的には、レビューまでもAIに任せられないかと模索するようになったといいます。</p>\n<p>「AIに書かせて自分がレビューしてとやっていると、非常に疲れるなというのが問題感としてありました」</p>\n<h2 id=\"heading-coderabbit\"><strong>CodeRabbitとの出会い</strong></h2>\n<p>RyoさんがCodeRabbitに出会ったきっかけは、他のAI開発支援ツールとの比較や試行の中でのことでした。当時はDevinやCursorなどのツールを並行して使用し、ドキュメントやレビューの自動化に取り組んでいたそうです。</p>\n<p>AIに仕様を理解させ、それに沿ったレビューやチェックを実現したいという強い思いから、試行錯誤を重ねる中で、CodeRabbitの精度や柔軟性に魅力を感じて導入に至りました。</p>\n<p>「レビューの指摘の対応やPRの状況の確認などにも利用できるので非常に柔軟なツールなところが気にいっています」</p>\n<h2 id=\"heading-coderabbit-1\"><strong>CodeRabbit導入の決定要因</strong></h2>\n<p>CodeRabbitを導入する決め手となったのは、レビューの質だけでなく、ツールとの対話ができる点だったといいます。指摘を受けたくないポイントを説明すれば理解してくれる柔軟性や、プロジェクトごとのカスタマイズ性が大きな魅力でした。</p>\n<p>また、単にレビューコメントを生成するだけでなく、レビュー後のフローに組み込みやすい点も導入を後押ししたそうです。</p>\n<p>「プロダクトの都合上、指摘を入れて欲しくない所は説明すれば学習してくれるのが嬉しいですね」</p>\n<h2 id=\"heading-coderabbit-2\"><strong>CodeRabbitの運用状況・効果</strong></h2>\n<p>現在では、reviewtaskやその他のプロジェクトにおいて、CodeRabbitによるレビューを標準フローとして組み込んでくれています。レビュー品質の維持と同時に、設計やドキュメント作成に集中できる時間が確保され、結果として開発効率が大きく向上しました。</p>\n<p>レビュー指摘の管理にはAIツールとの連携や自作ツールを駆使し、指摘をTODO化して確実に対応していくプロセスが構築されています。複数のプロジェクトを同時に進める現在の開発スタイルは、CodeRabbitの存在抜きには成り立たないといいます。</p>\n<p>「今はなくてはならないパートナーという感じです」</p>\n<h2 id=\"heading-kirlrpli5njgafjga7liknnlkgqkg\"><strong>実務での利用</strong></h2>\n<p>OSS開発だけでなく、業務での開発プロジェクトにおいてもCodeRabbitを導入し、レビューの効率化を図っています。特にレビューコストの高いチームにとっては、CodeRabbitが先に自動で指摘を洗い出してくれることで、人的リソースの負担が大きく軽減されました。</p>\n<p>導入後は、レビューの流れそのものが変わり、指摘が先に潰された状態でレビュワーに渡るため、確認作業の集中と精度向上につながっているとのことです。</p>\n<p>「ワークフローが完全に変わった感じがして良い評価が開発メンバーからも上がっています」</p>\n<h2 id=\"heading-coderabbit-3\"><strong>CodeRabbitに今後期待したいところ</strong></h2>\n<p>CodeRabbitへの要望としては、仕様学習の精度向上や、PRやIssueを横断的に管理できる機能の強化が挙げられました。VSCodeとの連携においても、詳細な指摘内容を取得し、IDE上でAIからのフィードバックを直接得られるようになることを期待されています。</p>\n<p>さらに、ドキュメントのわかりやすさや機能説明の具体性にも改善の余地があると感じており、ユーザーの立場からnoteなどで情報発信を続けていきたいとの意欲も語ってくださいました。</p>\n<p>「本当に素晴らしいプロダクトだと思っているので、ぜひこのすばらしいプロダクトを広めていただければと思っています！」</p>\n<p>CodeRabbitは今後も<a target=\"_blank\" href=\"https://biwakonbu.github.io/reviewtask/\">reviewtask</a>の開発をサポートしていきます！</p>\n",
      "summary": "Ryo HIGASHIGAWAさんは、OSSとして「reviewtask」というレビュー支援ツールを一人で開発されています。このツールは、AIが生成したコードに対して発生する膨大な指摘事項を、効率的かつ正確に管理するために設計されたものです。Ryoさん自身がAIによるコードレビューを日常的に活用し、その運用課題に直面する中で生まれた実践的なプロダクトとなっています。\nもともとは、GitHubのレビューコメントをAIで取得し、タスクに変換して管理するというアプローチを試していたものの、精度や手間の問題が大きく、より安定した運用を目指してreviewtaskの開発が始まりました。そんな背景を持つRyo HIGASHIGAWAさんに、reviewtaskとCodeRabbit活用についてお話を伺いました。\nreviewtaskの開発体制について\nreviewtaskはRyoさんが開発していますが、コード自体はほぼすべてAIによって生成されています。Ryoさん自身は、開発タスクの設計やバグ報告など、プロダクトマネージャーのような立場に徹し、コードを書く作業をAIに委ねています。\nGit操作やPull Request、Issue作成などの多くもAIに任せており、自らは開発プロセスの全体像を見ながらプロジェクトを前に進める役割に集中しているとのことです。\nコードレビューに関する課題\nAIによるコード生成を大量に行う中で、課題となったのがレビュー品質の担保でした。生成されたコードは量が多く、すべてを人の手でレビューするのは現実的ではなく、疲労感とボトルネックを生み出していたそうです。\nAIにコードを書かせることで生産性は大きく向上しましたが、その反面、レビューと品質管理にかかる時間と労力が爆発的に増加。最終的には、レビューまでもAIに任せられないかと模索するようになったといいます。\n「AIに書かせて自分がレビューしてとやっていると、非常に疲れるなというのが問題感としてありました」\nCodeRabbitとの出会い\nRyoさんがCodeRabbitに出会ったきっかけは、他のAI開発支援ツールとの比較や試行の中でのことでした。当時はDevinやCursorなどのツールを並行して使用し、ドキュメントやレビューの自動化に取り組んでいたそうです。\nAIに仕様を理解させ、それに沿ったレビューやチェックを実現したいという強い思いから、試行錯誤を重ねる中で、CodeRabbitの精度や柔軟性に魅力を感じて導入に至りました。\n「レビューの指摘の対応やPRの状況の確認などにも利用できるので非常に柔軟なツールなところが気にいっています」\nCodeRabbit導入の決定要因\nCodeRabbitを導入する決め手となったのは、レビューの質だけでなく、ツールとの対話ができる点だったといいます。指摘を受けたくないポイントを説明すれば理解してくれる柔軟性や、プロジェクトごとのカスタマイズ性が大きな魅力でした。\nまた、単にレビューコメントを生成するだけでなく、レビュー後のフローに組み込みやすい点も導入を後押ししたそうです。\n「プロダクトの都合上、指摘を入れて欲しくない所は説明すれば学習してくれるのが嬉しいですね」\nCodeRabbitの運用状況・効果\n現在では、reviewtaskやその他のプロジェクトにおいて、CodeRabbitによるレビューを標準フローとして組み込んでくれています。レビュー品質の維持と同時に、設計やドキュメント作成に集中できる時間が確保され、結果として開発効率が大きく向上しました。\nレビュー指摘の管理にはAIツールとの連携や自作ツールを駆使し、指摘をTODO化して確実に対応していくプロセスが構築されています。複数のプロジェクトを同時に進める現在の開発スタイルは、CodeRabbitの存在抜きには成り立たないといいます。\n「今はなくてはならないパートナーという感じです」\n実務での利用\nOSS開発だけでなく、業務での開発プロジェクトにおいてもCodeRabbitを導入し、レビューの効率化を図っています。特にレビューコストの高いチームにとっては、CodeRabbitが先に自動で指摘を洗い出してくれることで、人的リソースの負担が大きく軽減されました。\n導入後は、レビューの流れそのものが変わり、指摘が先に潰された状態でレビュワーに渡るため、確認作業の集中と精度向上につながっているとのことです。\n「ワークフローが完全に変わった感じがして良い評価が開発メンバーからも上がっています」\nCodeRabbitに今後期待したいところ\nCodeRabbitへの要望としては、仕様学習の精度向上や、PRやIssueを横断的に管理できる機能の強化が挙げられました。VSCodeとの連携においても、詳細な指摘内容を取得し、IDE上でAIからのフィードバックを直接得られるようになることを期待されています。\nさらに、ドキュメントのわかりやすさや機能説明の具体性にも改善の余地があると感じており、ユーザーの立場からnoteなどで情報発信を続けていきたいとの意欲も語ってくださいました。\n「本当に素晴らしいプロダクトだと思っているので、ぜひこのすばらしいプロダクトを広めていただければと思っています！」\nCodeRabbitは今後もreviewtaskの開発をサポートしていきます！",
      "publishedAt": "2025-09-15T01:00:13.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:12.176Z",
      "score": 0.025365197503205855
    },
    {
      "id": "cb74a9a02f87a185d427db67d4a4be54",
      "title": "Our security posture: How we safeguard your repositories",
      "url": "https://coderabbit.ai/blog/our-security-posture-how-we-safeguard-your-repositories",
      "content": "<p>Our customers trust us with their most valuable asset: their source code. That trust is why security is central to our mission of helping developers ship better code faster.</p>\n<p>When there’s a chance to strengthen our security posture, we act quickly and decisively. And when we design new systems, we design them with “security by default” in mind.</p>\n<p>We share below the architecture that makes CodeRabbit more resilient, limits the potential impact of any one component, and ensures that the data entrusted to us remains safe under all circumstances.</p>\n<h2 id=\"heading-overview\">Overview</h2>\n<p>Customers install CodeRabbit on their git platforms via the app marketplace. We integrate via webhooks with all popular Git providers such as GitHub, GitLab, Bitbucket &amp; and Azure DevOps. The integration allows us to register webhooks on events such as PR opened, user comment, etc.</p>\n<p>Each event is processed in complete isolation. We maintain a secure internal queue that verifies subscriptions, applies rate limits, and ensures that only authorized events are allowed through. Events are handled one at a time, with zero shared state and no assumptions about what came before or after.</p>\n<p>This model gives us something incredibly valuable: containment by default. If an attacker were to compromise one event, they would find nothing else to pivot to – no shared memory, no long-lived tokens, no context beyond that single, short-lived process. Every review starts from scratch, runs alone, and ends clean.</p>\n<h3 id=\"heading-our-architecture-at-a-glance\"><strong>Our architecture at a glance</strong></h3>\n<p>Here’s a high-level look at how our system is structured in our git-based, IDE, and CLI reviews:</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757969439935/8c97eef0-7f3d-4157-b2f6-8766584d36cc.png\" alt class=\"image--center mx-auto\" /></p>\n<p>This design is focused on limiting an attacker’s potential “blast radius” – or how much damage an attacker can do if they succeed at breaching one component. By isolating secrets, tightly scoping tokens, and strengthening our encryption, we’ve drastically reduced that radius.</p>\n<h3 id=\"heading-our-layered-approach\">Our layered approach</h3>\n<p>We use these layered strategies:</p>\n<h3 id=\"heading-1-sandbox\">1. Sandbox</h3>\n<p>We create a secure sandbox environment for each code review event to clone the codebase in order to read files, pull context from various sources in our knowledge base about your code and to run tools, linters, web search queries &amp; verification checks. Our sandboxed environment only has the short-lived token for that particular repository, but it contains absolutely no other secrets, API keys, or credentials. Even if an attacker were to achieve remote code execution within our sandbox environment or get out of the sandbox and break the sandbox kernel-based isolation mechanism, they would find nothing of value - no environment variables with tokens, no configuration files with secrets.</p>\n<p>Internal network access is also blocked from the sandbox. Tools may connect to the internet when required, but they cannot reach CodeRabbit’s internal services.</p>\n<h3 id=\"heading-2-token-service-separation\">2. Token Service Separation</h3>\n<p>To reinforce the isolation of workloads, we have fully embraced a model based on short-lived session tokens rather than long-lived secrets. Instead of passing environment variables or static credentials, every process is scoped with query or event-specific tokens. These git provider tokens are valid only for the duration of the event or process. These are customer-specific, short-lived tokens.  These tokens also have strict rate limiting and audit logging.</p>\n<p>This means that workloads never carry unnecessary privileges. They can only access the resources required to process a specific pull request – and nothing more.</p>\n<p>By removing persistent credentials from execution environments, we eliminate one of the most common attack surfaces. Even if a third-party tool were exploited, the attacker would see nothing beyond the minimal context of the current event.</p>\n<h3 id=\"heading-3-customer-data-isolation-amp-encryption\">3. Customer Data Isolation &amp; Encryption</h3>\n<p>Each customer's code review is completely isolated. We provision separate containers per code review and use customer-scoped tokens that can only access their specific repositories. There is no shared state between customers.</p>\n<p>We also ensure that our code index and all cached code is encrypted with a unique key per customer. Even CodeRabbit employees can't see any code-related data we store. You can also <a target=\"_blank\" href=\"https://docs.coderabbit.ai/reference/caching\">opt out of these features</a> if you don’t want a cached copy of your code.</p>\n<p>This layered approach ensures that even if an attacker were able to gain access, they would be unable to access anything critical.</p>\n<h2 id=\"heading-our-broader-security-posture\"><strong>Our broader security posture</strong></h2>\n<p>A security best practice is to layer multiple controls so that if one fails, others remain in place. We’ve implemented several layers of defense to protect customer code and data:</p>\n<p><strong>Automated sandbox enforcement</strong>: Every external tool must run in an isolated sandbox environment. This rule is enforced automatically.</p>\n<ul>\n<li><p><strong>Hardened deployment gates</strong>: We’ve added pre-deployment checks that verify no service can bypass sandbox isolation or attempt to run with escalated privileges.</p>\n</li>\n<li><p><strong>Encryption by customer key</strong>: Code indexes and cached code are encrypted with a per-customer key. This ensures that even if cache data were exposed, it would remain unreadable without the correct key.</p>\n</li>\n<li><p><strong>Auditing and monitoring</strong>: We’ve expanded our monitoring of sandboxed environments and added automated alerts for unexpected behavior or network activity.</p>\n</li>\n<li><p><strong>Expanded training</strong>: Every CodeRabbit engineer receives additional security training focused on secure-by-design practices and safe handling of secrets.</p>\n</li>\n<li><p><strong>Least privilege access:</strong> Users, processes, and systems are granted only the minimum level of permissions and access rights necessary to perform their specific tasks and nothing more.</p>\n</li>\n<li><p><strong>Vulnerability disclosure program (VDP):</strong> We maintain a formal program that invites independent security researchers to report potential issues responsibly. This ensures that if a weakness is discovered, it can be addressed quickly, transparently, and in partnership with the security community.</p>\n</li>\n<li><p><strong>Penetration testing and architectural reviews:</strong> We work with multiple third parties to conduct routine penetration testing and architectural reviews to routinely audit and improve our security posture.</p>\n</li>\n</ul>\n<h2 id=\"heading-looking-ahead\"><strong>Looking ahead</strong></h2>\n<p>We’re committed to building on this foundation by continuing to work with independent auditors, engaging with security researchers through responsible disclosure, and refining our internal practices.</p>\n<p>Our goal is to deliver world-class AI code reviews with the highest levels of security and reliability.</p>\n",
      "summary": "Our customers trust us with their most valuable asset: their source code. That trust is why security is central to our mission of helping developers ship better code faster.\nWhen there’s a chance to strengthen our security posture, we act quickly and decisively. And when we design new systems, we design them with “security by default” in mind.\nWe share below the architecture that makes CodeRabbit more resilient, limits the potential impact of any one component, and ensures that the data entrusted to us remains safe under all circumstances.\nOverview\nCustomers install CodeRabbit on their git platforms via the app marketplace. We integrate via webhooks with all popular Git providers such as GitHub, GitLab, Bitbucket & and Azure DevOps. The integration allows us to register webhooks on events such as PR opened, user comment, etc.\nEach event is processed in complete isolation. We maintain a secure internal queue that verifies subscriptions, applies rate limits, and ensures that only authorized events are allowed through. Events are handled one at a time, with zero shared state and no assumptions about what came before or after.\nThis model gives us something incredibly valuable: containment by default. If an attacker were to compromise one event, they would find nothing else to pivot to – no shared memory, no long-lived tokens, no context beyond that single, short-lived process. Every review starts from scratch, runs alone, and ends clean.\nOur architecture at a glance\nHere’s a high-level look at how our system is structured in our git-based, IDE, and CLI reviews:\n\nThis design is focused on limiting an attacker’s potential “blast radius” – or how much damage an attacker can do if they succeed at breaching one component. By isolating secrets, tightly scoping tokens, and strengthening our encryption, we’ve drastically reduced that radius.\nOur layered approach\nWe use these layered strategies:\n1. Sandbox\nWe create a secure sandbox environment for each code review event to clone the codebase in order to read files, pull context from various sources in our knowledge base about your code and to run tools, linters, web search queries & verification checks. Our sandboxed environment only has the short-lived token for that particular repository, but it contains absolutely no other secrets, API keys, or credentials. Even if an attacker were to achieve remote code execution within our sandbox environment or get out of the sandbox and break the sandbox kernel-based isolation mechanism, they would find nothing of value - no environment variables with tokens, no configuration files with secrets.\nInternal network access is also blocked from the sandbox. Tools may connect to the internet when required, but they cannot reach CodeRabbit’s internal services.\n2. Token Service Separation\nTo reinforce the isolation of workloads, we have fully embraced a model based on short-lived session tokens rather than long-lived secrets. Instead of passing environment variables or static credentials, every process is scoped with query or event-specific tokens. These git provider tokens are valid only for the duration of the event or process. These are customer-specific, short-lived tokens.  These tokens also have strict rate limiting and audit logging.\nThis means that workloads never carry unnecessary privileges. They can only access the resources required to process a specific pull request – and nothing more.\nBy removing persistent credentials from execution environments, we eliminate one of the most common attack surfaces. Even if a third-party tool were exploited, the attacker would see nothing beyond the minimal context of the current event.\n3. Customer Data Isolation & Encryption\nEach customer's code review is completely isolated. We provision separate containers per code review and use customer-scoped tokens that can only access their specific repositories. There is no shared state between customers.\nWe also ensure that our code index and all cached code is encrypted with a unique key per customer. Even CodeRabbit employees can't see any code-related data we store. You can also opt out of these features if you don’t want a cached copy of your code.\nThis layered approach ensures that even if an attacker were able to gain access, they would be unable to access anything critical.\nOur broader security posture\nA security best practice is to layer multiple controls so that if one fails, others remain in place. We’ve implemented several layers of defense to protect customer code and data:\nAutomated sandbox enforcement: Every external tool must run in an isolated sandbox environment. This rule is enforced automatically.\nHardened deployment gates: We’ve added pre-deployment checks that verify no service can bypass sandbox isolation or attempt to run with escalated privileges.\nEncryption by customer key: Code indexes and cached code are encrypted with a per-customer key. This ensures that even if cache data were exposed, it would remain unreadable without the correct key.\nAuditing and monitoring: We’ve expanded our monitoring of sandboxed environments and added automated alerts for unexpected behavior or network activity.\nExpanded training: Every CodeRabbit engineer receives additional security training focused on secure-by-design practices and safe handling of secrets.\nLeast privilege access: Users, processes, and systems are granted only the minimum level of permissions and access rights necessary to perform their specific tasks and nothing more.\nVulnerability disclosure program (VDP): We maintain a formal program that invites independent security researchers to report potential issues responsibly. This ensures that if a weakness is discovered, it can be addressed quickly, transparently, and in partnership with the security community.\nPenetration testing and architectural reviews: We work with multiple third parties to conduct routine penetration testing and architectural reviews to routinely audit and improve our security posture.\nLooking ahead\nWe’re committed to building on this foundation by continuing to work with independent auditors, engaging with security researchers through responsible disclosure, and refining our internal practices.\nOur goal is to deliver world-class AI code reviews with the highest levels of security and reliability.",
      "publishedAt": "2025-09-14T07:00:00.000Z",
      "author": "Rohit Khanna",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "ide",
        "testing",
        "observability",
        "governance"
      ],
      "ingestedAt": "2025-12-04T14:17:12.176Z",
      "score": 0.06611507558127173
    },
    {
      "id": "6db1a811cac14b58835bae10c5df0aae",
      "title": "フルリモート開発を加速するSalesNowのCodeRabbit活用法",
      "url": "https://coderabbit.ai/blog/salesnowcoderabbit",
      "content": "<p><a target=\"_blank\" href=\"https://top.salesnow.jp/\">株式会社SalesNow</a>は1,400万件超の企業情報を収録し、法人網羅率100%を誇る日本最大級の企業・組織データベース、AI企業データクラウド「SalesNow」を提供しています。同社では、プロダクト面のAI活用に加えて、社内の開発生産性向上を目的としたLLMやエージェントの導入にも積極的です。</p>\n<p>その一環としてAIコードレビューサービスのCodeRabbitを活用し、レビューの標準化と学習の仕組みづくりを進めています。今回はSalesNow社内におけるCodeRabbitの利用状況について、同社エンジニアの<a target=\"_blank\" href=\"https://x.com/sa9_sha9\">@sa9_sha9</a>さんにお話を伺いました。</p>\n<h2 id=\"heading-salesnow\">SalesNowの開発体制について</h2>\n<p>同社の開発は完全内製で、創業時からフルリモートを文化として定着しています。地理的に分散したメンバーが自律的に動けるよう、非同期コミュニケーションとプルリクエスト中心のフローを重視しています。</p>\n<p>体制はアプリケーション開発が6名、データ生成と収集を担うデータチームがフルタイム6名とインターン約4名。さらにデザイナーとPMが加わり、全体で20名ほどとなっています。主要スタックはPythonとReactで、データの信頼性と鮮度を軸に開発を進めています。</p>\n<h2 id=\"heading-44os44ot44ol44o85b6f44gh44gu44oc44oi44or44on44od44kv55m655sf44gm6kqy6agm\">レビュー待ちのボトルネック発生が課題</h2>\n<p>CodeRabbit導入前は、ドメイン知識や社内の開発流儀の判断が一部メンバーに集中し、レビュー待ちのボトルネックが発生していました。長く在籍しているからこそ分かる書き方や、過去の経緯に基づいた知見が人に依存し、属人化を招いていました。</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757307500016/279160eb-0729-45b9-97b0-45b4dff08fb4.jpeg\" alt class=\"image--center mx-auto\" /></p>\n<p>技術面では、プログラミング言語の進化に伴う細かな是正を人が指摘していました。たとえばPythonの型ヒントに関する記述の見直しなど、新しい機能に関する指摘ほど、人が丁寧に行う必要がありました。加えて静的解析では指摘が多く、重要度の見極めが難しかったと振り返ります。</p>\n<p>「レビューの質は落とさず、人にしかできない判断と機械で代替できる指摘を切り分けたいという課題感が常にありました」</p>\n<h2 id=\"heading-coderabbit\">CodeRabbitとの出会い</h2>\n<p>CodeRabbitと出会ったきっかけは、Xのタイムラインでした。実運用の事例も多く確認でき、プルリクエスト（以下PR）を起点に自動で一次レビューが進む点が自社にフィットすると判断したといいます。そこで、まずは@sa9_sha9さんのチームに限定して、小さく使い始めました。</p>\n<p>同社では他のAIツールも併用していましたが、PR駆動の自動レビューという仕組みは運用に乗せやすいと感じたといいます。非同期中心のワークスタイルにも自然に溶け込み、導入ハードルが低い点も後押しになりました。</p>\n<p>「小さく始めて徐々に広げるという進め方が取りやすく、現場の感触を早く得られました」</p>\n<h2 id=\"heading-coderabbit3\">CodeRabbit導入を決めた3つのポイント</h2>\n<p>SalesNow社が、CodeRabbit導入を決定した要因は以下の3つです。</p>\n<p>1. PR作成を起点に自動で一次レビューが実行されること<br />2. 日本語の自然なコメントと、プロジェクトガイドラインの読込に対応していること<br />3. レビューの指摘に対してやり取りを重ねることで、自然と暗黙知が体系化されていく体験がとても良かった</p>\n<p>自動的なコードレビューは、人の手が空いていなくてもレビューとディスカッションが先行し、手戻りが減る効果がありました。また、一般的なベストプラクティスと社内の流儀を橋渡しできる点も評価しています。</p>\n<p>「まずAIに通すことで基本的な抜け漏れを塞ぎ、人は本質的なレビューに集中できるようになっています」</p>\n<h2 id=\"heading-ai\">AIによるレビューは心理的摩擦が低い</h2>\n<p>現在はフルタイムとインターンを含む全開発メンバーに権限を付与し、PRを作るとCodeRabbitがレビューするのが当たり前になっています。新しいメンバーにはオンボーディングにて、レビューへの反応と判断のコメント化を周知しています。</p>\n<p>この工夫は、非同期な環境ではレビューコメントが放置されているのか、対応不要なものなのかを判別するためです。厳格なルール化はしていませんが、同社では実質的な規範として定着しています。</p>\n<p>導入後の効果として、一次レビューの網羅性が上がり、週末に働きたいメンバーも一人でレビューを回せるようになっています。ユニークな意見として、AIのレビューは人だと起こりがちな心理的摩擦が少なく、受け止めやすいとのこと。</p>\n<p>「CodeRabbitが一次レビューを行い、人は本質を見るという分担で、速度と丁寧さの良いバランスを得られています」</p>\n<h2 id=\"heading-coderabbit-1\">CodeRabbitに今後期待したいところ</h2>\n<p>SalesNow社では、深くCodeRabbitを活用しており、さまざまな要望が上がっているとのことです。</p>\n<p>「まず、要件定義やドメインロジックへの踏み込みを強化してほしいです。当社ではAsanaを利用していますが、そこから仕様の文脈をより確実に参照し、実装意図との整合性の確認や、やるべきでない変更の検知まで踏み込めると、より便利になると思います」</p>\n<p>他にもクロスリポジトリにおける整合性の強化が期待されています。APIとフロントエンド間ではOAS（OpenAPI Specification）を使っていますが、それでも十分に読み取れていない場合があるとの指摘がありました。</p>\n<p>「他にも設定のマージ（組織、リポジトリそれぞれの設定のマージ）や、もっとレポート機能を使いこなしたいと考えています」</p>\n<p>CodeRabbitは、今後もSalesNow社のサービス開発をサポートして参ります。</p>\n<hr />\n<p>SalesNowでは、Webリードエンジニアやデータエンジニア、バックエンドエンジニア、LLMエンジニアなどさまざまなエンジニアを募集しています。気になる方は、ぜひ<a target=\"_blank\" href=\"https://recruit.salesnow.jp/\">SalesNow採用情報</a>をご覧ください。</p>\n",
      "summary": "株式会社SalesNowは1,400万件超の企業情報を収録し、法人網羅率100%を誇る日本最大級の企業・組織データベース、AI企業データクラウド「SalesNow」を提供しています。同社では、プロダクト面のAI活用に加えて、社内の開発生産性向上を目的としたLLMやエージェントの導入にも積極的です。\nその一環としてAIコードレビューサービスのCodeRabbitを活用し、レビューの標準化と学習の仕組みづくりを進めています。今回はSalesNow社内におけるCodeRabbitの利用状況について、同社エンジニアの@sa9_sha9さんにお話を伺いました。\nSalesNowの開発体制について\n同社の開発は完全内製で、創業時からフルリモートを文化として定着しています。地理的に分散したメンバーが自律的に動けるよう、非同期コミュニケーションとプルリクエスト中心のフローを重視しています。\n体制はアプリケーション開発が6名、データ生成と収集を担うデータチームがフルタイム6名とインターン約4名。さらにデザイナーとPMが加わり、全体で20名ほどとなっています。主要スタックはPythonとReactで、データの信頼性と鮮度を軸に開発を進めています。\nレビュー待ちのボトルネック発生が課題\nCodeRabbit導入前は、ドメイン知識や社内の開発流儀の判断が一部メンバーに集中し、レビュー待ちのボトルネックが発生していました。長く在籍しているからこそ分かる書き方や、過去の経緯に基づいた知見が人に依存し、属人化を招いていました。\n\n技術面では、プログラミング言語の進化に伴う細かな是正を人が指摘していました。たとえばPythonの型ヒントに関する記述の見直しなど、新しい機能に関する指摘ほど、人が丁寧に行う必要がありました。加えて静的解析では指摘が多く、重要度の見極めが難しかったと振り返ります。\n「レビューの質は落とさず、人にしかできない判断と機械で代替できる指摘を切り分けたいという課題感が常にありました」\nCodeRabbitとの出会い\nCodeRabbitと出会ったきっかけは、Xのタイムラインでした。実運用の事例も多く確認でき、プルリクエスト（以下PR）を起点に自動で一次レビューが進む点が自社にフィットすると判断したといいます。そこで、まずは@sa9_sha9さんのチームに限定して、小さく使い始めました。\n同社では他のAIツールも併用していましたが、PR駆動の自動レビューという仕組みは運用に乗せやすいと感じたといいます。非同期中心のワークスタイルにも自然に溶け込み、導入ハードルが低い点も後押しになりました。\n「小さく始めて徐々に広げるという進め方が取りやすく、現場の感触を早く得られました」\nCodeRabbit導入を決めた3つのポイント\nSalesNow社が、CodeRabbit導入を決定した要因は以下の3つです。\n1. PR作成を起点に自動で一次レビューが実行されること\n2. 日本語の自然なコメントと、プロジェクトガイドラインの読込に対応していること\n3. レビューの指摘に対してやり取りを重ねることで、自然と暗黙知が体系化されていく体験がとても良かった\n自動的なコードレビューは、人の手が空いていなくてもレビューとディスカッションが先行し、手戻りが減る効果がありました。また、一般的なベストプラクティスと社内の流儀を橋渡しできる点も評価しています。\n「まずAIに通すことで基本的な抜け漏れを塞ぎ、人は本質的なレビューに集中できるようになっています」\nAIによるレビューは心理的摩擦が低い\n現在はフルタイムとインターンを含む全開発メンバーに権限を付与し、PRを作るとCodeRabbitがレビューするのが当たり前になっています。新しいメンバーにはオンボーディングにて、レビューへの反応と判断のコメント化を周知しています。\nこの工夫は、非同期な環境ではレビューコメントが放置されているのか、対応不要なものなのかを判別するためです。厳格なルール化はしていませんが、同社では実質的な規範として定着しています。\n導入後の効果として、一次レビューの網羅性が上がり、週末に働きたいメンバーも一人でレビューを回せるようになっています。ユニークな意見として、AIのレビューは人だと起こりがちな心理的摩擦が少なく、受け止めやすいとのこと。\n「CodeRabbitが一次レビューを行い、人は本質を見るという分担で、速度と丁寧さの良いバランスを得られています」\nCodeRabbitに今後期待したいところ\nSalesNow社では、深くCodeRabbitを活用しており、さまざまな要望が上がっているとのことです。\n「まず、要件定義やドメインロジックへの踏み込みを強化してほしいです。当社ではAsanaを利用していますが、そこから仕様の文脈をより確実に参照し、実装意図との整合性の確認や、やるべきでない変更の検知まで踏み込めると、より便利になると思います」\n他にもクロスリポジトリにおける整合性の強化が期待されています。APIとフロントエンド間ではOAS（OpenAPI Specification）を使っていますが、それでも十分に読み取れていない場合があるとの指摘がありました。\n「他にも設定のマージ（組織、リポジトリそれぞれの設定のマージ）や、もっとレポート機能を使いこなしたいと考えています」\nCodeRabbitは、今後もSalesNow社のサービス開発をサポートして参ります。\nSalesNowでは、Webリードエンジニアやデータエンジニア、バックエンドエンジニア、LLMエンジニアなどさまざまなエンジニアを募集しています。気になる方は、ぜひSalesNow採用情報をご覧ください。",
      "publishedAt": "2025-09-10T01:00:38.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:12.176Z",
      "score": 0.015529236540607927
    },
    {
      "id": "fc384d6b68f748c1ca67cf41abcf9e81",
      "title": "CodeRabbitがどうやって大規模コードベースで正確なAIコードレビューを実現しているか",
      "url": "https://coderabbit.ai/blog/how-coderabbit-delivers-accurate-ai-code-reviews-on-massive-codebases-ja",
      "content": "<p><a target=\"_blank\" href=\"https://www.coderabbit.ai/blog/how-coderabbit-delivers-accurate-ai-code-reviews-on-massive-codebases\">How CodeRabbit delivers accurate AI code reviews on massive codebases</a>の意訳です。</p>\n<p>大規模なコードベースは特別な存在です。数百のファイルに広がり、何年ものコミットで進化し、時にはなんとか組織的な記憶でつながっているように見えることもあります。その環境で変更をレビューするのは難しいだけでなく、まるで考古学の発掘作業のようです。この行が先週ここに移動したのは理由があったのか？他のファイルが密かに依存しているのではないか？</p>\n<p>まさにそこでCodeRabbitが力を発揮します。スケールに対応するよう設計されているため、ファイルごとのバラバラなコメントになることなく、大規模コードベース全体の履歴とアーキテクチャを考慮してレビューを行います。リポジトリが大きく古いほど、CodeRabbitは役立ちます。人間がプルリクエストの途中で忘れてしまいがちなパターン、依存関係、ルールを見抜けるからです。</p>\n<h2 id=\"heading-ai\">大規模コードベース？AIコードレビューにはより多くの文脈が必要！</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757096043094/d6910b50-edaa-43bc-8962-803f4665900b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>CodeRabbitは<a target=\"_blank\" href=\"https://x.com/qwertyu_alex/status/1956848505445654595\">大規模リポジトリで高いパフォーマンスを発揮することで知られています</a>。私たちのツールはプルリクエストを表面的に読むだけではなく、アーカイブ役のように振る舞います。コメントを残す前に、周辺のコードや多数の文脈を引き込みます。AIエージェントはそれらが履歴の中でどう動いてきたかを追跡し、チームのコーディング規約を適用し、スクリプトやツールで自らの推論を二重チェックします。</p>\n<p>その結果、レビューは異常なほど「文脈に詳しい」ものになります。クロスファイルの問題を事前にキャッチし、一貫性を強制しつつも不要な指摘は避け、複雑で長い過去を持つリポジトリ全体にスケールします。</p>\n<p>得られる結果は明確で、早い段階でのリスクに対するフィードバック、予期せぬ副作用の減少、そしてコードベース全体を理解したレビューになります。</p>\n<h2 id=\"heading-kirlt67liibjgadjgzhjga7jg6zjg5pjg6xjg7zjga7llypoyzngrnvvijmlofohijjgyzjgarjgytjgajkvzxjgyzotbfjgzpjgovjgyvvvikqkg\"><strong>差分だけのレビューの問題点（文脈がないと何が起こるか）</strong></h2>\n<p>コードの差分は必要ですが十分ではありません。大規模コードベースでは、10行の変更が複数サービスで共有されるヘルパーを密かに変えたり、公開されているAPIの要件を変更したり、差分ファイル以外のセキュリティ前提を崩したりすることがあります。</p>\n<p>差分だけを見るAIレビューは、大規模コードベースでは計器なしで飛んでいるようなものです。変更箇所がどこで参照されているのか、他に一緒に変わりやすいコードは何か、チケットの意図に合っているかが見えなければ、小さなコードベースでは通用しても大規模コードベースでは役立ちません。</p>\n<p>文脈がないと「これも更新してもらえますか？」というやり取りが繰り返され、マージ時に遅れて驚きが発生し、小さなリグレッションが積み重なります。紙の上ではレビューが良く見えても、本番では違う結果になるのです。</p>\n<h2 id=\"heading-pr\"><strong>レガシーコードベースに正しい文脈を構築する（それがPRをどう助けるか）</strong></h2>\n<p>CodeRabbitを「意見を出す前に調査ファイルを組み立てる存在」と考えてください。そのケースファイルには以下の要素が含まれ、それぞれがレビューに反映されます。</p>\n<ol>\n<li><h3 id=\"heading-codegraph\"><strong>コードの地図（Codegraph）</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757097642077/59f74d44-200a-48f4-8c3e-3f85f22e3180.png\" alt class=\"image--center mx-auto\" /></p>\n<p> CodeRabbitは定義と参照の軽量なマップを構築し、履歴をスキャンして頻繁に一緒に変更されるファイルを特定します。これにより、依存関係のマップを作成し、PR内の変更が他の依存関係を壊さないかを確認します。</p>\n<p> <strong>なぜ役立つか:</strong> 行単位ではなくファイル間で推論できる。</p>\n<p> <strong>実際の動作:</strong> Codegraphを使って関連ファイルを辿り、差分外で見つかったバグをまとめて通知します。</p>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757097586686/df6114b4-cc64-4d4e-9429-368ae9eff503.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><h3 id=\"heading-amp\"><strong>コードインデックス（セマンティック &amp; 類似検索）</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757090961514/f82b6444-2b1c-4eb5-8b85-a016a886768c.png\" alt class=\"image--center mx-auto\" /></p>\n<p> CodeRabbitは関数、クラス/モジュール、テスト、過去のPRや変更のセマンティックインデックス（埋め込み）を保持します。レビュー時にはキーワードではなく目的ベースで検索し、類似実装を見つけ、再利用すべきテストを引き出し、過去の修正方法を思い出します。</p>\n<p> <strong>なぜ役立つか:</strong> レガシーコードベースですでに解決している方法を参照でき、一貫性向上、手戻り削減、テスト拡充が速くなる。</p>\n<p> <strong>実際の動作:</strong> 類似検索により同じコールバックパターンを使った別のテストを提示し、同じ修正を提案します。</p>\n</li>\n<li><h3 id=\"heading-kirjg4hjg7zjg6dni6zoh6rjga7jg6vjg7zjg6vjgpllj43mmkaqkg\"><strong>チーム独自のルールを反映</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757090919900/00918dac-6fe3-44b4-9b4a-a823f89562bc.png\" alt class=\"image--center mx-auto\" /></p>\n<p> CodeRabbitのレビューはチームの規約（命名、エラーハンドリング、API境界、セキュリティ要件、性能要件、テスト規範など）に基づいて行われます。</p>\n<p> <strong>なぜ役立つか:</strong> 一般的なチェックリストではなく、チーム固有の基準に沿ったフィードバックが得られる。</p>\n<p> <strong>実際の動作:</strong> スキーマ変更後にPrismaのマイグレーション不足を指摘。開発者が「デプロイ時に自動生成される」と返答すると、CodeRabbitはそれを<strong>学習</strong>として保存し、将来の誤検出を避けます。</p>\n</li>\n<li><h3 id=\"heading-kirjg4tjg7zjg6vjgyvjgonjga7jgrfjgrdjg4rjg6sqkg\"><strong>ツールからのシグナル</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757091000646/a73d1f9d-d93e-4666-803d-843f356f720a.png\" alt class=\"image--center mx-auto\" /></p>\n<p> AIの推論と並行して、CodeRabbitはリンターやセキュリティ解析ツールを実行し、その結果をレビューに統合します。</p>\n<p> <strong>なぜ役立つか:</strong> AIとツールの両方に裏打ちされた具体的な改善提案が得られる。</p>\n<p> <strong>実際の動作:</strong> ESLintルールと行番号を示し、コールバックを型付き宣言に書き換え、オプショナルチェイニングで安全性を確保します。</p>\n</li>\n<li><h3 id=\"heading-kiroqlzmi6djgavln7rjgaxjgyvvijmpjzoqlzjgrnjgqjg6rjg5fjg4jvvikqkg\"><strong>証拠に基づく（検証スクリプト）</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757092768117/d45e60b4-6cdd-46b7-9c48-e470874544cf.png\" alt class=\"image--center mx-auto\" /></p>\n<p> 検証が必要な場合、CodeRabbitはシェル/Pythonスクリプト（grepやast-grepのようなもの）を生成し、仮定を確認したり証拠を抽出してからコメントを残します。</p>\n<p> <strong>なぜ役立つか:</strong> コメントに裏付けがあるため、ノイズが減り、実際にコードを改善する指摘だけが残る。</p>\n<p> <strong>実際の動作:</strong> ファイルとループを特定し、失敗モードを説明し、検証エージェントが解析後に導いた正確な修正案を提案します。</p>\n</li>\n</ol>\n<p>これは<strong>実践的なコンテキストエンジニアリング</strong>です。正しい情報を集め、絞り込み、整理してからモデルに判断させる。CodeRabbitは創業時からこのアプローチを核としてきました。</p>\n<p>成果はシンプルです。シグナルが強く、ノイズが少なく、システムを理解しているレビューになります。</p>\n<h2 id=\"heading-kirjgqjjg7pjgrjg7zjg5fjg6njgqtjgrropomqkhjg6rjg53jgrjjg4jjg6rjgbjjga7jgrnjgrhjg7zjg6rjg7pjgraqkg\"><strong>エンタープライズ規模リポジトリへのスケーリング</strong></h2>\n<p>CodeRabbitはスケールを意識して設計されたパイプラインにより、大規模・レガシーコードベースで強みを発揮します。</p>\n<p>PRが届くと、CodeRabbitは隔離された、短期間だけの安全な環境を立ち上げます。必要なものだけを取得し、文脈を構築し、検証を実行し、終了後に破棄します。ピーク時には多数のワーカーが並列実行され、レビュー速度は一定に保たれます。パスフィルターで不要なアセットを除外し、キャッシュやインデックスを選択的に有効化して繰り返しのレビューを高速化できます。</p>\n<p>要するに、範囲の選択で文脈を集中させ、隔離で安全性を確保し、弾力性ある実行方法で高速性を維持します。この手法はコードベースとリリーススケジュールに合わせてスケールします。</p>\n<h2 id=\"heading-coderabbit-ai\"><strong>CodeRabbit: 大規模コードベース向けAIコードレビューの正解</strong></h2>\n<p>CodeRabbitの強みは単一のトリックではありません。コンテキストエンジニアリングを端から端まで適用する姿勢にあります。変更が何に触れるかをマッピングし、意図に結びつけ、チームルールを適用し、ツールで検証し、証拠付きでコメントします。</p>\n<p>このやり方は「コンテキストエンジニアリング」という言葉が流行る前から一貫しており、スケールした環境で正確でノイズの少ないレビューを実現する唯一の方法です。</p>\n<p><strong><em>あなたの大規模コードベースで深い文脈を持つレビューを体験してみませんか？ →</em></strong> <a target=\"_blank\" href=\"https://coderabbit.link/sY5vXpT\"><strong><em>14日間のトライアルを開始する</em></strong></a></p>\n",
      "summary": "How CodeRabbit delivers accurate AI code reviews on massive codebasesの意訳です。\n大規模なコードベースは特別な存在です。数百のファイルに広がり、何年ものコミットで進化し、時にはなんとか組織的な記憶でつながっているように見えることもあります。その環境で変更をレビューするのは難しいだけでなく、まるで考古学の発掘作業のようです。この行が先週ここに移動したのは理由があったのか？他のファイルが密かに依存しているのではないか？\nまさにそこでCodeRabbitが力を発揮します。スケールに対応するよう設計されているため、ファイルごとのバラバラなコメントになることなく、大規模コードベース全体の履歴とアーキテクチャを考慮してレビューを行います。リポジトリが大きく古いほど、CodeRabbitは役立ちます。人間がプルリクエストの途中で忘れてしまいがちなパターン、依存関係、ルールを見抜けるからです。\n大規模コードベース？AIコードレビューにはより多くの文脈が必要！\n\nCodeRabbitは大規模リポジトリで高いパフォーマンスを発揮することで知られています。私たちのツールはプルリクエストを表面的に読むだけではなく、アーカイブ役のように振る舞います。コメントを残す前に、周辺のコードや多数の文脈を引き込みます。AIエージェントはそれらが履歴の中でどう動いてきたかを追跡し、チームのコーディング規約を適用し、スクリプトやツールで自らの推論を二重チェックします。\nその結果、レビューは異常なほど「文脈に詳しい」ものになります。クロスファイルの問題を事前にキャッチし、一貫性を強制しつつも不要な指摘は避け、複雑で長い過去を持つリポジトリ全体にスケールします。\n得られる結果は明確で、早い段階でのリスクに対するフィードバック、予期せぬ副作用の減少、そしてコードベース全体を理解したレビューになります。\n差分だけのレビューの問題点（文脈がないと何が起こるか）\nコードの差分は必要ですが十分ではありません。大規模コードベースでは、10行の変更が複数サービスで共有されるヘルパーを密かに変えたり、公開されているAPIの要件を変更したり、差分ファイル以外のセキュリティ前提を崩したりすることがあります。\n差分だけを見るAIレビューは、大規模コードベースでは計器なしで飛んでいるようなものです。変更箇所がどこで参照されているのか、他に一緒に変わりやすいコードは何か、チケットの意図に合っているかが見えなければ、小さなコードベースでは通用しても大規模コードベースでは役立ちません。\n文脈がないと「これも更新してもらえますか？」というやり取りが繰り返され、マージ時に遅れて驚きが発生し、小さなリグレッションが積み重なります。紙の上ではレビューが良く見えても、本番では違う結果になるのです。\nレガシーコードベースに正しい文脈を構築する（それがPRをどう助けるか）\nCodeRabbitを「意見を出す前に調査ファイルを組み立てる存在」と考えてください。そのケースファイルには以下の要素が含まれ、それぞれがレビューに反映されます。\nコードの地図（Codegraph）\n \n CodeRabbitは定義と参照の軽量なマップを構築し、履歴をスキャンして頻繁に一緒に変更されるファイルを特定します。これにより、依存関係のマップを作成し、PR内の変更が他の依存関係を壊さないかを確認します。\n なぜ役立つか: 行単位ではなくファイル間で推論できる。\n 実際の動作: Codegraphを使って関連ファイルを辿り、差分外で見つかったバグをまとめて通知します。\n \nコードインデックス（セマンティック & 類似検索）\n \n CodeRabbitは関数、クラス/モジュール、テスト、過去のPRや変更のセマンティックインデックス（埋め込み）を保持します。レビュー時にはキーワードではなく目的ベースで検索し、類似実装を見つけ、再利用すべきテストを引き出し、過去の修正方法を思い出します。\n なぜ役立つか: レガシーコードベースですでに解決している方法を参照でき、一貫性向上、手戻り削減、テスト拡充が速くなる。\n 実際の動作: 類似検索により同じコールバックパターンを使った別のテストを提示し、同じ修正を提案します。\nチーム独自のルールを反映\n \n CodeRabbitのレビューはチームの規約（命名、エラーハンドリング、API境界、セキュリティ要件、性能要件、テスト規範など）に基づいて行われます。\n なぜ役立つか: 一般的なチェックリストではなく、チーム固有の基準に沿ったフィードバックが得られる。\n 実際の動作: スキーマ変更後にPrismaのマイグレーション不足を指摘。開発者が「デプロイ時に自動生成される」と返答すると、CodeRabbitはそれを学習として保存し、将来の誤検出を避けます。\nツールからのシグナル\n \n AIの推論と並行して、CodeRabbitはリンターやセキュリティ解析ツールを実行し、その結果をレビューに統合します。\n なぜ役立つか: AIとツールの両方に裏打ちされた具体的な改善提案が得られる。\n 実際の動作: ESLintルールと行番号を示し、コールバックを型付き宣言に書き換え、オプショナルチェイニングで安全性を確保します。\n証拠に基づく（検証スクリプト）\n \n 検証が必要な場合、CodeRabbitはシェル/Pythonスクリプト（grepやast-grepのようなもの）を生成し、仮定を確認したり証拠を抽出してからコメントを残します。\n なぜ役立つか: コメントに裏付けがあるため、ノイズが減り、実際にコードを改善する指摘だけが残る。\n 実際の動作: ファイルとループを特定し、失敗モードを説明し、検証エージェントが解析後に導いた正確な修正案を提案します。\nこれは実践的なコンテキストエンジニアリングです。正しい情報を集め、絞り込み、整理してからモデルに判断させる。CodeRabbitは創業時からこのアプローチを核としてきました。\n成果はシンプルです。シグナルが強く、ノイズが少なく、システムを理解しているレビューになります。\nエンタープライズ規模リポジトリへのスケーリング\nCodeRabbitはスケールを意識して設計されたパイプラインにより、大規模・レガシーコードベースで強みを発揮します。\nPRが届くと、CodeRabbitは隔離された、短期間だけの安全な環境を立ち上げます。必要なものだけを取得し、文脈を構築し、検証を実行し、終了後に破棄します。ピーク時には多数のワーカーが並列実行され、レビュー速度は一定に保たれます。パスフィルターで不要なアセットを除外し、キャッシュやインデックスを選択的に有効化して繰り返しのレビューを高速化できます。\n要するに、範囲の選択で文脈を集中させ、隔離で安全性を確保し、弾力性ある実行方法で高速性を維持します。この手法はコードベースとリリーススケジュールに合わせてスケールします。\nCodeRabbit: 大規模コードベース向けAIコードレビューの正解\nCodeRabbitの強みは単一のトリックではありません。コンテキストエンジニアリングを端から端まで適用する姿勢にあります。変更が何に触れるかをマッピングし、意図に結びつけ、チームルールを適用し、ツールで検証し、証拠付きでコメントします。\nこのやり方は「コンテキストエンジニアリング」という言葉が流行る前から一貫しており、スケールした環境で正確でノイズの少ないレビューを実現する唯一の方法です。\nあなたの大規模コードベースで深い文脈を持つレビューを体験してみませんか？ → 14日間のトライアルを開始する",
      "publishedAt": "2025-09-08T09:03:45.000Z",
      "author": "Atsushi Nakatsugawa",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:12.176Z",
      "score": 0.013788452316073257
    },
    {
      "id": "c1a469fcd6c6ea5f00545fb70fb5c5f9",
      "title": "How CodeRabbit delivers accurate AI code reviews on massive codebases",
      "url": "https://coderabbit.ai/blog/how-coderabbit-delivers-accurate-ai-code-reviews-on-massive-codebases",
      "content": "<p>Massive codebases are a special kind of beast. They sprawl across hundreds of files, evolve over years of commits, and occasionally feel like they’re held together by equal parts duct tape and institutional memory. Reviewing changes in that environment isn’t just hard – it feels like an archaeological dig. Did this line move here last week for a reason? Is there another file quietly depending on it?</p>\n<p>That’s exactly where CodeRabbit shines. It was built for scale, so instead of drowning you in disconnected file-by-file comments, it reviews with the whole history and architecture of your massive codebase in mind. The larger and older your repository, the more useful CodeRabbit becomes because it can see the patterns, dependencies, and rules that humans usually forget about halfway through a pull request when trying to keep all the dependencies in that legacy code in their head.</p>\n<h2 id=\"heading-large-codebase-ai-code-reviews-need-more-context\">Large codebase? AI code reviews need more context!</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757096043094/d6910b50-edaa-43bc-8962-803f4665900b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>CodeRabbit is <a target=\"_blank\" href=\"https://x.com/qwertyu_alex/status/1956848505445654595\">known for performing great on large repos</a>. Our tool doesn’t just skim your pull requests; it goes full archivist. Before leaving a single comment, it gathers the surrounding code from your large codebase and pulls in dozens of points of context from your code. AI agents then trace how those pieces have moved through history, apply your team’s coding standards, and even double-check their own reasoning with scripts and tools.</p>\n<p>The effect is reviews that feel unusually…informed about your legacy codebase. It catches cross-file issues before they turn into production mysteries, enforces consistency without nitpicking, and scales comfortably across sprawling repos with long, complicated pasts.</p>\n<p>The power you gain through this is clearer, earlier feedback on real risks, fewer “wait, what else did that touch?” surprises, and reviews that actually reflect how your whole massive codebase fits together.</p>\n<h2 id=\"heading-the-problem-with-diff-only-reviews-or-what-goes-wrong-without-context\"><strong>The problem with diff-only reviews (or what goes wrong without context)</strong></h2>\n<p>Code diffs are necessary, but they’re not sufficient. In a massive codebase, a 10-line change can quietly alter a shared helper used by multiple services, shift a public API contract, or undermine a security assumption that lives outside the files in the diff.</p>\n<p>AI Bot reviewers who only see the diff are flying without instruments within a large codebase. AI that can’t see where the changed code is referenced, what else tends to change with it, or whether the change actually matches the ticket’s intent, might work for a smaller codebase but not for yours.</p>\n<p>Without the right context, you get ping-pong cycles (“Can you also update…?”), late surprises at merge time, and a steady drip of small regressions that add up. The review looks fine on paper, while production tells a different story.</p>\n<h2 id=\"heading-building-the-right-context-on-your-legacy-codebase-and-how-that-helps-your-prs\"><strong>Building the right context on your legacy codebase (and how that helps your PRs)</strong></h2>\n<p>Think of CodeRabbit as assembling a case file before giving an opinion. Here’s what goes into that case file and how each piece shows up in your reviews.</p>\n<ol>\n<li><h3 id=\"heading-a-map-of-your-code-codegraph\"><strong>A map of your code (Codegraph)</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757097642077/59f74d44-200a-48f4-8c3e-3f85f22e3180.png\" alt class=\"image--center mx-auto\" /></p>\n<p> CodeRabbit builds a lightweight map of definitions and references and scans commit history for files that frequently change together throughout your massive codebase. This creates a map of file dependencies that CodeRabbit uses to check if any changes in your PR will break other dependencies in your codebase.</p>\n<p> <strong>Why this helps:</strong> The review can reason across files, not just lines.</p>\n<p> <strong>Seeing it in action:</strong> CodeRabbit posts a summary listing bugs <strong>outside the diff range</strong> that CodeRabbit located by traversing related files with Codegraph.</p>\n<p> <strong>Here’s an example of the files that CodeGraph brings in from across a repository when completing a PR review.</strong></p>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757097586686/df6114b4-cc64-4d4e-9429-368ae9eff503.png\" alt class=\"image--center mx-auto\" /></p>\n</li>\n<li><h3 id=\"heading-code-index-semantic-amp-similarity-retrieval\"><strong>Code Index (semantic &amp; similarity retrieval)</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757090961514/f82b6444-2b1c-4eb5-8b85-a016a886768c.png\" alt class=\"image--center mx-auto\" /></p>\n<p> CodeRabbit maintains a semantic index (embeddings) of functions, classes/modules, tests, and prior PRs/changes. During review, it searches by purpose, not just keywords to surface parallel implementations to align with, pull relevant tests to reuse or extend, and recall how similar issues were fixed before.</p>\n<p> <strong>Why this helps:</strong> Suggestions are grounded in how your legacy codebase already solves similar problems, reducing rework, improving consistency, and speeding up test coverage.</p>\n<p> <strong>Seeing it in action:</strong> Using similarity retrieval, CodeRabbit surfaces a different test with the same callback pattern and proposes the same fix.</p>\n</li>\n<li><h3 id=\"heading-your-team-rules-not-generic-advice\"><strong>Your team rules, not generic advice</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757090919900/00918dac-6fe3-44b4-9b4a-a823f89562bc.png\" alt class=\"image--center mx-auto\" /></p>\n<p> CodeRabbit reviews are primed with your standards (naming, error handling, API boundaries, security requirements, performance expectations, testing norms) that you can share with us via coding guidelines and review instructions.</p>\n<p> <strong>Why this helps:</strong> Feedback reflects <em>your</em> standards and context, not a one-size-fits-all checklist.</p>\n<p> <strong>Seeing it in action:</strong> CodeRabbit flags a missing Prisma migration after a schema edit. A developer replies that migrations are auto-generated during deploy, a repo-specific rule. CodeRabbit stores that as a <strong>Learning</strong> to avoid future false positives.</p>\n</li>\n<li><h3 id=\"heading-signals-from-tools\"><strong>Signals from tools</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757091000646/a73d1f9d-d93e-4666-803d-843f356f720a.png\" alt class=\"image--center mx-auto\" /></p>\n<p> Alongside AI reasoning, CodeRabbit runs linters and security analyzers and folds their findings into our easy-to-read and understand reviews.</p>\n<p> <strong>Why this helps:</strong> You get grounded, actionable suggestions backed by both AI <em>and</em> recognizable tools.</p>\n<p> <strong>Seeing it in action:</strong> CodeRabbit will do things like point to the exact ESLint rule and line numbers, rewrites the callback as a typed declaration, and guards the call with optional chaining.</p>\n</li>\n<li><h3 id=\"heading-evidence-not-vibes-verification-scripts\"><strong>Evidence, not vibes (verification scripts)</strong></h3>\n<p> <img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1757092768117/d45e60b4-6cdd-46b7-9c48-e470874544cf.png\" alt class=\"image--center mx-auto\" /></p>\n<p> When something needs checking, CodeRabbit generates shell/Python checks (think grep, ast-grep) to confirm an assumption or extract proof from the codebase before we post the comment.</p>\n<p> <strong>Why this helps:</strong> Comments come with <strong><em>receipts</em></strong>. That translates into less noise and more comments that actually improve your code.</p>\n<p> <strong>Seeing it in action:</strong> The comment pinpoints the file and loop, explains the failure mode, and proposes the exact change produced by the verification agent after analyzing the parsing path.</p>\n</li>\n</ol>\n<p>This is <strong>context engineering in practice</strong>: gathering, filtering, and organizing the <em>right</em> information before asking the model to judge. It’s been core to CodeRabbit since day one.</p>\n<p>The payoff is simple: higher signal, lower noise, and reviews that feel like they understand your system.</p>\n<h2 id=\"heading-scaling-to-enterprise-size-repos\"><strong>Scaling to enterprise-size repos</strong></h2>\n<p>CodeRabbit has an advantage on massive codebases and legacy codebases because we designed our pipeline with scale in mind.</p>\n<p>When a PR arrives, CodeRabbit spins up an isolated, secure, short-lived environment to do the work. It pulls only what it needs, constructs the context, runs the checks, and tears everything down after. During busy hours, many of these workers run in parallel so review speed holds steady. You stay in control of scope by using path filters to keep bulky or generated assets out of the way, and choosing whether to enable caching or indexing to accelerate repeat reviews.</p>\n<p>In short: selective scope keeps context focused, isolation keeps it safe, and elastic execution keeps it fast. This approach scales with your codebase and your release calendar.</p>\n<h2 id=\"heading-coderabbit-large-codebase-ai-code-reviews-done-right\"><strong>CodeRabbit: Large codebase AI code reviews done right</strong></h2>\n<p>CodeRabbit’s advantage on massive codebases isn’t a single trick. It comes from how we approach context engineering end-to-end: map what the change touches, tie it to intent, apply your rules, verify with tools, then comment with evidence.</p>\n<p>We’ve operated this way from the start, well before “context engineering” became a buzzword, because it’s the only reliable path to accurate, low-noise reviews at scale.</p>\n<p><strong><em>Ready to see a deep-context review on your large codebase? →</em></strong> <a target=\"_blank\" href=\"https://coderabbit.link/sY5vXpT\"><strong><em>Start a 14-day trial</em></strong></a></p>\n",
      "summary": "Massive codebases are a special kind of beast. They sprawl across hundreds of files, evolve over years of commits, and occasionally feel like they’re held together by equal parts duct tape and institutional memory. Reviewing changes in that environment isn’t just hard – it feels like an archaeological dig. Did this line move here last week for a reason? Is there another file quietly depending on it?\nThat’s exactly where CodeRabbit shines. It was built for scale, so instead of drowning you in disconnected file-by-file comments, it reviews with the whole history and architecture of your massive codebase in mind. The larger and older your repository, the more useful CodeRabbit becomes because it can see the patterns, dependencies, and rules that humans usually forget about halfway through a pull request when trying to keep all the dependencies in that legacy code in their head.\nLarge codebase? AI code reviews need more context!\n\nCodeRabbit is known for performing great on large repos. Our tool doesn’t just skim your pull requests; it goes full archivist. Before leaving a single comment, it gathers the surrounding code from your large codebase and pulls in dozens of points of context from your code. AI agents then trace how those pieces have moved through history, apply your team’s coding standards, and even double-check their own reasoning with scripts and tools.\nThe effect is reviews that feel unusually…informed about your legacy codebase. It catches cross-file issues before they turn into production mysteries, enforces consistency without nitpicking, and scales comfortably across sprawling repos with long, complicated pasts.\nThe power you gain through this is clearer, earlier feedback on real risks, fewer “wait, what else did that touch?” surprises, and reviews that actually reflect how your whole massive codebase fits together.\nThe problem with diff-only reviews (or what goes wrong without context)\nCode diffs are necessary, but they’re not sufficient. In a massive codebase, a 10-line change can quietly alter a shared helper used by multiple services, shift a public API contract, or undermine a security assumption that lives outside the files in the diff.\nAI Bot reviewers who only see the diff are flying without instruments within a large codebase. AI that can’t see where the changed code is referenced, what else tends to change with it, or whether the change actually matches the ticket’s intent, might work for a smaller codebase but not for yours.\nWithout the right context, you get ping-pong cycles (“Can you also update…?”), late surprises at merge time, and a steady drip of small regressions that add up. The review looks fine on paper, while production tells a different story.\nBuilding the right context on your legacy codebase (and how that helps your PRs)\nThink of CodeRabbit as assembling a case file before giving an opinion. Here’s what goes into that case file and how each piece shows up in your reviews.\nA map of your code (Codegraph)\n \n CodeRabbit builds a lightweight map of definitions and references and scans commit history for files that frequently change together throughout your massive codebase. This creates a map of file dependencies that CodeRabbit uses to check if any changes in your PR will break other dependencies in your codebase.\n Why this helps: The review can reason across files, not just lines.\n Seeing it in action: CodeRabbit posts a summary listing bugs outside the diff range that CodeRabbit located by traversing related files with Codegraph.\n Here’s an example of the files that CodeGraph brings in from across a repository when completing a PR review.\n \nCode Index (semantic & similarity retrieval)\n \n CodeRabbit maintains a semantic index (embeddings) of functions, classes/modules, tests, and prior PRs/changes. During review, it searches by purpose, not just keywords to surface parallel implementations to align with, pull relevant tests to reuse or extend, and recall how similar issues were fixed before.\n Why this helps: Suggestions are grounded in how your legacy codebase already solves similar problems, reducing rework, improving consistency, and speeding up test coverage.\n Seeing it in action: Using similarity retrieval, CodeRabbit surfaces a different test with the same callback pattern and proposes the same fix.\nYour team rules, not generic advice\n \n CodeRabbit reviews are primed with your standards (naming, error handling, API boundaries, security requirements, performance expectations, testing norms) that you can share with us via coding guidelines and review instructions.\n Why this helps: Feedback reflects your standards and context, not a one-size-fits-all checklist.\n Seeing it in action: CodeRabbit flags a missing Prisma migration after a schema edit. A developer replies that migrations are auto-generated during deploy, a repo-specific rule. CodeRabbit stores that as a Learning to avoid future false positives.\nSignals from tools\n \n Alongside AI reasoning, CodeRabbit runs linters and security analyzers and folds their findings into our easy-to-read and understand reviews.\n Why this helps: You get grounded, actionable suggestions backed by both AI and recognizable tools.\n Seeing it in action: CodeRabbit will do things like point to the exact ESLint rule and line numbers, rewrites the callback as a typed declaration, and guards the call with optional chaining.\nEvidence, not vibes (verification scripts)\n \n When something needs checking, CodeRabbit generates shell/Python checks (think grep, ast-grep) to confirm an assumption or extract proof from the codebase before we post the comment.\n Why this helps: Comments come with receipts. That translates into less noise and more comments that actually improve your code.\n Seeing it in action: The comment pinpoints the file and loop, explains the failure mode, and proposes the exact change produced by the verification agent after analyzing the parsing path.\nThis is context engineering in practice: gathering, filtering, and organizing the right information before asking the model to judge. It’s been core to CodeRabbit since day one.\nThe payoff is simple: higher signal, lower noise, and reviews that feel like they understand your system.\nScaling to enterprise-size repos\nCodeRabbit has an advantage on massive codebases and legacy codebases because we designed our pipeline with scale in mind.\nWhen a PR arrives, CodeRabbit spins up an isolated, secure, short-lived environment to do the work. It pulls only what it needs, constructs the context, runs the checks, and tears everything down after. During busy hours, many of these workers run in parallel so review speed holds steady. You stay in control of scope by using path filters to keep bulky or generated assets out of the way, and choosing whether to enable caching or indexing to accelerate repeat reviews.\nIn short: selective scope keeps context focused, isolation keeps it safe, and elastic execution keeps it fast. This approach scales with your codebase and your release calendar.\nCodeRabbit: Large codebase AI code reviews done right\nCodeRabbit’s advantage on massive codebases isn’t a single trick. It comes from how we approach context engineering end-to-end: map what the change touches, tie it to intent, apply your rules, verify with tools, then comment with evidence.\nWe’ve operated this way from the start, well before “context engineering” became a buzzword, because it’s the only reliable path to accurate, low-noise reviews at scale.\nReady to see a deep-context review on your large codebase? → Start a 14-day trial",
      "publishedAt": "2025-09-05T17:20:16.000Z",
      "author": "Sahana Vijaya Prasad",
      "source": "rss",
      "feedName": "CodeRabbit",
      "sourceType": "competitor_blog",
      "company": "CodeRabbit",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval",
        "agents",
        "ide",
        "testing"
      ],
      "ingestedAt": "2025-12-04T14:17:12.177Z",
      "score": 0.03910765478021181
    },
    {
      "id": "e4104baabccbc7455885b5700c5f549a",
      "title": "How confessions can keep language models honest",
      "url": "https://openai.com/index/how-confessions-can-keep-language-models-honest",
      "content": "OpenAI researchers are testing “confessions,” a method that trains models to admit when they make mistakes or act undesirably, helping improve AI honesty, transparency, and trust in model outputs.",
      "summary": "OpenAI researchers are testing “confessions,” a method that trains models to admit when they make mistakes or act undesirably, helping improve AI honesty, transparency, and trust in model outputs.",
      "publishedAt": "2025-12-03T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [
        "code_review",
        "testing"
      ],
      "ingestedAt": "2025-12-04T14:17:12.374Z",
      "score": 6.894446299727548
    },
    {
      "id": "07086e73bc037c6565aaaff87c9c19e3",
      "title": "OpenAI to acquire Neptune",
      "url": "https://openai.com/index/openai-to-acquire-neptune",
      "content": "OpenAI is acquiring Neptune to deepen visibility into model behavior and strengthen the tools researchers use to track experiments and monitor training.",
      "summary": "OpenAI is acquiring Neptune to deepen visibility into model behavior and strengthen the tools researchers use to track experiments and monitor training.",
      "publishedAt": "2025-12-03T10:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:12.374Z",
      "score": 2.757778519891019
    },
    {
      "id": "9c758928cf8860b920470d92a9d01f1c",
      "title": "Announcing the initial People-First AI Fund grantees",
      "url": "https://openai.com/index/people-first-ai-fund-grantees",
      "content": "The OpenAI Foundation announces the initial recipients of the People-First AI Fund, awarding $40.5M in unrestricted grants to 208 nonprofits supporting community innovation and opportunity.",
      "summary": "The OpenAI Foundation announces the initial recipients of the People-First AI Fund, awarding $40.5M in unrestricted grants to 208 nonprofits supporting community innovation and opportunity.",
      "publishedAt": "2025-12-03T08:00:00.000Z",
      "source": "rss",
      "feedName": "OpenAI News",
      "sourceType": "platform_blog",
      "company": "OpenAI",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:12.375Z",
      "score": 10.965647710817905
    },
    {
      "id": "6f04776afdf5220467fe46b99749fd6d",
      "title": "Evaluating Deep Agents: Our Learnings",
      "url": "https://blog.langchain.com/evaluating-deep-agents-our-learnings/",
      "content": "<p>Over the past month at LangChain, we shipped four applications on top of the Deep Agents harness:</p><ul><li><a href=\"https://blog.langchain.com/introducing-deepagents-cli/\">DeepAgents CLI</a>: a coding agent</li><li>LangSmith Assist: an in-app agent to help with various things in LangSmith</li><li>Personal Email Assistant: an email assistant that learns from interactions with each user</li><li><a href=\"https://blog.langchain.com/langsmith-agent-builder-now-in-public-beta/\">Agent Builder</a>: a</li></ul>",
      "summary": "Over the past month at LangChain, we shipped four applications on top of the Deep Agents harness:\n\nDeepAgents CLI: a coding agent\nLangSmith Assist: an in-app agent to help with various things in LangSmith\nPersonal Email Assistant: an email assistant that learns from interactions with each user\nAgent Builder: a",
      "publishedAt": "2025-12-03T17:44:17.000Z",
      "author": "LangChain Accounts",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:12.829Z",
      "score": 9.40675291866189
    },
    {
      "id": "a7ae04488aabfb49ed48f4513bf6da98",
      "title": "LangSmith Agent Builder now in Public Beta",
      "url": "https://blog.langchain.com/langsmith-agent-builder-now-in-public-beta/",
      "content": "Now anyone can create production ready agents without writing code, just chat.\nAgent Builder guides you from initial idea to deployed agent, creating detailed prompts, selecting required tools, and even creating subagents.",
      "summary": "Now anyone can create production ready agents without writing code, just chat.\nAgent Builder guides you from initial idea to deployed agent, creating detailed prompts, selecting required tools, and even creating subagents.",
      "publishedAt": "2025-12-02T16:30:39.000Z",
      "author": "LangChain Accounts",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:12.829Z",
      "score": 13.08952008519962
    },
    {
      "id": "43f5d2c5069336ee5a1885ee81f94d3e",
      "title": "Using skills with Deep Agents",
      "url": "https://blog.langchain.com/using-skills-with-deep-agents/",
      "content": "<p>tl;dr: Anthropic recently introduced the idea of <a href=\"https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills?ref=blog.langchain.com\" rel=\"noopener noreferrer\">agent skills</a>. Skills are simply folders containing a <a href=\"http://skill.md/?ref=blog.langchain.com\" rel=\"noopener noreferrer\">SKILL.md</a> file along with any associated files (e.g., documents or scripts) that an agent can discover and load dynamically to perform better at specific tasks. <strong>We&apos;ve added skills support</strong> to</p>",
      "summary": "tl;dr: Anthropic recently introduced the idea of agent skills. Skills are simply folders containing a SKILL.md file along with any associated files (e.g., documents or scripts) that an agent can discover and load dynamically to perform better at specific tasks. We've added skills support to",
      "publishedAt": "2025-11-25T16:45:09.000Z",
      "author": "Lance Martin",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:12.829Z",
      "score": 5.826265530288733
    },
    {
      "id": "6e378ae46f9b2c5b62fc7629e4121c3b",
      "title": "How agents can use filesystems for context engineering",
      "url": "https://blog.langchain.com/how-agents-can-use-filesystems-for-context-engineering/",
      "content": "<p>By Nick Huang</p><p>A key feature of <a href=\"https://blog.langchain.com/deep-agents/\">deep agents</a> is their access to a set of filesystem tools. Deep agents can use these tools to read, write, edit, list, and search for files in their filesystem.</p><p>In this post, we&#x2019;ll walk through why we think filesystems are important</p>",
      "summary": "By Nick Huang\nA key feature of deep agents is their access to a set of filesystem tools. Deep agents can use these tools to read, write, edit, list, and search for files in their filesystem.\nIn this post, we’ll walk through why we think filesystems are important",
      "publishedAt": "2025-11-21T18:45:13.000Z",
      "author": "LangChain Accounts",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:12.829Z",
      "score": 4.004054435279921
    },
    {
      "id": "985131f8ceb79cd47bb3ef57884a8bfb",
      "title": "How Jimdo empower solopreneurs with AI-powered business assistance",
      "url": "https://blog.langchain.com/customers-jimdo/",
      "content": "See how Jimdo uses LangChain.js, LangGraph.js, and LangSmith to deliver personalized business insights that drive 50% more first customer contacts and 40% more overall customer activity.",
      "summary": "See how Jimdo uses LangChain.js, LangGraph.js, and LangSmith to deliver personalized business insights that drive 50% more first customer contacts and 40% more overall customer activity.",
      "publishedAt": "2025-11-20T01:47:31.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:12.829Z",
      "score": 4.60785289396183
    },
    {
      "id": "b27ae2bc05a737f23fcecb6269b3a5da",
      "title": "How ServiceNow uses LangSmith to get visibility into its customer success agents",
      "url": "https://blog.langchain.com/customers-servicenow/",
      "content": "<p><strong>Authors: </strong><em>Ganesh Srinivasan (ServiceNow), Linda Ye (LangChain), and Jake Broekhuizen (LangChain)</em></p><p>ServiceNow is a leading digital workflow platform that helps enterprises transform service management across IT, customer service, and other departments. To improve their internal sales and customer success operations, ServiceNow&apos;s AI team is using LangSmith and LangGraph</p>",
      "summary": "Authors: Ganesh Srinivasan (ServiceNow), Linda Ye (LangChain), and Jake Broekhuizen (LangChain)\nServiceNow is a leading digital workflow platform that helps enterprises transform service management across IT, customer service, and other departments. To improve their internal sales and customer success operations, ServiceNow's AI team is using LangSmith and LangGraph",
      "publishedAt": "2025-11-17T22:42:50.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:12.829Z",
      "score": 4.56694650147802
    },
    {
      "id": "8f5798e972d05abecb217992caf57325",
      "title": "Execute Code with Sandboxes for DeepAgents",
      "url": "https://blog.langchain.com/execute-code-with-sandboxes-for-deepagents/",
      "content": "<p>By Vivek Trivedy</p><p>Today we&apos;re excited to launch Sandboxes for DeepAgents, a new set of integrations that allow you to safely execute arbitrary DeepAgent code in remote sandboxes. We currently support sandboxes from 3 of our partners: <a href=\"https://www.runloop.ai/?ref=blog.langchain.com\">Runloop</a>, <a href=\"https://www.daytona.io/?ref=blog.langchain.com\">Daytona</a>, and <a href=\"https://modal.com/?ref=blog.langchain.com\">Modal</a>. Below, we dive into what you can</p>",
      "summary": "By Vivek Trivedy\nToday we're excited to launch Sandboxes for DeepAgents, a new set of integrations that allow you to safely execute arbitrary DeepAgent code in remote sandboxes. We currently support sandboxes from 3 of our partners: Runloop, Daytona, and Modal. Below, we dive into what you can",
      "publishedAt": "2025-11-13T16:22:20.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "product_launch",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:12.829Z",
      "score": 2.4697121318294357
    },
    {
      "id": "bfe9dd799455d3b59dc1944a15f55f52",
      "title": "Join LangChain at AWS re:Invent 2025",
      "url": "https://blog.langchain.com/join-langchain-at-aws-re-invent-2025/",
      "content": "<p>If you&apos;re attending AWS re:Invent in Las Vegas this year and working on agent development, here&apos;s what we have planned:</p><h2 id=\"visit-us-at-booth-524\">Visit Us at Booth #524</h2><p>We&apos;ll be at Booth #524 in the Venetian Expo Center, next to the Industry Pavilion, December 1-4. Our</p>",
      "summary": "If you're attending AWS re:Invent in Las Vegas this year and working on agent development, here's what we have planned:\nVisit Us at Booth #524\nWe'll be at Booth #524 in the Venetian Expo Center, next to the Industry Pavilion, December 1-4. Our",
      "publishedAt": "2025-11-11T00:58:44.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "general",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:12.829Z",
      "score": 0.9295762914394621
    },
    {
      "id": "35bf5cf8090b5db4f59fdb81c62895a6",
      "title": "Why We Rebuilt LangChain’s Chatbot and What We Learned",
      "url": "https://blog.langchain.com/rebuilding-chat-langchain/",
      "content": "<p><em>By Liam Bush</em></p><h2 id=\"background\">Background</h2><p>Every successful platform needs reliable support, but we realized our own team was spending hours tracking down answers to technical questions. This friction wasn&apos;t just slowing down our engineers&#x2014;it was a critical <strong>bottleneck</strong> for our users.</p><p>We set out to solve this</p>",
      "summary": "By Liam Bush\nBackground\nEvery successful platform needs reliable support, but we realized our own team was spending hours tracking down answers to technical questions. This friction wasn't just slowing down our engineers—it was a critical bottleneck for our users.\nWe set out to solve this",
      "publishedAt": "2025-11-05T16:28:53.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:12.829Z",
      "score": 0.38049392915002855
    },
    {
      "id": "fe24922605f234a8b32ff9e8ee404b05",
      "title": "Introducing DeepAgents CLI",
      "url": "https://blog.langchain.com/introducing-deepagents-cli/",
      "content": "<p><em>By </em><a href=\"https://www.linkedin.com/in/vivek-trivedy-433509134/?ref=blog.langchain.com\"><em>Vivek Trivedy</em></a></p><p>We&apos;re excited to introduce <strong>DeepAgents CLI</strong> for coding, research, and building agents with persistent memory. Now you can easily create and run custom DeepAgents directly from the terminal. It supports:</p><ul><li><strong>Read, write, and edit files</strong> in your project</li><li><strong>Execute shell commands</strong> with human approval</li><li><strong>Search</strong></li></ul>",
      "summary": "By Vivek Trivedy\nWe're excited to introduce DeepAgents CLI for coding, research, and building agents with persistent memory. Now you can easily create and run custom DeepAgents directly from the terminal. It supports:\n\nRead, write, and edit files in your project\nExecute shell commands with human approval\nSearch",
      "publishedAt": "2025-10-30T16:55:35.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:12.829Z",
      "score": 1.1582531447545688
    },
    {
      "id": "6be2480ae9b32bf3b491c4e502760868",
      "title": "Introducing LangSmith’s No Code Agent Builder",
      "url": "https://blog.langchain.com/langsmith-agent-builder/",
      "content": "<p><em>By Brace Sproul and Sam Crowder</em></p><p>Today, we&#x2019;re expanding who can build agents beyond developers. While a lot of the highest volume, customer-facing agents will be built by technical teams, nearly every business user has use cases for agentic applications in their daily routines. Our new <strong>LangSmith Agent</strong></p>",
      "summary": "By Brace Sproul and Sam Crowder\nToday, we’re expanding who can build agents beyond developers. While a lot of the highest volume, customer-facing agents will be built by technical teams, nearly every business user has use cases for agentic applications in their daily routines. Our new LangSmith Agent",
      "publishedAt": "2025-10-29T14:38:43.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:12.830Z",
      "score": 1.0711098640136876
    },
    {
      "id": "b7a1c14ff5fb54ec9b794f1291406698",
      "title": "Doubling down on DeepAgents",
      "url": "https://blog.langchain.com/doubling-down-on-deepagents/",
      "content": "<p>Two months ago <a href=\"https://blog.langchain.com/deep-agents/\">we wrote about Deep Agents</a> - a term we coined for agents that are able to do complex, open ended tasks over longer time horizons. We hypothesized that there were four key elements to those agents: a planning tool, access to a filesystem, subagents, and detailed prompts.</p>",
      "summary": "Two months ago we wrote about Deep Agents - a term we coined for agents that are able to do complex, open ended tasks over longer time horizons. We hypothesized that there were four key elements to those agents: a planning tool, access to a filesystem, subagents, and detailed prompts.",
      "publishedAt": "2025-10-28T17:02:22.000Z",
      "author": "LangChain Accounts",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:12.830Z",
      "score": 0.9326589552774137
    },
    {
      "id": "35f5e149ddd7b981222eeccafddaca18",
      "title": "Agent Frameworks, Runtimes, and Harnesses- oh my!",
      "url": "https://blog.langchain.com/agent-frameworks-runtimes-and-harnesses-oh-my/",
      "content": "<p>There are few different open source packages we maintain: <a href=\"https://docs.langchain.com/oss/python/langchain/quickstart?ref=blog.langchain.com\">LangChain</a> and <a href=\"https://docs.langchain.com/oss/python/langgraph/overview?ref=blog.langchain.com\">LangGraph</a> being the biggest ones, but <a href=\"https://docs.langchain.com/oss/python/deepagents/overview?ref=blog.langchain.com\">DeepAgents</a> being an increasingly popular one. I&#x2019;ve started using different terms to describe them: LangChain is an agent framework, LangGraph is an agent runtime, DeepAgents is an <a href=\"https://www.vtrivedy.com/posts/claude-code-sdk-haas-harness-as-a-service?ref=blog.langchain.com\">agent harness</a>. Other folks</p>",
      "summary": "There are few different open source packages we maintain: LangChain and LangGraph being the biggest ones, but DeepAgents being an increasingly popular one. I’ve started using different terms to describe them: LangChain is an agent framework, LangGraph is an agent runtime, DeepAgents is an agent harness. Other folks",
      "publishedAt": "2025-10-25T16:14:35.000Z",
      "author": "LangChain Accounts",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:12.830Z",
      "score": 0.5776796143940996
    },
    {
      "id": "6a3e63855e3e53cf75d5700936c60023",
      "title": "Improve agent quality with Insights Agent and Multi-turn Evals, now in LangSmith",
      "url": "https://blog.langchain.com/insights-agent-multiturn-evals-langsmith/",
      "content": "LangSmith's new Insights Agent and Multi-turn Evals help you understand what your agents are doing in production and whether they're accomplishing user goals.",
      "summary": "LangSmith's new Insights Agent and Multi-turn Evals help you understand what your agents are doing in production and whether they're accomplishing user goals.",
      "publishedAt": "2025-10-23T14:23:55.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:12.830Z",
      "score": 0.6474471173913238
    },
    {
      "id": "1befb3ac924097b0b30e6945eeb84153",
      "title": "LangChain and LangGraph Agent Frameworks Reach v1.0 Milestones",
      "url": "https://blog.langchain.com/langchain-langgraph-1dot0/",
      "content": "<p><em>By Sydney Runkle and the LangChain OSS team </em></p><p>We&apos;re releasing LangChain 1.0 and LangGraph 1.0 &#x2014; our first major versions of our open source frameworks! After years of feedback, we&apos;ve updated <code>langchain</code> to focus on the core agent loop, provide flexibility with a new</p>",
      "summary": "By Sydney Runkle and the LangChain OSS team \nWe're releasing LangChain 1.0 and LangGraph 1.0 — our first major versions of our open source frameworks! After years of feedback, we've updated langchain to focus on the core agent loop, provide flexibility with a new",
      "publishedAt": "2025-10-22T14:58:46.000Z",
      "author": "LangChain",
      "source": "rss",
      "feedName": "LangChain Blog",
      "sourceType": "platform_blog",
      "company": "LangChain",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:12.830Z",
      "score": 0.6503074100421651
    },
    {
      "id": "93b1c3f9c3a35488688fc512806e538a",
      "title": "How disconnected data can derail modern investigations",
      "url": "https://www.elastic.co/blog/disconnected-data-modern-investigations",
      "content": "Learn how Elastic can bring together isolated, unstructured data sources using a secure data mesh approach and speed investigations through data and AI using tools like the Elastic Agent Builder.",
      "summary": "Learn how Elastic can bring together isolated, unstructured data sources using a secure data mesh approach and speed investigations through data and AI using tools like the Elastic Agent Builder.",
      "publishedAt": "2025-12-03T00:00:00.000Z",
      "author": "Josh Phifer",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "general",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:13.050Z",
      "score": 6.692274087606426
    },
    {
      "id": "2e76b2ed323cf11e9016d03fdc43460d",
      "title": "Elastic Stack 9.2.2 released ",
      "url": "https://www.elastic.co/blog/elastic-stack-9-2-2-released",
      "content": "<p>Version 9.2.2 of the Elastic Stack was released today. We recommend you <a href=\"https://www.elastic.co/docs/deploy-manage/upgrade\">upgrade to this latest version</a>. We recommend 9.2.2 over the previous versions 9.2.1</p>\n<p>For details of the issues that have been fixed and a full list of changes for each product in this version, please refer to <a href=\"https://www.elastic.co/docs/release-notes\">the release notes</a>.</p>",
      "summary": "Version 9.2.2 of the Elastic Stack was released today. We recommend you upgrade to this latest version. We recommend 9.2.2 over the previous versions 9.2.1\nFor details of the issues that have been fixed and a full list of changes for each product in this version, please refer to the release notes.",
      "publishedAt": "2025-12-02T00:00:00.000Z",
      "author": "Julien Mailleret",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:13.051Z",
      "score": 7.061717617316387
    },
    {
      "id": "ebf9fc5df40d782d3321d040bea7d280",
      "title": "Elastic Stack 9.1.8 released",
      "url": "https://www.elastic.co/blog/elastic-stack-9-1-8-released",
      "content": "<p>Version 9.1.8 of the Elastic Stack was released today. We recommend you <a href=\"https://www.elastic.co/docs/deploy-manage/upgrade\">upgrade to this latest version</a>. We recommend 9.1.8 over the previous versions 9.1.7</p>\n<p>For details of the issues that have been fixed and a full list of changes for each product in this version, please refer to <a href=\"https://www.elastic.co/docs/release-notes\">the release notes</a>.</p>",
      "summary": "Version 9.1.8 of the Elastic Stack was released today. We recommend you upgrade to this latest version. We recommend 9.1.8 over the previous versions 9.1.7\nFor details of the issues that have been fixed and a full list of changes for each product in this version, please refer to the release notes.",
      "publishedAt": "2025-12-02T00:00:00.000Z",
      "author": "Nina Lee",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:13.051Z",
      "score": 7.061717617316387
    },
    {
      "id": "315a54bfa8ddfeea7534e6403bd7c055",
      "title": "Elastic Stack 8.19.8 released ",
      "url": "https://www.elastic.co/blog/elastic-stack-8-19-8-released",
      "content": "<p>Version 8.19.8 of the Elastic Stack was released today. We recommend you <a href=\"https://www.elastic.co/docs/deploy-manage/upgrade\">upgrade to this latest version</a>. We recommend 8.19.8 over the previous versions 8.19.7.</p>\n<p>For details of the issues that have been fixed and a full list of changes for each product in this version, please refer to <a href=\"https://www.elastic.co/guide/en/starting-with-the-elasticsearch-platform-and-its-solutions/8.19/new.html\">the release notes</a>.</p>",
      "summary": "Version 8.19.8 of the Elastic Stack was released today. We recommend you upgrade to this latest version. We recommend 8.19.8 over the previous versions 8.19.7.\nFor details of the issues that have been fixed and a full list of changes for each product in this version, please refer to the release notes.",
      "publishedAt": "2025-12-02T00:00:00.000Z",
      "author": "Nina Lee",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:13.051Z",
      "score": 7.061717617316387
    },
    {
      "id": "75c7a009ef0b90fda971252c5cc070d7",
      "title": "Navigating the Shai-Hulud Worm 2.0: Elastic's updated response to npm supply chain compromise",
      "url": "https://www.elastic.co/blog/shai-hulud-worm-2-0-updated-response",
      "content": "Learn about the steps Elastic took to monitor and put measures in place to mitigate the threat posed from the ever increasing number of compromised packages.",
      "summary": "Learn about the steps Elastic took to monitor and put measures in place to mitigate the threat posed from the ever increasing number of compromised packages.",
      "publishedAt": "2025-12-01T00:00:00.000Z",
      "author": "Mandy Andress",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "feature_update",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:13.051Z",
      "score": 5.801384500820703
    },
    {
      "id": "aa0b5d6b752e298d887a33eeb9ece1c6",
      "title": "Pioneering Elastic among first partners with AWS Agentic AI Specialization",
      "url": "https://www.elastic.co/blog/elastic-aws-competency-agentic-ai",
      "content": "Elastic has earned the AWS Agentic AI Specialization. This recognition validates our expertise in delivering agentic AI solutions that help organizations realize significant gains in business efficiency, creativity, and productivity on AWS.",
      "summary": "Elastic has earned the AWS Agentic AI Specialization. This recognition validates our expertise in delivering agentic AI solutions that help organizations realize significant gains in business efficiency, creativity, and productivity on AWS.",
      "publishedAt": "2025-12-01T00:00:00.000Z",
      "author": "Udayasimha Theepireddy (Uday),Brian Bergholm,Jenn Michel",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "general",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:13.051Z",
      "score": 5.801384500820703
    },
    {
      "id": "ecc4ca41c2fe2b8aaae18dda2f81699f",
      "title": "Elastic and Accenture Collaboration on Data Readiness for GenAI",
      "url": "https://www.elastic.co/blog/accenture-data-readiness-engine",
      "content": "Accenture and Elastic have jointly developed a unified, AI-ready knowledge base called the Data Readiness Engine. Available now on the AWS Marketplace, it helps organizations prepare enterprise data for advanced GenAI applications.",
      "summary": "Accenture and Elastic have jointly developed a unified, AI-ready knowledge base called the Data Readiness Engine. Available now on the AWS Marketplace, it helps organizations prepare enterprise data for advanced GenAI applications.",
      "publishedAt": "2025-12-01T00:00:00.000Z",
      "author": "Udayasimha Theepireddy (Uday),Rekha Thangellapalli",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "pricing_business",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:13.051Z",
      "score": 7.348420367706224
    },
    {
      "id": "afdb59cc40de29becf00a335faa1490a",
      "title": "Elastic 9.2: Agent Builder, DiskBBQ, Streams, Significant Events, and more",
      "url": "https://www.elastic.co/blog/whats-new-elastic-9-2-0",
      "content": "Elastic 9.2 simplifies the development of AI agents with Agent Builder, provides a cost-efficient balance between recall and latency with DiskBBQ, and establishes a new AI-powered foundation to get more from logs with Streams and Significant Events.",
      "summary": "Elastic 9.2 simplifies the development of AI agents with Agent Builder, provides a cost-efficient balance between recall and latency with DiskBBQ, and establishes a new AI-powered foundation to get more from logs with Streams and Significant Events.",
      "publishedAt": "2025-10-23T00:00:00.000Z",
      "author": "Dan Courcy",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:13.051Z",
      "score": 0.6441452565206155
    },
    {
      "id": "71366014124ae7276337a86f8c5e1876",
      "title": "Why data sovereignty is mission-critical for global defence organisations",
      "url": "https://www.elastic.co/blog/data-sovereignty",
      "content": "Data sovereignty is central to defence strategy, allowing organisations to maintain data control, protect sensitive information, act confidently, and ensure mission readiness.",
      "summary": "Data sovereignty is central to defence strategy, allowing organisations to maintain data control, protect sensitive information, act confidently, and ensure mission readiness.",
      "publishedAt": "2025-11-19T00:00:00.000Z",
      "author": "Alf Franklin",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "general",
      "tags": [
        "code_review",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:13.051Z",
      "score": 2.1336900428821757
    },
    {
      "id": "3684bc3972a88726a7a946abe8321b42",
      "title": "DevRel newsletter — November 2025",
      "url": "https://www.elastic.co/blog/devrel-newsletter-november-2025",
      "content": "In this newsletter blog, we cover Elastic 9.2, complimentary on-demand trainings, the latest blogs and videos, and upcoming events.",
      "summary": "In this newsletter blog, we cover Elastic 9.2, complimentary on-demand trainings, the latest blogs and videos, and upcoming events.",
      "publishedAt": "2025-11-20T00:00:00.000Z",
      "author": "Elastic DevRel team",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "general",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:13.051Z",
      "score": 1.2339769649643082
    },
    {
      "id": "1f8da5bfa5c75cb0e67bb93f0dfc8422",
      "title": "Elasticsearch: The context engine for grounding and orchestration in Microsoft Azure AI Foundry Agent Service",
      "url": "https://www.elastic.co/blog/elasticsearch-microsoft-azure-ai-foundry-agent-service",
      "content": "The native integration of Elasticsearch with Microsoft Azure AI Foundry Agent Service ensures that AI agents are always grounded in enterprise truth, delivering verifiable and relevant outcomes without using organization data to train public LLMs. ",
      "summary": "The native integration of Elasticsearch with Microsoft Azure AI Foundry Agent Service ensures that AI agents are always grounded in enterprise truth, delivering verifiable and relevant outcomes without using organization data to train public LLMs.",
      "publishedAt": "2025-11-18T00:00:00.000Z",
      "author": "Greg Crist,Matt Ryan",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:13.051Z",
      "score": 4.431644776017504
    },
    {
      "id": "e1bb738c400250d3f26fcfe9a4f70283",
      "title": "Detecting the undetectable: Building a fraud detection framework with Elastic",
      "url": "https://www.elastic.co/blog/building-fraud-detection-framework",
      "content": "This blog explains how to use the Elasticsearch Platform for fraud detection with built-in Elastic features like detection rules, machine learning jobs, and Attack Discovery.",
      "summary": "This blog explains how to use the Elasticsearch Platform for fraud detection with built-in Elastic features like detection rules, machine learning jobs, and Attack Discovery.",
      "publishedAt": "2025-11-17T00:00:00.000Z",
      "author": "Kyle Rozanitis",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "thought_leadership",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:13.052Z",
      "score": 0.5691226897115185
    },
    {
      "id": "df35dab81cd1d117280ffe91d253ea4f",
      "title": "Elastic wins 2025 Microsoft SDC Latin America Partner of the Year ",
      "url": "https://www.elastic.co/blog/elastic-wins-2025-microsoft-latam-partner-of-the-year",
      "content": "Elastic recognized as 2025 Microsoft SDC Partner of the Year across multiple regions, validating its continued innovative partnership with Microsoft as a Global SDC that enables customers to succeed through powerful search and GenAI. ",
      "summary": "Elastic recognized as 2025 Microsoft SDC Partner of the Year across multiple regions, validating its continued innovative partnership with Microsoft as a Global SDC that enables customers to succeed through powerful search and GenAI.",
      "publishedAt": "2025-11-12T00:00:00.000Z",
      "author": "Jake Pollock,Lisa Troshian,Greg Crist,Jeff Maddocks,Brian Bergholm",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:13.052Z",
      "score": 0.49774939548527136
    },
    {
      "id": "5d552c7c61b10d2fac63a270474fbdbd",
      "title": "Elastic named a Leader in the IDC MarketScape: Worldwide Observability Platforms 2025 Vendor Assessment",
      "url": "https://www.elastic.co/blog/elastic-observability-idc-marketscape-leader-2025",
      "content": "Elastic is a Leader in the IDC MarketScape: Worldwide Observability Platforms 2025 Vendor Assessment. Learn why Elastic was recognized in the report.",
      "summary": "Elastic is a Leader in the IDC MarketScape: Worldwide Observability Platforms 2025 Vendor Assessment. Learn why Elastic was recognized in the report.",
      "publishedAt": "2025-11-14T21:09:30.000Z",
      "author": "Natalie Blake",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "general",
      "tags": [
        "ide",
        "observability"
      ],
      "ingestedAt": "2025-12-04T14:17:13.052Z",
      "score": 1.1007111681303778
    },
    {
      "id": "3d3e320ea6a4c31015dc1b9b42e6ed15",
      "title": "It's time for the defense and intelligence community to upgrade endpoint security",
      "url": "https://www.elastic.co/blog/defense-and-intelligence-community-endpoint-security",
      "content": "Elastic’s endpoint security, built on Endgame’s proven tech, delivers battle-tested protection for defense and intelligence agencies — running seamlessly on Linux, in air-gapped networks, and in deployable fly-away kits.",
      "summary": "Elastic’s endpoint security, built on Endgame’s proven tech, delivers battle-tested protection for defense and intelligence agencies — running seamlessly on Linux, in air-gapped networks, and in deployable fly-away kits.",
      "publishedAt": "2025-11-13T00:00:00.000Z",
      "author": "Matt Isett",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "security_incident",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:13.052Z",
      "score": 2.88685875347193
    },
    {
      "id": "fa83293a6851e8b1dcbb1af7d53beb9d",
      "title": "Elastic named a Leader in the IDC MarketScape: Worldwide General-Purpose Knowledge Discovery 2025 Vendor Assessment",
      "url": "https://www.elastic.co/blog/elasticsearch-idc-marketscape-leader-2025",
      "content": "Elastic was named a Leader in the IDC MarketScape: Worldwide General-Purpose Knowledge Discovery 2025. Our innovation unifies search, analytics, and AI to power multimodal and agentic experiences across enterprise and ecommerce use cases.",
      "summary": "Elastic was named a Leader in the IDC MarketScape: Worldwide General-Purpose Knowledge Discovery 2025. Our innovation unifies search, analytics, and AI to power multimodal and agentic experiences across enterprise and ecommerce use cases.",
      "publishedAt": "2025-11-12T00:00:00.000Z",
      "author": "Natalie Blake",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:13.052Z",
      "score": 2.488746977426357
    },
    {
      "id": "8adec54f05f6d9ce2657ece4fc1cc6ef",
      "title": "Elastic Stack 9.1.7 released",
      "url": "https://www.elastic.co/blog/elastic-stack-9-1-7-released",
      "content": "<p>Version 9.1.7 of the Elastic Stack was released today. We recommend you <a href=\"https://www.elastic.co/docs/deploy-manage/upgrade\">upgrade to this latest version</a>. We recommend 9.1.7 over the previous versions 9.1.6</p>\n<p>For details of the issues that have been fixed and a full list of changes for each product in this version, please refer to <a href=\"https://www.elastic.co/docs/release-notes\">the release notes</a>.</p>",
      "summary": "Version 9.1.7 of the Elastic Stack was released today. We recommend you upgrade to this latest version. We recommend 9.1.7 over the previous versions 9.1.6\nFor details of the issues that have been fixed and a full list of changes for each product in this version, please refer to the release notes.",
      "publishedAt": "2025-11-11T00:00:00.000Z",
      "author": "Nina Lee",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:13.052Z",
      "score": 1.5756821815721467
    },
    {
      "id": "95450fd0f91444ae53bf899ea294b322",
      "title": "Elastic Stack 8.19.7 released ",
      "url": "https://www.elastic.co/blog/elastic-stack-8-19-7-released",
      "content": "<p>Version 8.19.7 of the Elastic Stack was released today. We recommend you <a href=\"https://www.elastic.co/docs/deploy-manage/upgrade\">upgrade to this latest version</a>. We recommend 8.19.7 over the previous version 8.19.6</p>\n<p>For details of the issues that have been fixed and a full list of changes for each product in this version, please refer to <a href=\"https://www.elastic.co/guide/en/starting-with-the-elasticsearch-platform-and-its-solutions/8.19/new.html\">the release notes</a>.</p>",
      "summary": "Version 8.19.7 of the Elastic Stack was released today. We recommend you upgrade to this latest version. We recommend 8.19.7 over the previous version 8.19.6\nFor details of the issues that have been fixed and a full list of changes for each product in this version, please refer to the release notes.",
      "publishedAt": "2025-11-11T00:00:00.000Z",
      "author": "Stamatis Kourkoutas",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:13.052Z",
      "score": 1.5756821815721467
    },
    {
      "id": "52fd4ca1627f0be4e1db799a203cc673",
      "title": "Elastic Stack 9.2.1 released",
      "url": "https://www.elastic.co/blog/elastic-stack-9-2-1-released",
      "content": "<p>Version 9.2.1 of the Elastic Stack was released today. We recommend you <a href=\"https://www.elastic.co/docs/deploy-manage/upgrade\">upgrade to this latest version</a>. We recommend 9.2.1 over the previous versions 9.2.0</p>\n<p>For details of the issues that have been fixed and a full list of changes for each product in this version, please refer to  <a href=\"https://www.elastic.co/docs/release-notes\">the release notes</a>.</p>",
      "summary": "Version 9.2.1 of the Elastic Stack was released today. We recommend you upgrade to this latest version. We recommend 9.2.1 over the previous versions 9.2.0\nFor details of the issues that have been fixed and a full list of changes for each product in this version, please refer to  the release notes.",
      "publishedAt": "2025-11-11T00:00:00.000Z",
      "author": "Vincent Deuschle",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "product_launch",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:13.052Z",
      "score": 1.5756821815721467
    },
    {
      "id": "d72ac9721e5dd4a63b97010d98bfcaf1",
      "title": "3 real-world generative AI strategies for executives",
      "url": "https://www.elastic.co/blog/generative-ai-strategies-for-executives",
      "content": "Learn how Elastic's real-world generative AI tools like Support Assistant, ElasticGPT, and AI Assistant delivered measurable business outcomes through a strategy that included executive ownership, high-quality data, and workflow integration.",
      "summary": "Learn how Elastic's real-world generative AI tools like Support Assistant, ElasticGPT, and AI Assistant delivered measurable business outcomes through a strategy that included executive ownership, high-quality data, and workflow integration.",
      "publishedAt": "2025-10-08T00:00:00.000Z",
      "author": "Jay Shah",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:13.052Z",
      "score": 0.08988706857560087
    },
    {
      "id": "0129a52b74e3d91f0bc0e8706853ed8b",
      "title": "KPMG Technology consulting deploys Elastic Security to cut storage costs, increase visibility, and reduce false positives ",
      "url": "https://www.elastic.co/blog/kpmg-technology-consulting-deploys-elastic-security",
      "content": "KPMG Technology consulting helps clients migrate from legacy SIEM platforms to Elastic Security, delivering 75% cost savings, 10x storage increase, and enhanced threat detection with AI-powered analytics and real-time monitoring capabilities.",
      "summary": "KPMG Technology consulting helps clients migrate from legacy SIEM platforms to Elastic Security, delivering 75% cost savings, 10x storage increase, and enhanced threat detection with AI-powered analytics and real-time monitoring capabilities.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "author": "Sarah Chandler,Tal Grinstein",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "security_incident",
      "tags": [
        "retrieval",
        "observability"
      ],
      "ingestedAt": "2025-12-04T14:17:13.052Z",
      "score": 1.393046330064468
    },
    {
      "id": "ac001ddf5e4a58907430c577410a29d3",
      "title": "Celebrating partner excellence: The 2025–2026 Elastic Partner Awards",
      "url": "https://www.elastic.co/blog/elastic-partner-awards-2025",
      "content": "The Elastic Partner Awards honor partners for driving customer impact through innovation, AI, and cloud adoption. Follow this blog for updates as winners are announced throughout 2025–2026.",
      "summary": "The Elastic Partner Awards honor partners for driving customer impact through innovation, AI, and cloud adoption. Follow this blog for updates as winners are announced throughout 2025–2026.",
      "publishedAt": "2025-11-06T00:00:00.000Z",
      "author": "Vicky Wells",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "feature_update",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:13.052Z",
      "score": 0.58365611474684
    },
    {
      "id": "3486ded4afa8089cc1cea509ec7123d9",
      "title": "Money20/20 USA 2025: Fintech’s inflection point has arrived",
      "url": "https://www.elastic.co/blog/elastic-money2020-usa-fintech-data-ai-trust",
      "content": "From AI-powered insight to stablecoin innovation, Money20/20 USA 2025 showed that fintech’s future is built on unified data, real-time intelligence, and the infrastructure of trust.",
      "summary": "From AI-powered insight to stablecoin innovation, Money20/20 USA 2025 showed that fintech’s future is built on unified data, real-time intelligence, and the infrastructure of trust.",
      "publishedAt": "2025-11-03T00:00:00.000Z",
      "author": "Karen Mcdermott",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:13.052Z",
      "score": 0.26171067131132697
    },
    {
      "id": "0df088ce44c988757761a234901839e2",
      "title": "Elastic Cloud Serverless pricing and packaging: Evolved for scale and simplicity",
      "url": "https://www.elastic.co/blog/elastic-cloud-serverless-pricing-packaging",
      "content": "Learn more about Elastic Cloud Serverless pricing and packaging, focusing on our approach for solution-specific pricing, pricing metrics, consumption, and support.",
      "summary": "Learn more about Elastic Cloud Serverless pricing and packaging, focusing on our approach for solution-specific pricing, pricing metrics, consumption, and support.",
      "publishedAt": "2025-11-01T00:00:00.000Z",
      "author": "Chris DiStasio,Gaurav Sharma",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "observability"
      ],
      "ingestedAt": "2025-12-04T14:17:13.053Z",
      "score": 0.9528590269842602
    },
    {
      "id": "3282fb0770a1ec8effa2575a8a933fa0",
      "title": "Elevating public sector cyber defense with AI-powered threat hunting",
      "url": "https://www.elastic.co/blog/public-sector-cyber-defense-ai-threat-hunting",
      "content": "Explore real-life examples, workflows, and AI-powered features that demonstrate how Elastic can be a game changer in threat hunting for public sector.",
      "summary": "Explore real-life examples, workflows, and AI-powered features that demonstrate how Elastic can be a game changer in threat hunting for public sector.",
      "publishedAt": "2025-10-31T00:00:00.000Z",
      "author": "Brixton Pizzuti",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:13.053Z",
      "score": 0.21123132722143706
    },
    {
      "id": "fff99ec33f03357709b3b0d040e3e697",
      "title": "Expedite your SIEM migration with Automatic Migration for Dashboards",
      "url": "https://www.elastic.co/blog/automatic-migration-for-dashboards",
      "content": "Take advantage of Automatic Migration for Dashboards (technical preview) to start planning your SIEM migration to Elastic Security. The agentic translation method handles complexity with ease for a smooth migration experience.",
      "summary": "Take advantage of Automatic Migration for Dashboards (technical preview) to start planning your SIEM migration to Elastic Security. The agentic translation method handles complexity with ease for a smooth migration experience.",
      "publishedAt": "2025-10-29T00:00:00.000Z",
      "author": "Charles Davison",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:13.053Z",
      "score": 0.9155588465158137
    },
    {
      "id": "49b8c841197f9c0ec38cb765a8957600",
      "title": "Build and buy: Why a durable enterprise architecture delivers business impact at scale",
      "url": "https://www.elastic.co/blog/build-and-buy-ai-strategy",
      "content": "Learn why a durable enterprise architecture is essential for a “build and buy” AI strategy. Build an enterprise architecture that uses existing SaaS investments to ensure context-aware AI outputs and technology performance with observability. ",
      "summary": "Learn why a durable enterprise architecture is essential for a “build and buy” AI strategy. Build an enterprise architecture that uses existing SaaS investments to ensure context-aware AI outputs and technology performance with observability.",
      "publishedAt": "2025-10-28T00:00:00.000Z",
      "author": "Chris Blaisure",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "pricing_business",
      "tags": [
        "code_review",
        "observability"
      ],
      "ingestedAt": "2025-12-04T14:17:13.053Z",
      "score": 0.7160519222804829
    },
    {
      "id": "380b07f0a6859b03cf34ac69370ae6e1",
      "title": "Stopping USB-borne attacks at the endpoint with Elastic Security Device Control",
      "url": "https://www.elastic.co/blog/elastic-security-device-control",
      "content": "Stop USB-borne threats with Elastic Security Device Control. Manage external storage access to block malware, prevent data loss, and reduce insider risks — adding a vital layer to your endpoint defense.",
      "summary": "Stop USB-borne threats with Elastic Security Device Control. Manage external storage access to block malware, prevent data loss, and reduce insider risks — adding a vital layer to your endpoint defense.",
      "publishedAt": "2025-10-28T00:00:00.000Z",
      "author": "Roxana Gheorghe",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "security_incident",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:13.053Z",
      "score": 0.8865404752044074
    },
    {
      "id": "a81bb5b53c3922e49daf67ba5729eb5f",
      "title": "AI can do what now?! Accelerating SIEM migration",
      "url": "https://www.elastic.co/blog/accelerating-siem-migration",
      "content": "AI automation turns SIEM migration from months to minutes. Using LLMs and RAG, Elastic converts legacy rules, maps data, normalizes queries, and flags issues—speeding migration while keeping human review for accuracy.",
      "summary": "AI automation turns SIEM migration from months to minutes. Using LLMs and RAG, Elastic converts legacy rules, maps data, normalizes queries, and flags issues—speeding migration while keeping human review for accuracy.",
      "publishedAt": "2025-10-27T00:00:00.000Z",
      "author": "Elastic Security Team",
      "source": "rss",
      "feedName": "Elasticsearch Blog",
      "sourceType": "infra_blog",
      "company": "Elastic",
      "contentType": "general",
      "tags": [
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.053Z",
      "score": 0.2539768735889048
    },
    {
      "id": "4749bb4bc12385f84eacdb91572dd251",
      "title": "AWS 2025 Rising Star Technology Partner",
      "url": "https://weaviate.io/blog/aws-2025-partner-award",
      "content": "Weaviate recognized by AWS Partners in Benelux as leaders in helping customers drive innovation",
      "summary": "Weaviate recognized by AWS Partners in Benelux as leaders in helping customers drive innovation",
      "publishedAt": "2025-12-03T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "general",
      "tags": [
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.294Z",
      "score": 3.5692121237926577
    },
    {
      "id": "f9839010ab8223029e55770a0727824e",
      "title": "Announcing the new Weaviate Java Client v6",
      "url": "https://weaviate.io/blog/weaviate-java-client-v6",
      "content": "The Weaviate Java client v6 is now generally available! This release brings a completely redesigned API that embraces modern Java patterns, simplifies common operations, and makes working with vector databases more intuitive than ever.",
      "summary": "The Weaviate Java client v6 is now generally available! This release brings a completely redesigned API that embraces modern Java patterns, simplifies common operations, and makes working with vector databases more intuitive than ever.",
      "publishedAt": "2025-12-02T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "product_launch",
      "tags": [
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 8.307901403329225
    },
    {
      "id": "1e802ce4ceb9c901e8602ad55232d70d",
      "title": "Bringing RAG to Life with Dify and Weaviate",
      "url": "https://weaviate.io/blog/dify-and-weaviate",
      "content": "Learn how to use the Dify and Weaviate integration to build RAG applications.",
      "summary": "Learn how to use the Dify and Weaviate integration to build RAG applications.",
      "publishedAt": "2025-11-20T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "thought_leadership",
      "tags": [
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 2.291671044087327
    },
    {
      "id": "e3af55eae5673c9951805a293075367c",
      "title": "Weaviate 1.34 Release",
      "url": "https://weaviate.io/blog/weaviate-1-34-release",
      "content": "1.34 introduces flat index support with RQ quantization, server-side batching improvements, new client libraries, Contextual AI integration and much more.",
      "summary": "1.34 introduces flat index support with RQ quantization, server-side batching improvements, new client libraries, Contextual AI integration and much more.",
      "publishedAt": "2025-11-11T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 2.0391177076844365
    },
    {
      "id": "f62804b712d1ada5001e8a9a1f688921",
      "title": "Weaviate security release - Medium and High severity fixes for CVEs",
      "url": "https://weaviate.io/blog/weaviate-security-release-november-2025",
      "content": "Weaviate announces two CVEs that are fixed in updated versions of our product.",
      "summary": "Weaviate announces two CVEs that are fixed in updated versions of our product.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 1.5323506552325827
    },
    {
      "id": "ec6048630c0893f3c102e80046236cb0",
      "title": "From Kitchen Experiments to Five Star Service: The Weaviate Development Journey",
      "url": "https://weaviate.io/blog/day0-day1-day2-operations",
      "content": "What building AI apps with Weaviate and cooking have in common? Let’s find out!",
      "summary": "What building AI apps with Weaviate and cooking have in common? Let’s find out!",
      "publishedAt": "2025-11-06T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "general",
      "tags": [
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 0.5188053311062288
    },
    {
      "id": "c0b8796e8d8ada550a22d3b93c07d6a9",
      "title": "Evals and Guardrails in Enterprise workflows (Part 3)",
      "url": "https://weaviate.io/blog/evals-enterprise-workflows-3",
      "content": "Hands-on patterns: Design pattern for gen-AI enterprise applications, with Arize AI.",
      "summary": "Hands-on patterns: Design pattern for gen-AI enterprise applications, with Arize AI.",
      "publishedAt": "2025-11-04T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "pricing_business",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 1.0681345800453426
    },
    {
      "id": "e2aa7c1c97c1a9a707d2755b8a58267e",
      "title": "Unleash Real-Time Agentic AI with Streaming Agents on Confluent Cloud and Weaviate",
      "url": "https://weaviate.io/blog/confluent-streaming-agents-and-weaviate",
      "content": "Learn how Confluent’s Streaming Agents and Weaviate combine real-time context with semantic understanding for agentic AI.",
      "summary": "Learn how Confluent’s Streaming Agents and Weaviate combine real-time context with semantic understanding for agentic AI.",
      "publishedAt": "2025-10-30T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "feature_update",
      "tags": [
        "retrieval",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 0.8653461842803032
    },
    {
      "id": "cfad02c62500a1043203f07f108506a9",
      "title": "A Simpler, More Transparent Pricing Model for Weaviate Cloud",
      "url": "https://weaviate.io/blog/weaviate-cloud-pricing-update",
      "content": "Weaviate Cloud gets an updated pricing model.",
      "summary": "Weaviate Cloud gets an updated pricing model.",
      "publishedAt": "2025-10-28T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "feature_update",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 0.7501494827857429
    },
    {
      "id": "3536d381c72630d4f8c31c8aad40e45d",
      "title": "Legacy data to RAG : Modernise Your Apps with Amazon Sagemaker Unified Studio",
      "url": "https://weaviate.io/blog/sagemaker-studio-rag",
      "content": "A guide to seamlessly transform data sitting in lakes and warehouses for GenAI capable applications",
      "summary": "A guide to seamlessly transform data sitting in lakes and warehouses for GenAI capable applications",
      "publishedAt": "2025-10-16T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "thought_leadership",
      "tags": [
        "retrieval",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 0.13023125619254095
    },
    {
      "id": "0bd8d8f5fb92164de163cd1ef54145a5",
      "title": "When Good Models Go Bad",
      "url": "https://weaviate.io/blog/when-good-models-go-bad",
      "content": "A strategic guide to the costs, risks and rewards of upgrading embedding models in production AI",
      "summary": "A strategic guide to the costs, risks and rewards of upgrading embedding models in production AI",
      "publishedAt": "2025-10-09T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "thought_leadership",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 0.1316487495561114
    },
    {
      "id": "b26470af99b2cfdccef4f8a42f313045",
      "title": "Rethinking Vector Search at Scale: Weaviate's Native, Efficient and Optimized Multi-Tenancy",
      "url": "https://weaviate.io/blog/weaviate-multi-tenancy-architecture-explained",
      "content": "Learn how Weaviate's native multi-tenancy architecture delivers scalable vector search with one shard per tenant, dynamic resource management, and true data isolation.",
      "summary": "Learn how Weaviate's native multi-tenancy architecture delivers scalable vector search with one shard per tenant, dynamic resource management, and true data isolation.",
      "publishedAt": "2025-10-08T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "general",
      "tags": [
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 0.06537240037667831
    },
    {
      "id": "b1cb267d9e67667504f6fa4dcfb7cb00",
      "title": "Weaviate 1.33 Release",
      "url": "https://weaviate.io/blog/weaviate-1-33-release",
      "content": "1.33 brings compression by default for optimal resource utilization, powerful 1-bit rotational quantization (RQ), streamlined server-side batch imports, enhanced OIDC group management, and collection aliases become generally available (GA).",
      "summary": "1.33 brings compression by default for optimal resource utilization, powerful 1-bit rotational quantization (RQ), streamlined server-side batch imports, enhanced OIDC group management, and collection aliases become generally available (GA).",
      "publishedAt": "2025-10-02T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "product_launch",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 0.11711187094729686
    },
    {
      "id": "fd8d89053b485b841a4f2344ee9ca19f",
      "title": "Building Multi-Agent Systems with Crew AI and Weaviate",
      "url": "https://weaviate.io/blog/building-multi-agent-systems",
      "content": "Learn about how you can build multi-agent systems with CrewAI and Weaviate.",
      "summary": "Learn about how you can build multi-agent systems with CrewAI and Weaviate.",
      "publishedAt": "2025-10-01T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "general",
      "tags": [
        "retrieval",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 0.05947554769119765
    },
    {
      "id": "98bed1c66bf5cc653b5f2c21d57d109d",
      "title": "Evals and Guardrails in Enterprise workflows (Part 2)",
      "url": "https://weaviate.io/blog/evals-guardrails-enterprise-workflows-2",
      "content": "Hands-on patterns: LLM-as-Judge with LangChain and W&B",
      "summary": "Hands-on patterns: LLM-as-Judge with LangChain and W&B",
      "publishedAt": "2025-09-25T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "pricing_business",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 0.06134576666232052
    },
    {
      "id": "09124efbf8ea2687f970f5989ec2f3a0",
      "title": "Weaviate is now ISO 27001 compliant",
      "url": "https://weaviate.io/blog/weaviate-iso-compliant",
      "content": "Announcing Weaviate has achieved ISO 27001 compliance.",
      "summary": "Announcing Weaviate has achieved ISO 27001 compliance.",
      "publishedAt": "2025-09-24T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "product_launch",
      "tags": [
        "retrieval",
        "governance"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 0.08417206741512873
    },
    {
      "id": "cd24f6d402a3a5e6660f389e5ef9b0fc",
      "title": "Search Mode Benchmarking",
      "url": "https://weaviate.io/blog/search-mode-benchmarking",
      "content": "Learn how Search Mode compares against Hybrid Search on the BEIR, LoTTe, BRIGHT, EnronQA, and WixQA Information Retrieval benchmarks.",
      "summary": "Learn how Search Mode compares against Hybrid Search on the BEIR, LoTTe, BRIGHT, EnronQA, and WixQA Information Retrieval benchmarks.",
      "publishedAt": "2025-09-23T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "benchmark_eval",
      "tags": [
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 0.027989099664630056
    },
    {
      "id": "c570821a2ef4b1316081a2e8ae555569",
      "title": "Accelerating Data Workflows with Query Agent, now GA",
      "url": "https://weaviate.io/blog/query-agent-generally-available",
      "content": "The Query Agent is now generally available! Learn how the interface to databases is shifting with the introduction of agentic retrievers – domain experts at using Weaviate’s APIs with your data.",
      "summary": "The Query Agent is now generally available! Learn how the interface to databases is shifting with the introduction of agentic retrievers – domain experts at using Weaviate’s APIs with your data.",
      "publishedAt": "2025-09-17T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "product_launch",
      "tags": [
        "retrieval",
        "agents"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 0.04375966249600652
    },
    {
      "id": "e3cd2e2d17440071f3f948dce4683447",
      "title": "Using Weaviate Cloud Queries in MacOS apps",
      "url": "https://weaviate.io/blog/apple-and-weaviate-2",
      "content": "A practical guide on using Weaviate Cloud Queries in MacOS apps.",
      "summary": "A practical guide on using Weaviate Cloud Queries in MacOS apps.",
      "publishedAt": "2025-09-09T00:00:00.000Z",
      "source": "rss",
      "feedName": "Weaviate Blog",
      "sourceType": "infra_blog",
      "company": "Weaviate",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:13.295Z",
      "score": 0.016474582949625453
    },
    {
      "id": "eff9c5ef04ba3c92308f4b17b981b796",
      "title": "Why Life Sciences AI Is a Search Problem (Part 4 of 5)",
      "url": "https://blog.vespa.ai/why-life-sciences-ai-is-a-search-problem-4-of-5/",
      "content": "The future of GenAI in pharma and healthcare isn’t about building bigger models — it’s about smarter retrieval.",
      "summary": "The future of GenAI in pharma and healthcare isn’t about building bigger models — it’s about smarter retrieval.",
      "publishedAt": "2025-12-04T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "thought_leadership",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.510Z",
      "score": 6.229406648704178
    },
    {
      "id": "8798bba87373704a6d6e6886bcf403a5",
      "title": "Vespa Newsletter, December 2025",
      "url": "https://blog.vespa.ai/vespa-newsletter-december-2025/",
      "content": "Advances in Vespa features and performance include automated ANN tuning, accelerated vector distance calculations with Google Highway, precise chunk-level matching with enhanced sameElement, and expressive proximity queries with nested NEAR operators.\n",
      "summary": "Advances in Vespa features and performance include automated ANN tuning, accelerated vector distance calculations with Google Highway, precise chunk-level matching with enhanced sameElement, and expressive proximity queries with nested NEAR operators.",
      "publishedAt": "2025-12-03T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.510Z",
      "score": 6.246120106422251
    },
    {
      "id": "a89d5b1c6d4f339a363c10039e8215cc",
      "title": "Why Life Sciences AI Is a Search Problem (Part 3 of 5)",
      "url": "https://blog.vespa.ai/why-life-sciences-ai-is-a-search-problem-3-of-5/",
      "content": "The future of GenAI in pharma and healthcare isn’t about building bigger models — it’s about smarter retrieval.",
      "summary": "The future of GenAI in pharma and healthcare isn’t about building bigger models — it’s about smarter retrieval.",
      "publishedAt": "2025-12-03T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "thought_leadership",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.510Z",
      "score": 5.799968670249234
    },
    {
      "id": "494473dead113e025414dfa616788cad",
      "title": "Why Life Sciences AI Is a Search Problem (Part 2 of 5)",
      "url": "https://blog.vespa.ai/why-life-sciences-ai-is-a-search-problem-2-of-5/",
      "content": "The future of GenAI in pharma and healthcare isn't about building bigger models - it's about smarter retrieval.",
      "summary": "The future of GenAI in pharma and healthcare isn't about building bigger models - it's about smarter retrieval.",
      "publishedAt": "2025-12-02T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "thought_leadership",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.510Z",
      "score": 5.4001349523184965
    },
    {
      "id": "5bf60131d3eb4b2d6925b324ff982ac2",
      "title": "Why Life Sciences AI Is a Search Problem (Part 1 of 5)",
      "url": "https://blog.vespa.ai/why-life-sciences-ai-is-a-search-problem-1-of-5/",
      "content": "The future of GenAI in pharma and healthcare isn't about building bigger models - it's about smarter retrieval.",
      "summary": "The future of GenAI in pharma and healthcare isn't about building bigger models - it's about smarter retrieval.",
      "publishedAt": "2025-12-01T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "thought_leadership",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.510Z",
      "score": 5.027864659482509
    },
    {
      "id": "d491b574209dab34acda8d91b58852b4",
      "title": "Beyond Vector Search: The Move to Tensor-Based Retrieval",
      "url": "https://blog.vespa.ai/beyond-vector-search/",
      "content": "Tensors preserve critical context, making them far better suited for advanced retrieval tasks where precision and explainability matter.",
      "summary": "Tensors preserve critical context, making them far better suited for advanced retrieval tasks where precision and explainability matter.",
      "publishedAt": "2025-11-28T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.510Z",
      "score": 4.370238703610275
    },
    {
      "id": "6311f7aff2d19dc06e6706bd873b5f21",
      "title": "Vector Search Is Reaching Its Limit. Here’s What Comes Next",
      "url": "https://blog.vespa.ai/vector-search-is-reaching-its-limit/",
      "content": "As RAG applications evolve, they require richer data representations that capture relationships within and across modalities, like text, images and video.",
      "summary": "As RAG applications evolve, they require richer data representations that capture relationships within and across modalities, like text, images and video.",
      "publishedAt": "2025-11-27T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:13.510Z",
      "score": 4.650247537546844
    },
    {
      "id": "44062e08278234e6e2136621de5d4edb",
      "title": "🎄 Advent of Tensors 2025 🎅",
      "url": "https://blog.vespa.ai/advent-of-tensors-2025/",
      "content": "We’re excited to announce Advent of Tensors 2025 — a 24-day coding challenge for anyone curious about tensors.",
      "summary": "We’re excited to announce Advent of Tensors 2025 — a 24-day coding challenge for anyone curious about tensors.",
      "publishedAt": "2025-11-21T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:13.510Z",
      "score": 0.9466727728580087
    },
    {
      "id": "6e892103e737a8d9330999d1ae73d9ca",
      "title": "LLMs, Vespa, and a side of Summer Debugging",
      "url": "https://blog.vespa.ai/interns-mcp-server/",
      "content": "We built a standalone MCP server in Python, then rewrote it in Java for full Vespa integration — and lived to tell the tale.",
      "summary": "We built a standalone MCP server in Python, then rewrote it in Java for full Vespa integration — and lived to tell the tale.",
      "publishedAt": "2025-11-07T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "feature_update",
      "tags": [
        "agents",
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:13.510Z",
      "score": 1.4626980927351407
    },
    {
      "id": "15b157668ad9c0f2ef4467b2eb30685a",
      "title": "Protein models Need a PLM Store: Turning Model Outputs into Searchable Biological Intelligence- beyond LLM's",
      "url": "https://blog.vespa.ai/protein-models-need-a-plm-store/",
      "content": "A story about bridging AI models, LIMS data, and real-world biologics discovery. ",
      "summary": "A story about bridging AI models, LIMS data, and real-world biologics discovery.",
      "publishedAt": "2025-11-03T00:00:00.000Z",
      "source": "rss",
      "feedName": "Vespa Blog",
      "sourceType": "infra_blog",
      "company": "Vespa",
      "contentType": "general",
      "tags": [
        "code_review"
      ],
      "ingestedAt": "2025-12-04T14:17:13.510Z",
      "score": 0.5757632588792789
    },
    {
      "id": "994aa90a7567744ce191abc6bba1dcf5",
      "title": "Average DRAM price in USD over last 18 months",
      "url": "https://pcpartpicker.com/trends/price/memory/",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46142100\">Comments</a>",
      "summary": "Comments",
      "publishedAt": "2025-12-04T00:08:26.000Z",
      "source": "rss",
      "feedName": "Hacker News",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "code_review",
        "retrieval"
      ],
      "ingestedAt": "2025-12-04T14:17:13.906Z",
      "score": 6.232011039633891
    },
    {
      "id": "93ca3245718d9611863166669c76d31c",
      "title": "1D Conway's Life glider found, 3.7B cells long",
      "url": "https://conwaylife.com/forums/viewtopic.php?&p=222136#p222136",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46137253\">Comments</a>",
      "summary": "Comments",
      "publishedAt": "2025-12-03T17:24:49.000Z",
      "source": "rss",
      "feedName": "Hacker News",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:13.906Z",
      "score": 2.819299708486928
    },
    {
      "id": "632889959b436a4d5a136d25ccd8b0dc",
      "title": "Reverse engineering a $1B Legal AI tool exposed 100k+ confidential files",
      "url": "https://alexschapiro.com/security/vulnerability/2025/12/02/filevine-api-100k",
      "content": "<a href=\"https://news.ycombinator.com/item?id=46137514\">Comments</a>",
      "summary": "Comments",
      "publishedAt": "2025-12-03T17:44:33.000Z",
      "source": "rss",
      "feedName": "Hacker News",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [
        "ide"
      ],
      "ingestedAt": "2025-12-04T14:17:13.906Z",
      "score": 2.822060691531041
    },
    {
      "id": "13d79167cdff86f58feba59232c28f5c",
      "title": "Meta is killing off the external Facebook Like button (1 minute read)",
      "url": "https://www.engadget.com/meta-is-killing-off-the-external-facebook-like-button-205207354.html?utm_source=tldrnewsletter",
      "content": "Facebook's Like and Share buttons for third-party websites will be discontinued on February 10.",
      "summary": "Facebook's Like and Share buttons for third-party websites will be discontinued on February 10.",
      "publishedAt": "2025-11-11T00:00:00.000Z",
      "author": "TLDR",
      "source": "rss",
      "feedName": "TLDR Tech",
      "sourceType": "curated_ai",
      "contentType": "general",
      "tags": [],
      "ingestedAt": "2025-12-04T14:17:14.463Z",
      "score": 0.3707483161265575
    }
  ],
  "lastSync": "2025-12-04T14:17:14.466Z"
}