# Full Context Dump (Last 7 Days)

Generated: 2025-12-02T19:16:53.488Z
Total Items: 245

## [9/10] MemoriesDB: A Temporal-Semantic-Relational Database for Long-Term Agent Memory / Modeling Experience as a Graph of Temporal-Semantic Surfaces
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251106179W/abstract
**Reasoning:** MemoriesDB is directly related to context engineering and memory management for AI agents, making it highly relevant.
**Authors:** Ward, Joel

**Content/Abstract:**
> We introduce MemoriesDB, a unified data architecture designed to avoid decoherence across time, meaning, and relation in long-term computational memory. Each memory is a time-semantic-relational entity-a structure that simultaneously encodes when an event occurred, what it means, and how it connects to other events. Built initially atop PostgreSQL with pgvector extensions, MemoriesDB combines the properties of a time-series datastore, a vector database, and a graph system within a single append-only schema. Each memory is represented as a vertex uniquely labeled by its microsecond timestamp and accompanied by low- and high-dimensional normalized embeddings that capture semantic context. Directed edges between memories form labeled relations with per-edge metadata, enabling multiple contextual links between the same vertices. Together these constructs form a time-indexed stack of temporal-semantic surfaces, where edges project as directional arrows in a 1+1-dimensional similarity field, tracing the evolution of meaning through time while maintaining cross-temporal coherence. This formulation supports efficient time-bounded retrieval, hybrid semantic search, and lightweight structural reasoning in a single query path. A working prototype demonstrates scalable recall and contextual reinforcement using standard relational infrastructure, and we discuss extensions toward a columnar backend, distributed clustering, and emergent topic modeling.

---

## [9/10] CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251112159Z/abstract
**Reasoning:** CriticSearch involves search agents and tool-integrated reasoning, which aligns with our primary interest in AI agents in software engineering.
**Authors:** Zhang, Yaocheng, Huang, Haohuan, Song, Zijun, Zhu, Yuanheng, Zhang, Qichao, Zhao, Zijie, Zhao, Dongbin

**Content/Abstract:**
> Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.

---

## [9/10] AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251109005C/abstract
**Reasoning:** The paper discusses a RAG-powered multi-agent pipeline, which is highly relevant to context engineering and AI agents in software engineering.
**Authors:** Chauhan, Alvin

**Content/Abstract:**
> Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of the search space in a gradual, incremental, and sequential (GIS) manner. Stated succinctly, high-quality reasoning is a controlled, incremental search. To test this framework, we investigate the efficacy of recursive refinement (RR)--an iterative process of self-criticism, adversarial stress-testing, and integrating critical feedback--as a practical method for implementing GIS search. We designed an experiment comparing a simple, linear pipeline against a complex, explicitly structured pipeline leveraging a recursive refinement layer. The multi-agent models were constructed to reflect the historical personas of three US Founding Fathers (Hamilton, Jefferson, and Madison) using RAG-powered corpora and were prompted to generate responses to three contemporary political issues. Model performance was evaluated using a two-tiered approach: a quantitative score from an LLM arbiter agent and qualitative human judgment. Our results revealed that the complex model consistently outperformed the simple model across all nine test cases with an average arbiter-outputted score of 88.3 versus 71.7. The complex model's arguments were superior in analytical depth, structural nuance, and strategic framing. We conclude that recursive refinement is a robust architectural feature for enhancing LLM reasoning via GIS search.

---

## [9/10] MALBO: Optimizing LLM-Based Multi-Agent Teams via Multi-Objective Bayesian Optimization
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251111788S/abstract
**Reasoning:** The paper discusses optimizing LLM-based multi-agent systems, which is highly relevant to AI agents in software engineering.
**Authors:** Sabbatella, Antonio

**Content/Abstract:**
> The optimal assignment of Large Language Models (LLMs) to specialized roles in multi-agent systems is a significant challenge, defined by a vast combinatorial search space, expensive black-box evaluations, and an inherent trade-off between performance and cost. Current optimization methods focus on single-agent settings and lack a principled framework for this multi-agent, multi-objective problem. This thesis introduces MALBO (Multi-Agent LLM Bayesian Optimization), a systematic framework designed to automate the efficient composition of LLM-based agent teams. We formalize the assignment challenge as a multi-objective optimization problem, aiming to identify the Pareto front of configurations between task accuracy and inference cost. The methodology employs multi-objective Bayesian Optimization (MOBO) with independent Gaussian Process surrogate models. By searching over a continuous feature-space representation of the LLMs, this approach performs a sample-efficient exploration guided by the expected hypervolume improvement. The primary contribution is a principled and automated methodology that yields a Pareto front of optimal team configurations. Our results demonstrate that the Bayesian optimization phase, compared to an initial random search, maintained a comparable average performance while reducing the average configuration cost by over 45%. Furthermore, MALBO identified specialized, heterogeneous teams that achieve cost reductions of up to 65.8% compared to homogeneous baselines, all while maintaining maximum performance. The framework thus provides a data-driven tool for deploying cost-effective and highly specialized multi-agent AI systems.

---

## [9/10] Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T16:59:28.000Z
**URL:** https://arxiv.org/abs/2512.01878v1
**Reasoning:** This article discusses knowledge graph reasoning, which is directly related to context engineering and RAG.
**Authors:** Gaganpreet Jhajj

**Content/Abstract:**
> In this work, we propose that reasoning in knowledge graph (KG) networks can be guided by surprise minimization. Entities that are close in graph distance will have lower surprise than those farther apart. This connects the Free Energy Principle (FEP) from neuroscience to KG systems, where the KG serves as the agent's generative model. We formalize surprise using the shortest-path distance in directed graphs and provide a framework for KG-based agents. Graph distance appears in graph neural networks as message passing depth and in model-based reinforcement learning as world model trajectories. This work-in-progress study explores whether distance-based surprise can extend recent work showing that syntax minimizes surprise and free energy via tree structures.

---

## [9/10] MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T14:16:57.000Z
**URL:** https://arxiv.org/abs/2512.01710v1
**Reasoning:** The article discusses memory-augmented generation for LLMs, which is directly related to AI Agents in SE.
**Authors:** Stefano Zeppieri

**Content/Abstract:**
> Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.

---

## [9/10] HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T13:31:06.000Z
**URL:** https://arxiv.org/abs/2512.01659v1
**Reasoning:** This article discusses RAG systems and knowledge graph alignment, which are directly relevant to context engineering.
**Authors:** Valentin Noël

**Content/Abstract:**
> Legal AI systems powered by retrieval-augmented generation (RAG) face a critical accountability challenge: when an AI assistant cites case law, statutes, or contractual clauses, practitioners need verifiable guarantees that generated text faithfully represents source documents. Existing hallucination detectors rely on semantic similarity metrics that tolerate entity substitutions, a dangerous failure mode when confusing parties, dates, or legal provisions can have material consequences. We introduce HalluGraph, a graph-theoretic framework that quantifies hallucinations through structural alignment between knowledge graphs extracted from context, query, and response. Our approach produces bounded, interpretable metrics decomposed into \textit{Entity Grounding} (EG), measuring whether entities in the response appear in source documents, and \textit{Relation Preservation} (RP), verifying that asserted relationships are supported by context. On structured control documents, HalluGraph achieves near-perfect discrimination ($&gt;$400 words, $&gt;$20 entities), HalluGraph achieves $AUC = 0.979$, while maintaining robust performance ($AUC \approx 0.89$) on challenging generative legal task, consistently outperforming semantic similarity baselines. The framework provides the transparency and traceability required for high-stakes legal applications, enabling full audit trails from generated assertions back to source passages.

---

## [9/10] Building Intelligent AI Agents with MongoDB Atlas: A Bidirectional Data Flow Architecture
**Source:** The Practical Developer | **Date:** 2025-12-02T12:55:48.000Z
**URL:** https://dev.to/pash10g/building-intelligent-ai-agents-with-mongodb-atlas-a-bidirectional-data-flow-architecture-2obl
**Reasoning:** Directly discusses building AI agents with RAG and semantic search, highly relevant to context engineering.
**Authors:** Pash10g

**Content/Abstract:**
> <p>As AI agents become increasingly sophisticated, the way applications interact with databases is fundamentally changing. Gone are the days of simple CRUD operations and static queries. Modern AI-powered applications require a <strong>bidirectional data flow</strong> where:</p> 
>  
> <ol> 
> <li> 
> <strong>Agents feed from the database</strong> - Using semantic search and retrieval-augmented generation (RAG) to access relevant data</li> 
> <li> 
> <strong>Agents feed back to the database</strong> - Storing conversation context, user interactions, and learned preferences</li> 
> <li> 
> <strong>Agents transform the UI</strong> - Dynamically updating search filters, results, and interface elements based on natural language understanding</li> 
> </ol> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5tyxm32nsyc9ok9zvbzf.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5tyxm32nsyc9ok9zvbzf.png" alt="Application" width="800" height="436"></a></p> 
>  
> <p>In this article, I'll walk you through a production-ready rental property search application that demonstrates how MongoDB Atlas's Document Model and Vector Search capabilities make this bidirectional agent-database architecture not just possible, but elegant and performant.</p> 
>  
> <blockquote> 
> <p>Looking for realistic sample data? All of the screenshots and demos below use the 6k-listing Airbnb dataset that MongoDB published on Hugging Face: <a href="https://huggingface.co/datasets/MongoDB/airbnb_embeddings">https://huggingface.co/datasets/MongoDB/airbnb_embeddings</a>. The repo ships with <code>seed-hf-airbnb-data.js</code>, which downloads that dataset, loads it into Atlas (including the vector field), and makes the entire experience turnkey.</p> 
> </blockquote> 
>  
> <h2> 
>    
>    
>   Why MongoDB Atlas is Perfect for AI Agent Applications 
> </h2> 
>  
> <p>Before diving into the code, let's understand why MongoDB Atlas stands out for agent-based architectures:</p> 
>  
> <h3> 
>    
>    
>   1. <strong>Flexible Document Model</strong> 
> </h3> 
>  
> <p>AI agents work with diverse, semi-structured data - user conversations, property details, embeddings, and metadata. MongoDB's document model handles this naturally without rigid schemas:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>{</span> 
>   <span>"</span><span>_id</span><span>"</span><span>:</span> <span>ObjectId</span><span>(</span><span>"</span><span>...</span><span>"</span><span>),</span> 
>   <span>"</span><span>sessionId</span><span>"</span><span>:</span> <span>"</span><span>user-session-123</span><span>"</span><span>,</span> 
>   <span>"</span><span>userId</span><span>"</span><span>:</span> <span>ObjectId</span><span>(</span><span>"</span><span>...</span><span>"</span><span>),</span> 
>   <span>"</span><span>messages</span><span>"</span><span>:</span> <span>[</span> 
>     <span>{</span> 
>       <span>"</span><span>role</span><span>"</span><span>:</span> <span>"</span><span>user</span><span>"</span><span>,</span> 
>       <span>"</span><span>content</span><span>"</span><span>:</span> <span>"</span><span>Find me a 2BR in Manhattan under $200</span><span>"</span><span>,</span> 
>       <span>"</span><span>timestamp</span><span>"</span><span>:</span> <span>ISODate</span><span>(</span><span>"</span><span>2024-01-15T10:30:00Z</span><span>"</span><span>),</span> 
>       <span>"</span><span>metadata</span><span>"</span><span>:</span> <span>{</span> 
>         <span>"</span><span>context</span><span>"</span><span>:</span> <span>{</span> <span>"</span><span>filters</span><span>"</span><span>:</span> <span>{</span> <span>"</span><span>bedrooms</span><span>"</span><span>:</span> <span>2</span><span>,</span> <span>"</span><span>location</span><span>"</span><span>:</span> <span>"</span><span>New York</span><span>"</span> <span>}</span> <span>}</span> 
>       <span>}</span> 
>     <span>},</span> 
>     <span>{</span> 
>       <span>"</span><span>role</span><span>"</span><span>:</span> <span>"</span><span>assistant</span><span>"</span><span>,</span> 
>       <span>"</span><span>content</span><span>"</span><span>:</span> <span>"</span><span>I found 15 properties matching your criteria...</span><span>"</span><span>,</span> 
>       <span>"</span><span>timestamp</span><span>"</span><span>:</span> <span>ISODate</span><span>(</span><span>"</span><span>2024-01-15T10:30:05Z</span><span>"</span><span>),</span> 
>       <span>"</span><span>metadata</span><span>"</span><span>:</span> <span>{</span> 
>         <span>"</span><span>tool_calls_made</span><span>"</span><span>:</span> <span>1</span><span>,</span> 
>         <span>"</span><span>search_performed</span><span>"</span><span>:</span> <span>true</span><span>,</span> 
>         <span>"</span><span>rental_ids</span><span>"</span><span>:</span> <span>[</span><span>123</span><span>,</span> <span>456</span><span>,</span> <span>789</span><span>]</span> 
>       <span>}</span> 
>     <span>}</span> 
>   <span>],</span> 
>   <span>"</span><span>metadata</span><span>"</span><span>:</span> <span>{</span> 
>     <span>"</span><span>totalMessages</span><span>"</span><span>:</span> <span>2</span><span>,</span> 
>     <span>"</span><span>lastActivity</span><span>"</span><span>:</span> <span>ISODate</span><span>(</span><span>"</span><span>2024-01-15T10:30:05Z</span><span>"</span><span>)</span> 
>   <span>}</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   2. <strong>Native Vector Search</strong> 
> </h3> 
>  
> <p>Atlas Vector Search enables semantic understanding at the database layer. No need for external vector databases or complex integrations:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>{</span> 
>   <span>$vectorSearch</span><span>:</span> <span>{</span> 
>     <span>index</span><span>:</span> <span>"</span><span>rental_vector_search</span><span>"</span><span>,</span> 
>     <span>path</span><span>:</span> <span>"</span><span>text_embeddings</span><span>"</span><span>,</span> 
>     <span>queryVector</span><span>:</span> <span>[</span><span>0.1234</span><span>,</span> <span>-</span><span>0.5678</span><span>,</span> <span>...],</span> <span>// 1536-dimensional embedding</span> 
>     <span>numCandidates</span><span>:</span> <span>100</span><span>,</span> 
>     <span>limit</span><span>:</span> <span>10</span><span>,</span> 
>     <span>filter</span><span>:</span> <span>{</span> 
>       <span>"</span><span>address.market</span><span>"</span><span>:</span> <span>{</span> <span>$eq</span><span>:</span> <span>"</span><span>New York</span><span>"</span> <span>},</span> 
>       <span>"</span><span>price</span><span>"</span><span>:</span> <span>{</span> <span>$lte</span><span>:</span> <span>200</span> <span>},</span> 
>       <span>"</span><span>bedrooms</span><span>"</span><span>:</span> <span>{</span> <span>$gte</span><span>:</span> <span>2</span> <span>}</span> 
>     <span>}</span> 
>   <span>}</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   3. <strong>Rich Querying and Aggregations</strong> 
> </h3> 
>  
> <p>MongoDB's aggregation pipeline lets you combine vector search with traditional filters, scoring, and transformations in a single operation.</p> 
>  
> <h3> 
>    
>    
>   4. <strong>Unified Platform</strong> 
> </h3> 
>  
> <p>Store embeddings, conversation history, user profiles, and application data in one database. No data synchronization headaches.</p> 
>  
> <h2> 
>    
>    
>   Architecture Overview: The Bidirectional Data Flow 
> </h2> 
>  
> <p>Our rental search application demonstrates three key data flows:<br> 
> </p> 
>  
> <div> 
> <pre><code>┌─────────────────────────────────────────────────────────────┐ 
> │                         User Interface                       │ 
> │              (Natural Language + Dynamic Filters)            │ 
> └───────────────────────┬─────────────────────────────────────┘ 
>                         │ 
>                         ▼ 
> ┌─────────────────────────────────────────────────────────────┐ 
> │                    OpenAI Agents SDK                         │ 
> │              (GPT-5-mini with Custom Tools)                 │ 
> └───────┬───────────────────────────┬─────────────────────────┘ 
>         │                           │ 
>         │ ① Agents Feed FROM DB     │ ② Agents Feed TO DB 
>         ▼                           ▼ 
> ┌─────────────────────┐     ┌──────────────────────────────┐ 
> │  Vector Search      │     │  Conversation Storage        │ 
> │  • Embeddings       │     │  • Chat History             │ 
> │  • Semantic Query   │     │  • User Context             │ 
> │  • Filters          │     │  • Search Metadata          │ 
> └─────────────────────┘     └──────────────────────────────┘ 
>         │                           │ 
>         └───────────┬───────────────┘ 
>                     ▼ 
>         ③ Agents Transform UI 
>     ┌──────────────────────────┐ 
>     │  • Update Search Filters │ 
>     │  • Display Results       │ 
>     │  • Modify Interface      │ 
>     └──────────────────────────┘ 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Let's explore each flow in detail.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Flow 1: Agents Feed FROM the Database (RAG Pattern) 
> </h2> 
>  
> <p>The first and most critical flow is how agents access relevant data to answer user queries. This is the classic <strong>Retrieval-Augmented Generation (RAG)</strong> pattern.</p> 
>  
> <h3> 
>    
>    
>   Step 1: Vector Embeddings as Data Foundation 
> </h3> 
>  
> <p>Every rental property in our database includes a 1536-dimensional embedding generated from its description, amenities, and location:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>{</span> 
>   <span>"</span><span>_id</span><span>"</span><span>:</span> <span>12345</span><span>,</span> 
>   <span>"</span><span>name</span><span>"</span><span>:</span> <span>"</span><span>Luxury Manhattan Loft</span><span>"</span><span>,</span> 
>   <span>"</span><span>description</span><span>"</span><span>:</span> <span>"</span><span>Stunning 2-bedroom loft in heart of SoHo...</span><span>"</span><span>,</span> 
>   <span>"</span><span>property_type</span><span>"</span><span>:</span> <span>"</span><span>Loft</span><span>"</span><span>,</span> 
>   <span>"</span><span>price</span><span>"</span><span>:</span> <span>175</span><span>,</span> 
>   <span>"</span><span>bedrooms</span><span>"</span><span>:</span> <span>2</span><span>,</span> 
>   <span>"</span><span>address</span><span>"</span><span>:</span> <span>{</span> 
>     <span>"</span><span>market</span><span>"</span><span>:</span> <span>"</span><span>New York</span><span>"</span><span>,</span> 
>     <span>"</span><span>neighbourhood</span><span>"</span><span>:</span> <span>"</span><span>SoHo</span><span>"</span><span>,</span> 
>     <span>"</span><span>country</span><span>"</span><span>:</span> <span>"</span><span>United States</span><span>"</span> 
>   <span>},</span> 
>   <span>"</span><span>amenities</span><span>"</span><span>:</span> <span>[</span><span>"</span><span>WiFi</span><span>"</span><span>,</span> <span>"</span><span>Kitchen</span><span>"</span><span>,</span> <span>"</span><span>Elevator</span><span>"</span><span>,</span> <span>"</span><span>Gym</span><span>"</span><span>],</span> 
>   <span>"</span><span>text_embeddings</span><span>"</span><span>:</span> <span>[</span><span>0.023</span><span>,</span> <span>-</span><span>0.145</span><span>,</span> <span>0.891</span><span>,</span> <span>...],</span> <span>// ← Generated from OpenAI</span> 
>   <span>"</span><span>review_scores</span><span>"</span><span>:</span> <span>{</span> 
>     <span>"</span><span>review_scores_rating</span><span>"</span><span>:</span> <span>95</span> 
>   <span>}</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>Key Insight</strong>: Embeddings are stored alongside the data they represent, eliminating the need for separate vector stores and JOIN operations.</p> 
>  
> <h3> 
>    
>    
>   Step 2: Agent Tool Definition 
> </h3> 
>  
> <p>Using the OpenAI Agents SDK, we define a <code>searchRentals</code> tool that the agent can invoke:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>import</span> <span>{</span> <span>Agent</span><span>,</span> <span>tool</span> <span>}</span> <span>from</span> <span>'</span><span>@openai/agents</span><span>'</span><span>;</span> 
> <span>import</span> <span>{</span> <span>z</span> <span>}</span> <span>from</span> <span>'</span><span>zod</span><span>'</span><span>;</span> 
>  
> <span>this</span><span>.</span><span>searchRentalsTool</span> <span>=</span> <span>tool</span><span>({</span> 
>   <span>name</span><span>:</span> <span>'</span><span>searchRentals</span><span>'</span><span>,</span> 
>   <span>description</span><span>:</span> <span>"</span><span>'Search for rental properties using semantic search based on user preferences.',</span><span>"</span> 
>   <span>parameters</span><span>:</span> <span>z</span><span>.</span><span>object</span><span>({</span> 
>     <span>query</span><span>:</span> <span>z</span><span>.</span><span>string</span><span>().</span><span>describe</span><span>(</span><span>'</span><span>Natural language search query</span><span>'</span><span>),</span> 
>     <span>filters</span><span>:</span> <span>z</span><span>.</span><span>object</span><span>({</span> 
>       <span>min_price</span><span>:</span> <span>z</span><span>.</span><span>number</span><span>().</span><span>nullable</span><span>().</span><span>optional</span><span>(),</span> 
>       <span>max_price</span><span>:</span> <span>z</span><span>.</span><span>number</span><span>().</span><span>nullable</span><span>().</span><span>optional</span><span>(),</span> 
>       <span>min_bedrooms</span><span>:</span> <span>z</span><span>.</span><span>number</span><span>().</span><span>nullable</span><span>().</span><span>optional</span><span>(),</span> 
>       <span>location</span><span>:</span> <span>z</span><span>.</span><span>string</span><span>().</span><span>nullable</span><span>().</span><span>optional</span><span>(),</span> 
>       <span>superhost_only</span><span>:</span> <span>z</span><span>.</span><span>boolean</span><span>().</span><span>nullable</span><span>().</span><span>optional</span><span>()</span> 
>     <span>}).</span><span>nullable</span><span>().</span><span>optional</span><span>(),</span> 
>     <span>limit</span><span>:</span> <span>z</span><span>.</span><span>number</span><span>().</span><span>default</span><span>(</span><span>5</span><span>)</span> 
>   <span>}),</span> 
>   <span>execute</span><span>:</span> <span>this</span><span>.</span><span>handleSearchRentals</span><span>.</span><span>bind</span><span>(</span><span>this</span><span>)</span> 
> <span>});</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>What makes this powerful</strong>: The agent understands the tool's capabilities through the description and parameter schema, deciding when and how to invoke it based on user intent.</p> 
>  
> <h3> 
>    
>    
>   Step 3: Hybrid Search Implementation 
> </h3> 
>  
> <p>When the agent invokes the tool, we perform a <strong>hybrid search</strong> combining vector similarity with traditional filters:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>async</span> <span>hybridSearch</span><span>(</span><span>queryText</span><span>,</span> <span>filters</span> <span>=</span> <span>{},</span> <span>limit</span> <span>=</span> <span>10</span><span>)</span> <span>{</span> 
>   <span>// Generate query embedding</span> 
>   <span>const</span> <span>queryEmbedding</span> <span>=</span> <span>await</span> <span>this</span><span>.</span><span>generateEmbedding</span><span>(</span><span>queryText</span><span>);</span> 
>  
>   <span>// Build vector search pipeline</span> 
>   <span>const</span> <span>pipeline</span> <span>=</span> <span>[</span> 
>     <span>{</span> 
>       <span>$vectorSearch</span><span>:</span> <span>{</span> 
>         <span>index</span><span>:</span> <span>"</span><span>rental_vector_search</span><span>"</span><span>,</span> 
>         <span>path</span><span>:</span> <span>"</span><span>text_embeddings</span><span>"</span><span>,</span> 
>         <span>queryVector</span><span>:</span> <span>queryEmbedding</span><span>,</span> 
>         <span>numCandidates</span><span>:</span> <span>100</span><span>,</span> 
>         <span>limit</span><span>:</span> <span>limit</span><span>,</span> 
>         <span>filter</span><span>:</span> <span>{</span> 
>           <span>// Combine semantic + structured filters</span> 
>           <span>"</span><span>address.market</span><span>"</span><span>:</span> <span>filters</span><span>.</span><span>location</span> <span>?</span> <span>{</span> <span>$eq</span><span>:</span> <span>filters</span><span>.</span><span>location</span> <span>}</span> <span>:</span> <span>undefined</span><span>,</span> 
>           <span>"</span><span>price</span><span>"</span><span>:</span> <span>{</span> 
>             <span>$gte</span><span>:</span> <span>filters</span><span>.</span><span>min_price</span> <span>||</span> <span>0</span><span>,</span> 
>             <span>$lte</span><span>:</span> <span>filters</span><span>.</span><span>max_price</span> <span>||</span> <span>999999</span> 
>           <span>},</span> 
>           <span>"</span><span>bedrooms</span><span>"</span><span>:</span> <span>{</span> <span>$gte</span><span>:</span> <span>filters</span><span>.</span><span>min_bedrooms</span> <span>||</span> <span>0</span> <span>},</span> 
>           <span>"</span><span>host.host_is_superhost</span><span>"</span><span>:</span> <span>filters</span><span>.</span><span>superhost_only</span> <span>?</span> <span>{</span> <span>$eq</span><span>:</span> <span>true</span> <span>}</span> <span>:</span> <span>undefined</span> 
>         <span>}</span> 
>       <span>}</span> 
>     <span>},</span> 
>     <span>{</span> 
>       <span>$project</span><span>:</span> <span>{</span> 
>         <span>name</span><span>:</span> <span>1</span><span>,</span> 
>         <span>description</span><span>:</span> <span>"</span><span>1,</span><span>"</span> 
>         <span>property_type</span><span>:</span> <span>1</span><span>,</span> 
>         <span>price</span><span>:</span> <span>1</span><span>,</span> 
>         <span>bedrooms</span><span>:</span> <span>1</span><span>,</span> 
>         <span>"</span><span>address.market</span><span>"</span><span>:</span> <span>1</span><span>,</span> 
>         <span>"</span><span>address.country</span><span>"</span><span>:</span> <span>1</span><span>,</span> 
>         <span>score</span><span>:</span> <span>{</span> <span>$meta</span><span>:</span> <span>"</span><span>vectorSearchScore</span><span>"</span> <span>}</span> <span>// ← Similarity score</span> 
>       <span>}</span> 
>     <span>}</span> 
>   <span>];</span> 
>  
>   <span>return</span> <span>await</span> <span>collection</span><span>.</span><span>aggregate</span><span>(</span><span>pipeline</span><span>).</span><span>toArray</span><span>();</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>MongoDB's Superpower Here</strong>:</p> 
>  
> <ul> 
> <li>Vector search and traditional filters execute in a <strong>single database query</strong> 
> </li> 
> <li>No post-processing, no multiple round-trips</li> 
> <li>Results are sorted by semantic relevance (cosine similarity)</li> 
> </ul> 
>  
> <h3> 
>    
>    
>   Step 4: Agent Processes and Responds 
> </h3> 
>  
> <p>The agent receives structured results and generates a natural language response:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>async</span> <span>handleSearchRentals</span><span>({</span> <span>query</span><span>,</span> <span>filters</span><span>,</span> <span>limit</span> <span>})</span> <span>{</span> 
>   <span>const</span> <span>results</span> <span>=</span> <span>await</span> <span>vectorSearchService</span><span>.</span><span>hybridSearch</span><span>(</span><span>query</span><span>,</span> <span>filters</span><span>,</span> <span>limit</span><span>);</span> 
>  
>   <span>// Format for agent consumption</span> 
>   <span>const</span> <span>formattedResults</span> <span>=</span> <span>results</span><span>.</span><span>map</span><span>((</span><span>rental</span><span>,</span> <span>index</span><span>)</span> <span>=&gt;</span> <span>({</span> 
>     <span>rank</span><span>:</span> <span>index</span> <span>+</span> <span>1</span><span>,</span> 
>     <span>id</span><span>:</span> <span>rental</span><span>.</span><span>_id</span><span>,</span> 
>     <span>name</span><span>:</span> <span>rental</span><span>.</span><span>name</span><span>,</span> 
>     <span>price</span><span>:</span> <span>rental</span><span>.</span><span>price</span><span>,</span> 
>     <span>bedrooms</span><span>:</span> <span>rental</span><span>.</span><span>bedrooms</span><span>,</span> 
>     <span>location</span><span>:</span> <span>`</span><span>${</span><span>rental</span><span>.</span><span>address</span><span>.</span><span>neighbourhood</span><span>}</span><span>, </span><span>${</span><span>rental</span><span>.</span><span>address</span><span>.</span><span>country</span><span>}</span><span>`</span><span>,</span> 
>     <span>rating</span><span>:</span> <span>(</span><span>rental</span><span>.</span><span>review_scores</span><span>.</span><span>review_scores_rating</span> <span>/</span> <span>20</span><span>).</span><span>toFixed</span><span>(</span><span>1</span><span>),</span> 
>     <span>similarity_score</span><span>:</span> <span>rental</span><span>.</span><span>score</span><span>.</span><span>toFixed</span><span>(</span><span>3</span><span>)</span> 
>   <span>}));</span> 
>  
>   <span>return</span> <span>JSON</span><span>.</span><span>stringify</span><span>({</span> 
>     <span>total_found</span><span>:</span> <span>results</span><span>.</span><span>length</span><span>,</span> 
>     <span>query_used</span><span>:</span> <span>query</span><span>,</span> 
>     <span>results</span><span>:</span> <span>formattedResults</span> 
>   <span>});</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>The Result</strong>: User asks "Find me a cozy apartment in Barcelona for under €150" → Agent extracts intent → Searches MongoDB with semantic understanding → Returns relevant properties.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Flow 2: Agents Feed TO the Database (Context Persistence) 
> </h2> 
>  
> <p>What makes AI agents truly intelligent is <strong>memory</strong>. Every interaction teaches the system about user preferences and context. MongoDB's document model makes this persistence natural.</p> 
>  
> <h3> 
>    
>    
>   Conversation Storage Pattern 
> </h3> 
>  
>  
>  
> <div> 
> <pre><code><span>export</span> <span>class</span> <span>ConversationModel</span> <span>{</span> 
>   <span>static</span> <span>async</span> <span>addMessage</span><span>(</span><span>sessionId</span><span>,</span> <span>role</span><span>,</span> <span>content</span><span>,</span> <span>metadata</span> <span>=</span> <span>{},</span> <span>userId</span> <span>=</span> <span>null</span><span>)</span> <span>{</span> 
>     <span>const</span> <span>message</span> <span>=</span> <span>{</span> 
>       <span>id</span><span>:</span> <span>new</span> <span>ObjectId</span><span>().</span><span>toString</span><span>(),</span> 
>       <span>role</span><span>,</span> <span>// 'user' or 'assistant'</span> 
>       <span>content</span><span>,</span> 
>       <span>timestamp</span><span>:</span> <span>new</span> <span>Date</span><span>(),</span> 
>       <span>metadata</span><span>:</span> <span>{</span> 
>         <span>...</span><span>metadata</span><span>,</span> 
>         <span>userId</span><span>:</span> <span>userId</span> <span>||</span> <span>null</span><span>,</span> 
>         <span>isAuthenticated</span><span>:</span> <span>userId</span> <span>!==</span> <span>null</span> 
>       <span>}</span> 
>     <span>};</span> 
>  
>     <span>// Upsert pattern: Create conversation if not exists</span> 
>     <span>await</span> <span>collection</span><span>.</span><span>updateOne</span><span>(</span> 
>       <span>{</span> <span>sessionId</span> <span>},</span> 
>       <span>{</span> 
>         <span>$push</span><span>:</span> <span>{</span> <span>messages</span><span>:</span> <span>message</span> <span>},</span> 
>         <span>$inc</span><span>:</span> <span>{</span> <span>'</span><span>metadata.totalMessages</span><span>'</span><span>:</span> <span>1</span> <span>},</span> 
>         <span>$set</span><span>:</span> <span>{</span> 
>           <span>updatedAt</span><span>:</span> <span>new</span> <span>Date</span><span>(),</span> 
>           <span>'</span><span>metadata.lastActivity</span><span>'</span><span>:</span> <span>new</span> <span>Date</span><span>()</span> 
>         <span>},</span> 
>         <span>$setOnInsert</span><span>:</span> <span>{</span> 
>           <span>userId</span><span>:</span> <span>userId</span><span>,</span> 
>           <span>createdAt</span><span>:</span> <span>new</span> <span>Date</span><span>()</span> 
>         <span>}</span> 
>       <span>},</span> 
>       <span>{</span> <span>upsert</span><span>:</span> <span>true</span> <span>}</span> 
>     <span>);</span> 
>   <span>}</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>Why This Matters</strong>:</p> 
>  
> <ul> 
> <li> 
> <strong>Upsert Pattern</strong>: Create conversation on first message, append to existing ones</li> 
> <li> 
> <strong>Nested Documents</strong>: Messages are embedded in conversation, no JOINs needed</li> 
> <li> 
> <strong>Atomic Updates</strong>: <code>$push</code>, <code>$inc</code>, <code>$set</code> operations are atomic and efficient</li> 
> <li> 
> <strong>Rich Metadata</strong>: Store context about tool calls, search results, user state</li> 
> </ul> 
>  
> <h3> 
>    
>    
>   Storing Agent Metadata 
> </h3> 
>  
> <p>After the agent responds, we capture what it did:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>// Store assistant response with rich metadata</span> 
> <span>await</span> <span>ConversationModel</span><span>.</span><span>addMessage</span><span>(</span><span>sessionId</span><span>,</span> <span>'</span><span>assistant</span><span>'</span><span>,</span> <span>response</span><span>.</span><span>message</span><span>,</span> <span>{</span> 
>   <span>tool_calls_made</span><span>:</span> <span>response</span><span>.</span><span>toolCalls</span><span>?.</span><span>length</span> <span>||</span> <span>0</span><span>,</span> 
>   <span>has_rental_results</span><span>:</span> <span>response</span><span>.</span><span>metadata</span><span>?.</span><span>search_performed</span> <span>||</span> <span>false</span><span>,</span> 
>   <span>search_metadata</span><span>:</span> <span>{</span> 
>     <span>query</span><span>:</span> <span>response</span><span>.</span><span>metadata</span><span>.</span><span>search_query</span><span>,</span> 
>     <span>filters_applied</span><span>:</span> <span>response</span><span>.</span><span>metadata</span><span>.</span><span>search_filters</span><span>,</span> 
>     <span>rental_ids</span><span>:</span> <span>response</span><span>.</span><span>metadata</span><span>.</span><span>rental_ids</span> <span>// ← IDs of returned properties</span> 
>   <span>},</span> 
>   <span>timestamp</span><span>:</span> <span>new</span> <span>Date</span><span>().</span><span>toISOString</span><span>()</span> 
> <span>},</span> <span>userId</span><span>);</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>The Power</strong>: Later queries can reference previous searches, compare properties, or recall user preferences - all because we stored structured metadata alongside conversational content.</p> 
>  
> <h3> 
>    
>    
>   User Activity Tracking 
> </h3> 
>  
> <p>MongoDB's flexible schema lets us track diverse user actions:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>await</span> <span>UserModel</span><span>.</span><span>updateActivity</span><span>(</span><span>userId</span><span>,</span> <span>{</span> 
>   <span>$push</span><span>:</span> <span>{</span> 
>     <span>activity_log</span><span>:</span> <span>{</span> 
>       <span>action</span><span>:</span> <span>'</span><span>search_performed</span><span>'</span><span>,</span> 
>       <span>timestamp</span><span>:</span> <span>new</span> <span>Date</span><span>(),</span> 
>       <span>details</span><span>:</span> <span>{</span> 
>         <span>query</span><span>:</span> <span>userMessage</span><span>,</span> 
>         <span>results_count</span><span>:</span> <span>results</span><span>.</span><span>length</span><span>,</span> 
>         <span>filters_used</span><span>:</span> <span>filters</span> 
>       <span>}</span> 
>     <span>}</span> 
>   <span>},</span> 
>   <span>$inc</span><span>:</span> <span>{</span> <span>'</span><span>stats.total_searches</span><span>'</span><span>:</span> <span>1</span> <span>},</span> 
>   <span>$set</span><span>:</span> <span>{</span> <span>'</span><span>stats.last_search_date</span><span>'</span><span>:</span> <span>new</span> <span>Date</span><span>()</span> <span>}</span> 
> <span>});</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>Real-World Use Case</strong>: Build personalized recommendations, identify power users, analyze search patterns - all from this rich behavioral data.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Flow 3: Agents Transform the UI (Dynamic Interface Updates) 
> </h2> 
>  
> <p>The most magical aspect of agent-database integration is when the agent's understanding directly manipulates the user interface.</p> 
>  
> <h3> 
>    
>    
>   The Metadata Bridge 
> </h3> 
>  
> <p>When an agent performs a search, it returns not just conversational text, but structured metadata:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>{</span> 
>   <span>success</span><span>:</span> <span>true</span><span>,</span> 
>   <span>message</span><span>:</span> <span>"</span><span>I found 15 great properties in Barcelona under €150...</span><span>"</span><span>,</span> 
>   <span>metadata</span><span>:</span> <span>{</span> 
>     <span>search_performed</span><span>:</span> <span>true</span><span>,</span> 
>     <span>search_query</span><span>:</span> <span>"</span><span>cozy apartment in Barcelona under €150</span><span>"</span><span>,</span> 
>     <span>search_filters</span><span>:</span> <span>{</span> 
>       <span>location</span><span>:</span> <span>"</span><span>Barcelona</span><span>"</span><span>,</span> 
>       <span>max_price</span><span>:</span> <span>150</span><span>,</span> 
>       <span>property_type</span><span>:</span> <span>"</span><span>Apartment</span><span>"</span> 
>     <span>},</span> 
>     <span>rental_ids</span><span>:</span> <span>[</span><span>12345</span><span>,</span> <span>12346</span><span>,</span> <span>12347</span><span>,</span> <span>...]</span> 
>   <span>}</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   Frontend Integration 
> </h3> 
>  
> <p>The UI watches for this metadata and reacts:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>async</span> <span>function</span> <span>sendMessage</span><span>()</span> <span>{</span> 
>   <span>const</span> <span>response</span> <span>=</span> <span>await</span> <span>fetch</span><span>(</span><span>'</span><span>/chat</span><span>'</span><span>,</span> <span>{</span> 
>     <span>method</span><span>:</span> <span>'</span><span>POST</span><span>'</span><span>,</span> 
>     <span>body</span><span>:</span> <span>JSON</span><span>.</span><span>stringify</span><span>({</span> 
>       <span>message</span><span>:</span> <span>userInput</span><span>,</span> 
>       <span>context</span><span>:</span> <span>{</span> 
>         <span>current_search</span><span>:</span> <span>searchBar</span><span>.</span><span>value</span><span>,</span> 
>         <span>filters</span><span>:</span> <span>getCurrentFilters</span><span>()</span> 
>       <span>}</span> 
>     <span>})</span> 
>   <span>});</span> 
>  
>   <span>const</span> <span>data</span> <span>=</span> <span>await</span> <span>response</span><span>.</span><span>json</span><span>();</span> 
>  
>   <span>// Display conversational response</span> 
>   <span>displayMessage</span><span>(</span><span>data</span><span>.</span><span>message</span><span>);</span> 
>  
>   <span>// Check if agent performed a search</span> 
>   <span>if </span><span>(</span><span>data</span><span>.</span><span>metadata</span><span>.</span><span>search_performed</span><span>)</span> <span>{</span> 
>     <span>// ① Update UI filters based on agent's understanding</span> 
>     <span>updateFiltersUI</span><span>(</span><span>data</span><span>.</span><span>metadata</span><span>.</span><span>search_filters</span><span>);</span> 
>  
>     <span>// ② Fetch and display the rental results</span> 
>     <span>const</span> <span>rentals</span> <span>=</span> <span>await</span> <span>fetchRentalsByIds</span><span>(</span><span>data</span><span>.</span><span>metadata</span><span>.</span><span>rental_ids</span><span>);</span> 
>     <span>displayRentals</span><span>(</span><span>rentals</span><span>);</span> 
>  
>     <span>// ③ Update URL and browser history</span> 
>     <span>updateURLParams</span><span>(</span><span>data</span><span>.</span><span>metadata</span><span>.</span><span>search_filters</span><span>);</span> 
>   <span>}</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>User Experience</strong>:<br> 
> </p> 
>  
> <div> 
> <pre><code>User: "Show me 2 bedroom apartments in Manhattan under $200" 
>          ↓ 
> Agent: [Understands intent, extracts filters, searches MongoDB] 
>          ↓ 
> UI: ✨ Location dropdown changes to "New York" 
>     ✨ Bedrooms filter updates to "2+" 
>     ✨ Price slider moves to "$0-$200" 
>     ✨ Results grid displays matching properties 
>     ✨ Chat shows: "I found 15 properties matching your criteria..." 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   Bidirectional Filter Sync 
> </h3> 
>  
> <p>The genius is that filters work both ways:</p> 
>  
> <ol> 
> <li> 
> <strong>Manual Filter → Agent Context</strong>: User adjusts UI filters → Passed to agent in next message</li> 
> <li> 
> <strong>Agent Understanding → UI Filters</strong>: Agent extracts intent from natural language → Updates UI filters 
> </li> 
> </ol> 
>  
> <div> 
> <pre><code><span>// Sending filter context to agent</span> 
> <span>const</span> <span>chatPayload</span> <span>=</span> <span>{</span> 
>   <span>message</span><span>:</span> <span>userInput</span><span>,</span> 
>   <span>context</span><span>:</span> <span>{</span> 
>     <span>filters</span><span>:</span> <span>{</span> 
>       <span>location</span><span>:</span> <span>locationDropdown</span><span>.</span><span>value</span><span>,</span> 
>       <span>min_price</span><span>:</span> <span>priceSlider</span><span>.</span><span>min</span><span>,</span> 
>       <span>max_price</span><span>:</span> <span>priceSlider</span><span>.</span><span>max</span><span>,</span> 
>       <span>bedrooms</span><span>:</span> <span>bedroomFilter</span><span>.</span><span>value</span> 
>     <span>}</span> 
>   <span>}</span> 
> <span>};</span> 
>  
> <span>// Agent enhances message with current filter state</span> 
> <span>if </span><span>(</span><span>context</span><span>.</span><span>filters</span> <span>&amp;&amp;</span> <span>Object</span><span>.</span><span>keys</span><span>(</span><span>context</span><span>.</span><span>filters</span><span>).</span><span>length</span> <span>&gt;</span> <span>0</span><span>)</span> <span>{</span> 
>   <span>enhancedMessage</span> <span>+=</span> <span>` Current filters: </span><span>${</span><span>formatFilters</span><span>(</span><span>context</span><span>.</span><span>filters</span><span>)}</span><span>`</span><span>;</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>Why This Works</strong>: MongoDB stores both the agent's understanding (in conversation metadata) and the current UI state (in user preferences), creating a single source of truth.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Advanced Patterns: Going Beyond Basic RAG 
> </h2> 
>  
> <h3> 
>    
>    
>   Pattern 1: Saved Rentals with Agent Integration 
> </h3> 
>  
> <p>Users can save favorite properties, and the agent accesses this data:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>this</span><span>.</span><span>getSavedRentalsTool</span> <span>=</span> <span>tool</span><span>({</span> 
>   <span>name</span><span>:</span> <span>'</span><span>getSavedRentals</span><span>'</span><span>,</span> 
>   <span>description</span><span>:</span> <span>'</span><span>Get the user</span><span>\'</span><span>s saved rental properties for comparison and recommendations.</span><span>'</span><span>,</span> 
>   <span>parameters</span><span>:</span> <span>z</span><span>.</span><span>object</span><span>({</span> 
>     <span>includeDetails</span><span>:</span> <span>z</span><span>.</span><span>boolean</span><span>().</span><span>default</span><span>(</span><span>false</span><span>)</span> 
>   <span>}),</span> 
>   <span>execute</span><span>:</span> <span>async </span><span>({</span> <span>includeDetails</span> <span>})</span> <span>=&gt;</span> <span>{</span> 
>     <span>const</span> <span>savedRentals</span> <span>=</span> <span>await</span> <span>UserModel</span><span>.</span><span>getSavedRentals</span><span>(</span><span>userId</span><span>);</span> 
>  
>     <span>if </span><span>(</span><span>includeDetails</span><span>)</span> <span>{</span> 
>       <span>// Fetch full property data using rental IDs</span> 
>       <span>const</span> <span>detailedRentals</span> <span>=</span> <span>await</span> <span>Promise</span><span>.</span><span>all</span><span>(</span> 
>         <span>savedRentals</span><span>.</span><span>map</span><span>(</span><span>saved</span> <span>=&gt;</span> <span>RentalModel</span><span>.</span><span>findById</span><span>(</span><span>saved</span><span>.</span><span>rental_id</span><span>))</span> 
>       <span>);</span> 
>       <span>return</span> <span>JSON</span><span>.</span><span>stringify</span><span>(</span><span>detailedRentals</span><span>);</span> 
>     <span>}</span> 
>  
>     <span>return</span> <span>JSON</span><span>.</span><span>stringify</span><span>(</span><span>savedRentals</span><span>);</span> 
>   <span>}</span> 
> <span>});</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>User Experience</strong>:<br> 
> </p> 
>  
> <div> 
> <pre><code>User: "Compare my saved properties in terms of price and location" 
>          ↓ 
> Agent: [Calls getSavedRentals with includeDetails=true] 
>          ↓ 
> MongoDB: Returns full property documents 
>          ↓ 
> Agent: "Here's a comparison of your 3 saved properties: 
>         1. Manhattan Loft ($175/night) - SoHo, great for nightlife 
>         2. Barcelona Apartment (€120/night) - Gothic Quarter, historic charm 
>         3. Sydney Studio ($140/night) - Bondi, beach vibes 
>  
>         The Barcelona option offers the best value, while Manhattan is ideal 
>         if you prioritize being in the center of the action." 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   Pattern 2: Context-Aware Property Details 
> </h3> 
>  
> <p>When a user views a property, that context is passed to the agent:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>const</span> <span>chatPayload</span> <span>=</span> <span>{</span> 
>   <span>message</span><span>:</span> <span>"</span><span>Tell me about the neighborhood</span><span>"</span><span>,</span> 
>   <span>context</span><span>:</span> <span>{</span> 
>     <span>current_property</span><span>:</span> <span>{</span> 
>       <span>id</span><span>:</span> <span>12345</span><span>,</span> 
>       <span>name</span><span>:</span> <span>"</span><span>Luxury Manhattan Loft</span><span>"</span><span>,</span> 
>       <span>location</span><span>:</span> <span>{</span> <span>market</span><span>:</span> <span>"</span><span>New York</span><span>"</span><span>,</span> <span>neighbourhood</span><span>:</span> <span>"</span><span>SoHo</span><span>"</span> <span>},</span> 
>       <span>features</span><span>:</span> <span>{</span> <span>bedrooms</span><span>:</span> <span>2</span><span>,</span> <span>price</span><span>:</span> <span>175</span> <span>}</span> 
>     <span>}</span> 
>   <span>}</span> 
> <span>};</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>The agent receives this context and provides targeted advice:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>if </span><span>(</span><span>context</span><span>.</span><span>current_property</span><span>)</span> <span>{</span> 
>   <span>const</span> <span>property</span> <span>=</span> <span>context</span><span>.</span><span>current_property</span><span>;</span> 
>   <span>enhancedMessage</span> <span>+=</span> <span>` User is currently viewing: "</span><span>${</span><span>property</span><span>.</span><span>name</span><span>}</span><span>" in </span><span>${</span><span>property</span><span>.</span><span>location</span><span>.</span><span>neighbourhood</span><span>}</span><span>`</span><span>;</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>Agent Response</strong>: "SoHo is one of Manhattan's most vibrant neighborhoods, known for its cast-iron architecture, upscale boutiques, and art galleries. You'll be walking distance from great restaurants and nightlife. At $175/night for a 2-bedroom, this is competitive for the area."</p> 
>  
> <h3> 
>    
>    
>   Pattern 3: Hybrid Search with Scoring 
> </h3> 
>  
> <p>Combine vector similarity with business logic:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>const</span> <span>pipeline</span> <span>=</span> <span>[</span> 
>   <span>{</span> 
>     <span>$vectorSearch</span><span>:</span> <span>{</span> 
>       <span>index</span><span>:</span> <span>"</span><span>rental_vector_search</span><span>"</span><span>,</span> 
>       <span>path</span><span>:</span> <span>"</span><span>text_embeddings</span><span>"</span><span>,</span> 
>       <span>queryVector</span><span>:</span> <span>queryEmbedding</span><span>,</span> 
>       <span>numCandidates</span><span>:</span> <span>100</span><span>,</span> 
>       <span>limit</span><span>:</span> <span>50</span> <span>// Get more candidates for scoring</span> 
>     <span>}</span> 
>   <span>},</span> 
>   <span>{</span> 
>     <span>$addFields</span><span>:</span> <span>{</span> 
>       <span>vector_score</span><span>:</span> <span>{</span> <span>$meta</span><span>:</span> <span>"</span><span>vectorSearchScore</span><span>"</span> <span>},</span> 
>       <span>rating_score</span><span>:</span> <span>{</span> 
>         <span>$divide</span><span>:</span> <span>[</span><span>"</span><span>$review_scores.review_scores_rating</span><span>"</span><span>,</span> <span>100</span><span>]</span> 
>       <span>},</span> 
>       <span>superhost_bonus</span><span>:</span> <span>{</span> 
>         <span>$cond</span><span>:</span> <span>[</span><span>"</span><span>$host.host_is_superhost</span><span>"</span><span>,</span> <span>0.1</span><span>,</span> <span>0</span><span>]</span> 
>       <span>}</span> 
>     <span>}</span> 
>   <span>},</span> 
>   <span>{</span> 
>     <span>$addFields</span><span>:</span> <span>{</span> 
>       <span>final_score</span><span>:</span> <span>{</span> 
>         <span>$add</span><span>:</span> <span>[</span> 
>           <span>{</span> <span>$multiply</span><span>:</span> <span>[</span><span>"</span><span>$vector_score</span><span>"</span><span>,</span> <span>0.6</span><span>]</span> <span>},</span>      <span>// 60% semantic relevance</span> 
>           <span>{</span> <span>$multiply</span><span>:</span> <span>[</span><span>"</span><span>$rating_score</span><span>"</span><span>,</span> <span>0.3</span><span>]</span> <span>},</span>      <span>// 30% ratings</span> 
>           <span>{</span> <span>$multiply</span><span>:</span> <span>[</span><span>"</span><span>$superhost_bonus</span><span>"</span><span>,</span> <span>0.1</span><span>]</span> <span>}</span>    <span>// 10% superhost boost</span> 
>         <span>]</span> 
>       <span>}</span> 
>     <span>}</span> 
>   <span>},</span> 
>   <span>{</span> 
>     <span>$sort</span><span>:</span> <span>{</span> <span>final_score</span><span>:</span> <span>-</span><span>1</span> <span>}</span> 
>   <span>},</span> 
>   <span>{</span> 
>     <span>$limit</span><span>:</span> <span>10</span> 
>   <span>}</span> 
> <span>];</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>Result</strong>: Properties ranked by a combination of semantic relevance, user ratings, and business rules - all computed in MongoDB.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   MongoDB Atlas Setup for Production 
> </h2> 
>  
> <h3> 
>    
>    
>   1. Vector Search Index Configuration 
> </h3> 
>  
>  
>  
> <div> 
> <pre><code><span>{</span><span> 
>   </span><span>"fields"</span><span>:</span><span> </span><span>[</span><span> 
>     </span><span>{</span><span> 
>       </span><span>"numDimensions"</span><span>:</span><span> </span><span>1536</span><span>,</span><span> 
>       </span><span>"path"</span><span>:</span><span> </span><span>"text_embeddings"</span><span>,</span><span> 
>       </span><span>"similarity"</span><span>:</span><span> </span><span>"cosine"</span><span>,</span><span> 
>       </span><span>"type"</span><span>:</span><span> </span><span>"vector"</span><span> 
>     </span><span>},</span><span> 
>     </span><span>{</span><span> 
>       </span><span>"path"</span><span>:</span><span> </span><span>"property_type"</span><span>,</span><span> 
>       </span><span>"type"</span><span>:</span><span> </span><span>"filter"</span><span> 
>     </span><span>},</span><span> 
>     </span><span>{</span><span> 
>       </span><span>"path"</span><span>:</span><span> </span><span>"address.market"</span><span>,</span><span> 
>       </span><span>"type"</span><span>:</span><span> </span><span>"filter"</span><span> 
>     </span><span>},</span><span> 
>     </span><span>{</span><span> 
>       </span><span>"path"</span><span>:</span><span> </span><span>"price"</span><span>,</span><span> 
>       </span><span>"type"</span><span>:</span><span> </span><span>"filter"</span><span> 
>     </span><span>},</span><span> 
>     </span><span>{</span><span> 
>       </span><span>"path"</span><span>:</span><span> </span><span>"bedrooms"</span><span>,</span><span> 
>       </span><span>"type"</span><span>:</span><span> </span><span>"filter"</span><span> 
>     </span><span>},</span><span> 
>     </span><span>{</span><span> 
>       </span><span>"path"</span><span>:</span><span> </span><span>"host.host_is_superhost"</span><span>,</span><span> 
>       </span><span>"type"</span><span>:</span><span> </span><span>"filter"</span><span> 
>     </span><span>}</span><span> 
>   </span><span>]</span><span> 
> </span><span>}</span><span> 
> </span></code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>Key Points</strong>:</p> 
>  
> <ul> 
> <li> 
> <code>vector</code> field for semantic search</li> 
> <li> 
> <code>filter</code> fields for structured filtering</li> 
> <li>Cosine similarity for 1536-dim OpenAI embeddings</li> 
> </ul> 
>  
> <h3> 
>    
>    
>   2. Supporting Indexes 
> </h3> 
>  
>  
>  
> <div> 
> <pre><code><span>// Conversation history lookup</span> 
> <span>db</span><span>.</span><span>conversations</span><span>.</span><span>createIndex</span><span>({</span> <span>"</span><span>sessionId</span><span>"</span><span>:</span> <span>1</span> <span>});</span> 
> <span>db</span><span>.</span><span>conversations</span><span>.</span><span>createIndex</span><span>({</span> <span>"</span><span>userId</span><span>"</span><span>:</span> <span>1</span><span>,</span> <span>"</span><span>metadata.lastActivity</span><span>"</span><span>:</span> <span>-</span><span>1</span> <span>});</span> 
>  
> <span>// User activity queries</span> 
> <span>db</span><span>.</span><span>users</span><span>.</span><span>createIndex</span><span>({</span> <span>"</span><span>username</span><span>"</span><span>:</span> <span>1</span> <span>},</span> <span>{</span> <span>unique</span><span>:</span> <span>true</span> <span>});</span> 
> <span>db</span><span>.</span><span>users</span><span>.</span><span>createIndex</span><span>({</span> <span>"</span><span>saved_rentals.rental_id</span><span>"</span><span>:</span> <span>1</span> <span>});</span> 
>  
> <span>// Rental property queries</span> 
> <span>db</span><span>.</span><span>rentals</span><span>.</span><span>createIndex</span><span>({</span> <span>"</span><span>address.market</span><span>"</span><span>:</span> <span>1</span><span>,</span> <span>"</span><span>price</span><span>"</span><span>:</span> <span>1</span> <span>});</span> 
> <span>db</span><span>.</span><span>rentals</span><span>.</span><span>createIndex</span><span>({</span> <span>"</span><span>bedrooms</span><span>"</span><span>:</span> <span>1</span><span>,</span> <span>"</span><span>accommodates</span><span>"</span><span>:</span> <span>1</span> <span>});</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   3. Aggregation Pipeline Optimization 
> </h3> 
>  
> <p><strong>Use <code>$project</code> early</strong> to reduce data transfer:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>{</span> 
>   <span>$vectorSearch</span><span>:</span> <span>{</span> <span>/* ... */</span> <span>}</span> 
> <span>},</span> 
> <span>{</span> 
>   <span>$project</span><span>:</span> <span>{</span> 
>     <span>name</span><span>:</span> <span>1</span><span>,</span> 
>     <span>price</span><span>:</span> <span>1</span><span>,</span> 
>     <span>bedrooms</span><span>:</span> <span>1</span><span>,</span> 
>     <span>"</span><span>address.market</span><span>"</span><span>:</span> <span>1</span><span>,</span> 
>     <span>score</span><span>:</span> <span>{</span> <span>$meta</span><span>:</span> <span>"</span><span>vectorSearchScore</span><span>"</span> <span>}</span> 
>     <span>// Only fetch what you need</span> 
>   <span>}</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
>  
>  
>  
> <h2> 
>    
>    
>   Performance Considerations 
> </h2> 
>  
> <h3> 
>    
>    
>   Embedding Generation Strategy 
> </h3> 
>  
>  
>  
> <div> 
> <pre><code><span>// Cache embeddings at data ingestion</span> 
> <span>async</span> <span>function</span> <span>seedRental</span><span>(</span><span>rental</span><span>)</span> <span>{</span> 
>   <span>const</span> <span>embeddingText</span> <span>=</span> <span>`</span><span>${</span><span>rental</span><span>.</span><span>name</span><span>}</span><span>. </span><span>${</span><span>rental</span><span>.</span><span>description</span><span>}</span><span>. 
>     Located in </span><span>${</span><span>rental</span><span>.</span><span>address</span><span>.</span><span>market</span><span>}</span><span>, </span><span>${</span><span>rental</span><span>.</span><span>address</span><span>.</span><span>country</span><span>}</span><span>. 
>     </span><span>${</span><span>rental</span><span>.</span><span>property_type</span><span>}</span><span> with </span><span>${</span><span>rental</span><span>.</span><span>bedrooms</span><span>}</span><span> bedrooms. 
>     Amenities: </span><span>${</span><span>rental</span><span>.</span><span>amenities</span><span>.</span><span>join</span><span>(</span><span>'</span><span>, </span><span>'</span><span>)}</span><span>.`</span><span>;</span> 
>  
>   <span>rental</span><span>.</span><span>text_embeddings</span> <span>=</span> <span>await</span> <span>generateEmbedding</span><span>(</span><span>embeddingText</span><span>);</span> 
>  
>   <span>await</span> <span>db</span><span>.</span><span>rentals</span><span>.</span><span>insertOne</span><span>(</span><span>rental</span><span>);</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>Never generate embeddings at query time</strong> - pre-compute and store them.</p> 
>  
> <h3> 
>    
>    
>   Conversation History Management 
> </h3> 
>  
>  
>  
> <div> 
> <pre><code><span>// Limit conversation history to last 20 messages</span> 
> <span>const</span> <span>conversation</span> <span>=</span> <span>await</span> <span>collection</span><span>.</span><span>findOne</span><span>(</span> 
>   <span>{</span> <span>sessionId</span> <span>},</span> 
>   <span>{</span> 
>     <span>projection</span><span>:</span> <span>{</span> 
>       <span>messages</span><span>:</span> <span>{</span> <span>$slice</span><span>:</span> <span>-</span><span>20</span> <span>},</span> <span>// Only get last 20</span> 
>       <span>metadata</span><span>:</span> <span>1</span> 
>     <span>}</span> 
>   <span>}</span> 
> <span>);</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>Why</strong>: Sending entire conversation history to LLMs is expensive. Recent context is usually sufficient.</p> 
>  
> <h3> 
>    
>    
>   Connection Pooling 
> </h3> 
>  
>  
>  
> <div> 
> <pre><code><span>const</span> <span>client</span> <span>=</span> <span>new</span> <span>MongoClient</span><span>(</span><span>uri</span><span>,</span> <span>{</span> 
>   <span>maxPoolSize</span><span>:</span> <span>50</span><span>,</span> 
>   <span>minPoolSize</span><span>:</span> <span>10</span><span>,</span> 
>   <span>maxIdleTimeMS</span><span>:</span> <span>30000</span> 
> <span>});</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>Production Tip</strong>: Pool size should match expected concurrent users/requests.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Security Best Practices 
> </h2> 
>  
> <h3> 
>    
>    
>   1. User-Scoped Data Access 
> </h3> 
>  
>  
>  
> <div> 
> <pre><code><span>// NEVER trust client-provided userId</span> 
> <span>const</span> <span>userId</span> <span>=</span> <span>await</span> <span>verifyJWT</span><span>(</span><span>authToken</span><span>);</span> 
>  
> <span>// All queries scoped to authenticated user</span> 
> <span>const</span> <span>savedRentals</span> <span>=</span> <span>await</span> <span>db</span><span>.</span><span>users</span><span>.</span><span>findOne</span><span>(</span> 
>   <span>{</span> <span>_id</span><span>:</span> <span>ObjectId</span><span>(</span><span>userId</span><span>)</span> <span>},</span> 
>   <span>{</span> <span>projection</span><span>:</span> <span>{</span> <span>saved_rentals</span><span>:</span> <span>1</span> <span>}</span> <span>}</span> 
> <span>);</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   2. Input Sanitization 
> </h3> 
>  
>  
>  
> <div> 
> <pre><code><span>// Validate and sanitize before DB operations</span> 
> <span>const</span> <span>filters</span> <span>=</span> <span>{</span> 
>   <span>min_price</span><span>:</span> <span>Math</span><span>.</span><span>max</span><span>(</span><span>0</span><span>,</span> <span>parseInt</span><span>(</span><span>filters</span><span>.</span><span>min_price</span><span>)</span> <span>||</span> <span>0</span><span>),</span> 
>   <span>max_price</span><span>:</span> <span>Math</span><span>.</span><span>min</span><span>(</span><span>10000</span><span>,</span> <span>parseInt</span><span>(</span><span>filters</span><span>.</span><span>max_price</span><span>)</span> <span>||</span> <span>10000</span><span>),</span> 
>   <span>location</span><span>:</span> <span>sanitizeString</span><span>(</span><span>filters</span><span>.</span><span>location</span><span>)</span> 
> <span>};</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   3. Rate Limiting 
> </h3> 
>  
>  
>  
> <div> 
> <pre><code><span>// Track API usage per user</span> 
> <span>await</span> <span>db</span><span>.</span><span>users</span><span>.</span><span>updateOne</span><span>(</span> 
>   <span>{</span> <span>_id</span><span>:</span> <span>userId</span> <span>},</span> 
>   <span>{</span> 
>     <span>$inc</span><span>:</span> <span>{</span> <span>'</span><span>rate_limits.api_calls_today</span><span>'</span><span>:</span> <span>1</span> <span>},</span> 
>     <span>$set</span><span>:</span> <span>{</span> <span>'</span><span>rate_limits.last_call</span><span>'</span><span>:</span> <span>new</span> <span>Date</span><span>()</span> <span>}</span> 
>   <span>}</span> 
> <span>);</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
>  
>  
>  
> <h2> 
>    
>    
>   Real-World Results: What We Achieved 
> </h2> 
>  
> <h3> 
>    
>    
>   Performance Metrics 
> </h3> 
>  
> <ul> 
> <li> 
> <strong>Average search latency</strong>: 150-300ms (embedding generation + vector search + formatting)</li> 
> <li> 
> <strong>Vector search alone</strong>: 50-80ms for 5,000+ properties</li> 
> <li> 
> <strong>Conversation storage</strong>: &lt;10ms per message (upsert with indexing)</li> 
> <li> 
> <strong>Concurrent users</strong>: Tested up to 100 simultaneous chat sessions</li> 
> </ul> 
>  
> <h3> 
>    
>    
>   User Experience Wins 
> </h3> 
>  
> <ul> 
> <li> 
> <strong>Natural language accuracy</strong>: 90%+ intent extraction on first try</li> 
> <li> 
> <strong>Filter synchronization</strong>: Seamless bidirectional updates</li> 
> <li> 
> <strong>Context retention</strong>: Agent remembers previous searches and user preferences</li> 
> <li> 
> <strong>Multi-turn conversations</strong>: Supports complex, multi-step property searches</li> 
> </ul> 
>  
> <h3> 
>    
>    
>   Developer Experience 
> </h3> 
>  
> <ul> 
> <li> 
> <strong>Single database</strong>: No data synchronization between vector DB and app DB</li> 
> <li> 
> <strong>Unified query language</strong>: MongoDB aggregation for everything</li> 
> <li> 
> <strong>Flexible schema</strong>: Add new metadata fields without migrations</li> 
> <li> 
> <strong>Rich ecosystem</strong>: Works with Mongoose, native driver, Prisma, etc.</li> 
> </ul> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Lessons Learned &amp; Best Practices 
> </h2> 
>  
> <h3> 
>    
>    
>   1. Design Your Document Schema for Agent Access 
> </h3> 
>  
>  
>  
> <div> 
> <pre><code><span>// ❌ Bad: Deeply nested, agent can't navigate</span> 
> <span>{</span> 
>   <span>"</span><span>data</span><span>"</span><span>:</span> <span>{</span> 
>     <span>"</span><span>property_info</span><span>"</span><span>:</span> <span>{</span> 
>       <span>"</span><span>details</span><span>"</span><span>:</span> <span>{</span> 
>         <span>"</span><span>location</span><span>"</span><span>:</span> <span>{</span> <span>...</span> <span>}</span> 
>       <span>}</span> 
>     <span>}</span> 
>   <span>}</span> 
> <span>}</span> 
>  
> <span>// ✅ Good: Flat, predictable structure</span> 
> <span>{</span> 
>   <span>"</span><span>name</span><span>"</span><span>:</span> <span>"</span><span>...</span><span>"</span><span>,</span> 
>   <span>"</span><span>address</span><span>"</span><span>:</span> <span>{</span> <span>"</span><span>market</span><span>"</span><span>:</span> <span>"</span><span>...</span><span>"</span><span>,</span> <span>"</span><span>country</span><span>"</span><span>:</span> <span>"</span><span>...</span><span>"</span> <span>},</span> 
>   <span>"</span><span>price</span><span>"</span><span>:</span> <span>150</span><span>,</span> 
>   <span>"</span><span>bedrooms</span><span>"</span><span>:</span> <span>2</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   2. Include Both Structured and Unstructured Data 
> </h3> 
>  
>  
>  
> <div> 
> <pre><code><span>{</span> 
>   <span>"</span><span>name</span><span>"</span><span>:</span> <span>"</span><span>Cozy Manhattan Loft</span><span>"</span><span>,</span> 
>   <span>"</span><span>description</span><span>"</span><span>:</span> <span>"</span><span>Full natural language description...</span><span>"</span><span>,</span> <span>// ← For embeddings</span> 
>   <span>"</span><span>property_type</span><span>"</span><span>:</span> <span>"</span><span>Loft</span><span>"</span><span>,</span>                              <span>// ← For filtering</span> 
>   <span>"</span><span>bedrooms</span><span>"</span><span>:</span> <span>2</span><span>,</span>                                        <span>// ← For filtering</span> 
>   <span>"</span><span>amenities</span><span>"</span><span>:</span> <span>[</span><span>"</span><span>WiFi</span><span>"</span><span>,</span> <span>"</span><span>Kitchen</span><span>"</span><span>],</span>                     <span>// ← For filtering</span> 
>   <span>"</span><span>text_embeddings</span><span>"</span><span>:</span> <span>[...]</span>                              <span>// ← For vector search</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   3. Store Agent Metadata Richly 
> </h3> 
>  
>  
>  
> <div> 
> <pre><code><span>// Don't just store the conversation</span> 
> <span>{</span> 
>   <span>"</span><span>role</span><span>"</span><span>:</span> <span>"</span><span>assistant</span><span>"</span><span>,</span> 
>   <span>"</span><span>content</span><span>"</span><span>:</span> <span>"</span><span>I found 5 properties...</span><span>"</span> 
> <span>}</span> 
>  
> <span>// Store what the agent DID</span> 
> <span>{</span> 
>   <span>"</span><span>role</span><span>"</span><span>:</span> <span>"</span><span>assistant</span><span>"</span><span>,</span> 
>   <span>"</span><span>content</span><span>"</span><span>:</span> <span>"</span><span>I found 5 properties...</span><span>"</span><span>,</span> 
>   <span>"</span><span>metadata</span><span>"</span><span>:</span> <span>{</span> 
>     <span>"</span><span>tool_calls</span><span>"</span><span>:</span> <span>[</span><span>"</span><span>searchRentals</span><span>"</span><span>],</span> 
>     <span>"</span><span>filters_applied</span><span>"</span><span>:</span> <span>{</span> <span>"</span><span>location</span><span>"</span><span>:</span> <span>"</span><span>New York</span><span>"</span><span>,</span> <span>"</span><span>max_price</span><span>"</span><span>:</span> <span>200</span> <span>},</span> 
>     <span>"</span><span>rental_ids</span><span>"</span><span>:</span> <span>[</span><span>123</span><span>,</span> <span>456</span><span>],</span> 
>     <span>"</span><span>user_satisfied</span><span>"</span><span>:</span> <span>true</span> <span>// Track based on follow-up</span> 
>   <span>}</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   4. Optimize for Agent Token Limits 
> </h3> 
>  
>  
>  
> <div> 
> <pre><code><span>// Return concise summaries to the agent</span> 
> <span>const</span> <span>formattedResults</span> <span>=</span> <span>results</span><span>.</span><span>map</span><span>(</span><span>r</span> <span>=&gt;</span> <span>({</span> 
>   <span>id</span><span>:</span> <span>r</span><span>.</span><span>_id</span><span>,</span> 
>   <span>name</span><span>:</span> <span>r</span><span>.</span><span>name</span><span>,</span> 
>   <span>price</span><span>:</span> <span>r</span><span>.</span><span>price</span><span>,</span> 
>   <span>location</span><span>:</span> <span>`</span><span>${</span><span>r</span><span>.</span><span>address</span><span>.</span><span>market</span><span>}</span><span>, </span><span>${</span><span>r</span><span>.</span><span>address</span><span>.</span><span>country</span><span>}</span><span>`</span><span>,</span> 
>   <span>bedrooms</span><span>:</span> <span>r</span><span>.</span><span>bedrooms</span> 
>   <span>// Skip description, images, etc. - retrieve on-demand</span> 
> <span>}));</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   5. Enable Agent Self-Discovery 
> </h3> 
>  
>  
>  
> <div> 
> <pre><code><span>// Provide tools for agents to explore data</span> 
> <span>this</span><span>.</span><span>exploreDataTool</span> <span>=</span> <span>tool</span><span>({</span> 
>   <span>name</span><span>:</span> <span>'</span><span>exploreAvailableMarkets</span><span>'</span><span>,</span> 
>   <span>description</span><span>:</span> <span>'</span><span>Get list of available cities/markets in the database</span><span>'</span><span>,</span> 
>   <span>execute</span><span>:</span> <span>async </span><span>()</span> <span>=&gt;</span> <span>{</span> 
>     <span>const</span> <span>markets</span> <span>=</span> <span>await</span> <span>db</span><span>.</span><span>rentals</span><span>.</span><span>distinct</span><span>(</span><span>'</span><span>address.market</span><span>'</span><span>);</span> 
>     <span>return</span> <span>JSON</span><span>.</span><span>stringify</span><span>(</span><span>markets</span><span>);</span> 
>   <span>}</span> 
> <span>});</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
>  
>  
>  
> <h2> 
>    
>    
>   The Future: What's Next for Agent-Database Integration 
> </h2> 
>  
> <h3> 
>    
>    
>   1. Agent-Driven Schema Evolution 
> </h3> 
>  
> <p>Imagine agents that suggest new fields based on user queries:<br> 
> </p> 
>  
> <div> 
> <pre><code>Agent: "I notice users frequently ask about 'pet-friendly' properties, 
>         but this field doesn't exist. Should I add it to the schema?" 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   2. Semantic Caching 
> </h3> 
>  
> <p>MongoDB could cache embedding+filter combinations:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>{</span> 
>   <span>"</span><span>query_hash</span><span>"</span><span>:</span> <span>"</span><span>sha256(...)</span><span>"</span><span>,</span> 
>   <span>"</span><span>embedding</span><span>"</span><span>:</span> <span>[...],</span> 
>   <span>"</span><span>filters</span><span>"</span><span>:</span> <span>{</span> <span>"</span><span>location</span><span>"</span><span>:</span> <span>"</span><span>New York</span><span>"</span> <span>},</span> 
>   <span>"</span><span>cached_results</span><span>"</span><span>:</span> <span>[...],</span> 
>   <span>"</span><span>valid_until</span><span>"</span><span>:</span> <span>ISODate</span><span>(</span><span>"</span><span>2024-01-15T12:00:00Z</span><span>"</span><span>)</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   3. Multi-Agent Coordination 
> </h3> 
>  
> <p>Different specialized agents sharing the same MongoDB instance:</p> 
>  
> <ul> 
> <li> 
> <strong>Search Agent</strong>: Finds properties</li> 
> <li> 
> <strong>Booking Agent</strong>: Handles reservations</li> 
> <li> 
> <strong>Recommendation Agent</strong>: Suggests based on history</li> 
> <li>All coordinating through shared conversation and user state</li> 
> </ul> 
>  
> <h3> 
>    
>    
>   4. Continuous Learning from Feedback 
> </h3> 
>  
>  
>  
> <div> 
> <pre><code><span>// User indicates result quality</span> 
> <span>{</span> 
>   <span>"</span><span>search_query</span><span>"</span><span>:</span> <span>"</span><span>cozy apartment in Barcelona</span><span>"</span><span>,</span> 
>   <span>"</span><span>results_shown</span><span>"</span><span>:</span> <span>[</span><span>123</span><span>,</span> <span>456</span><span>,</span> <span>789</span><span>],</span> 
>   <span>"</span><span>user_clicked</span><span>"</span><span>:</span> <span>456</span><span>,</span>        <span>// Implicit feedback</span> 
>   <span>"</span><span>user_saved</span><span>"</span><span>:</span> <span>[</span><span>456</span><span>],</span>        <span>// Strong signal</span> 
>   <span>"</span><span>user_booked</span><span>"</span><span>:</span> <span>456</span>          <span>// Conversion</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Use this data to fine-tune embeddings or ranking algorithms.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Conclusion: MongoDB Atlas as the Foundation for Intelligent Applications 
> </h2> 
>  
> <p>Building AI agents that truly understand and serve users requires more than just a language model. You need a database that:</p> 
>  
> <p>✅ <strong>Stores semantic understanding</strong> (vectors) alongside structured data (filters)<br> 
> ✅ <strong>Handles dynamic, evolving schemas</strong> (conversations, metadata, user context)<br> 
> ✅ <strong>Enables bidirectional data flow</strong> (agents read, write, and transform)<br> 
> ✅ <strong>Performs at scale</strong> (millisecond searches across thousands of documents)<br> 
> ✅ <strong>Provides a unified platform</strong> (no juggling multiple databases)</p> 
>  
> <p>MongoDB Atlas delivers all of this with its Document Model and Vector Search capabilities. As we've seen in this rental search application:</p> 
>  
> <ol> 
> <li> 
> <strong>Agents feed FROM the database</strong> using semantic vector search combined with traditional filters</li> 
> <li> 
> <strong>Agents feed TO the database</strong> by storing rich conversation context and metadata</li> 
> <li> 
> <strong>Agents transform the UI</strong> through structured metadata that synchronizes with interface elements</li> 
> </ol> 
>  
> <p>This bidirectional architecture represents the future of AI-powered applications. And MongoDB Atlas makes it not just possible, but elegant, performant, and production-ready.</p> 
>  
>  
> <h2> 
>    
>    
>   Try It Yourself 
> </h2> 
>  
> <p>The complete code for this project is available on GitHub: <a href="https://github.com/mongodb-developer/mongodb-openai-agentic-rentals">mongodb-openai-agentic-rentals</a></p> 
>  
> <p><strong>Quick Start</strong>:<br> 
> </p> 
>  
> <div> 
> <pre><code>git clone https://github.com/mongodb-developer/mongodb-openai-agentic-rentals.git 
>  
> <span>cd </span>mongodb-openai-agentic-rentals 
> bun <span>install</span> 
> <span># Configure .env with your MongoDB Atlas URI and OpenAI API key</span> 
> node seed-hf-airbnb-data.js 
> bun start 
> <span># Visit http://localhost:5000/index.html</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>What to explore</strong>:</p> 
>  
> <ol> 
> <li>Try natural language queries: "Find me a beachfront property in Sydney"</li> 
> <li>Watch the UI filters update automatically</li> 
> <li>Check the MongoDB conversation collection to see stored context</li> 
> <li>Examine the aggregation pipelines in <code>src/services/vector-search.service.js</code> 
> </li> 
> <li>Extend the agent with new tools in <code>src/agents/rental-rag-agent.js</code> 
> </li> 
> </ol> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Additional Resources 
> </h2> 
>  
> <ul> 
> <li><a href="https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-overview/">MongoDB Atlas Vector Search Documentation</a></li> 
> <li><a href="https://openai.github.io/openai-agents-js/">OpenAI Agents SDK</a></li> 
> <li><a href="https://www.mongodb.com/docs/manual/core/aggregation-pipeline/">MongoDB Aggregation Pipeline</a></li> 
> <li><a href="https://www.mongodb.com/docs/manual/core/data-modeling-introduction/">Document Model Design Best Practices</a></li> 
> </ul> 
>  
>  
>  
>  
> <p><strong>About the Author</strong>: Pavel Duchovny is a Developer Advocate at MongoDB, passionate about helping developers build intelligent, scalable applications. Connect on <a href="https://twitter.com/mongodb">Twitter</a> or <a href="https://linkedin.com/company/mongodb">LinkedIn</a>.</p> 
>  
>  
>  
>  
> <p><em>Have questions or feedback? Open an issue on the GitHub repo or reach out to the MongoDB Developer Community.</em></p>

---

## [9/10] I Built an AI That Finds Every Article Similar to Your Text — Here’s How It Works
**Source:** The Practical Developer | **Date:** 2025-12-02T12:34:19.000Z
**URL:** https://dev.to/stokry/i-built-an-ai-that-finds-every-article-similar-to-your-text-heres-how-it-works-bhb
**Reasoning:** The article discusses building an AI for semantic search, which is directly related to codebase indexing and retrieval.
**Authors:** Stokry

**Content/Abstract:**
> <p>Most search engines will show you pages that <em>mention</em> the same keywords.</p> 
>  
> <p>But what if you want to find articles that express the <strong>same ideas</strong>, even if they’re written completely differently?</p> 
>  
> <p>That’s the problem that pushed me into building something new.</p> 
>  
> <p>A few months ago, I needed to verify whether parts of my content were being reused online. Google wasn’t helpful — keyword search missed half of the semantically similar pages. Rephrased paragraphs were completely invisible.</p> 
>  
> <p>So I built my own engine.</p> 
>  
> <p>Today, I’m excited to introduce <strong><a href="https://nooth.dev/">Nooth</a></strong>, an AI-powered system that takes <em>any text you enter</em> and searches the web for content with similar themes, ideas, and semantic meaning.</p> 
>  
> <p>Drop in a paragraph → get URLs, similarity percentages, and deep analytical insights.</p> 
>  
> <p>Here’s how it works under the hood.</p> 
>  
> <h2> 
>    
>    
>   <strong>🚀 The Idea</strong> 
> </h2> 
>  
> <p>Traditional search relies on keyword overlap.</p> 
>  
> <p>If two texts don’t share the same words, the engine assumes they’re unrelated.</p> 
>  
> <p>But humans don’t think that way.</p> 
>  
> <p>We recognize similarity through <strong>meaning</strong> — the structure of ideas, not characters.</p> 
>  
> <p>So the goal was simple:</p> 
>  
> <blockquote> 
> <p><strong>Build an engine that understands what your text is about, then find everything on the internet that expresses the same ideas.</strong></p> 
> </blockquote> 
>  
> <p>That meant two pieces were crucial:</p> 
>  
> <ol> 
> <li><p><strong>A way to measure meaning</strong></p></li> 
> <li><p><strong>A way to scan the web, extract clean text, and compare it</strong></p></li> 
> </ol> 
>  
> <h2> 
>    
>    
>   <strong>🧠 Step 1: Turning Text Into Meaning (Embeddings)</strong> 
> </h2> 
>  
> <p>At the core of Nooth is a semantic model that converts text into embeddings — numerical vectors that represent meaning.</p> 
>  
> <p>Example:</p> 
>  
> <blockquote> 
> <p>“How to speed up your Node.js API”</p> 
>  
> <p>and</p> 
>  
> <p>“Optimizing performance in a JavaScript backend”</p> 
> </blockquote> 
>  
> <p>These look different, but in vector space, their embeddings are neighbors.</p> 
>  
> <p>Every time you paste text into Nooth:</p> 
>  
> <ul> 
> <li><p>it cleans and normalizes it</p></li> 
> <li><p>creates sentence-level embeddings</p></li> 
> <li><p>composes them into a unified “semantic fingerprint”</p></li> 
> </ul> 
>  
> <p>That fingerprint becomes the reference point for the search.</p> 
>  
> <h2> 
>    
>    
>   <strong>🌐 Step 2: Finding Content Across the Web</strong> 
> </h2> 
>  
> <p>Nooth crawls and indexes content using:</p> 
>  
> <ul> 
> <li><p>lightweight scrapers</p></li> 
> <li><p>structured metadata</p></li> 
> <li><p>boilerplate removal</p></li> 
> <li><p>language detection</p></li> 
> <li><p>canonical URL resolution</p></li> 
> <li><p>and chunking for long-form content</p></li> 
> </ul> 
>  
> <p>The goal isn’t to store the entire HTML — only the <strong>meaningful part</strong>:</p> 
>  
> <p>titles, headings, paragraphs, and contextual metadata.</p> 
>  
> <p>Each chunk gets its own embedding.</p> 
>  
> <p>That allows highly precise, paragraph-level similarity detection.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   <strong>📊 Step 3: Computing Semantic Similarity</strong> 
> </h2> 
>  
> <p>Once all candidates are scraped, Nooth performs:</p> 
>  
> <ul> 
> <li><p>cosine similarity</p></li> 
> <li><p>conceptual overlap scoring</p></li> 
> <li><p>syntactic variance weighting</p></li> 
> <li><p>partial-match scoring (for rewritten content)</p></li> 
> </ul> 
>  
> <p>This combination is what lets Nooth detect even heavily rephrased articles.</p> 
>  
> <p>It doesn’t just ask:</p> 
>  
> <p><strong>“Do these texts share words?”</strong></p> 
>  
> <p>It asks:</p> 
>  
> <p><strong>“Do these texts express the same thing?”</strong></p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   <strong>🔍 Step 4: Presenting Clear Evidence</strong> 
> </h2> 
>  
> <p>The final result looks like this (example flow):</p> 
>  
> <ol> 
> <li><p>You paste a paragraph</p></li> 
> <li><p>Engine breaks it into meaning blocks</p></li> 
> <li><p>It fetches semantically closest content across the web</p></li> 
> <li><p>You get:</p></li> 
> </ol> 
>  
> <div> 
> <pre><code>-   URL 
>  
> -   domain 
>  
> -   similarity percentage 
>  
> -   highlighted overlapping ideas 
>  
> -   reasoning on _why_ the match is close 
> </code></pre> 
>  
> </div> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxc7fbzsdrckb2qtfmd1s.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxc7fbzsdrckb2qtfmd1s.png" alt="Ui Nooth" width="800" height="489"></a></p> 
>  
> <p>This makes it useful for:</p> 
>  
> <ul> 
> <li><p>discovering who’s copying your content</p></li> 
> <li><p>researching related topics</p></li> 
> <li><p>finding competitors</p></li> 
> <li><p>understanding what else exists on the same theme</p></li> 
> <li><p>generating new ideas based on existing clusters</p></li> 
> </ul> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsyj9aklbsfzco6g1pq9x.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsyj9aklbsfzco6g1pq9x.png" alt="UI NOOTH" width="800" height="748"></a></p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   <strong>🧭 Why I Built It</strong> 
> </h2> 
>  
> <p>I didn’t want another Google-like keyword search.</p> 
>  
> <p>I wanted a lens into the <strong>semantic web</strong> — the world of ideas, not strings.</p> 
>  
> <p>Whether you’re a researcher, writer, builder, or SEO nerd, sometimes you need to find content that’s <em>conceptually</em> similar, not textually identical.</p> 
>  
> <p>Nooth solves that by combining:</p> 
>  
> <ul> 
> <li><p>modern embeddings</p></li> 
> <li><p>optimized scraping</p></li> 
> <li><p>semantic search</p></li> 
> <li><p>scoring heuristics</p></li> 
> <li><p>and clean UI on top of everything</p></li> 
> </ul> 
>  
> <p>It shows you what’s related on an idea level, not just word level.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   <strong>🧪 Try It Out</strong> 
> </h2> 
>  
> <p>If you want to test it:</p> 
>  
> <p>👉 Paste any text.</p> 
>  
> <p>👉 See what the web holds that’s semantically connected.</p> 
>  
> <p>👉 Explore clusters of related ideas you didn’t even know existed.</p> 
>  
> <p><a href="https://nooth.dev/">https://nooth.dev/</a></p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   <strong>🔮 What’s Next</strong> 
> </h2> 
>  
> <p>I’m currently working on:</p> 
>  
> <ul> 
> <li><p>deeper plagiarism detection</p></li> 
> <li><p>topic extraction</p></li> 
> <li><p>content clustering</p></li> 
> <li><p>semantic timelines</p></li> 
> <li><p>alerts when new similar content appears</p></li> 
> <li><p>full API for developers</p></li> 
> </ul> 
>  
> <p>If any of this sounds interesting, I’d love feedback.</p> 
>  
> <p>Thanks for reading — and welcome to the world of semantic discovery.</p>

---

## [8/10] Answering Constraint Path Queries over Graphs
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T14:40:35.000Z
**URL:** https://arxiv.org/abs/2512.01733v1
**Reasoning:** The paper on constraint path queries over graphs is relevant to knowledge graphs, which are of secondary interest.
**Authors:** Heyang Li

**Content/Abstract:**
> Constraints are powerful declarative constructs that allow users to 
>   conveniently restrict variable values that potentially range over an 
>   infinite domain. In this paper, we propose a constraint path query language 
>   over property graphs, 
>   which extends Regular Path Queries (RPQs) with SMT constraints on data 
>   attributes in the form of equality constraints and Linear 
>   Real Arithmetic (LRA) constraints. We provide efficient algorithms 
>   for evaluating such path queries over property graphs, which exploits 
>   optimization of macro-states (among others, using theory-specific 
>   techniques). 
>   In particular, we demonstrate how such an algorithm may effectively utilize 
>   highly optimized SMT solvers for resolving such constraints over paths. 
>   We implement our algorithm in MillenniumDB, an open-source graph engine 
>   supporting property graph queries and GQL. Our extensive empirical 
>   evaluation in a real-world setting demonstrates the viability of our 
>   approach.

---

## [8/10] Introducing Forgetful - Shared Knowledge and Memory across Agents
**Source:** The Practical Developer | **Date:** 2025-12-02T12:03:17.000Z
**URL:** https://dev.to/scott_raisbeck_24ea5fbc1e/introducing-forgetful-shared-knowledge-and-memory-across-agents-40o6
**Reasoning:** The article discusses shared knowledge and memory across agents, which is relevant to AI agents in software engineering.
**Authors:** Scott Raisbeck

**Content/Abstract:**
> <p>I, like many of us over the last year or so, have been on an interesting journey when it comes to software development. </p> 
>  
> <p>From using ChatGPT to show me how to use sk-learn to build a classifier in 2022, seeing tab completion reach super saiyan level with copilot in 2023 (or was it 2024?! - It all seems like ancient history) on an engineers machine at work, using Cursor to remove the friction of copy/pasting out of chatgpt in 2024, to adopting my own form of the <a href="https://github.com/bmad-code-org/BMAD-METHOD?tab=readme-ov-file">BMAD method</a> to build out some really nice pet projects using a variety of tools like Claude Code, Cursor and Codex here in 2025.</p> 
>  
> <p>That's my personal experience compressed into a small paragraph, possibly a similar experience had by many on here have already had. The past few years have been a hell of a ride and I am finding I am now using more AI tools than ever. </p> 
>  
> <p>Over that time I have enjoyed reading many an article on DEV.TO on how many of us have our own approaches to get the best in this new paradigm, coupled with the 'X AI Feature/Product.. is Insane' YouTube videos (seriously guys, please stop). </p> 
>  
> <p>There have been some real nuggets in this that I have found, I've already mentioned the BMAD Method (which I still use to this day, albeit a bit more lightweight). Managing context window was another, and probably my favourite of all was <a href="https://github.com/upstash/context7">context7</a>.  </p> 
>  
> <p>Using context7 also made me realise something, the models are more accurate when the information is inside their context window and they are not having to rely on training data.</p> 
>  
> <p>For me the training data is almost like a necessary evil, I want them to rely on it less and less, please allow me to explain a bit further. </p> 
>  
> <p>Let's imagine I am looking to implement a Model Context Protocol (MCP) server. If you were looking to do this a few months back, the models straight up didn't know about MCP, so you were forced to ask it to use context7 and a web search to bring back the necessary data. </p> 
>  
> <p>Even today, with the models with the more recent training data, things are developing so fast in MCP, what it is likely to surface could be out of date and is more than likely not the optimal implementation pattern, so using Context7 I was able to be more confident in the approaches the Agent would suggest, especially when they could cite the sources and I could go validate them myself.  </p> 
>  
> <p>Like any other developer, once you solve a problem, you like to keep that pattern for other projects and over time your toolbox gets bigger and better. It should be no different with AIs, in fact, if anything it is even more important to ensure that the AIs have your taste, preferences and best practices in mind whenever they are implementing on your behalf. </p> 
>  
> <p>Simultaneously, I was working on my own AI agents, I've been developing agents for about six months now. As a learning exercise mostly, but I found it to actually be really fun. </p> 
>  
> <p>The first thing that anyone engineering an agent will realise is that you need memory. Even if it is just reinjecting the conversation history back to the agent so you can have continuity between requests. </p> 
>  
> <p>As you work on these you start to envisage better systems for memory. You often start to architect different types of memory, short term (a simple version being what I described in the previous paragraph), long term memory - for example some kind of automated retrieval of relevant facts/conversations that had been during previous interactions. </p> 
>  
> <p>There's also the breakdown of episodic (temporal and spatial context - "I had a meeting last week about the new payroll implementation"), semantic (facts and concepts - "payroll is a term to describe the business process of paying employees"), procedural (motor skills, and how to knowledge - "to put up a shelf I first.." &lt;- I am still to learn this one).</p> 
>  
> <p>Ultimately the answer usually ends up with some kind of persistence layer, a search mechanism and some service to pull it all together (maybe even some sub-agent specifically to manage retrieval and storage of memories) at inference time.</p> 
>  
> <p>So with this in mind I went away and vibe coded up an MCP server for agentic systems to store memories in. I did it in an evening and spent the rest of the weekend dogfooding it on systems like Claude Code, Claude Desktop, Codex and Cursor. I had built this tool for my own agents that I was building, but I actually found it really useful in my coding agents. I quickly set about encoding the code for all my projects into memories, getting encoding agents to attach documents and code snippets to sit alongside memories so that an agent querying the knowledge base could get an idea of a particular pattern, or the way something worked, just from what was in the knowledge base and dig into the code if needed. </p> 
>  
> <p>In this sense, the MCP server I had built became very much like my own little mini context7, well actually a bit more than that really. I could ask it to look at the code in those repos of course, but it would still lack information about design decisions, information for how we resolved an issue, preferred patterns and when to use them. It just felt like I was working in the same session across multiple projects/agents, it was really refreshing. </p> 
>  
> <p>I had hosted this remotely and managed to connect it to Claude Mobile via <a href="https://dev.to/scott_raisbeck_24ea5fbc1e/i-spent-a-week-building-oauth-plumbing-that-shouldnt-exist-1h34">Dynamic Client Registration</a>, </p> 
>  
> <p>So now when I was out having a walk or something and I had an idea around something related to one of my projects, Claude had semantic understanding of it. It meant I could have meaningful conversations about it without it having to scan through code. </p> 
>  
> <p>I could even make decisions with Claude while out on that walk, get it to record a memory (and an implementation plan document) and then once I was back home just tell Claude Code to go ahead and implement. The same applied to using other agentic coding applications, such as Cursor, Copilot and Codex. </p> 
>  
> <p>I found myself logging every design decision in the memory, and then just having built in commands to fetch context for when I started out a new session. This worked across Claude Code, Codex, OpenCode and Copilot. Which has been my arsenal of AI coding tools for the past few months (could I get by with just one? Yes, but those YouTube videos I mentioned earlier.. they're unfortunately really effective - so please stop guys, I have kids to feed). </p> 
>  
> <p>Anyhow, it didn't take long for me to realise that I needed to build a proper version of this as I was depending on it more and more in my daily work. While it all worked fine and felt pretty robust, I didn't like the way Claude had implemented it, maybe I shouldn't care but I do. In order to understand something, I need to build it and all that good stuff, I believe dr Feinman said something along those lines, I am certainly no Richard Feinman but I can still appreciate that sentiment. </p> 
>  
> <p>So I set about building out a new version of it, gave it a name (Forgetful) and decided I'd make it open source to see if it would be useful for others - be it for people using coding agents (which was a use case I had found myself accidently having) to AI engineers build agents themselves. </p> 
>  
> <p>I did all this before checking if anyone else had built one by the way, there are several out there now, some paid and some for free. I would encourage you to check them out and see what works for you. I do see this as the next paradigm in AI, cross agent memory solutions are going to be key, especially as the AI ecosystem seems to open up more and more for third party apps on some of the big AI labs systems and I cannot tell you how much this has helped my own use of coding agents. I am working on something right now for my day job that I plan on showcasing, but that is for another post (as it's not finished) and Forgetful sits at the heart of it.</p> 
>  
> <p>It's not some 'I 10xd my AI workflow' hack, I don't know what kind of gains this has given me, I don't really have a way to measure it. </p> 
>  
> <p>I just feel more comfortable switching between projects and agents now and perhaps more importantly getting the agents more familiar with the patterns you use and  reducing the need to have the same conversation with AI's to tackle the same problem elsewhere. </p> 
>  
> <h2> 
>    
>    
>   So What Did I Actually Build? 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5u0mcmc5s3p5og7nz9kw.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5u0mcmc5s3p5og7nz9kw.png" alt="Architecture Diagram" width="800" height="416"></a></p> 
>  
> <p>When I sat down to build Forgetful properly, I had to make some decisions about how memories should be structured. This is where I got opinionated.</p> 
>  
> <p>Most memory systems are essentially vector dumps — throw everything in, embed it, retrieve by cosine similarity, hope for the best. It works, but it's messy. You end up with overlapping chunks, no clear boundaries between concepts, and retrieval that's good enough but never precise, reranking helps to a degree, but I see this as something other than just a store. </p> 
>  
> <p>I wanted something more like how I use Obsidian. Atomic notes. One concept per note. Links between related ideas. A graph that emerges from the connections rather than being imposed from above. I had been using Obsidian to follow the <a href="https://en.wikipedia.org/wiki/Zettelkasten">Zettelkasten principle</a> — a note-taking method where each note captures exactly one idea, is self-contained enough to understand on its own, and links explicitly to related notes.</p> 
>  
> <p>I had stumbled across <a href="https://www.youtube.com/watch?v=nSh2BYJ29kY&amp;t=15s">A video of someone implementing semantic encoding of their own Obisidan Notes</a> and then things started to come together. I did my usually back and forth with Claude when out for one of my walks, it threw up some papers around where similar concepts had already been tried: The <a href="https://arxiv.org/abs/2502.12110">A-Mem paper</a> on agentic memory systems found that structured, self-organising memory significantly improves retrieval precision. </p> 
>  
> <p>In Forgetful, every memory must have (these are configurable as environment variables):</p> 
>  
> <ul> 
> <li>A clear title (forced brevity — 200 char limit)</li> 
> <li>Content covering one concept (~300-400 words max)</li> 
> <li>Context around what the agent was doing when it created the memory</li> 
> <li>Keywords and tags for additional retrieval paths</li> 
> </ul> 
>  
> <p>This might seem restrictive, but that's the point. When an agent goes to store something, it has to think about what the atomic unit of knowledge actually is. No more dumping entire conversations or documents into memory and hoping retrieval figures it out later.</p> 
>  
> <h2> 
>    
>    
>   Auto-Linking 
> </h2> 
>  
> <p>Here's where it gets interesting. When you create a memory, Forgetful doesn't just store it — it finds its place in the graph. Now I would stipulate this is also configurable, I think the best practice is to have an agent dedicated to memory management, who takes in raw input and decides what memories are worth keeping, and how they should fit inside the knowledge base, and whether existing memories need updating or need to be made obsolete as a result of the new interactions. This however is not something I have built yet for my own development memory management and indeed might not be something others want to build. So as a starting point I added automated memory linking, and so far it has worked just fine. </p> 
>  
> <p>The process:</p> 
>  
> <ol> 
> <li>Generate an embedding for the new memory </li> 
> <li>Search for semantically similar existing memories</li> 
> <li>Any memories above a 0.7 similarity threshold and cross encoder ranked get automatically linked</li> 
> <li>These links are bidirectional — the graph builds itself</li> 
> </ol> 
>  
> <p>So if I store a memory about "choosing Stripe over PayPal for payment processing" and I already have memories about "PCI compliance requirements" and "subscription billing architecture", Forgetful will automatically connect them. No manual linking required.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F7jte83j54yk3pcpv4kcg.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F7jte83j54yk3pcpv4kcg.png" alt="Memory Auto-Linking" width="593" height="499"></a></p> 
>  
> <p>When an agent later queries "how should I handle payments?", it doesn't just get the Stripe decision — it gets the linked context about compliance and billing architecture too. One-hop graph traversal is included by default, what I mean by that is, for every memory that is retrieved through search, all linked memories 1-hop away are returned as well. </p> 
>  
> <p>This is what I mean by "Obsidian for AI agents". The same way your Obsidian vault becomes more valuable as connections emerge between notes, your agent's memory becomes more useful as the knowledge graph densifies.</p> 
>  
> <h2> 
>    
>    
>   Why Not Neo4j? 
> </h2> 
>  
> <p>I can already hear some of you asking: "If you're building a knowledge graph, why not use a graph database?"</p> 
>  
> <p>The honest answer is I've never worked with a Graph database, I'm only familiar with the relational database such as PostgreSQL, MySQL, MSSQL etc.<br> 
> So Forgetful stores memories in PostgreSQL (or SQLite for the zero-config local experience) with pgvector for embeddings. The graph relationships are just rows in a links table. So no elegant graph theory, maybe it's something I'll consider in the future. I've architectued Forgetful in a way that I can add adapters quite easily for different types of implementation layers, hence why I can switch between Postgres and SQLite, so adding a Graph Database or even a dedicated Vector Database later down the line wouldn't involve a total re-write. </p> 
>  
> <p>The relational database appraoch appears to  works fine for my scale. For the access patterns agents actually use (store a memory, find related memories, traverse one hop), a relational model with proper indexing handles it without breaking a sweat. Maybe at massive scale I'd revisit this, but I'd rather ship something useful than architect for problems I don't have yet.</p> 
> <h2> 
>    
>    
>   Making Retrieval Actually Good 
> </h2> 
>  
> <p>Storing memories is the easy part. Retrieval is where most systems fall down.</p> 
>  
> <p>A naive approach: embed the query, find the top-k most similar memories by cosine distance, return them. This works but has problems. Embedding similarity is fuzzy — semantically related doesn't always mean actually relevant to what the agent needs right now.</p> 
>  
> <p>Forgetful uses a multi-stage approach:</p> 
>  
> <p><strong>Stage 1: Dense retrieval</strong><br> 
> Embed the query, pull back candidate memories using vector similarity. Cast a wide net.</p> 
>  
> <p><strong>Stage 2: Cross-encoder reranking</strong><br> 
> Here's the trick — when an agent searches, it provides not just the query but a <code>query_context</code> explaining <em>why</em> it's searching. "I'm implementing payment integration" gives different results than "I'm debugging a checkout error" even if both search for "payments". The cross-encoder uses this full context to rerank candidates.</p> 
>  
> <p>The cross-encoder scores each candidate against the full query context, reranks them, and the top results go back to the agent.</p> 
>  
> <p>Is this overkill? Maybe. But retrieval precision is everything. Returning the wrong context is worse than returning nothing — it confidently misleads the agent.</p> 
> <h2> 
>    
>    
>   Token Budget Management 
> </h2> 
>  
> <p>Even with great retrieval, you can still overwhelm an agent's context window. Twenty highly relevant memories might be 15,000 tokens — and that's before the agent's actual task.</p> 
>  
> <p>Forgetful manages this with a configurable token budget (default 8K). Results are prioritised by:</p> 
>  
> <ol> 
> <li>Importance score (9-10 rated memories first)</li> 
> <li>Recency (newest within each importance tier)</li> 
> </ol> 
>  
> <p>If the budget fills up, lower-priority memories get truncated. The agent always gets the most critical context without the LLM choking on input length.</p> 
> <h2> 
>    
>    
>   From Single Machine to Cloud 
> </h2> 
>  
> <p>One thing I wanted to get right: Forgetful should scale with your needs.</p> 
>  
> <p><strong>Just trying it out?</strong><br> 
> </p> 
>  
> <div> 
> <pre><code>uvx forgetful-ai 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>That's it. SQLite database, stored in your home directory, stdio transport for MCP. Zero configuration.</p> 
>  
> <p>The default setup runs completely offline — embeddings are generated locally using FastEmbed with the BAAI/bge-small-en-v1.5 model. No OpenAI API key required, no data leaves your machine. If you want cloud embeddings (Azure OpenAI, Google), you can configure that, but it's entirely optional.</p> 
>  
> <p><strong>Running it for real?</strong><br> 
> Docker Compose with PostgreSQL, HTTP transport, proper authentication. Same codebase, same API, just different deployment.</p> 
>  
> <p><strong>Multi-device access?</strong><br> 
> Host it somewhere with an endpoint, configure your MCP clients to point at it. I run mine on a VPS and connect from Claude Desktop, Claude Mobile, Cursor, and Claude Code — all hitting the same knowledge base.</p> 
>  
> <p>The progression should feel natural. Start local, go remote when you need to.</p> 
> <h2> 
>    
>    
>   What Else Is In There? 
> </h2> 
>  
> <p>Memories are the core, but Forgetful has a few other concepts that emerged from real usage:</p> 
>  
> <p><strong>Entities</strong>: Concrete things — people, organisations, products, infrastructure. These can have relationships to each other ("Jordan works_for TechFlow") and link to memories. Useful for building out knowledge about your team, your systems, your clients.</p> 
>  
> <p><strong>Projects</strong>: Scope for memories. When I'm working on the e-commerce platform, I don't need memories from the trading bot project polluting my context. Project scoping keeps retrieval focused.</p> 
>  
> <p><strong>Documents</strong>: Sometimes you need more than 400 words. Documents store long-form content, with the expectation that you'll extract atomic memories from them that link back to the parent.</p> 
>  
> <p><strong>Code Artifacts</strong>: Reusable code snippets attached to memories. The agent can retrieve not just the concept but a working example.</p> 
>  
> <p>All of these link together. An entity can relate to memories, which belong to projects, which have associated documents and code artifacts. The graph extends beyond just memory-to-memory connections.</p> 
> <h2> 
>    
>    
>   The Meta-Tools Pattern 
> </h2> 
>  
> <p>One last implementation detail that I'm quite pleased with.</p> 
>  
> <p>MCP clients see three tools from Forgetful:</p> 
>  
> <ul> 
> <li> 
> <code>discover_tools</code> — what's available?</li> 
> <li> 
> <code>how_to_use</code> — how does a specific tool work?</li> 
> <li> 
> <code>execute_tool</code> — run a tool with arguments</li> 
> </ul> 
>  
> <p>Behind that facade sit 42 actual tools. The agent discovers what it needs, learns how to use it, then executes. This keeps the tool list in the agent's context minimal while still exposing full functionality.</p> 
>  
> <p>It's a small thing, but context window discipline matters. Every token spent on tool definitions is a token not available for actual reasoning. The numbers matter here: exposing dozens of tools with full JSON schemas would consume thousands of tokens before the agent even starts working. The three meta-tools keep context overhead minimal while still providing full access to everything Forgetful can do.</p> 
>  
>  
>  
> <p>That's Forgetful. An opinionated memory system built on Zettelkasten principles, with auto-linking, proper retrieval, and a deployment model that scales from "just trying it" to "running in production".</p> 
>  
> <p>If you want to try it:<br> 
> </p> 
>  
> <div> 
> <pre><code>uvx forgetful-ai 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>GitHub: <a href="https://github.com/ScottRBK/forgetful">github.com/ScottRBK/forgetful</a></p> 
>  
> <p>Discord: <a href="https://discord.gg/ngaUjKWkFJ">If you are into building AI Agents or even just like talking about coding agents and AI in general head over to my discord</a></p> 
>  
> <p>I'd love to hear how you use it, what breaks, and what's missing. I'd imagine that this will be something i continually work on as part of agent development, so having the input of others will be most helpful! </p> 
>  
> <p>Next post: I'll show what I'm building on top of Forgetful for my day job — a system for semantic understanding of 250+ repositories. But that's for another time.</p>

---

## [7/10] Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251107943X/abstract
**Reasoning:** Thinker discusses hierarchical thinking for deep search, which is relevant to retrieval and context management.
**Authors:** Xu, Jun, Du, Xinkai, Ao, Yu, Zhao, Peilong, Li, Yang, Zhong, Ling, Yuan, Lin, Bo, Zhongpu, Wang, Xiaorui, Sun, Mengshu, Gui, Zhengke, Zhang, Dalong, Wang, Zhaoyang, Wang, Qiwei, Hou, Yangyang, Yin, Zhiying, Wang, Haofen, Chen, Huajun, Liang, Lei, Zhou, Jun

**Content/Abstract:**
> Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.

---

## [7/10] Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251107581V/abstract
**Reasoning:** Orion discusses adaptive search strategies with LLMs, relevant to retrieval and context management.
**Authors:** Vijay, Supriti, Priyanshu, Aman, Vellore, Anu, Saglam, Baturay, Karbasi, Amin

**Content/Abstract:**
> Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.

---

## [7/10] Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251102770C/abstract
**Reasoning:** The paper discusses multi-query retrieval, which is relevant to codebase indexing and retrieval.
**Authors:** Chen, Hung-Ting, Liu, Xiang, Ravfogel, Shauli, Choi, Eunsol

**Content/Abstract:**
> Most text retrievers generate \emph{one} query vector to retrieve relevant documents. Yet, the conditional distribution of relevant documents for the query may be multimodal, e.g., representing different interpretations of the query. We first quantify the limitations of existing retrievers. All retrievers we evaluate struggle more as the distance between target document embeddings grows. To address this limitation, we develop a new retriever architecture, \emph{A}utoregressive \emph{M}ulti-\emph{E}mbedding \emph{R}etriever (AMER). Our model autoregressively generates multiple query vectors, and all the predicted query vectors are used to retrieve documents from the corpus. We show that on the synthetic vectorized data, the proposed method could capture multiple target distributions perfectly, showing 4x better performance than single embedding model. We also fine-tune our model on real-world multi-answer retrieval datasets and evaluate in-domain. AMER presents 4 and 21\% relative gains over single-embedding baselines on two datasets we evaluate on. Furthermore, we consistently observe larger gains on the subset of dataset where the embeddings of the target documents are less similar to each other. We demonstrate the potential of using a multi-query vector retriever and open up a new direction for future work.

---

## [7/10] Coordination-Free Lane Partitioning for Convergent ANN Search
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251104221K/abstract
**Reasoning:** Coordination-Free Lane Partitioning involves vector search systems, which is related to our interest in codebase indexing and retrieval.
**Authors:** Kugblenu, Carl, Vuorimaa, Petri

**Content/Abstract:**
> Production vector search systems often fan out each query across parallel lanes (threads, replicas, or shards) to meet latency service-level objectives (SLOs). In practice, these lanes rediscover the same candidates, so extra compute does not increase coverage. We present a coordination-free lane partitioner that turns duplication into complementary work at the same cost and deadline. For each query we (1) build a deterministic candidate pool sized to the total top-k budget, (2) apply a per-query pseudorandom permutation, and (3) assign each lane a disjoint slice of positions. Lanes then return different results by construction, with no runtime coordination. At equal cost with four lanes (total candidate budget 64), on SIFT1M (1M SIFT feature vectors) with Hierarchical Navigable Small World graphs (HNSW) recall@10 rises from 0.249 to 0.999 while lane overlap falls from nearly 100% to 0%. On MS MARCO (8.8M passages) with HNSW, hit@10 improves from 0.200 to 0.601 and Mean Reciprocal Rank at 10 (MRR@10) from 0.133 to 0.330. For inverted file (IVF) indexes we see smaller but consistent gains (for example, +11% on MS MARCO) by de-duplicating list routing. A microbenchmark shows planner overhead of ~37 microseconds per query (mean at the main setting) with linear growth in the number of merged candidates. These results yield a simple operational guideline: size the per-query pool to the total budget, deterministically partition positions across lanes, and turn redundant fan-out into complementary coverage without changing budget or deadline.

---

## [7/10] Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251113057P/abstract
**Reasoning:** The study on vector retrieval and compression is relevant to codebase indexing and retrieval, particularly in optimizing storage and performance.
**Authors:** Pati, Satyanarayan

**Content/Abstract:**
> Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the "performance loss" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective "sweet spot," achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.

---

## [7/10] How Far Are We from Genuinely Useful Deep Research Agents?
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T17:58:59.000Z
**URL:** https://arxiv.org/abs/2512.01948v1
**Reasoning:** The exploration of Deep Research Agents relates to autonomous coding and agent memory architectures.
**Authors:** Dingling Zhang

**Content/Abstract:**
> Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.

---

## [7/10] Stop Prompting the Hard Way: How Mellea Gives You Supercharged AI Results
**Source:** The Practical Developer | **Date:** 2025-12-02T12:10:47.000Z
**URL:** https://dev.to/manikandan/stop-prompting-the-hard-way-how-mellea-gives-you-supercharged-ai-results-2080
**Reasoning:** The article discusses structured prompt design, which is relevant to AI agents in software engineering.
**Authors:** Manikandan Mariappan

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwueyp4a9b0tavjkpe0r2.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><h2>  
>     
>     
>   Introduction  
> </h2>  
>   
> <p>Prompting used to feel like magic. Sometimes the AI gave great results sometimes it didn’t. But modern AI workloads require consistency, structure, validation, and reliability — not guesswork.</p>  
>   
> <p>This is where <code>Mellea</code> steps in. </p>  
>   
> <p>Mellea is changing the way developers craft prompts by introducing structured, modular, and reusable prompt design. Instead of writing plain text prompts, Mellea lets you create smart, dynamic, and interactive prompt components.</p>  
>   
> <blockquote>  
> <p>Mellea transforms ordinary text prompts into well-defined, self-correcting AI functions that produce predictable and error-free outputs.</p>  
> </blockquote>  
>   
> <h2>  
>     
>     
>   What Is Mellea?  
> </h2>  
>   
> <p>Mellea is a Python-based framework that allows you to build <strong>prompt<br>  
> objects</strong>, reuse them, and execute them like code. This makes prompting more consistent, scalable, and easier to maintain.</p>  
>   
> <p><code>Mellea is a runtime layer</code> that sits between your <code>application</code> and an <code>LLM</code>. It forces the AI to follow structured, rule-based, validated outputs.</p>  
>   
> <p>Think of Mellea as:</p>  
>   
> <ul>  
> <li>A <strong>compiler</strong> for your AI instructions</li>  
> <li>A <strong>validator</strong> that checks if AI output matches your schema</li>  
> <li>A <strong>controller</strong> that retries/corrects the AI when it goes off-track</li>  
> </ul>  
> <h3>  
>     
>     
>   Without Mellea vs. With Mellea  
> </h3>  
>   
>   
> <div>  
> <pre><code>  
> <span>Without</span> <span>Mellea</span><span>:</span>  
>   
> <span>Prompt</span> <span>→</span> <span>AI</span> <span>→</span> <span>(</span><span>Sometimes</span> <span>correct</span><span>,</span> <span>sometimes</span> <span>messy</span><span>)</span>  
>   
>   
> <span>With</span> <span>Mellea</span><span>:</span>  
>   
> <span>Prompt</span> <span>+</span> <span>Schema</span> <span>→</span> <span>Mellea</span> <span>→</span> <span>AI</span> <span>→</span> <span>Validate</span> <span>→</span> <span>Correct</span> <span>→</span> <span>Perfect</span> <span>Output</span>  
>   
> </code></pre>  
>   
> </div>  
>   
> <h2>  
>     
>     
>   Why Prompting Alone Is No Longer Enough  
> </h2>  
> <h3>  
>     
>     
>   ❌ Traditional Prompting Problems  
> </h3>  
>   
> <ul>  
> <li><p>AI may hallucinate</p></li>  
> <li><p>Output may not follow structure</p></li>  
> <li><p>Difficult to embed into production apps</p></li>  
> <li><p>No validation layer</p></li>  
> <li><p>High retry cost</p></li>  
> </ul>  
> <h3>  
>     
>     
>   ✔️ Mellea Solves All of These  
> </h3>  
>   
> <p>Mellea adds <strong>guaranteed structure:</strong></p>  
>   
> <ul>  
> <li><p>Enforces strict JSON schemas</p></li>  
> <li><p>Validates output</p></li>  
> <li><p>Auto-corrects when wrong</p></li>  
> <li><p>Reduces hallucinations</p></li>  
> <li><p>Makes AI outputs predictable</p></li>  
> </ul>  
>   
> <p>This is why developers say:</p>  
>   
> <blockquote>  
> <p>“Mellea makes LLMs behave like functions, not guessing machines.”</p>  
> </blockquote>  
> <h2>  
>     
>     
>   How Mellea Upgrades Prompting: A Visual Diagram  
> </h2>  
> <h3>  
>     
>     
>   Architecture Overview  
> </h3>  
>   
>   
> <div>  
> <pre><code><span>+</span><span>------------------------+</span>  
> <span>|</span>        <span>Your</span> <span>App</span>        <span>|</span>  
> <span>+</span><span>------------------------+</span>  
>             <span>|</span>  
>             <span>v</span>  
> <span>+</span><span>------------------------+</span>  
> <span>|</span>         <span>Mellea</span>         <span>|</span>  
> <span>|</span>  <span>-</span> <span>Function</span> <span>schemas</span>    <span>|</span>  
> <span>|</span>  <span>-</span> <span>Validators</span>          <span>|</span>  
> <span>|</span>  <span>-</span> <span>Auto</span><span>-</span><span>correct</span>        <span>|</span>  
> <span>+</span><span>------------------------+</span>  
>             <span>|</span>  
>             <span>v</span>  
> <span>+</span><span>------------------------+</span>  
> <span>|</span>        <span>LLM</span> <span>Model</span>       <span>|</span>  
> <span>+</span><span>------------------------+</span>  
>   
> </code></pre>  
>   
> </div>  
>   
> <h3>  
>     
>     
>   What Happens Internally?  
> </h3>  
>   
>   
> <div>  
> <pre><code>  
> <span>Input</span> <span>→</span> <span>Parse</span> <span>→</span> <span>Send</span> <span>to</span> <span>LLM</span> <span>→</span> <span>Validate</span> <span>Output</span>  
>          <span>|</span>                     <span>|</span>  
>          <span>|&lt;</span><span>-- retry/fix if needed</span>  
>   
> </code></pre>  
>   
> </div>  
>   
>   
> <blockquote>  
> <p>Mellea behaves like an AI compiler.</p>  
> </blockquote>  
> <h2>  
>     
>     
>   Real Example: Prompting Without vs With Mellea  
> </h2>  
> <h3>  
>     
>     
>   ❌ Without Mellea  
> </h3>  
>   
>   
> <div>  
> <pre><code><span>Prompt</span><span>:</span>  
> <span>"</span><span>Extract all products from this HTML page.</span><span>"</span>  
>   
> <span>AI</span> <span>Output</span><span>:</span>  
> <span>Maybe</span> <span>JSON</span><span>,</span> <span>maybe</span> <span>text</span><span>,</span> <span>maybe</span> <span>missing</span> <span>fields</span><span>,</span> <span>maybe</span> <span>hallucinated</span> <span>items</span><span>.</span>  
> </code></pre>  
>   
> </div>  
>   
> <h3>  
>     
>     
>   ✔️ With Mellea  
> </h3>  
>   
>   
> <div>  
> <pre><code><span>Define</span> <span>a</span> <span>function</span>  
> <span>@mellea.function</span>  
> <span>def</span> <span>extract_products</span><span>(</span><span>html</span><span>:</span> <span>str</span><span>)</span> <span>-&gt;</span> <span>List</span><span>[</span><span>Product</span><span>]:</span>  
>     <span>"""</span><span>Extract product list from HTML.</span><span>"""</span>  
> </code></pre>  
>   
> </div>  
>   
>   
> <p>Call it<br>  
> </p>  
>   
> <div>  
> <pre><code><span>products</span> <span>=</span> <span>extract_products</span><span>(</span><span>html</span><span>)</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <h3>  
>     
>     
>   Guaranteed Output  
> </h3>  
>   
>   
>   
> <div>  
> <pre><code><span>[</span><span>  
>   </span><span>{</span><span>"name"</span><span>:</span><span> </span><span>"Laptop"</span><span>,</span><span> </span><span>"price"</span><span>:</span><span> </span><span>899</span><span>,</span><span> </span><span>"availability"</span><span>:</span><span> </span><span>true</span><span>},</span><span>  
>   </span><span>{</span><span>"name"</span><span>:</span><span> </span><span>"Headphones"</span><span>,</span><span> </span><span>"price"</span><span>:</span><span> </span><span>129</span><span>,</span><span> </span><span>"availability"</span><span>:</span><span> </span><span>false</span><span>}</span><span>  
> </span><span>]</span><span>  
> </span></code></pre>  
>   
> </div>  
>   
>   
>   
> <blockquote>  
> <p>Structured. Clean. Valid. No hallucinations. No formatting issues.</p>  
> </blockquote>  
>   
> <h2>  
>     
>     
>   Why Do We Need This Tool?  
> </h2>  
>   
> <ul>  
> <li>  Traditional prompts are unstructured and hard to debug.</li>  
> <li>  Teams struggle to reuse prompt logic across projects.</li>  
> <li>  Scaling prompts for large applications is tedious and error-prone.</li>  
> </ul>  
>   
> <p>Mellea solves all of this with a programmable prompting interface.</p>  
>   
> <h2>  
>     
>     
>   Example: Simple Prompt  
> </h2>  
>   
>   
>   
> <div>  
> <pre><code><span>from</span> <span>mellea.core</span> <span>import</span> <span>Prompt</span>  
>   
> <span>p</span> <span>=</span> <span>Prompt</span><span>(</span><span>"</span><span>Write a poem about {topic}.</span><span>"</span><span>)</span>  
> <span>print</span><span>(</span><span>p</span><span>(</span><span>topic</span><span>=</span><span>"</span><span>snow</span><span>"</span><span>))</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <h2>  
>     
>     
>   Example: Operator System  
> </h2>  
>   
>   
>   
> <div>  
> <pre><code><span>from</span> <span>mellea.core</span> <span>import</span> <span>Operator</span>  
>   
> <span>class</span> <span>Adder</span><span>(</span><span>Operator</span><span>):</span>  
>     <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>y</span><span>):</span>  
>         <span>return</span> <span>x</span> <span>+</span> <span>y</span>  
>   
> <span>adder</span> <span>=</span> <span>Adder</span><span>()</span>  
> <span>print</span><span>(</span><span>adder</span><span>(</span><span>10</span><span>,</span> <span>20</span><span>))</span>  <span># Output: 30  
> </span></code></pre>  
>   
> </div>  
>   
>   
>   
> <h2>  
>     
>     
>   Real Use Case Example  
> </h2>  
>   
>   
>   
> <div>  
> <pre><code><span>from</span> <span>mellea.core</span> <span>import</span> <span>Prompt</span><span>,</span> <span>Operator</span>  
>   
> <span>class</span> <span>Summarizer</span><span>(</span><span>Operator</span><span>):</span>  
>     <span>prompt</span> <span>=</span> <span>Prompt</span><span>(</span><span>"</span><span>Summarize the following text:  
> {text}</span><span>"</span><span>)</span>  
>   
> <span>summary</span> <span>=</span> <span>Summarizer</span><span>()</span>  
> <span>print</span><span>(</span><span>summary</span><span>(</span><span>text</span><span>=</span><span>"</span><span>AI is transforming the world...</span><span>"</span><span>))</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <h2>  
>     
>     
>   Why This Tool Is Really Needed  
> </h2>  
>   
> <p><strong>1. AI is too unpredictable for production apps</strong></p>  
>   
> <p>Companies need reliability, not creativity.</p>  
>   
> <p><strong>2. Schema validation is mandatory in real workflows</strong></p>  
>   
> <p>APIs, pipelines, agents → All require structured outputs.</p>  
>   
> <p><strong>3. LLMs hallucinate and misformat</strong></p>  
>   
> <p>Mellea guards against this.</p>  
>   
> <p><strong>4. Prompts alone do not scale</strong></p>  
>   
> <p>Mellea gives a programming paradigm, not instructions.</p>  
>   
> <h2>  
>     
>     
>   What Is the Future of Mellea?  
> </h2>  
>   
> <p>Mellea will shape the future of AI development in multiple ways:</p>  
>   
> <p><strong>1. Turning AI into composable functions</strong></p>  
>   
> <p>Every AI model becomes a strict function block—like software components.</p>  
>   
> <p><strong>2. More agent frameworks will depend on Mellea-like logic</strong></p>  
>   
> <p>Agents need deterministic outputs to perform actions.</p>  
>   
> <p><strong>3. AI pipelines will become fully typed</strong></p>  
>   
> <p>Just like TypeScript changed JS, Mellea-style validation will change LLM development.</p>  
>   
> <p><strong>4. Enterprise adoption will explode</strong></p>  
>   
> <p>Compliance + Reliability = Must-have for companies</p>  
>   
> <p><strong>5. Integrated into major AI tools &amp; platforms</strong></p>  
>   
> <p>Mellea-like layers will become standard in LLM SDKs.</p>  
>   
> <h2>  
>     
>     
>   How Mellea Impacts Our Daily Routine  
> </h2>  
>   
> <p><strong>1. Faster development</strong></p>  
>   
> <p>No more debugging AI output.</p>  
>   
> <p><strong>2. Cleaner responses</strong></p>  
>   
> <p>Every prompt becomes predictable.</p>  
>   
> <p><strong>3. Better safety</strong></p>  
>   
> <p>Schemas stop AI from generating harmful or irrelevant content.</p>  
>   
> <p><strong>4. Low cognitive load</strong></p>  
>   
> <p>Developers stop writing long prompts; they define functions instead.</p>  
>   
> <p><strong>5. More reliable AI systems</strong></p>  
>   
> <p>Great for automations, bots, monitoring, summarization, extraction, and more.</p>  
>   
> <h2>  
>     
>     
>   Final Thoughts  
> </h2>  
>   
> <p>Mellea is not <strong>“just another AI tool.”</strong> It is a <strong>revolution in prompting</strong>.</p>  
>   
> <p>It transforms:</p>  
>   
> <p>❌ Messy, unpredictable prompts -&gt; ✔️ Structured, safe, validated AI functions.</p>  
>   
> <p><strong>Mellea is the future.</strong></p>  
>   
> <h2>  
>     
>     
>   🔗 References  
> </h2>  
>   
> <ul>  
> <li>  Mellea GitHub: <a href="https://github.com/generative-computing/mellea">https://github.com/generative-computing/mellea</a>  
> </li>  
> <li>  Tutorial:  
> <a href="https://github.com/generative-computing/mellea/blob/main/docs/tutorial.md">https://github.com/generative-computing/mellea/blob/main/docs/tutorial.md</a>  
> </li>  
> </ul>

---

## [7/10] Creating Autonomous AI Agents – A Practical Guide for Businesses
**Source:** The Practical Developer | **Date:** 2025-12-02T11:18:33.000Z
**URL:** https://dev.to/harshada_75eaf5c6bf7a194a/creating-autonomous-ai-agents-a-practical-guide-for-businesses-384k
**Reasoning:** The article discusses creating autonomous AI agents, which is related to AI agents in software engineering.
**Authors:** harshada

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fq08uh01hg9ie79x95wx0.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>Artificial intelligence is quickly evolving from simple prompt-based systems into autonomous agents capable of reasoning, planning, and acting independently. Organizations across industries are now exploring the process of <a href="https://resurs.ai/">creating autonomous AI agents</a><br>  
>  to streamline operations, reduce manual work, and unlock intelligent automation at scale.</p>  
>   
> <p>Autonomous agents can execute tasks, interact with software tools, analyze data, and refine their performance—all with minimal human oversight. This makes them a powerful upgrade from traditional chatbots, static RPA, or single-step AI workflows.</p>  
>   
> <p>What Are Autonomous AI Agents?</p>  
>   
> <p>Autonomous AI agents are software entities designed to perform goal-driven tasks independently. They rely on a mix of large language models, memory systems, reasoning engines, and tool execution frameworks to complete workflows.</p>  
>   
> <p>These agents can:</p>  
>   
> <p>Break tasks into actionable steps</p>  
>   
> <p>Execute commands across systems</p>  
>   
> <p>Adapt to changing requirements</p>  
>   
> <p>Validate their results</p>  
>   
> <p>Improve performance over time</p>  
>   
> <p>Unlike simple automation scripts, autonomous agents can think, not just execute.</p>  
>   
> <p>Why Businesses Are Building Autonomous AI Agents</p>  
>   
> <p>Companies are deploying agentic systems to solve real operational challenges, including:</p>  
>   
> <p>High workflows dependency on human decision-making</p>  
>   
> <p>Time-consuming manual tasks</p>  
>   
> <p>Complex multi-step workflows</p>  
>   
> <p>Legacy automation limitations</p>  
>   
> <p>Scalability and personnel constraints</p>  
>   
> <p>This new generation of automation delivers:</p>  
>   
> <p>Benefit Impact<br>  
> Efficiency gains    Faster execution of processes<br>  
> Accuracy improvement    Fewer errors and quality checks<br>  
> Cost savings    Reduced labor and operational overhead<br>  
> 24/7 automation Full-time digital workforce<br>  
> Adaptability    Continuous learning and refinement</p>  
>   
> <p>The adoption curve is accelerating across finance, IT ops, HR automation, legal, cybersecurity, and logistics.</p>  
>   
> <p>Core System Requirements for Building Autonomous AI Agents</p>  
>   
> <p>To successfully start creating autonomous AI agents, organizations need:</p>  
>   
> <ol>  
> <li>A Reasoning LLM Core</li>  
> </ol>  
>   
> <p>The language model performs planning, problem-solving, and decision-making.</p>  
>   
> <ol>  
> <li>Tool Execution Environment</li>  
> </ol>  
>   
> <p>Agents require access to APIs, workflow automation platforms, or agentic AI workflow tools.</p>  
>   
> <ol>  
> <li>Memory Framework</li>  
> </ol>  
>   
> <p>Short-term and long-term memory support context, personalization, and iterative learning.</p>  
>   
> <ol>  
> <li>Monitoring &amp; Validation Layer</li>  
> </ol>  
>   
> <p>Ensures output accuracy, compliance, and safety guardrails.</p>  
>   
> <ol>  
> <li>Agentic AI Orchestration Layer</li>  
> </ol>  
>   
> <p>This enables multi-agent collaboration, task delegation, and lifecycle management.</p>  
>   
> <p>Well-architected orchestration is essential for enterprise adoption.</p>  
>   
> <p>Steps to Creating Autonomous AI Agents</p>  
>   
> <p>To simplify implementation, here’s a proven framework used by leading AI innovators:</p>  
>   
> <p>Step 1 — Define Use Case and Expected Output</p>  
>   
> <p>Start with measurable, repeatable workflows like data extraction, reporting, or request handling.</p>  
>   
> <p>Step 2 — Design Agent Capabilities</p>  
>   
> <p>Define whether the agent will retrieve information, automate tasks, evaluate output, or make decisions.</p>  
>   
> <p>Step 3 — Set Up Tools and Integrations</p>  
>   
> <p>Connect required systems such as CRM, ERP, cloud tools, messaging platforms, or internal applications.</p>  
>   
> <p>Step 4 — Add Memory and Feedback Loops</p>  
>   
> <p>Enable learning over time to improve performance and avoid repeating mistakes.</p>  
>   
> <p>Step 5 — Test, Observe, and Optimize</p>  
>   
> <p>Deploy in controlled environments before full-scale enterprise rollout.</p>  
>   
> <p>This structured approach ensures the agent is reliable, safe, and aligned with business goals.</p>  
>   
> <p>Real-World Use Cases for Autonomous Agents</p>  
>   
> <p>Companies are now using autonomous agents to:</p>  
>   
> <p>Process customer support and escalate complex cases</p>  
>   
> <p>Detect cyber threats and trigger automated responses</p>  
>   
> <p>Generate financial reports and reconcile data</p>  
>   
> <p>Run marketing campaigns and CRM workflows</p>  
>   
> <p>Manage IT operations and automated troubleshooting</p>  
>   
> <p>As maturity increases, these agents evolve into fully autonomous digital employees.</p>  
>   
> <p>Future of Autonomous Agent Systems</p>  
>   
> <p>With advances in reasoning models, memory, and orchestration, we will soon see:</p>  
>   
> <p>Autonomous teams of specialized AI agents</p>  
>   
> <p>Industry-specific prebuilt agent templates</p>  
>   
> <p>Policy-driven enterprise intelligence layers</p>  
>   
> <p>Self-healing and self-maintaining AI systems</p>  
>   
> <p>This represents a transformational shift in digital workforce infrastructure.</p>  
>   
> <p>Conclusion</p>  
>   
> <p>Organizations exploring creating autonomous AI agents<br>  
>  are positioning themselves ahead of the next wave of intelligent automation. By combining reasoning, workflow execution, and structured orchestration, enterprises can create scalable AI systems capable of delivering 10x productivity and operational resilience.</p>  
>   
> <p>FAQs</p>  
>   
> <ol>  
> <li><p>How difficult is it to build autonomous agents?<br>  
> With the right frameworks and tools, businesses can deploy their first agent within weeks—not months.</p></li>  
> <li><p>Do autonomous agents replace employees?<br>  
> They augment teams by handling routine and repetitive tasks, allowing humans to focus on strategic work.</p></li>  
> <li><p>What skills are needed to build agentic systems?<br>  
> Engineering expertise helps, but many modern platforms support low-code and no-code deployment.</p></li>  
> <li><p>How do autonomous agents learn?<br>  
> Through memory, feedback loops, result monitoring, and iterative refinement.</p></li>  
> <li><p>Can multiple agents work together?<br>  
> Yes, with proper agentic AI orchestration, agents can collaborate and distribute complex tasks.</p></li>  
> </ol>

---

## [6/10] SARCH: Multimodal Search for Archaeological Archives
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251105667S/abstract
**Reasoning:** The multimodal search system for archaeological archives touches on search and retrieval, but not specifically for codebases.
**Authors:** Sinha, Nivedita, Khanijo, Bharati, Singh, Sanskar, Mahant, Priyansh, Roy, Ashutosh, Singh Bhadouria, Saubhagya, Jain, Arpan, Ramanath, Maya

**Content/Abstract:**
> In this paper, we describe a multi-modal search system designed to search old archaeological books and reports. This corpus is digitally available as scanned PDFs, but varies widely in the quality of scans. Our pipeline, designed for multi-modal archaeological documents, extracts and indexes text, images (classified into maps, photos, layouts, and others), and tables. We evaluated different retrieval strategies, including keyword-based search, embedding-based models, and a hybrid approach that selects optimal results from both modalities. We report and analyze our preliminary results and discuss future work in this exciting vertical.

---

## [6/10] SQuaD: The Software Quality Dataset
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251111265R/abstract
**Reasoning:** The Software Quality Dataset is relevant to software engineering trends but does not directly address codebase indexing or context engineering.
**Authors:** Robredo, Mikel, Esposito, Matteo, Taibi, Davide, Peñaloza, Rafael, Lenarduzzi, Valentina

**Content/Abstract:**
> Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).

---

## [6/10] FLOWER: Flow-Oriented Entity-Relationship Tool
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251113357M/abstract
**Reasoning:** While it deals with entity-relationship models and data storytelling, it doesn't directly relate to codebase indexing or context engineering.
**Authors:** Moskalev, Dmitry

**Content/Abstract:**
> Exploring relationships across data sources is a crucial optimization for entities recognition. Since databases can store big amount of information with synthetic and organic data, serving all quantity of objects correctly is an important task to deal with. However, the decision of how to construct entity relationship model is associated with human factor. In this paper, we present flow-oriented entity-relationship tool. This is first and unique end-to-end solution that eliminates routine and resource-intensive problems of processing, creating and visualizing both of explicit and implicit dependencies for prominent SQL dialects on-the-fly. Once launched, FLOWER automatically detects built-in constraints and starting to create own correct and necessary one using dynamic sampling and robust data analysis techniques. This approach applies to improve entity-relationship model and data storytelling to better understand the foundation of data and get unseen insights from DB sources using SQL or natural language. Evaluated on state-of-the-art STATS benchmark, experiments show that FLOWER is superior to reservoir sampling by 2.4x for distribution representation and 2.6x for constraint learning with 2.15x acceleration. For data storytelling, our tool archives 1.19x for accuracy enhance with 1.86x context decrease compare to LLM. Presented tool is also support 23 languages and compatible with both of CPU and GPU. Those results show that FLOWER can manage with real-world data a way better to ensure with quality, scalability and applicability for different use-cases.

---

## [6/10] Agentic Policy Optimization via Instruction-Policy Co-Evolution
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T17:56:29.000Z
**URL:** https://arxiv.org/abs/2512.01945v1
**Reasoning:** The discussion on agentic policy optimization touches on autonomous agents, which is somewhat relevant to AI agents in software engineering.
**Authors:** Han Zhou

**Content/Abstract:**
> Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.

---

## [6/10] MIT Lincoln Laboratory: A Case Study on Improving Software Support for Research Projects
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T13:22:58.000Z
**URL:** https://arxiv.org/abs/2512.01649v1
**Reasoning:** The article discusses software engineering culture and trends, which is of secondary interest.
**Authors:** Daniel Strassler

**Content/Abstract:**
> Software plays an ever increasing role in complex system development and prototyping, and in recent years, MIT Lincoln Laboratory has sought to improve both the effectiveness and culture surrounding software engineering in execution of its mission. The Homeland Protection and Air Traffic Control Division conducted an internal study to examine challenges to effective and efficient research software development, and to identify ways to strengthen both the culture and execution for greater impact on our mission. Key findings of this study fell into three main categories: project attributes that influence how software development activities must be conducted and managed, potential efficiencies from centralization, opportunities to improve staffing and culture with respect to software practitioners. The study delivered actionable recommendations, including centralizing and standardizing software support tooling, developing a common database to help match the right software talent and needs to projects, and creating a software stakeholder panel to assist with continued improvement.

---

## [6/10] ONLYOFFICE updated: AI agents & custom hotkeys in new releases
**Source:** The Practical Developer | **Date:** 2025-12-02T13:31:04.000Z
**URL:** https://dev.to/onlyoffice/onlyoffice-updated-ai-agents-custom-hotkeys-in-new-releases-hem
**Reasoning:** The article mentions AI agents in ONLYOFFICE, which is somewhat relevant to AI in software engineering.
**Authors:** Kseniya Fedoruk

**Content/Abstract:**
> <p>We are excited to announce major updates across our ecosystem: ONLYOFFICE DocSpace 3.6 and ONLYOFFICE Docs 9.2. These releases introduce a new layer of intelligent assistance with AI Agents in DocSpace and bring significant productivity enhancements to the editors, including customizable hotkeys and macro recording. Let’s explore what’s new for developers and power users.</p> 
>  
> <h2> 
>    
>    
>   Meet your new AI agents in DocSpace 3.6 
> </h2> 
>  
> <p>The main highlight of DocSpace 3.6 is the introduction of AI agents, bringing intelligent assistance directly into your collaborative workspace. These agents are designed to help you and your team work faster and more efficiently.</p> 
>  
> <p>You can set up an AI agent tailored to your specific needs. Once configured, you can interact with it through a dedicated chat interface. Simply ask questions or describe your task, and the agent is ready to help.</p> 
>  
> <p>The AI can perform a wide range of tasks to support your projects:</p> 
>  
> <ul> 
> <li> 
> <strong>Analyze files:</strong> Dive deep into documents to check for accuracy, suggest improvements, or summarize key points.</li> 
> <li> 
> <strong>Generate content:</strong> Create text, brainstorm ideas, or draft communications based on your prompts.</li> 
> <li> 
> <strong>Search for information:</strong> Look up information across the web and your own personalized knowledge base.</li> 
> <li> 
> <strong>Manage your DocSpace:</strong> Organize files, structure rooms, add users, and keep your workspace tidy.</li> 
> <li> 
> <strong>Invite teammates:</strong> Collaborate with colleagues directly within the AI agent chat.</li> 
> </ul> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frrftwlc56ouv4irz41gr.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frrftwlc56ouv4irz41gr.png" alt="AI agents in DocSpace" width="800" height="456"></a></p> 
>  
> <h3> 
>    
>    
>   Getting started with AI agents 
> </h3> 
>  
> <p>Enabling AI agents in your DocSpace is straightforward. A new AI Settings section serves as your central hub for managing all AI-related functionalities.</p> 
>  
> <p><strong>1. Add an AI provider</strong><br> 
> First, you'll need to connect an AI provider to power your agents. Make sure you have an API key from your chosen provider. Currently, we support OpenAI, Anthropic, TogetherAI, and OpenRouter, with more options planned for future releases.</p> 
>  
> <p><strong>2. Enable MCP Server</strong><br> 
> Next, enable the MCP (Master Control Program) server. You can activate the ready-to-use ONLYOFFICE DocSpace MCP Server, which empowers AI agents to interact with and manage elements within your DocSpace, like creating rooms or organizing files. You can also connect any other MCP server for enhanced capabilities.</p> 
>  
> <p><strong>3. Connect web search and knowledge base</strong><br> 
> To expand your AI's capabilities, enable the web search engine. This allows the agent to pull information from the internet. You should also activate the knowledge base, which indexes your documents, allowing the AI to perform intelligent, question-based searches through your own data.</p> 
>  
> <p><strong>4. Create and manage your first agent</strong><br> 
> Once the setup is complete, you can build your first agent. Give it a name, cover, and tags. You can also provide specific instructions to define its purpose, such as, "Chats in this room are for discussing our startup project. Please stay on topic." You can also set storage quotas for your AI agents to control memory consumption.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftmx7st4vyo4stwvd60y2.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftmx7st4vyo4stwvd60y2.png" alt="Creating an AI agent" width="800" height="456"></a></p> 
>  
> <p>When you invite collaborators, you can assign specific roles:</p> 
>  
> <ul> 
> <li> 
> <strong>Agent Managers</strong> have full control over settings, users, and the shared result space.</li> 
> <li> 
> <strong>Content Creators</strong> can edit files, upload knowledge base content, and view results.</li> 
> <li> 
> <strong>Viewers</strong> have read-only access to the shared result space.</li> 
> </ul> 
>  
> <p>All AI-generated files can be saved to a dedicated Result Storage space, where you can continue editing or share them. The Chat History lets you revisit previous prompts and results at any time.</p> 
> <h2> 
>    
>    
>   What's new in ONLYOFFICE Docs 9.2? 
> </h2> 
>  
> <p>Beyond the platform-level AI in DocSpace, ONLYOFFICE Docs 9.2 introduces powerful features directly into the editors, focusing on productivity and customization.</p> 
> <h3> 
>    
>    
>   AI-powered grammar &amp; spelling 
> </h3> 
>  
> <p>The <a href="https://www.onlyoffice.com/ai-assistants">AI plugin</a> now includes integrated spell and grammar checking. To use it, simply navigate to the AI tab and select Grammar &amp; Spelling. The AI analyzes your text and provides suggestions with explanations, which you can accept or reject. It's a quick way to ensure your documents are polished and professional.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F4f2iqluzedbk1ydds7gs.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F4f2iqluzedbk1ydds7gs.png" alt="AI-powered grammar checking" width="800" height="456"></a></p> 
> <h3> 
>    
>    
>   Customizable keyboard shortcuts 
> </h3> 
>  
> <p>For many developers, an efficient workflow relies on keyboard shortcuts. You can now customize these shortcuts to match your personal preferences. Go to the File tab, open Advanced Settings, and configure your preferred key combinations for a truly personalized editing experience.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fw5egb8yg18d4trwbzgzu.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fw5egb8yg18d4trwbzgzu.png" alt="Customizable keyboard shortcuts" width="800" height="456"></a></p> 
> <h3> 
>    
>    
>   Record actions as macros 
> </h3> 
>  
> <p>Repetitive tasks can slow down your progress. With the new macro recording feature, you can automate these actions. Simply record a sequence of actions, save it as a macro, and run it whenever you need to perform that task again. This is a powerful way to streamline your work and save valuable time.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fyu2n50iuvm59rjhlioo4.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fyu2n50iuvm59rjhlioo4.png" alt="Record actions as macros" width="800" height="456"></a></p> 
> <h2> 
>    
>    
>   Other improvements in Docs and DocSpace 
> </h2> 
>  
> <ul> 
> <li> 
> <strong>Enhanced PDF redaction:</strong> You can now customize the color used for the Redact option in the PDF Editor, giving you more visual control when preparing documents for sharing.</li> 
> <li> 
> <strong>Improved form creation:</strong> Add descriptive text labels to checkboxes and radio buttons. You can also assign specific roles to fields when inserting them into a form, ensuring they are correctly tailored to different users.</li> 
> <li> 
> <strong>New format support:</strong> DocSpace 3.6 adds support for the HML format and allows you to convert presentations to TXT files.</li> 
> <li> 
> <strong>Redesigned data import:</strong> The data import tool has been overhauled for a smoother and more intuitive experience when migrating your files.</li> 
> </ul> 
> <h2> 
>    
>    
>   Get the latest versions 
> </h2> 
>  
> <p><iframe allowfullscreen="allowfullscreen" width="710" height="399" src="https://www.inoreader.com/yt-embed/?v=mwdKgLTNeRI" referrerpolicy="strict-origin-when-cross-origin" style="width:100%;aspect-ratio:16/9;height:auto;display:block;border:0;"> 
> </iframe> 
> </p> 
>  
> <p>These updates are designed to make your work more intelligent, efficient, and customized. The new AI agents in DocSpace 3.6 provide a collaborative assistant for your projects, while the features in Docs 9.2, like customizable hotkeys and macro recording, offer significant productivity gains.</p> 
>  
> <p>The latest updates are already available in the cloud. You can sign into your DocSpace to try all the new features or <a href="https://www.onlyoffice.com/docspace-registration">create a free account</a> if you are new to ONLYOFFICE. <a href="https://www.onlyoffice.com/download">Self-hosted builds</a> are also available.</p>

---

## [6/10] Cloud Cost Optimization for AI & Data-Intensive Systems: Save While Scaling
**Source:** The Practical Developer | **Date:** 2025-12-02T11:22:29.000Z
**URL:** https://dev.to/oleksii_samoilenko/cloud-cost-optimization-for-ai-data-intensive-systems-save-while-scaling-ofb
**Reasoning:** The article discusses cloud cost optimization for AI systems, which is somewhat relevant to infrastructure trends.
**Authors:** Oleksii Samoilenko

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ffo1ru3g8ba45cufvmm9g.webp" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>Modern AI systems, LLM-powered applications, and data-intensive platforms generate enormous value — but they also generate enormous cloud bills. As organizations scale their machine learning pipelines, vector databases, real-time analytics, and GPU-heavy inference workloads, cloud costs can quickly spiral out of control. The result is familiar: impressive AI results paired with a CFO asking why the monthly cloud invoice suddenly doubled.</p>  
>   
> <p>This is where cloud cost optimization becomes essential. Companies that strategically design, architect, and operate AI workloads in the cloud can reduce costs by 30–70% without sacrificing performance. Effective cloud optimization isn’t just about cutting expenses — it’s about enabling sustainable scaling, predictable operations, and better resource management across the entire AI lifecycle.</p>  
>   
> <p>In this article, we break down the causes of high AI cloud spend, the most effective cloud cost optimization strategies, and actionable approaches to achieving meaningful cloud infrastructure cost optimization while still supporting rapid AI growth.</p>  
>   
> <p>Why AI &amp; Data Workloads Become Expensive</p>  
>   
> <p>AI workloads are fundamentally different from traditional applications. They require:</p>  
>   
> <ul>  
> <li>GPU-intensive compute for training and inference</li>  
> <li>High-performance storage for large datasets</li>  
> <li>Massive data movement across networks</li>  
> <li>Always-on services for real-time applications</li>  
> <li>Distributed infrastructure for scalability</li>  
> </ul>  
>   
> <p>Because of these factors, poor cloud planning can lead to unnecessary overspending. The biggest cost drivers include:</p>  
>   
> <p><strong>GPU Overprovisioning</strong></p>  
>   
> <p>Data teams often spin up the largest GPU instances available (e.g., A100 or H100) even when workloads don’t require that power.</p>  
>   
> <p><strong>Idle Compute Resources</strong></p>  
>   
> <p>Training jobs, MLOps pipelines, and inference services often run 24/7 — even when not in use.</p>  
>   
> <p><strong>Inefficient Storage</strong></p>  
>   
> <p>Storing large datasets in high-cost storage tiers or duplicating data across environments dramatically increases bills.</p>  
>   
> <p><strong>Lack of Autoscaling</strong></p>  
>   
> <p>Without autoscaling policies, systems remain over-allocated during low-traffic periods.</p>  
>   
> <p><strong>Poor Observability &amp; Cost Governance</strong></p>  
>   
> <p>Teams don’t have enough visibility over cost centers, resulting in runaway cloud bills.</p>  
>   
> <p>Key Cloud Cost Optimization Strategies for AI Teams</p>  
>   
> <p>To ensure sustainable scaling, organizations must adopt a combination of engineering practices, architectural choices, and ongoing operational monitoring. Below are the most effective cloud cost optimization strategies for AI-driven systems.</p>  
>   
> <p><strong>Choose the Right Hardware for the Job</strong></p>  
>   
> <p>AI workloads often rely heavily on GPUs — but the “biggest GPU available” is not always the optimal choice.</p>  
>   
> <p>Strategies that work:</p>  
>   
> <ul>  
> <li>Use smaller GPUs (e.g., T4, L4) for inference instead of A100/H100.</li>  
> <li>Utilize spot GPU instances for training jobs with checkpoints.</li>  
> <li>Consider ARM-based processors (e.g., AWS Graviton) for preprocessing and ETL tasks.</li>  
> <li>Mix GPU and CPU-based inference where latency allows.  
> By right-sizing compute to workload requirements, companies often achieve 30–50% savings immediately.</li>  
> </ul>  
>   
> <p><strong>Implement Autoscaling and Right-Sizing Policies</strong></p>  
>   
> <p>AI systems frequently experience unpredictable traffic spikes. Autoscaling ensures that compute resources expand during peak usage and contract during low-demand periods.</p>  
>   
> <p>Best practices:</p>  
>   
> <ul>  
> <li>Use horizontal pod autoscaler (HPA) on Kubernetes.</li>  
> <li>Set up scale-to-zero for non-essential services.</li>  
> <li>Leverage serverless options for vector search, embeddings, or scheduled jobs.</li>  
> <li>Continuously track workloads with usage-based alerts to recommend right-sizing.</li>  
> </ul>  
>   
> <blockquote>  
> <p>Autoscaling alone can cut 20–40% of unnecessary spend.</p>  
> </blockquote>  
>   
> <p><strong>Optimize Cloud Storage for Data Pipelines</strong></p>  
>   
> <p>Storing AI datasets, embeddings, check-pointed training models, and log files can quickly get out of control.</p>  
>   
> <p>Effective optimization approaches:</p>  
>   
> <ul>  
> <li>Move historical datasets to cheaper storage tiers (e.g., S3 Glacier, Azure Archive).</li>  
> <li>Use columnar formats like Parquet to reduce storage size.</li>  
> <li>Deduplicate datasets with data versioning tools like DVC or LakeFS.</li>  
> <li>Archive ML logs and checkpoints automatically after validations.</li>  
> </ul>  
>   
> <blockquote>  
> <p>A well-designed data lifecycle plan is a key pillar of cloud infrastructure cost optimization, reducing storage costs by up to 80%.</p>  
> </blockquote>  
>   
> <p><strong>Use Efficient Vector Databases and Search Architectures</strong></p>  
>   
> <p>Vector search systems (Pinecone, Weaviate, Qdrant, Milvus) are essential for RAG, LLM retrieval, and semantic search, but they can be cost-heavy.</p>  
>   
> <p>Tips to control costs:</p>  
>   
> <ul>  
> <li>Use hybrid indexing to reduce vector storage.</li>  
> <li>Offload cold embeddings to object storage.</li>  
> <li>Use sharding and partial scale-out instead of overprovisioning large clusters.</li>  
> <li>Consider open-source solutions hosted on your own Kubernetes cluster.</li>  
> </ul>  
>   
> <p>Choosing the right database topology can reduce costs by 30–60%.</p>  
>   
> <p><strong>Compress, Quantize, and Optimize Models</strong></p>  
>   
> <p>Model compression dramatically reduces inference costs by allowing smaller or cheaper compute instances to serve requests.</p>  
>   
> <p>Popular optimization methods:</p>  
>   
> <ul>  
> <li>Quantization (FP16, INT8, INT4)</li>  
> <li>Pruning and distillation</li>  
> <li>Token-level caching for LLMs</li>  
> <li>Serving with optimized runtimes like ONNX Runtime or TensorRT</li>  
> </ul>  
>   
> <p>For many companies, model optimization means cutting inference costs in half with minimal accuracy loss.</p>  
>   
> <p>Use Spot Instances for Training</p>  
>   
> <p>Training LLMs, CV models, and deep neural networks is expensive, but spot GPU instances can slash cost if jobs are checkpointed.</p>  
>   
> <p>For example:</p>  
>   
> <ul>  
> <li>AWS EC2 Spot</li>  
> <li>GCP Preemptible Instances</li>  
> <li>Azure Spot VMs</li>  
> </ul>  
>   
> <p>Spot training can reduce costs by 70–90%, especially for long-running batch tasks.</p>  
>   
> <p><strong>Improve Observability and Cost Governance</strong></p>  
>   
> <p>Without proper monitoring, cost leaks remain invisible.</p>  
>   
> <p>Tools your team should use:</p>  
>   
> <ul>  
> <li>AWS Cost Explorer / Azure Cost Management</li>  
> <li>Kubecost for Kubernetes</li>  
> <li>DataDog or Grafana for resource usage</li>  
> <li>MLflow or Weights &amp; Biases to track training costs</li>  
> </ul>  
>   
> <p>For full cloud cost optimization, every team — AI, engineering, product — must see and own their usage patterns.</p>  
>   
> <p><strong>Adopt a Zero-Waste Cloud Philosophy</strong></p>  
>   
> <p>These advanced methods ensure minimal waste across the infrastructure:</p>  
>   
> <ul>  
> <li>Delete unused snapshots, volumes, clusters, and load balancers.</li>  
> <li>Shut down dev environments at night/weekends.</li>  
> <li>Separate dev/stage/prod with strict quotas.</li>  
> <li>Automate resource cleanup with cron jobs or Lambdas.</li>  
> </ul>  
>   
> <p>Teams that adopt zero-waste practices save up to 20% monthly with no engineering effort.</p>  
>   
> <p>Optimization Strategies for AI Training vs. Inference</p>  
>   
> <p>AI workloads fall into two categories — training and inference — and both require different optimization tactics.</p>  
>   
> <p>Training Optimization</p>  
>   
> <p>Training is GPU-heavy, long-running, and typically done in batches.</p>  
>   
> <p>Best strategies:</p>  
>   
> <ul>  
> <li>Use spot GPUs</li>  
> <li>Enable gradient checkpointing</li>  
> <li>Select smaller batch sizes</li>  
> <li>Choose cheaper regions</li>  
> <li>Perform distributed training when needed</li>  
> <li>Use autoscaling clusters like SageMaker or Vertex AI</li>  
> </ul>  
>   
> <p>Inference Optimization</p>  
>   
> <p>Inference must be fast, scalable, and cost-efficient.</p>  
>   
> <p>Best strategies:</p>  
>   
> <ul>  
> <li>Use small or quantized models</li>  
> <li>Deploy on smaller GPUs (T4/L4 or CPU for light tasks)</li>  
> <li>Use token streaming and caching</li>  
> <li>Autoscale aggressively</li>  
> <li>Use serverless inference (AWS Lambda + EFS, Vertex AI Serverless)</li>  
> </ul>  
>   
> <p>Building a Cloud Cost Optimization Culture</p>  
>   
> <p>Technology alone can’t solve the cost challenge — teams must adopt the right mindset.</p>  
>   
> <p>This includes:</p>  
>   
> <ul>  
> <li>Engineering estimating cloud impact before development</li>  
> <li>Architecture teams reviewing infra decisions</li>  
> <li>Product managers understanding budget implications</li>  
> <li>Finance collaborating with tech leaders</li>  
> <li>Automated alerts when cost thresholds are reached</li>  
> </ul>  
>   
> <p>Companies that follow this culture see long-term success with cloud infrastructure cost optimization.</p>  
>   
> <p>Scale AI Smartly, Not Expensively</p>  
>   
> <p>AI-driven systems and data-intensive workloads are inherently resource-hungry, but they don’t have to be financially unsustainable. By combining engineering best practices, architectural decisions, automation, and operational discipline, organizations can dramatically reduce cloud spend without compromising performance.</p>  
>   
> <p>The key to effective cloud cost optimization is simple:<br>  
> Pay only for what brings value — eliminate everything else.</p>  
>   
> <p>With the right set of cloud cost optimization strategies, you can scale LLMs, vector databases, analytics pipelines, and real-time AI systems efficiently, confidently, and cost-effectively.</p>

---

## [6/10] How to Protect Model Context Protocol (MCP) Servers with OpenAM and OpenIG
**Source:** The Practical Developer | **Date:** 2025-12-02T11:02:35.000Z
**URL:** https://dev.to/maximthomas/how-to-protect-model-context-protocol-mcp-servers-with-openam-and-openig-h83
**Reasoning:** The article discusses protecting Model Context Protocol servers, which is somewhat related to context management in AI.
**Authors:** Maxim Thomas

**Content/Abstract:**
> <h2> 
>    
>    
>   Introduction 
> </h2> 
>  
> <p>Large language model (LLM) agents can perform various tasks, from writing code or texts to booking airline tickets. Agents consist of a client that interacts with the user and a server that performs the required tasks. The interaction between the client, server, and LLM occurs via the Model Context Protocol <a href="https://modelcontextprotocol.io/docs/getting-started/intro">MCP</a>.</p> 
>  
> <p>MCP servers often have access to sensitive information, such as an internal source code repository or a customer database. Of course, not all users should have access to this data, even through an agent. To protect against unauthorized access, the Model Context Protocol specification describes the possibility of authorization based on OAuth 2.1: <a href="https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization">https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization</a>.</p> 
>  
> <p>In this article, we will deploy a simple MCP server developed based on <a href="https://spring.io/projects/spring-ai">Spring AI</a> and close it with the <a href="https://github.com/OpenIdentityPlatform/OpenIG">OpenIG</a> authorization gateway. The <a href="https://github.com/OpenIdentityPlatform/OpenAM">OpenAM</a> authentication service will be responsible for authentication. </p> 
>  
> <p>We will use VS Code with the Copilot extension as the MCP client.</p> 
>  
> <h2> 
>    
>    
>   Project description 
> </h2> 
>  
> <p>The source code for the OpenAM, OpenIG, and MCP server configuration is available at: <a href="https://github.com/OpenIdentityPlatform/openam-openig-mcp-example">https://github.com/OpenIdentityPlatform/openam-openig-mcp-example</a></p> 
>  
> <p>The project consists of three services described in the <code>docker-compose.yml</code> file.<br> 
> </p> 
>  
> <div> 
> <pre><code><span>services</span><span>:</span> 
>   <span>openig</span><span>:</span> 
>     <span>build</span><span>:</span> 
>       <span>context</span><span>:</span> <span>./openig-docker</span> 
>       <span>dockerfile</span><span>:</span> <span>Dockerfile</span> 
>     <span>container_name</span><span>:</span> <span>openig</span> 
>     <span>volumes</span><span>:</span> 
>       <span>-</span> <span>./openig-config:/usr/local/openig-config:ro</span> 
>     <span>ports</span><span>:</span>   
>       <span>-</span> <span>"</span><span>8081:8080"</span> 
>     <span>environment</span><span>:</span> 
>       <span>CATALINA_OPTS</span><span>:</span> <span>-Dopenig.base=/usr/local/openig-config -Dopenam=http://openam.example.org:8080/openam</span> 
>     <span>networks</span><span>:</span> 
>       <span>openam_network</span><span>:</span> 
>         <span>aliases</span><span>:</span> 
>           <span>-</span> <span>openig.example.org</span> 
>  
>   <span>openam</span><span>:</span> 
>     <span>build</span><span>:</span> 
>       <span>context</span><span>:</span> <span>./openam-docker</span> 
>       <span>dockerfile</span><span>:</span> <span>Dockerfile</span> 
>     <span>container_name</span><span>:</span> <span>openam</span> 
>     <span>restart</span><span>:</span> <span>always</span> 
>     <span>hostname</span><span>:</span> <span>openam.example.org</span> 
>     <span>ports</span><span>:</span> 
>       <span>-</span> <span>"</span><span>8080:8080"</span> 
>     <span>volumes</span><span>:</span> 
>       <span>-</span> <span>openam-data:/usr/openam/config</span> 
>     <span>networks</span><span>:</span> 
>       <span>openam_network</span><span>:</span> 
>         <span>aliases</span><span>:</span> 
>           <span>-</span> <span>openam.example.org</span> 
>  
>   <span>time-mcp-server</span><span>:</span> 
>     <span>build</span><span>:</span> 
>       <span>context</span><span>:</span> <span>./timeserver</span> 
>       <span>dockerfile</span><span>:</span> <span>Dockerfile</span> 
>     <span>container_name</span><span>:</span> <span>time-mcp-server</span> 
>     <span>ports</span><span>:</span> 
>       <span>-</span> <span>"</span><span>8082:8080"</span> 
>     <span>networks</span><span>:</span> 
>       <span>openam_network</span><span>:</span> 
>         <span>aliases</span><span>:</span> 
>           <span>-</span> <span>timeserver.example.org</span> 
> <span>networks</span><span>:</span> 
>   <span>openam_network</span><span>:</span> 
>     <span>driver</span><span>:</span> <span>bridge</span> 
>  
> <span>volumes</span><span>:</span> 
>   <span>openam-data</span><span>:</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h2> 
>    
>    
>   Preparing for launch 
> </h2> 
>  
> <p>For example, the host name for OpenAM will be <code>openam.example.org</code>, and for OpenIG it will be <code>openig.example.org</code>. Open the <code>hosts</code> file and add the host names and IP addresses to it, for example<br> 
> </p> 
>  
> <div> 
> <pre><code>127.0.0.1 openam.example.org openig.example.org 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>On Windows systems, the hosts file is located in the <code>C:\Windows/System32/drivers/etc/hosts</code> directory, and on Linux or Mac OS in <code>/etc/hosts</code>.</p> 
>  
> <h2> 
>    
>    
>   MCP Server 
> </h2> 
>  
> <p>The MCP server has a method for returning the current time in ISO 8601 format.<br> 
> </p> 
>  
> <div> 
> <pre><code><span>@Service</span> 
> <span>public</span> <span>class</span> <span>TimeService</span> <span>{</span> 
>  
>     <span>@Tool</span><span>(</span><span>name</span> <span>=</span> <span>"current_time_service"</span><span>,</span> <span>description</span> <span>=</span> <span>"Returns current time in ISO 8601 format"</span><span>)</span> 
>     <span>public</span> <span>String</span> <span>getTime</span><span>()</span> <span>{</span> 
>         <span>return</span>  <span>Instant</span><span>.</span><span>now</span><span>().</span><span>toString</span><span>();</span> 
>     <span>}</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>For more details on creating an MCP server, please refer to the <a href="https://docs.spring.io/spring-ai/reference/api/mcp/mcp-server-boot-starter-docs.html">documentation</a> or in the Spring AI <a href="https://spring.io/blog/2025/09/16/spring-ai-mcp-intro-blog">blog</a>.</p> 
>  
> <p>Start the OpenAM, OpenIG, and MCP server Docker containers with the command:<br> 
> </p> 
>  
> <div> 
> <pre><code>docker compose up <span>--build</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Check the availability of the running MCP server with the command:<br> 
> </p> 
>  
> <div> 
> <pre><code>curl <span>-X</span> POST  <span>--location</span>  <span>"http://localhost:8082/mcp"</span> <span>\</span> 
>     <span>-H</span> <span>"Content-Type: application/json"</span> <span>\</span> 
>     <span>-H</span> <span>"Accept: application/json, text/event-stream"</span> <span>\</span> 
>     <span>-d</span> <span>'{ 
>   "jsonrpc": "2.0", 
>   "id": 0, 
>   "method": "initialize", 
>   "params": { 
>     "protocolVersion": "2025-06-18", 
>     "capabilities": {} 
>   } 
> }'</span> 
>  
> <span>{</span> 
>   <span>"id"</span>: 0, 
>   <span>"jsonrpc"</span>: <span>"2.0"</span>, 
>   <span>"result"</span>: <span>{</span> 
>     <span>"capabilities"</span>: <span>{</span> 
>       <span>"completions"</span>: <span>{}</span>, 
>       <span>"prompts"</span>: <span>{</span> 
>         <span>"listChanged"</span>: <span>false</span> 
>       <span>}</span>, 
>       <span>"resources"</span>: <span>{</span> 
>         <span>"listChanged"</span>: <span>false</span>, 
>         <span>"subscribe"</span>: <span>false</span> 
>       <span>}</span>, 
>       <span>"tools"</span>: <span>{</span> 
>         <span>"listChanged"</span>: <span>false</span> 
>       <span>}</span> 
>     <span>}</span>, 
>     <span>"protocolVersion"</span>: <span>"2025-03-26"</span>, 
>     <span>"serverInfo"</span>: <span>{</span> 
>       <span>"name"</span>: <span>"time-server-mcp"</span>, 
>       <span>"version"</span>: <span>"0.0.1"</span> 
>     <span>}</span> 
>   <span>}</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Let's check the availability of tools in the MCP server:<br> 
> </p> 
>  
> <div> 
> <pre><code>curl <span>-X</span> POST  <span>--location</span>  <span>"http://localhost:8082/mcp"</span> <span>\</span> 
>     <span>-H</span> <span>"Content-Type: application/json"</span> <span>\</span> 
>     <span>-H</span> <span>"Accept: application/json, text/event-stream"</span> <span>\</span> 
>     <span>-d</span> <span>'{ 
>   "jsonrpc": "2.0", 
>   "id": 1, 
>   "method": "tools/list", 
>   "params": {} 
> }'</span>  
>  
> <span>{</span> 
>   <span>"jsonrpc"</span>: <span>"2.0"</span>, 
>   <span>"id"</span>: 1, 
>   <span>"result"</span>: <span>{</span> 
>     <span>"tools"</span>: <span>[</span> 
>       <span>{</span> 
>         <span>"name"</span>: <span>"current_time_service"</span>, 
>         <span>"description"</span>: <span>"Returns current time in ISO 8601 format"</span>, 
>         <span>"inputSchema"</span>: <span>{</span> 
>           <span>"type"</span>: <span>"object"</span>, 
>           <span>"properties"</span>: <span>{}</span>, 
>           <span>"required"</span>: <span>[]</span>, 
>           <span>"additionalProperties"</span>: <span>false</span> 
>         <span>}</span> 
>       <span>}</span> 
>     <span>]</span> 
>   <span>}</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h2> 
>    
>    
>   Configuring OpenAM 
> </h2> 
>  
> <p>OpenAM will be responsible for user authentication, issuing OAuth 2 <code>access_token</code> tokens, and validating them.</p> 
>  
> <p>If you have not yet configured OpenAM, perform a quick setup by running the command:<br> 
> </p> 
>  
> <div> 
> <pre><code>docker <span>exec</span> <span>-w</span> <span>'/usr/openam/ssoconfiguratortools'</span> openam bash <span>-c</span> <span>\</span> 
> <span>'echo "ACCEPT_LICENSES=true 
> SERVER_URL=http://openam.example.org:8080 
> DEPLOYMENT_URI=/$OPENAM_PATH 
> BASE_DIR=$OPENAM_DATA_DIR 
> locale=en_US 
> PLATFORM_LOCALE=en_US 
> AM_ENC_KEY= 
> ADMIN_PWD=passw0rd 
> AMLDAPUSERPASSWD=p@passw0rd 
> COOKIE_DOMAIN=example.org 
> ACCEPT_LICENSES=true 
> DATA_STORE=embedded 
> DIRECTORY_SSL=SIMPLE 
> DIRECTORY_SERVER=openam.example.org 
> DIRECTORY_PORT=50389 
> DIRECTORY_ADMIN_PORT=4444 
> DIRECTORY_JMX_PORT=1689 
> ROOT_SUFFIX=dc=openam,dc=example,dc=org 
> DS_DIRMGRDN=cn=Directory Manager 
> DS_DIRMGRPASSWD=passw0rd" &gt; conf.file &amp;&amp; java -jar openam-configurator-tool*.jar --file conf.file'</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   Configuring OAuth 2 in OpenAM 
> </h3> 
>  
> <p>Open the OpenAM console at <a href="http://openam.example.org:8080/openam/console">http://openam.example.org:8080/openam/console</a>. Enter the administrator login and password in the <code>User Name</code> and <code>Password</code> fields. In this case, they will be <code>amadmin</code> and <code>passw0rd</code>, respectively. </p> 
>  
> <p>Select <code>Top Level Realm</code> from the Realm list.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbg2088rk643g42jlw4ud.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbg2088rk643g42jlw4ud.png" alt="OpenAM Realms List" width="800" height="433"></a></p> 
>  
> <p>Next, <code>Configure OAuth Provider</code>.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fy0vepni7wpwcw983n71m.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fy0vepni7wpwcw983n71m.png" alt="OpenAM Configure OAuth Provider" width="800" height="538"></a></p> 
>  
> <p>Then select <code>Configure OAuth 2.0</code>.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg746yp3369nzj8ve8l97.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg746yp3369nzj8ve8l97.png" alt="OpenAM Configure OAuth 2.0" width="800" height="424"></a></p> 
>  
> <p>In the form that opens, you can leave the default settings unchanged. Click <code>Create</code>.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Famjie9iib7as12cejjjz.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Famjie9iib7as12cejjjz.png" alt="OpenAM Configure OAuth 2.0 Step 2" width="800" height="525"></a></p> 
>  
> <p>In the Realm settings, select Services from the menu on the left and open the OAuth2 Provider settings.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fdtxwnk9a0av5yiz4eaod.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fdtxwnk9a0av5yiz4eaod.png" alt="OpenAM Realm Services" width="800" height="356"></a></p> 
>  
> <p>Add the value <code>profile</code> to the <code>Scopes</code> and <code>Default Clients Scopes</code> settings. This scope will allow you to obtain basic information about the user. Enable the <code>Issue Refresh Tokens</code> and <code>Issue Refresh Tokens on Refreshing Access Tokens</code> options. Also, allow dynamic client registration by enabling the <code>Allow Open Dynamic Client Registration</code> option. This will allow the MCP client (VS Code) to automatically register with OpenAM without requiring any additional action from the user.</p> 
>  
> <p>For more information on configuring OpenAM, please refer to the <a href="https://doc.openidentityplatform.org/openam/">documentation</a>.</p> 
>  
> <h2> 
>    
>    
>   Configuring OpenIG 
> </h2> 
>  
> <p>OpenIG will be responsible for authorizing requests. It will check the validity of <code>access_token</code> issued by OpenAM and proxy requests to OpenAM and the MCP server.</p> 
>  
> <p>Now let's check the OpenIG route configuration for proxying requests.</p> 
>  
> <h3> 
>    
>    
>   Proxying requests to the MCP server. 
> </h3> 
>  
> <p>The route will receive the <code>access_token</code> issued by OpenAM, passed in the <code>Authorization</code> header. If the <code>access_token</code> is valid, it will pass the request to the MCP server and return a response. If the <code>access_token</code> is invalid, OpenIG will return HTTP status 401.</p> 
>  
> <p><code>openig-config/config/routes/10-mcp.json</code><br> 
> </p> 
>  
> <div> 
> <pre><code><span>{</span><span> 
>    </span><span>"name"</span><span>:</span><span> </span><span>"${matches(request.uri.path, '^/mcp')}"</span><span>,</span><span> 
>    </span><span>"condition"</span><span>:</span><span> </span><span>"${matches(request.uri.path, '^/mcp')}"</span><span>,</span><span> 
>    </span><span>"monitor"</span><span>:</span><span> </span><span>true</span><span>,</span><span> 
>    </span><span>"timer"</span><span>:</span><span> </span><span>true</span><span>,</span><span> 
>    </span><span>"handler"</span><span>:</span><span> </span><span>{</span><span> 
>       </span><span>"type"</span><span>:</span><span> </span><span>"Chain"</span><span>,</span><span> 
>       </span><span>"config"</span><span>:</span><span> </span><span>{</span><span> 
>          </span><span>"filters"</span><span>:</span><span> </span><span>[</span><span> 
>             </span><span>{</span><span> 
>                </span><span>"type"</span><span>:</span><span> </span><span>"OAuth2ResourceServerFilter"</span><span>,</span><span> 
>                </span><span>"config"</span><span>:</span><span> </span><span>{</span><span> 
>                   </span><span>"requireHttps"</span><span>:</span><span> </span><span>false</span><span>,</span><span> 
>                   </span><span>"providerHandler"</span><span>:</span><span> </span><span>"ClientHandler"</span><span>,</span><span>  
>                   </span><span>"scopes"</span><span>:</span><span> </span><span>[</span><span> 
>                      </span><span>"profile"</span><span> 
>                   </span><span>],</span><span> 
>                   </span><span>"tokenInfoEndpoint"</span><span>:</span><span> </span><span>"${system['openam'].concat('/oauth2/tokeninfo')}"</span><span>  
>                </span><span>}</span><span> 
>             </span><span>},</span><span> 
>             </span><span>{</span><span> 
>                </span><span>"type"</span><span>:</span><span> </span><span>"ConditionEnforcementFilter"</span><span>,</span><span> 
>                </span><span>"config"</span><span>:</span><span> </span><span>{</span><span> 
>                   </span><span>"condition"</span><span>:</span><span> </span><span>"${not empty contexts['oauth2']}"</span><span>,</span><span> 
>                   </span><span>"failureHandler"</span><span>:</span><span> </span><span>"RequireAuth"</span><span> 
>                </span><span>}</span><span> 
>             </span><span>}</span><span> 
>          </span><span>],</span><span> 
>          </span><span>"handler"</span><span>:</span><span> </span><span>"EndpointHandler"</span><span> 
>       </span><span>}</span><span> 
>    </span><span>},</span><span> 
>    </span><span>"heap"</span><span>:</span><span> </span><span>[</span><span> 
>       </span><span>{</span><span> 
>          </span><span>"name"</span><span>:</span><span> </span><span>"RequireAuth"</span><span>,</span><span> 
>          </span><span>"type"</span><span>:</span><span> </span><span>"StaticResponseHandler"</span><span>,</span><span> 
>          </span><span>"config"</span><span>:</span><span> </span><span>{</span><span> 
>             </span><span>"status"</span><span>:</span><span> </span><span>401</span><span>,</span><span> 
>             </span><span>"headers"</span><span>:</span><span> </span><span>{</span><span> 
>                </span><span>"WWW-Authenticate"</span><span>:</span><span> </span><span>[</span><span> 
>                   </span><span>"Bearer realm=</span><span>\"</span><span>OpenIG</span><span>\"</span><span>"</span><span> 
>                </span><span>]</span><span> 
>             </span><span>},</span><span> 
>             </span><span>"entity"</span><span>:</span><span> </span><span>"Authentication required"</span><span> 
>          </span><span>}</span><span> 
>       </span><span>},</span><span> 
>       </span><span>{</span><span> 
>          </span><span>"name"</span><span>:</span><span> </span><span>"EndpointHandler"</span><span>,</span><span> 
>          </span><span>"type"</span><span>:</span><span> </span><span>"DispatchHandler"</span><span>,</span><span> 
>          </span><span>"config"</span><span>:</span><span> </span><span>{</span><span> 
>             </span><span>"bindings"</span><span>:</span><span> </span><span>[</span><span> 
>                </span><span>{</span><span> 
>                   </span><span>"handler"</span><span>:</span><span> </span><span>"ClientHandler"</span><span>,</span><span> 
>                   </span><span>"baseURI"</span><span>:</span><span> </span><span>"http://time-mcp-server:8080/mcp"</span><span> 
>                </span><span>}</span><span> 
>             </span><span>]</span><span> 
>          </span><span>}</span><span> 
>       </span><span>}</span><span> 
>    </span><span>]</span><span> 
> </span><span>}</span><span> 
> </span></code></pre> 
>  
> </div> 
>  
>  
>  
> <p>The route consists of two filters. The first filter, <code>OAuth2ResourceServerFilter</code>, validates the <code>access_token</code> and, if successful, writes the data received from the <code>access_token</code> to the request context. The second filter, <code>ConditionEnforcementFilter</code>, checks the context and, if successful, forwards the request to the MCP server. Otherwise, it returns HTTP status 401.</p> 
>  
> <p>Let's make an unauthorized request to the MCP server and verify that OpenIG requires authorization.<br> 
> </p> 
>  
> <div> 
> <pre><code>curl <span>-v</span> http://openig.example.org:8081/mcp 
> <span>*</span>   Trying 127.0.0.1:8081... 
> <span>*</span> Connected to openig.example.org <span>(</span>127.0.0.1<span>)</span> port 8081 <span>(</span><span>#0)</span> 
> <span>&gt;</span> GET /mcp HTTP/1.1 
> <span>&gt;</span> Host: openig.example.org:8081 
> <span>&gt;</span> User-Agent: curl/8.1.2 
> <span>&gt;</span> Accept: <span>*</span>/<span>*</span> 
> <span>&gt;</span>  
> &lt; HTTP/1.1 401  
> &lt; WWW-Authenticate: Bearer <span>realm</span><span>=</span><span>"OpenIG"</span> 
> &lt; Content-Length: 0 
> &lt; Date: Mon, 22 Sep 2025 08:00:48 GMT 
>  
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Proxying to <code>.well-known</code> endpoints</p> 
>  
> <p>According to the MCP specification, the client obtains data about the authorization server from endpoints located at the URL <code>&lt;MCP server host&gt;/.well-known/*</code>. The endpoints are located on OpenAM at the URL <code>&lt;OpenAM host&gt;/openam/.well-known</code>. The route for forwarding HTTP requests to MCP on OpenAM is as follows:</p> 
>  
> <p><code>openig-config/config/routes/20-well-known.json</code><br> 
> </p> 
>  
> <div> 
> <pre><code><span>{</span><span> 
>   </span><span>"name"</span><span>:</span><span> </span><span>"${matches(request.uri.path, '^/.well-known/.*}"</span><span>,</span><span> 
>   </span><span>"condition"</span><span>:</span><span> </span><span>"${matches(request.uri.path, '^/.well-known/.*')}"</span><span>,</span><span> 
>   </span><span>"monitor"</span><span>:</span><span> </span><span>true</span><span>,</span><span> 
>   </span><span>"timer"</span><span>:</span><span> </span><span>true</span><span>,</span><span> 
>   </span><span>"handler"</span><span>:</span><span> </span><span>{</span><span> 
>     </span><span>"type"</span><span>:</span><span> </span><span>"Chain"</span><span>,</span><span> 
>     </span><span>"config"</span><span>:</span><span> </span><span>{</span><span> 
>       </span><span>"filters"</span><span>:</span><span> </span><span>[</span><span> 
>         </span><span>{</span><span> 
>           </span><span>"type"</span><span>:</span><span> </span><span>"HeaderFilter"</span><span>,</span><span> 
>           </span><span>"config"</span><span>:</span><span> </span><span>{</span><span> 
>             </span><span>"messageType"</span><span>:</span><span> </span><span>"REQUEST"</span><span>,</span><span> 
>             </span><span>"add"</span><span>:</span><span> </span><span>{</span><span> 
>               </span><span>"Host"</span><span>:</span><span> </span><span>[</span><span> 
>                 </span><span>"${matchingGroups(system['openam'],</span><span>\"</span><span>(http|https):</span><span>\/\/</span><span>(.[^</span><span>\/</span><span>]*)</span><span>\"</span><span>)[2]}"</span><span> 
>               </span><span>]</span><span> 
>             </span><span>},</span><span> 
>             </span><span>"remove"</span><span>:</span><span> </span><span>[</span><span> 
>               </span><span>"Host"</span><span>,</span><span> 
>               </span><span>"Origin"</span><span> 
>             </span><span>]</span><span> 
>           </span><span>}</span><span> 
>         </span><span>}</span><span> 
>       </span><span>],</span><span> 
>       </span><span>"handler"</span><span>:</span><span> </span><span>"EndpointHandler"</span><span> 
>     </span><span>}</span><span> 
>   </span><span>},</span><span> 
>   </span><span>"heap"</span><span>:</span><span> </span><span>[</span><span> 
>     </span><span>{</span><span> 
>       </span><span>"name"</span><span>:</span><span> </span><span>"EndpointHandler"</span><span>,</span><span> 
>       </span><span>"type"</span><span>:</span><span> </span><span>"DispatchHandler"</span><span>,</span><span> 
>       </span><span>"config"</span><span>:</span><span> </span><span>{</span><span> 
>         </span><span>"bindings"</span><span>:</span><span> </span><span>[</span><span> 
>           </span><span>{</span><span> 
>             </span><span>"expression"</span><span>:</span><span> </span><span>"${matches(request.uri.path, '^/.well-known/openid-configuration$')}"</span><span>,</span><span> 
>             </span><span>"handler"</span><span>:</span><span> </span><span>"ClientHandler"</span><span>,</span><span> 
>             </span><span>"baseURI"</span><span>:</span><span> </span><span>"${system['openam'].concat('/oauth2/.well-known/openid-configuration')}"</span><span> 
>           </span><span>}</span><span> 
>         </span><span>]</span><span> 
>       </span><span>}</span><span> 
>     </span><span>}</span><span> 
>   </span><span>]</span><span> 
> </span><span>}</span><span> 
> </span></code></pre> 
>  
> </div> 
>  
>  
>  
> <p>The <code>HeaderFilter</code> filter adds the OpenAM Host HTTP header specified in the <code>openam</code> system parameter in the <code>docker-compose.yaml</code> file, and the <code>EndpointHandler</code> handler forwards the request to the <code>/openam/.well-known/openid-configuration</code> endpoint deployed in the <code>openam</code> Docker container.</p> 
>  
> <p>Let's check the endpoint operation:<br> 
> </p> 
>  
> <div> 
> <pre><code> curl <span>-v</span> http://openig.example.org:8081/.well-known/openid-configuration 
>  
>  <span>{</span> 
>    <span>"acr_values_supported"</span> : <span>[]</span>, 
>    <span>"authorization_endpoint"</span> : <span>"http://openam.example.org:8080/openam/oauth2/authorize"</span>, 
>    <span>"check_session_iframe"</span> : <span>"http://openam.example.org:8080/openam/oauth2/connect/checkSession"</span>, 
>    <span>"claims_parameter_supported"</span> : <span>false</span>, 
>    <span>"claims_supported"</span> : <span>[]</span>, 
>    <span>"device_authorization_endpoint"</span> : <span>"http://openam.example.org:8080/openam/oauth2/device/code"</span>, 
>    <span>"end_session_endpoint"</span> : <span>"http://openam.example.org:8080/openam/oauth2/connect/endSession"</span>, 
>    <span>"id_token_encryption_alg_values_supported"</span> : <span>[</span> 
>       <span>"RSA-OAEP"</span>, 
>       <span>"RSA-OAEP-256"</span>, 
>       <span>"A128KW"</span>, 
>       <span>"RSA1_5"</span>, 
>       <span>"A256KW"</span>, 
>       <span>"dir"</span>, 
>       <span>"A192KW"</span> 
>    <span>]</span>, 
>    <span>"id_token_encryption_enc_values_supported"</span> : <span>[</span> 
>       <span>"A256GCM"</span>, 
>       <span>"A192GCM"</span>, 
>       <span>"A128GCM"</span>, 
>       <span>"A128CBC-HS256"</span>, 
>       <span>"A192CBC-HS384"</span>, 
>       <span>"A256CBC-HS512"</span> 
>    <span>]</span>, 
>    <span>"id_token_signing_alg_values_supported"</span> : <span>[</span> 
>       <span>"ES384"</span>, 
>       <span>"HS256"</span>, 
>       <span>"HS512"</span>, 
>       <span>"ES256"</span>, 
>       <span>"RS256"</span>, 
>       <span>"HS384"</span>, 
>       <span>"ES512"</span> 
>    <span>]</span>, 
>    <span>"issuer"</span> : <span>"http://openam.example.org:8080/openam/oauth2"</span>, 
>    <span>"jwks_uri"</span> : <span>"http://openam.example.org:8080/openam/oauth2/connect/jwk_uri"</span>, 
>    <span>"registration_endpoint"</span> : <span>"http://openam.example.org:8080/openam/oauth2/connect/register"</span>, 
>    <span>"response_types_supported"</span> : <span>[</span> 
>       <span>"code"</span>, 
>       <span>"code token"</span>, 
>       <span>"token"</span> 
>    <span>]</span>, 
>    <span>"scopes_supported"</span> : <span>[]</span>, 
>    <span>"subject_types_supported"</span> : <span>[</span> 
>       <span>"public"</span> 
>    <span>]</span>, 
>    <span>"token_endpoint"</span> : <span>"http://openam.example.org:8080/openam/oauth2/access_token"</span>, 
>    <span>"token_endpoint_auth_methods_supported"</span> : <span>[</span> 
>       <span>"client_secret_post"</span>, 
>       <span>"private_key_jwt"</span>, 
>       <span>"none"</span>, 
>       <span>"client_secret_basic"</span> 
>    <span>]</span>, 
>    <span>"userinfo_endpoint"</span> : <span>"http://openam.example.org:8080/openam/oauth2/userinfo"</span>, 
>    <span>"version"</span> : <span>"3.0"</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>For more details on configuring OpenIG, please refer to the documentation.</p> 
>  
> <p>Configuring VS Code to work with the MCP server</p> 
>  
> <p>You must have the extensions for working with Copilot, GitHub Copilot, and GitHub Copilot Chat installed and configured. Instructions on how to do this are available at: <a href="https://code.visualstudio.com/docs/copilot/setup">https://code.visualstudio.com/docs/copilot/setup</a>.</p> 
>  
> <p>Add the MCP server to VS Code.</p> 
>  
> <p>For example, to add MCP to your workspace, create a file named <code>mcp.json</code> in the <code>.vscode</code> directory of your workspace:</p> 
>  
> <p><code>mcp.json</code>:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>{</span><span> 
>   </span><span>"servers"</span><span>:</span><span> </span><span>{</span><span> 
>     </span><span>"time-mcp-server"</span><span>:</span><span> </span><span>{</span><span> 
>       </span><span>"type"</span><span>:</span><span> </span><span>"http"</span><span>,</span><span> 
>       </span><span>"url"</span><span>:</span><span> </span><span>"http://openig.example.org:8081/mcp"</span><span> 
>     </span><span>}</span><span> 
>   </span><span>}</span><span> 
> </span><span>}</span><span> 
> </span></code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Other ways to add an MCP server are described at: <a href="https://code.visualstudio.com/docs/copilot/customization/mcp-servers#_add-an-mcp-server">https://code.visualstudio.com/docs/copilot/customization/mcp-servers#_add-an-mcp-server</a></p> 
>  
> <p>In the VS Code extensions list, click on the settings for the added MCP server and select <code>Start Server</code> from the menu that appears.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft4ift973bsahq0a77jwa.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft4ift973bsahq0a77jwa.png" alt="VS Code Start MCP Server" width="800" height="404"></a></p> 
>  
> <p>Allow the MCP server to authenticate on the OpenIG host</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwx4g8q6u925u8tbyuaga.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwx4g8q6u925u8tbyuaga.png" alt="VS Code MCP Authenticate" width="788" height="518"></a></p> 
>  
> <p>A browser window with authentication will open. </p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fai8d21r5wiidn4vhrr66.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fai8d21r5wiidn4vhrr66.png" alt="OpenAM Login" width="800" height="572"></a></p> 
>  
> <p>Enter the login and password for the test user: <code>demo</code>  and <code>changeit</code>, respectively</p> 
>  
> <p>Confirm access to data for the Visual Studio Code application. If you want to disable the data access confirmation dialog, enable <code>Allow clients to skip consent</code> in the OAuth2 Provider settings in OpenAM. </p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fp331k8ti4jsh57rrlm0a.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fp331k8ti4jsh57rrlm0a.png" alt="OpenAM OAuth2 Consent" width="800" height="535"></a></p> 
>  
> <p>After confirming, you will be redirected back to VS Code.</p> 
>  
> <p>Open the chat with GitHub Copilot. To do this, select <strong>Show and Run Commands</strong> from the command menu:</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2ye5o9bzgr5pxfo10ltb.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2ye5o9bzgr5pxfo10ltb.png" alt="OpenAM Run Commands" width="800" height="267"></a></p> 
>  
> <p>Then select <strong>Chat: New Chat</strong>:</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fpikum1km0ec98wqb787z.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fpikum1km0ec98wqb787z.png" alt="OpenAM New Chat" width="800" height="209"></a></p> 
>  
> <p>In the chat window that opens, enter the question: <code>What is the current time?</code> . Copilot will respond that it does not have access to the current time:</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwv9hwmcw3suz3qzap08i.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwv9hwmcw3suz3qzap08i.png" alt="OpenAM Time Request" width="800" height="643"></a></p> 
>  
> <p>Now switch the chat to Agent mode at the bottom and ask the question again:</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fs2oy54n8j2e75r9uxi86.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fs2oy54n8j2e75r9uxi86.png" alt="Copilot Set the Agent Mode" width="800" height="202"></a></p> 
>  
> <p>Allow access to the function for obtaining the current time in the MCP server:</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgidwbwyssb4z0bm42762.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgidwbwyssb4z0bm42762.png" alt="Copilot Allow MCP Call" width="800" height="286"></a></p> 
>  
> <p>Copilot will receive information about the current time from the MCP server and return the correct response.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fm2elpzc3f3osaxb97khl.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fm2elpzc3f3osaxb97khl.png" alt="Copilot Successfol response" width="800" height="235"></a></p> 
>  
> <h2> 
>    
>    
>   Conclusion 
> </h2> 
>  
> <p>In this article, we demonstrated the practical integration of OpenAM and OpenIG to provide secure access to the MCP server based on OAuth 2.1. OpenAM acts as a reliable authentication and authorization center, issuing and validating tokens, while OpenIG filters requests, blocking unauthorized access and proxying traffic to protected resources. This approach minimizes the risk of sensitive data leaks - from internal repositories to customer databases.</p> 
>  
> <p>Download the source code from GitHub, test the configuration, and integrate it into your projects. For in-depth study, refer to the official documentation: <a href="https://doc.openidentityplatform.org/openam/">OpenAM</a> and <a href="https://doc.openidentityplatform.org/openig/">OpenIG</a>.</p>

---

## [5/10] RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251104491A/abstract
**Reasoning:** The benchmarking of LLM reasoning in complex tables is somewhat relevant to understanding LLM capabilities, but not directly tied to code or context.
**Authors:** Abhyankar, Nikhil, Chaurasia, Purvi, Kabra, Sanchit, Srivastava, Ananya, Gupta, Vivek, Reddy, Chandan K.

**Content/Abstract:**
> Existing tabular reasoning benchmarks mostly test models on small, uniform tables, underrepresenting the complexity of real-world data and giving an incomplete view of Large Language Models' (LLMs) reasoning abilities. Real tables are long, heterogeneous, and domain-specific, mixing structured fields with free text and requiring multi-hop reasoning across thousands of tokens. To address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from 2031 real-world tables spanning two domains: i) RB-Science (NSF grant records) and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates LLMs jointly across scale, heterogeneity, domain specificity, and reasoning complexity. Experiments with open-source and proprietary models show that LLMs struggle with heterogeneous schemas and complex multi-hop inference, revealing persistent weaknesses in current architectures and prompting strategies. RUST-BENCH establishes a challenging new testbed for advancing tabular reasoning research.

---

## [5/10] An Empirical Study of Agent Developer Practices in AI Agent Frameworks
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T17:52:15.000Z
**URL:** https://arxiv.org/abs/2512.01939v1
**Reasoning:** The study of agent developer practices is tangentially related to AI agents in software engineering.
**Authors:** Yanlin Wang

**Content/Abstract:**
> The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.

---

## [5/10] Understanding Amazon Q Custom Agents: Concepts, Architecture & Inner Workings
**Source:** The Practical Developer | **Date:** 2025-12-02T14:09:35.000Z
**URL:** https://dev.to/aws-builders/understanding-amazon-q-custom-agents-concepts-architecture-inner-workings-362
**Reasoning:** The article discusses custom agents by Amazon, which is relevant to AI Agents in SE but not directly focused on our primary interests.
**Authors:** Sarvar Nadaf

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F79ww1qnd9fijumax5sjc.webp" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>👋 Hey there, tech enthusiasts! </p>  
>   
> <p>I'm Sarvar, a Cloud Architect with a passion for transforming complex technological challenges into elegant solutions. With extensive experience spanning Cloud Operations (AWS &amp; Azure), Data Operations, Analytics, DevOps, and Generative AI, I've had the privilege of architecting solutions for global enterprises that drive real business impact. Through this article series, I'm excited to share practical insights, best practices, and hands-on experiences from my journey in the tech world. Whether you're a seasoned professional or just starting out, I aim to break down complex concepts into digestible pieces that you can apply in your projects.</p>  
>   
> <p>Let's dive in and explore the fascinating world of cloud technology together! 🚀</p>  
>   
>   
>   
>   
> <p>Imagine Sarah, a DevOps engineer at a growing startup. Every day, she receives dozens of questions from developers:</p>  
>   
> <blockquote>  
> <p>“What's the status of our production deployment?”<br>  
> “Can you check the database performance metrics?”<br>  
> “How much are we spending on EC2 this month?”</p>  
> </blockquote>  
>   
> <p>Sarah spends hours manually checking AWS consoles, running CLI commands, and compiling reports.</p>  
>   
> <p>What if Sarah could create an intelligent assistant that automatically handles these requests?</p>  
>   
> <p><strong>Enter Amazon Q Custom Agents:</strong> AI-powered assistants that can understand natural language, access your AWS resources, and provide intelligent responses.</p>  
>   
> <p>Today, we'll explore how to build these powerful agents, covering everything from basic concepts to advanced integrations.</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   <strong>What We're Covering Today</strong>  
> </h2>  
>   
> <ul>  
> <li>Understanding Amazon Q Custom Agents</li>  
> <li>Types of Custom Agents</li>  
> <li>What is a Knowledge Base?</li>  
> <li>What is an Index?</li>  
> <li>Advanced Indexing Strategies</li>  
> <li>Storage &amp; Integration Options for Knowledge Bases</li>  
> <li>Custom Agent Deployment Types</li>  
> <li>How Amazon Q Custom Agent Architecture Works</li>  
> <li>Pricing Breakdown</li>  
> <li>Conclusion</li>  
> </ul>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>Understanding Amazon Q Custom Agents</strong>  
> </h1>  
>   
> <p>Amazon Q Custom Agents are intelligent, model-driven assistants designed to adapt to your organization’s unique workflows and data. They combine large language models with your internal knowledge sources and AWS services to deliver accurate, contextual, and actionable responses.</p>  
>   
> <p>The core structure of an Amazon Q Business Application is organized as follows:<br>  
> </p>  
>   
> <div>  
> <pre><code>Amazon Q Business Application  
> └── Custom Agent  
>     ├── Instructions (System Prompts)  
>     ├── Guardrails (Content Filtering)  
>     ├── Knowledge Bases  
>     │   └── Amazon Bedrock Knowledge Base  
>     │       ├── Data Source (S3)  
>     │       ├── Vector Store (OpenSearch Serverless)  
>     │       └── Embedding Model (Bedrock)  
>     └── Action Groups  
>         ├── Lambda Functions  
>         └── OpenAPI Schema Definitions  
> </code></pre>  
>   
> </div>  
>   
>   
>   
>   
>   
>   
> <h2>  
>     
>     
>   <strong>Types of Custom Agents</strong>  
> </h2>  
>   
> <h3>  
>     
>     
>   <strong>1. Knowledge Base Agents</strong>  
> </h3>  
>   
> <ul>  
> <li>  
> <strong>What it does:</strong> Answers questions using documents, wikis, and internal knowledge sources.</li>  
> <li>  
> <strong>Best for:</strong> FAQ systems, documentation queries, policy lookups.</li>  
> <li>  
> <strong>Real-time example:</strong> <em>“What’s our company’s vacation policy?”</em> → Searches HR documents and returns accurate details.</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   <strong>2. Action Agents</strong>  
> </h3>  
>   
> <ul>  
> <li>  
> <strong>What it does:</strong> Executes tasks through API calls and AWS service integrations.</li>  
> <li>  
> <strong>Best for:</strong> Operational tasks, resource provisioning, automated workflows.</li>  
> <li>  
> <strong>Real-time example:</strong> <em>“Scale up our production environment”</em> → Increases EC2 capacity and updates load balancers.</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   <strong>3. Hybrid Agents</strong>  
> </h3>  
>   
> <ul>  
> <li>  
> <strong>What it does:</strong> Combines information retrieval with task execution.</li>  
> <li>  
> <strong>Best for:</strong> Complex workflows requiring both insights and actions.</li>  
> <li>  
> <strong>Real-time example:</strong> <em>“Check database performance and optimize if needed”</em> → Retrieves metrics, analyzes them, and applies tuning.</li>  
> </ul>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>What is a Knowledge Base?</strong>  
> </h1>  
>   
> <p>A Knowledge Base is the brain of a custom agent a centralized repository that serves as its memory, containing all the documents, procedures, and structured information needed to answer queries accurately.</p>  
>   
> <h3>  
>     
>     
>   <strong>Key Components</strong>  
> </h3>  
>   
> <ul>  
> <li>  
> <strong>Documents:</strong> PDFs, Word files, text content, wikis, web pages</li>  
> <li>  
> <strong>Structured Data:</strong> Databases, APIs, config files</li>  
> <li>  
> <strong>Real-time Data:</strong> Monitoring systems, AWS service metrics, logs</li>  
> </ul>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>What is an Index?</strong>  
> </h1>  
>   
> <p>An Index is a structured system that converts your Knowledge Base content into searchable vector representations, allowing the agent to retrieve relevant information quickly and accurately.</p>  
>   
> <h2>  
>     
>     
>   <strong>How Indexing Works</strong>  
> </h2>  
>   
> <ol>  
> <li>  
> <strong>Document Processing</strong> – Splits documents into meaningful chunks</li>  
> <li>  
> <strong>Vectorization</strong> – Converts text into numerical embeddings</li>  
> <li>  
> <strong>Semantic Search</strong> – Understands context instead of relying on keywords</li>  
> <li>  
> <strong>Retrieval</strong> – Returns the most relevant content based on user queries</li>  
> </ol>  
>   
> <h2>  
>     
>     
>   <strong>Example Index Structure</strong>  
> </h2>  
>   
>   
>   
> <div>  
> <pre><code>AWS Documentation Index  
> ├── EC2 Best Practices (Vector: [0.2, 0.8, 0.1...])  
> ├── S3 Security Guidelines (Vector: [0.5, 0.3, 0.9...])  
> ├── Cost Optimization Tips (Vector: [0.1, 0.7, 0.4...])  
> └── Troubleshooting Guides (Vector: [0.8, 0.2, 0.6...])  
> </code></pre>  
>   
> </div>  
>   
>   
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>Advanced Indexing Strategies</strong>  
> </h1>  
>   
> <h2>  
>     
>     
>   <strong>1. Vector Embedding Models</strong>  
> </h2>  
>   
> <ul>  
> <li>  
> <strong>Amazon Titan Text Embeddings:</strong> Optimized for English, up to 8,192 tokens</li>  
> <li>  
> <strong>Cohere Embed:</strong> Multilingual support for diverse content</li>  
> <li>  
> <strong>Custom Models:</strong> Fine-tuned vectors for domain-specific knowledge</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   <strong>2. Chunking Strategies</strong>  
> </h2>  
>   
> <ul>  
> <li>  
> <strong>Fixed-size chunking:</strong> 500–1000 tokens</li>  
> <li>  
> <strong>Semantic chunking:</strong> Breaks at logical document boundaries</li>  
> <li>  
> <strong>Overlapping chunks:</strong> 10–20% overlap for better continuity</li>  
> <li>  
> <strong>Hierarchical chunking:</strong> Varies chunk size based on content type</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   <strong>3. Retrieval Methods</strong>  
> </h2>  
>   
> <ul>  
> <li>  
> <strong>Semantic Search:</strong> Context-aware retrieval using vector similarity</li>  
> <li>  
> <strong>Hybrid Search:</strong> Combines keyword + semantic search</li>  
> <li>  
> <strong>Metadata Filtering:</strong> Filters results based on attributes (tags, authors, etc.)</li>  
> <li>  
> <strong>Re-ranking:</strong> Reorders retrieved documents for maximum relevance</li>  
> </ul>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>Storage &amp; Integration Options for Knowledge Bases</strong>  
> </h1>  
>   
> <p>Storage &amp; Integration Options define where your knowledge is stored and how it connects to your custom agent, enabling seamless access to documents, databases, and third-party applications for accurate responses. Below are commonly used storage and integration choices for building robust knowledge bases.</p>  
>   
> <h2>  
>     
>     
>   <strong>Primary Storage Solutions</strong>  
> </h2>  
>   
> <h3>  
>     
>     
>   <strong>1. Amazon S3</strong>  
> </h3>  
>   
> <ul>  
> <li>  
> <strong>Use Case:</strong> Document storage, archives, data lakes</li>  
> <li>  
> <strong>Benefits:</strong> Scalable, cost-effective, easy AWS integration</li>  
> <li>  
> <strong>Best For:</strong> Large document libraries and media files</li>  
> <li>  
> <strong>Pricing:</strong> ~<strong>$0.023/GB per month</strong>  
> </li>  
> </ul>  
>   
> <h3>  
>     
>     
>   <strong>2. Amazon OpenSearch</strong>  
> </h3>  
>   
> <ul>  
> <li>  
> <strong>Use Case:</strong> Real-time search, indexing, analytics</li>  
> <li>  
> <strong>Benefits:</strong> Fast retrieval, advanced search, near real-time indexing</li>  
> <li>  
> <strong>Best For:</strong> Frequently updated or search-intensive environments</li>  
> <li>  
> <strong>Pricing:</strong> Starts at ~<strong>$0.088/hour</strong> (t3.small.search)</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   <strong>3. Amazon RDS / DynamoDB</strong>  
> </h3>  
>   
> <ul>  
> <li>  
> <strong>Use Case:</strong> Structured or transactional data</li>  
> <li>  
> <strong>Benefits:</strong> High performance, fully managed, reliable</li>  
> <li>  
> <strong>Best For:</strong> Profiles, catalogs, operational metrics</li>  
> <li>  
> <p><strong>Pricing:</strong></p>  
>   
> <ul>  
> <li>  
> <strong>RDS:</strong> From ~<strong>$0.017/hour</strong>  
> </li>  
> <li>  
> <strong>DynamoDB:</strong> ~<strong>$0.25 per million reads</strong>  
> </li>  
> </ul>  
>   
>   
> </li>  
>   
> </ul>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   <strong>Integration Options</strong>  
> </h2>  
>   
> <h3>  
>     
>     
>   <strong>Direct Integrations</strong>  
> </h3>  
>   
> <ul>  
> <li>SharePoint Online</li>  
> <li>Salesforce</li>  
> <li>ServiceNow</li>  
> <li>Confluence</li>  
> <li>Jira</li>  
> <li>Microsoft Teams</li>  
> <li>Google Workspace</li>  
> <li>Slack</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   <strong>Custom Integrations</strong>  
> </h3>  
>   
> <ul>  
> <li>REST APIs</li>  
> <li>Database connectors</li>  
> <li>File system crawlers</li>  
> <li>Real-time streams (Kinesis, EventBridge)</li>  
> <li>Git repositories</li>  
> </ul>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>Custom Agent Deployment Types</strong>  
> </h1>  
>   
> <h2>  
>     
>     
>   <strong>1. Single-Region Deployment</strong>  
> </h2>  
>   
> <ul>  
> <li>  
> <strong>Use Case:</strong> Simple, cost-efficient setups</li>  
> <li>  
> <strong>Benefits:</strong> Low latency, minimal complexity</li>  
> <li>  
> <strong>Considerations:</strong> Single point of failure</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   <strong>2. Multi-Region Deployment</strong>  
> </h2>  
>   
> <ul>  
> <li>  
> <strong>Use Case:</strong> Global user base, disaster recovery</li>  
> <li>  
> <strong>Benefits:</strong> High availability, reduced latency globally</li>  
> <li>  
> <strong>Considerations:</strong> Sync overhead, added cost</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   <strong>3. Hybrid Deployment</strong>  
> </h2>  
>   
> <ul>  
> <li>  
> <strong>Use Case:</strong> Compliance or sensitive data</li>  
> <li>  
> <strong>Benefits:</strong> Local control + cloud scalability</li>  
> <li>  
> <strong>Considerations:</strong> Connectivity, governance requirements</li>  
> </ul>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>How Amazon Q Custom Agent Architecture Works</strong>  
> </h1>  
>   
> <p>Let’s understand how an Amazon Q Custom Agent works in real time. The following high-level architecture shows the simple flow of how a request is processed from start to finish. When a user asks a question, the Amazon Q Agent interprets the request and determines whether it needs to retrieve information or perform an action. If the request is informational, the agent searches the connected Knowledge Base, retrieves the most relevant documents, and generates a clear response for the user. If the request requires an operation such as creating a resource or retrieving system data the agent triggers a Lambda function or OpenAPI action to execute the task on AWS services. Once the action is completed, the result is returned to the agent, which then converts it into a user-friendly answer.<br>  
> </p>  
>   
> <div>  
> <pre><code>┌────────────────────────┐  
> │      User Question     │  
> └─────────────┬──────────┘  
>               │  
>               ▼  
> ┌────────────────────────┐  
> │      Amazon Q Agent    │  
> └─────────────┬──────────┘  
>               │  
>               ▼  
> ┌────────────────────────┐  
> │      Processing Logic  │  
> └─────────────┬──────────┘  
>               │  
>               ▼  
> ┌────────────────────────┐        ┌────────────────────────┐  
> │     Document Search    │ ◄ ──── │     Knowledge Base     │  
> └─────────────┬──────────┘        └────────────────────────┘  
>               │  
>               ▼  
> ┌────────────────────────┐  
> │    Response Generation │  
> └─────────────┬──────────┘  
>               │  
>        ┌──────┴─────────────────────────────┐  
>        ▼                                    ▼  
> ┌────────────────────────┐    ┌────────────────────────┐  
> │       User Answer      │    │    Action Execution    │  
> └────────────────────────┘    └─────────────┬──────────┘  
>                                             │  
>                                             ▼  
>                                ┌────────────────────────┐  
>                                │      AWS Services      │  
>                                └────────────────────────┘  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>This simple flow helps illustrate how the agent intelligently switches between knowledge retrieval and operational execution, allowing it to both answer questions and perform real-world tasks seamlessly.</p>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>Pricing Breakdown</strong>  
> </h1>  
>   
> <p><em>Important:</em> All these prices were collected using the AWS Pricing Calculator MCP Server integrated with Amazon Q Pro. If this sounds interesting, follow this guide: <a href="https://dev.to/aws-builders/real-time-aws-cost-estimation-using-the-pricing-mcp-server-and-amazon-q-cli-1nc6">Link</a></p>  
>   
> <h2>  
>     
>     
>   <strong>Amazon Q Business Pricing</strong>  
> </h2>  
>   
> <ul>  
> <li>  
> <strong>Pro Tier</strong>: $20 per user per month</li>  
> <li>  
> <strong>Lite Tier</strong>: $3 per user per month</li>  
> <li>  
> <strong>Includes</strong>: Agent interactions, knowledge base queries, basic actions</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   <strong>Knowledge Base Components</strong>  
> </h2>  
>   
> <ul>  
> <li>  
> <strong>Document Processing</strong>: $0.10 per 1,000 documents</li>  
> <li>  
> <strong>Vector Storage</strong>: $0.30 per GB per month</li>  
> <li>  
> <strong>Query Processing</strong>: $0.004 per query</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   <strong>Storage Costs (Monthly)</strong>  
> </h2>  
>   
> <ul>  
> <li>  
> <strong>S3 Standard</strong>: $0.023 per GB</li>  
> <li>  
> <strong>OpenSearch</strong>: $0.088/hour (t3.small) ≈ $63/month</li>  
> <li>  
> <strong>RDS</strong>: Starting at $12/month (db.t3.micro)</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   <strong>Integration Costs</strong>  
> </h2>  
>   
> <ul>  
> <li>  
> <strong>API Gateway</strong>: $3.50 per million API calls</li>  
> <li>  
> <strong>Lambda</strong>: $0.20 per 1 million requests</li>  
> <li>  
> <strong>CloudWatch</strong>: $0.30 per GB ingested</li>  
> </ul>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>Advanced Features and Integrations</strong>  
> </h1>  
>   
> <h2>  
>     
>     
>   <strong>1. Multi-Modal Capabilities</strong>  
> </h2>  
>   
> <ul>  
> <li>  
> <strong>Text Processing:</strong> Documents, emails, chat logs</li>  
> <li>  
> <strong>Image Analysis:</strong> Diagrams, screenshots, charts</li>  
> <li>  
> <strong>Voice Integration:</strong> Speech-to-text for voice-based queries</li>  
> </ul>  
>   
> <h1>  
>     
>     
>   <strong>2. Security and Compliance</strong>  
> </h1>  
>   
> <ul>  
> <li>  
> <strong>Identity Integration:</strong> AWS IAM, Active Directory, SAML</li>  
> <li>  
> <strong>Data Encryption:</strong> Encryption at rest and in transit</li>  
> <li>  
> <strong>Access Controls:</strong> Role-based permissions, data-level filtering</li>  
> <li>  
> <strong>Audit Logging:</strong> Full interaction history for compliance</li>  
> </ul>  
>   
> <h1>  
>     
>     
>   <strong>3. Enterprise Integrations</strong>  
> </h1>  
>   
> <ul>  
> <li>  
> <strong>Slack / Microsoft Teams:</strong> Direct chat-based interactions</li>  
> <li>  
> <strong>ServiceNow:</strong> Ticket creation, updates, and workflow automation</li>  
> <li>  
> <strong>Jira:</strong> Issue tracking and project updates</li>  
> <li>  
> <strong>Salesforce:</strong> Access to customer and sales data</li>  
> </ul>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>Best Practices for Custom Agent Development</strong>  
> </h1>  
>   
> <h2>  
>     
>     
>   <strong>4. Design Principles</strong>  
> </h2>  
>   
> <ul>  
> <li>  
> <strong>Start Simple:</strong> Begin with a basic knowledge base; add actions later</li>  
> <li>  
> <strong>User-Centric:</strong> Align conversations with real user workflows</li>  
> <li>  
> <strong>Iterative Improvement:</strong> Refine responses based on user behavior</li>  
> <li>  
> <strong>Security First:</strong> Implement proper access controls early</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   <strong>5. Performance Optimization</strong>  
> </h2>  
>   
> <ul>  
> <li>  
> <strong>Chunk Size:</strong> Optimize chunking for better document retrieval</li>  
> <li>  
> <strong>Index Strategy:</strong> Use indexing that fits your content patterns</li>  
> <li>  
> <strong>Caching:</strong> Cache responses for frequently asked questions</li>  
> <li>  
> <strong>Monitoring:</strong> Track latency, usage, and user satisfaction</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   <strong>6. Content Management</strong>  
> </h2>  
>   
> <ul>  
> <li>  
> <strong>Regular Updates:</strong> Keep documents and knowledge up to date</li>  
> <li>  
> <strong>Version Control:</strong> Track content changes through Git or similar tools</li>  
> <li>  
> <strong>Quality Assurance:</strong> Test accuracy of responses regularly</li>  
> <li>  
> <strong>Feedback Loop:</strong> Collect user feedback for continuous improvement</li>  
> </ul>  
>   
>   
>   
>   
> <blockquote>  
> <p><em>Conclusion: Amazon Q Custom Agents offer a transformative approach to knowledge management and task automation by combining large language models with your organization’s specific context to reduce manual work and improve accuracy. Whether you’re creating a simple FAQ bot or a complex operational assistant, success depends on understanding user needs, carefully curating your knowledge base, and continuously enhancing the agent’s capabilities. The investment quickly pays off through reduced support efforts, faster problem resolution, and increased productivity. As organizations generate more data and face growing operational complexity, intelligent agents will become essential for maintaining a competitive advantage.</em></p>  
> </blockquote>  
>   
> <p><strong>The future of work is collaborative intelligence between humans and AI agents - and with Amazon Q, that future is available today.</strong></p>  
>   
> <p><strong>Stay tuned for my dev.to article series on AWS Custom Agents. I’m planning to build and showcase the custom agents mentioned in this article, and I’ll continue sharing updates and new implementations as the series progresses.</strong></p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   📌 Wrapping Up  
> </h2>  
>   
> <p>Thank you for reading! I hope this article gave you practical insights and a clearer perspective on the topic.</p>  
>   
> <p><strong>Was this helpful?</strong></p>  
>   
> <ul>  
> <li>❤️ Like if it added value</li>  
> <li>🦄 Unicorn if you’re applying it today</li>  
> <li>💾 Save for your next optimization session</li>  
> <li>🔄 Share with your team</li>  
> </ul>  
>   
> <p><strong>Follow me for more on:</strong></p>  
>   
> <ul>  
> <li>AWS architecture patterns</li>  
> <li>FinOps automation</li>  
> <li>Multi-account strategies</li>  
> <li>AI-driven DevOps</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   💡 What’s Next  
> </h2>  
>   
> <p>More deep dives coming soon on cloud operations, GenAI, Agentic-AI, DevOps, and data workflows follow for weekly insights.</p>  
>   
> <h2>  
>     
>     
>   🤝 Let’s Connect  
> </h2>  
>   
> <p>I’d love to hear your thoughts drop a comment or connect with me on <a href="https://www.linkedin.com/in/sarvar04/">LinkedIn</a>.</p>  
>   
> <p>Happy Learning 🚀</p>

---

## [5/10] Breaking the doom-prompting loop with spec-driven development
**Source:** The Practical Developer | **Date:** 2025-12-02T13:38:26.000Z
**URL:** https://dev.to/vburckhardt/breaking-the-doom-prompting-loop-with-spec-driven-development-19hj
**Reasoning:** The article on spec-driven development in AI-assisted coding touches on engineering culture and trends.
**Authors:** Vincent Burckhardt

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fuobjyis03xrl2taq602t.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p><em>Bringing software engineering discipline to AI-assisted coding</em></p>  
>   
> <p>Every developer using AI coding tools has experienced the loop. You prompt, the AI generates code, something isn't quite right, you prompt again, the AI breaks something else while fixing the first issue, you prompt again. An hour later you're deeper in the hole than when you started, caught in what's now called <a href="https://www.cio.com/article/4056586/doomprompting-endless-tinkering-with-ai-outputs-can-cripple-it-results.html">doomprompting</a>: you keep going because you've already invested so much time.  </p>  
>   
> <p>This is the dark side of <a href="https://x.com/karpathy/status/1886192184808149383">vibe coding</a>, Andrej Karpathy's term for fully surrendering to AI-generated code without really understanding it. Karpathy himself noted it's "not too bad for throwaway weekend projects." For anything more substantial, the approach tends to collapse.  </p>  
>   
> <p>I've been using <a href="https://github.com/github/spec-kit">spec-kit</a>, GitHub's toolkit for spec-driven development, and it's changed how I think about AI-assisted coding. The core insight is simple: catching problems in specifications costs far less than catching them in code.</p>  
>   
> <h2>  
>     
>     
>   The shift-left principle applied to AI coding  
> </h2>  
>   
> <p>Shift-left testing is the idea that catching defects earlier in development is cheaper than catching them later. Everyone who's debugged a production issue knows this intuitively: finding a problem in requirements costs almost nothing, finding it in code review costs some rework, finding it in production costs a lot more.  </p>  
>   
> <p>Spec-kit applies this principle to AI-assisted development, but shifts even further left. Instead of catching issues through testing code, you catch them through reviewing specifications. The four-phase workflow makes this explicit: Specify, Plan, Tasks, then Implement. Each phase has a gate where you review before proceeding.  </p>  
>   
> <p>This feels familiar to anyone who studied software engineering formally. I remember university projects where we spent weeks on specifications and architecture before writing a line of code. The discipline felt excessive at the time, but the coding phase was remarkably smooth when we finally got there. Spec-kit brings that same rigor to AI-assisted development.</p>  
>   
> <h2>  
>     
>     
>   <strong>What spec-kit actually provides</strong>  
> </h2>  
>   
> <p>The toolkit is agent-agnostic and works with Claude Code, GitHub Copilot, Cursor, and other AI coding tools. At its core, it's a set of slash commands that guide you through structured phases:  </p>  
>   
> <p>The <code>/specify</code> command forces you to articulate what you're building. The <code>/plan</code> command generates research and technical direction. The <code>/tasks</code> command breaks the plan into discrete implementation steps. Finally, <code>/implement</code> executes those tasks.  </p>  
>   
> <p>Each phase produces markdown files that serve as both documentation and AI context. The specifications, plans, and task lists persist across sessions, acting as memory that keeps the AI aligned with your intent.  </p>  
>   
> <p>Spec-kit also introduces what it calls a "constitution" (I prefer "principles," but the concept matters more than the name). This file establishes cross-cutting rules for your project: testing approach, coding standards, architectural constraints. These non-functional requirements apply to everything the AI generates.</p>  
>   
> <h2>  
>     
>     
>   How the flow changes day-to-day work  
> </h2>  
>   
> <p>My workflow with spec-kit looks different from the typical AI coding loop. I spend time reviewing and editing the specifications and task list, then let the AI implement the full feature. I treat the AI less like a pair programmer and more like a developer I'm delegating work to. I review the resulting code the way I'd review a pull request from a human team member.  </p>  
>   
> <p>This mental model matters. With pair programming, you're watching every keystroke. With delegation, you're reviewing outcomes against specifications. The latter scales better with AI tools that can implement substantial features autonomously.  </p>  
>   
> <p>The plan phase has become the most valuable. The AI performs research on the technical direction, and I've learned things from this process. More importantly, I catch misunderstandings early. During one project, the plan revealed the AI assumed an IBM Cloud serverless service was deployed on a VPC, which is incorrect. Catching that during plan review was far cheaper than discovering it through broken infrastructure code.  </p>  
>   
> <p>I don't review every single code change anymore. Instead, I review the specifications carefully, let implementation run with auto-accept enabled, do smoke testing, then review the full changeset. If issues emerge, I iterate through the full flow (plan to tasks to implementation) rather than jumping straight to code fixes. This keeps the specifications accurate and aligned with what actually got built.</p>  
>   
> <h2>  
>     
>     
>   The overhead question  
> </h2>  
>   
> <p>Spec-kit adds overhead. For simple tasks, that overhead isn't worth it.  </p>  
>   
> <p>But for larger features, I've found the investment pays back. The specifications force me to think through requirements properly. Architectural problems surface during plan review rather than after I've invested in code. And I avoid the doom-prompting loop because ambiguities in my thinking get resolved during specification, not through trial-and-error prompting.  </p>  
>   
> <p>This parallels traditional development. Some developers code first and spend months fixing bugs and refactoring. Others invest in architecture and specifications upfront. Both approaches can work, but they have different risk profiles. For complex work, the methodical approach tends to win. The same applies to AI-assisted development.  </p>  
>   
> <p>The token usage goes up when using spec-kit. You're generating specifications, plans, and task lists before writing code. But these tokens typically pay for themselves by avoiding the doom-prompting loop where you might burn through tokens endlessly without making progress.</p>  
>   
> <h2>  
>     
>     
>   Prompt-based flows versus coded pipelines  
> </h2>  
>   
> <p>One aspect of spec-kit's design surprised me. My initial instinct would have been to implement most of the workflow in a traditional programming language with explicit control flow. Instead, spec-kit encapsulates the flow in detailed prompts with minimal supporting scripts.  </p>  
>   
> <p>This approach works well with frontier models. The prompts describe phases in natural language, and the AI follows them reliably. The templating approach with gates provides deterministic outcomes without requiring coded orchestration nodes like you'd find in LangGraph.  </p>  
>   
> <p>I suspect this approach would be less reliable with non-frontier models. The ability to follow complex, multi-phase instructions consistently requires the kind of instruction-following that frontier models do well.  </p>  
>   
> <p>Beyond the underlying model, I've noticed the tools available in each AI assistant matter. The plan phase benefits from web search, codebase search, and other research capabilities. Claude Code includes these out of the box, including deep search for thorough research. Other AI assistants may lack some of these capabilities, and I've seen the most variance in plan quality when research tools are limited.  </p>  
>   
> <p>Configuring <a href="https://modelcontextprotocol.io/">MCP</a> tools before running through the flow also improves results. For instance, I configure tools for Terraform module registry search and cloud provider documentation lookup. These help the AI generate better-informed plans.</p>  
>   
> <h2>  
>     
>     
>   Adapting for infrastructure as code  
> </h2>  
>   
> <p>When I started using spec-kit, I thought it would apply directly to infrastructure as code. As I progressed, I realized IaC has specific characteristics that need different handling: the declarative nature of tools like Terraform, the need to separate cloud-agnostic requirements from provider-specific implementations, governance concerns around security and cost that differ from application code, and validation against actual cloud provider APIs and module registries.  </p>  
>   
> <p>I ended up creating <a href="https://github.com/IBM/iac-spec-kit">iac-spec-kit</a> and open-sourced it to get more collaboration on the approach. It started as a fork, but ended up as a complete reimplementation of the commands, instructions, and templates. The only common layer is around the installer and the overall approach. The templates and prompts needed to be tuned specifically for infrastructure concerns.  </p>  
>   
> <p>The goal is to fill a gap where users can start with a high-level requirement like "deploy WordPress" or "set up a three-tier web app" and have the AI guide them through specification, planning, and code generation with review gates at each phase. The toolkit is cloud-agnostic and works with AWS, Azure, GCP, IBM Cloud, and others. Early tests look promising. I documented one end-to-end example at <a href="https://github.com/vburckhardt/wordpress-ibm-cloud">vburckhardt/wordpress-ibm-cloud</a>, which shows the full workflow from initial requirements through generated Terraform code.  </p>  
>   
> <p>A specific focus has been getting AI to compose higher-level Terraform modules rather than using lower-level providers directly. AI-generated code that glues together curated, supported modules is more maintainable and supportable than code that reinvents infrastructure patterns using primitives. It's similar to teaching AI to use a well-designed library instead of writing everything from scratch.</p>  
>   
> <h2>  
>     
>     
>   What this enables  
> </h2>  
>   
> <p>Spec-kit enables going beyond vibe coding. The structured flow feels right because it aligns with how sound engineering should work. You're not just prompting and hoping. You're defining intent, reviewing plans, and delegating implementation.  </p>  
>   
> <p>The specifications also work well for collaboration. They're markdown files that can be checked into source control and versioned. I can see workflows where teams have validation gates on specifications and plans before implementation begins. The artifacts serve as shared understanding, not just AI context.  </p>  
>   
> <p>For resuming sessions, the specification and task files act as memory. Instead of re-explaining context to the AI, the toolkit instructs it to load the existing artifacts. This makes long-running projects more manageable.  </p>  
>   
> <p>The structured flow also enables working on multiple features in parallel. While the AI implements one feature autonomously, I can work on specifications for the next one. This pattern is emerging more broadly with tools like <a href="https://openai.com/index/introducing-codex/">OpenAI Codex</a> that explicitly support parallel task execution. I expect this to become more common. The implications cut both ways: it lets independent developers and small startups move faster with limited headcount, but it also raises questions about expectations placed on developers in corporate settings.  </p>  
>   
> <p>The flow does require discipline. It's tempting to skip straight to implementation when you think you know what you want. But ambiguity in your thinking becomes apparent when you try to write it down as a specification. That's the point. The specification phase forces clarity before you've invested in code.</p>  
>   
> <h2>  
>     
>     
>   <strong>When it's worth it</strong>  
> </h2>  
>   
> <p>Spec-kit won't eliminate all the friction from AI-assisted development. The overhead is real, and it's not worth it for every task. But for substantial features where you'd otherwise end up in a doom-prompting loop, the structured approach catches problems when they're cheap to fix.  </p>  
>   
> <p>The shift-left principle applies: review specifications, not just code. Treat AI implementation as delegation, not pair programming. Invest in the plan phase when research can improve technical direction.  </p>  
>   
> <p>If you're frustrated with vibe coding results on anything beyond weekend projects, spec-driven development is worth trying. The discipline feels familiar to anyone who's done rigorous software engineering, and the payoff is similar: smoother implementation because the thinking happened upfront.</p>

---

## [5/10] Exclusive: Researchers trick Claude plug-in into deploying ransomware - Axios
**Source:** "Anthropic" - Google News | **Date:** 2025-12-02T13:31:59.000Z
**URL:** https://news.google.com/rss/articles/CBMihwFBVV95cUxNcVFCdEpSZklaMFJoR2dZR25EeVh3WTZXS3ZwcTFiaDQwZ1NIeU9LcmFrRlJYOVFBWXlVTk9uSDYtVllaeWU1SFJyLXdyX1B4dFFybF9mQUIzcG94QzhSeDZ1R2ZJbmFSTkxZVTBCN1Y1OUlQbFp3cHlGQnpjamxZRTUxNXlvbEE?oc=5
**Reasoning:** The article discusses a security issue with Claude, which is relevant due to its impact on AI model reliability.

**Content/Abstract:**
> <a href="https://news.google.com/rss/articles/CBMihwFBVV95cUxNcVFCdEpSZklaMFJoR2dZR25EeVh3WTZXS3ZwcTFiaDQwZ1NIeU9LcmFrRlJYOVFBWXlVTk9uSDYtVllaeWU1SFJyLXdyX1B4dFFybF9mQUIzcG94QzhSeDZ1R2ZJbmFSTkxZVTBCN1Y1OUlQbFp3cHlGQnpjamxZRTUxNXlvbEE?oc=5">Exclusive: Researchers trick Claude plug-in into deploying ransomware</a>  Axios

---

## [5/10] Anthropic study says AI agents develops $4.6M in smart contract bugs - TradingView
**Source:** "Anthropic" - Google News | **Date:** 2025-12-02T12:23:17.000Z
**URL:** https://news.google.com/rss/articles/CBMiywFBVV95cUxNUVpLcDhQaU9MdHdVSkRJVldobXA4M3NZT1VqU1ZWRGtzRmpubU1RRlFZX091bTZ0Y3hmTUxmWklUMEpMVnpmZzI5WFdGZHpQN0VjaVZFSUZQcWlkRmFvT0FkeTRTaVlBM0Q4blZHM21lUUVYVFdIRkdyclVhUXE4blZaX3JPWk5rOHNrNlpIOVpRdUxvRWs1aXR2STQxaVVqY0t4TTZybmFVY1dOWnpLLTI4Z2xvRERlTGwtNDRydjdEblRfRWVHWHczYw?oc=5
**Reasoning:** The article discusses AI agents and smart contract bugs, which is somewhat relevant to AI in software engineering.

**Content/Abstract:**
> <a href="https://news.google.com/rss/articles/CBMiywFBVV95cUxNUVpLcDhQaU9MdHdVSkRJVldobXA4M3NZT1VqU1ZWRGtzRmpubU1RRlFZX091bTZ0Y3hmTUxmWklUMEpMVnpmZzI5WFdGZHpQN0VjaVZFSUZQcWlkRmFvT0FkeTRTaVlBM0Q4blZHM21lUUVYVFdIRkdyclVhUXE4blZaX3JPWk5rOHNrNlpIOVpRdUxvRWs1aXR2STQxaVVqY0t4TTZybmFVY1dOWnpLLTI4Z2xvRERlTGwtNDRydjdEblRfRWVHWHczYw?oc=5">Anthropic study says AI agents develops $4.6M in smart contract bugs</a>  TradingView

---

## [5/10] AI Killed the Full-Stack Roadmap. Here's the New System.
**Source:** The Practical Developer | **Date:** 2025-12-02T13:12:22.000Z
**URL:** https://dev.to/thinkaddict/ai-killed-the-full-stack-roadmap-heres-the-new-system-38ba
**Reasoning:** Discusses AI's impact on software engineering roles, relevant to engineering culture and trends.
**Authors:** Think Addict

**Content/Abstract:**
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fpollinations.ai%2Fp%2FSynthwave%2520vector%2520icon%2520of%2520a%2520neural%2520network%2520roadmap%252C%2520glowing%2520neon%2520lines.%2520Text%2520%2522FUTURE%2520STACK%2522%2520in%2520retro%2520pixel%2520font%2520at%2520bottom.%252C%2520retro-futuristic%2520vector%2520art%252C%2520synthwave%2520style%252C%2520neon%2520gradient%2520colors%252C%2520dark%2520background%252C%2520high%2520quality%252C%25208k%252C%2520dribbble%2520style%3Fwidth%3D1280%26height%3D720%26seed%3D988079%26model%3Dflux%26nologo%3Dtrue"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fpollinations.ai%2Fp%2FSynthwave%2520vector%2520icon%2520of%2520a%2520neural%2520network%2520roadmap%252C%2520glowing%2520neon%2520lines.%2520Text%2520%2522FUTURE%2520STACK%2522%2520in%2520retro%2520pixel%2520font%2520at%2520bottom.%252C%2520retro-futuristic%2520vector%2520art%252C%2520synthwave%2520style%252C%2520neon%2520gradient%2520colors%252C%2520dark%2520background%252C%2520high%2520quality%252C%25208k%252C%2520dribbble%2520style%3Fwidth%3D1280%26height%3D720%26seed%3D988079%26model%3Dflux%26nologo%3Dtrue" alt="Cover" width="1024" height="576"></a></p> 
>  
> <h1> 
>    
>    
>   AI Killed the Full-Stack Roadmap. Here's the New System. 
> </h1> 
>  
> <h2>The Paradox: You're Climbing a Ladder That's Already Obsolete</h2> 
>  
> <p>You’ve seen the charts. The endless tree of technologies you’re supposed to learn to call yourself “full-stack.” HTML, CSS, JavaScript, then React, then Node.js, then SQL, then NoSQL, then Docker, then Kubernetes… it never ends. But here’s the brutal truth: <b>you’re preparing for a world that no longer exists.</b> While you’re memorizing syntax, AI is building entire applications from a single prompt.</p> 
>  
> <p>The game has changed. The value is no longer in being a human code compiler. The value has shifted from writing the lines to architecting the system. Your old roadmap is a trap, designed to make you a commodity in a world that now values unique insight above all.</p> 
>  
> <h2>The Analysis: From Code Writer to System Director</h2> 
>  
> <p>The fundamental shift is this: we are moving from an era of *information retrieval* to an era of *intelligence augmentation*. Previously, a developer's worth was tied to their ability to recall specific library functions or boilerplate patterns. Stuck? You’d search Stack Overflow. Now, you ask an AI co-pilot, and it generates the code instantly.</p> 
>  
> <blockquote>The best developers are no longer code writers; they are system architects who speak human, machine, and now, AI.</blockquote> 
>  
> <p>This terrifies those who built their identity on being a syntax wizard. But it should liberate you. It frees up your cognitive bandwidth to focus on what truly matters: the architecture, the user experience, the business logic, and the security of the system as a whole. Chasing the hot new JavaScript framework is a losing battle. The frameworks will come and go, churned out faster than ever with AI assistance. Your ability to think, however, is timeless.</p> 
>  
> <h3>The New Skill Stack: Principles Over Patterns</h3> 
>  
> <ul> 
>   <li> 
> <b>Deep Fundamentals:</b> AI can write a React component, but it can’t explain the fundamental trade-offs between server-side rendering and client-side rendering for your specific use case. You must.</li> 
>   <li> 
> <b>System Design:</b> How do services communicate? How does data flow? How do you ensure scalability and resilience? This is the blueprint. AI is the construction crew.</li> 
>   <li> 
> <b>AI Literacy:</b> You don't need to build foundational models, but you must know how to wield them. This means mastering prompt engineering, understanding API integrations, and knowing the limitations of the tools.</li> 
> </ul> 
>  
> <h2>The System: The AI-Proof Developer Roadmap</h2> 
>  
> <p>Stop collecting technologies. Start building a system for thinking. This is the new path forward.</p> 
>  
> <p><b>1. Master the First Principles:</b> Don't just learn Express.js; master the HTTP protocol. Don't just learn PostgreSQL; master relational database theory. Principles are durable. Frameworks are fleeting.</p> 
>  
> <p><b>2. Think Like an Architect, Act Like a Director:</b> Spend 80% of your time on whiteboarding flows, defining data models, and planning the user journey. Use AI to generate the boilerplate and initial code. Your role is to be the editor, the curator, the director who says “no, that’s not quite right, try it this way.”</p> 
>  
> <blockquote>Stop chasing frameworks. Start mastering principles. AI can write boilerplate; it can't replicate deep understanding.</blockquote> 
>  
> <p><b>3. Build Your Leverage Stack:</b> Your stack is no longer just MERN or LAMP. It’s your brain, layered with your communication skills, your design sense, and your ability to leverage AI tools. This is your unique, uncopyable advantage. Forget the roadmap that everyone else is following. The real path is building a skill set so unique and principled that you can’t be easily replaced—by a human or a machine.</p> 
>  
>  
>  
>  
> <h3> 
>    
>    
>   🚀 Upgrade Your Mindset 
> </h3> 
>  
> <p><strong><a href="https://t.me/ThinkAddictGlobal">👉 JOIN THE SYSTEM</a></strong></p> 
>  
> <p><em>Visual by Think Addict System.</em></p>

---

## [5/10] Security Holes in MCP Servers and How To Plug Them
**Source:** The Practical Developer | **Date:** 2025-12-02T12:35:43.000Z
**URL:** https://dev.to/thenjdevopsguy/security-holes-in-mcp-servers-and-how-to-plug-them-d61
**Reasoning:** Discusses security in AI protocols, relevant to AI agent security concerns.
**Authors:** Michael Levan

**Content/Abstract:**
> <p>Model Context Protocol (MCP), has officially hit one year old as of November 25th and although there have been some amazing innovations within MCP, one issue still persists - the gaping security hole. This is no secret as just about every organization is talking about it. The long-running joke so far has been “The S in MCP stands for security”.</p> 
>  
> <p>Aside from prompt injections, MCP Server security is arguably the biggest issue in the AI security world right now.</p> 
>  
> <p>In this blog post, you’ll learn how to fix the gaps.</p> 
>  
> <h2> 
>    
>    
>   Prerequisites 
> </h2> 
>  
> <p>To follow along with this blog post, you should have:</p> 
>  
> <ol> 
> <li>A Kubernetes cluster running (it can be local).</li> 
> <li>Kubernetes Gateway API CRDs installed, which you can find <a href="https://gateway-api.sigs.k8s.io/guides/">here</a>.</li> 
> </ol> 
>  
> <h2> 
>    
>    
>   Why Security Matters For MCP 
> </h2> 
>  
> <p>There are two forms of MCP servers:</p> 
>  
> <ol> 
> <li><code>stdio</code></li> 
> <li><code>StreamableHTTP</code></li> 
> </ol> 
>  
> <p><code>stdio</code> (the communication method) stands for standard input/output. When using standard input/output, you’re typically targeting a pre-built MCP Server that’s written in, typically, Python or JS (Go is up and coming in this space) that’s called to locally via a command like <code>uvx</code> or <code>npx</code> (depending on how the MCP Server is built). It’s not a standard library download (like a <code>pip install</code>), but instead stored in cache (e.g - <code>~/.local/share/uv</code>).</p> 
>  
> <p><code>StreamableHTTP</code> is an external (or even internal) server that you’re reaching out to that’s not cached locally. A good example of this is the GitHub Copilot MCP Server. It’s an MCP Server that you’re reaching out to over the HTTP protocol instead of via a local cache.</p> 
>  
> <p>💡 There was another option called SSE which you may see, but it is now deprecated as of June 2025.</p> 
>  
> <p>Regardless of which option you choose, there are many security holes.</p> 
>  
> <p>One (of the many) problem with <code>stdio</code> MCP Servers is that you can't run them through a Gateway. That means no AuthN/Z, no rate limiting, and no tool control. The MCP Servers are effectively open and usable by anyone in an organization unless you’re manually locking each computer down with Claude Desktop configurations (which, spoiler alert: no ones doing).</p> 
>  
> <p>With Streamable HTTP, you’re in the dark. You’re connecting Agents to some black box running in someones datacenter with who knows what (if any) security protocols, and even if there are security protocols, that doesn’t help from an overall AuthN/Z perspective for your organization. There’s also no way to even test the security without a proper pentest, which wouldn’t be legal without explicit permission from the organization hosting the MCP Server. The only way to do it would be to put an Agent in front of the MCP Server, but then you're not actually securing the MCP Server, you're securing the Agent.</p> 
>  
> <p>As Model Context Protocol stands right now, there’s only one true way to secure it - with a proper AI Gateway.</p> 
>  
> <p>Solo Enterprise For Agentgateway implements everything from locking down tools to proper user and system-level authentication to MCP Servers with or without Agents. In the following sections, you’ll see how to configure security for MCP.</p> 
>  
> <h2> 
>    
>    
>   Deploy An MCP Server and Agentgateway 
> </h2> 
>  
> <p>The first step is to deploy an MCP Server and a Gateway via agentgateway enterprise so we not only have an MCP Server to test with, but a proper AI gateway to secure our MCP connectivity.</p> 
>  
> <ol> 
> <li>Create a new Kubernetes Deployment pointing to the test MCP Server that is containerized. You’ll also see a Service that gets deployed so the Gateway can properly connect to it. 
> </li> 
> </ol> 
>  
> <div> 
> <pre><code><span>kubectl</span> <span>apply</span> <span>-</span><span>f</span> <span>-</span> <span>&lt;</span><span>&lt;</span><span>EOF</span> 
> <span>apiVersion</span><span>:</span> <span>apps</span><span>/</span><span>v1</span> 
> <span>kind</span><span>:</span> <span>Deployment</span> 
> <span>metadata</span><span>:</span> 
>   <span>name</span><span>:</span> <span>mcp-website-fetcher</span> 
>   <span>namespace</span><span>:</span> <span>default</span> 
> <span>spec</span><span>:</span> 
>   <span>selector</span><span>:</span> 
>     <span>matchLabels</span><span>:</span> 
>       <span>app</span><span>:</span> <span>mcp-website-fetcher</span> 
>   <span>template</span><span>:</span> 
>     <span>metadata</span><span>:</span> 
>       <span>labels</span><span>:</span> 
>         <span>app</span><span>:</span> <span>mcp-website-fetcher</span> 
>     <span>spec</span><span>:</span> 
>       <span>containers</span><span>:</span> 
>       <span>-</span> <span>name</span><span>:</span> <span>mcp-website-fetcher</span> 
>         <span>image</span><span>:</span> <span>ghcr</span><span>.</span><span>io</span><span>/</span><span>peterj</span><span>/</span><span>mcp-website-fetcher</span><span>:</span><span>main</span> 
>         <span>imagePullPolicy</span><span>:</span> <span>Always</span> 
> <span>---</span> 
> <span>apiVersion</span><span>:</span> <span>v1</span> 
> <span>kind</span><span>:</span> <span>Service</span> 
> <span>metadata</span><span>:</span> 
>   <span>name</span><span>:</span> <span>mcp-website-fetcher</span> 
>   <span>namespace</span><span>:</span> <span>default</span> 
> <span>spec</span><span>:</span> 
>   <span>selector</span><span>:</span> 
>     <span>app</span><span>:</span> <span>mcp-website-fetcher</span> 
>   <span>ports</span><span>:</span> 
>   <span>-</span> <span>port</span><span>:</span> <span>80</span> 
>     <span>targetPort</span><span>:</span> <span>8000</span> 
>     <span>appProtocol</span><span>:</span> <span>kgateway</span><span>.</span><span>dev</span><span>/</span><span>mcp</span> 
> <span>EOF</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ol> 
> <li>Deploy the Backend so agentgateway knows what to route to. In this case, it’s routing to the MCP Server service that you deployed in step 1. 
> </li> 
> </ol> 
>  
> <div> 
> <pre><code><span>kubectl</span> <span>apply</span> <span>-</span><span>f</span> <span>-</span> <span>&lt;</span><span>&lt;</span><span>EOF</span> 
> <span>apiVersion</span><span>:</span> <span>gateway</span><span>.</span><span>kgateway</span><span>.</span><span>dev</span><span>/</span><span>v1alpha1</span> 
> <span>kind</span><span>:</span> <span>Backend</span> 
> <span>metadata</span><span>:</span> 
>   <span>name</span><span>:</span> <span>mcp-backend</span> 
>   <span>namespace</span><span>:</span> <span>gloo-system</span> 
> <span>spec</span><span>:</span> 
>   <span>type</span><span>:</span> <span>MCP</span> 
>   <span>mcp</span><span>:</span> 
>     <span>targets</span><span>:</span> 
>     <span>-</span> <span>name</span><span>:</span> <span>mcp-target</span> 
>       <span>static</span><span>:</span> 
>         <span>host</span><span>:</span> <span>mcp-website-fetcher</span><span>.</span><span>default</span><span>.</span><span>svc</span><span>.</span><span>cluster</span><span>.</span><span>local</span> 
>         <span>port</span><span>:</span> <span>80</span> 
>         <span>protocol</span><span>:</span> <span>StreamableHTTP</span> 
> <span>EOF</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ol> 
> <li>Deploy the Gateway using the agentgateway enterprise class. This Gateway is what will be used for MCP Inspector to connect to (more on Inspector coming up). 
> </li> 
> </ol> 
>  
> <div> 
> <pre><code><span>kubectl</span> <span>apply</span> <span>-</span><span>f</span> <span>-</span> <span>&lt;</span><span>&lt;</span><span>EOF</span> 
> <span>apiVersion</span><span>:</span> <span>gateway</span><span>.</span><span>networking</span><span>.</span><span>k8s</span><span>.</span><span>io</span><span>/</span><span>v1</span> 
> <span>kind</span><span>:</span> <span>Gateway</span> 
> <span>metadata</span><span>:</span> 
>   <span>name</span><span>:</span> <span>agentgateway</span> 
>   <span>namespace</span><span>:</span> <span>gloo-system</span> 
> <span>spec</span><span>:</span> 
>   <span>gatewayClassName</span><span>:</span> <span>agentgateway-enterprise</span> 
>   <span>listeners</span><span>:</span> 
>   <span>-</span> <span>name</span><span>:</span> <span>http</span> 
>     <span>port</span><span>:</span> <span>8080</span> 
>     <span>protocol</span><span>:</span> <span>HTTP</span> 
>     <span>allowedRoutes</span><span>:</span> 
>       <span>namespaces</span><span>:</span> 
>         <span>from</span><span>:</span> <span>Same</span> 
> <span>EOF</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ol> 
> <li>Create an HTTP route so you can route the traffic to the backend, which you created in step 3. 
> </li> 
> </ol> 
>  
> <div> 
> <pre><code><span>kubectl</span> <span>apply</span> <span>-</span><span>f</span> <span>-</span> <span>&lt;</span><span>&lt;</span><span>EOF</span> 
> <span>apiVersion</span><span>:</span> <span>gateway</span><span>.</span><span>networking</span><span>.</span><span>k8s</span><span>.</span><span>io</span><span>/</span><span>v1</span> 
> <span>kind</span><span>:</span> <span>HTTPRoute</span> 
> <span>metadata</span><span>:</span> 
>   <span>name</span><span>:</span> <span>mcp-route</span> 
>   <span>namespace</span><span>:</span> <span>gloo-system</span> 
> <span>spec</span><span>:</span> 
>   <span>parentRefs</span><span>:</span> 
>   <span>-</span> <span>name</span><span>:</span> <span>agentgateway</span> 
>   <span>rules</span><span>:</span> 
>   <span>-</span> <span>backendRefs</span><span>:</span> 
>     <span>-</span> <span>name</span><span>:</span> <span>mcp-backend</span> 
>       <span>group</span><span>:</span> <span>gateway</span><span>.</span><span>kgateway</span><span>.</span><span>dev</span> 
>       <span>kind</span><span>:</span> <span>Backend</span> 
> <span>EOF</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ol> 
> <li>Capture the Gateway ALB IP in an environment variable to be used when connecting to the MCP Server. If you’re running this locally, you can do a <code>port-forward</code> on the Gateway and use <a href="http://localhost"><code>localhost</code></a> within the IP address section when connecting to it. 
> </li> 
> </ol> 
>  
> <div> 
> <pre><code><span>export</span> <span>GATEWAY_IP</span><span>=</span><span>$</span><span>(</span><span>kubectl</span> <span>get</span> <span>svc</span> <span>agentgateway</span> <span>-</span><span>n</span> <span>gloo</span><span>-</span><span>system</span> <span>-</span><span>o</span> <span>jsonpath</span><span>=</span><span>'</span><span>{.status.loadBalancer.ingress[0].ip}</span><span>'</span><span>)</span> 
> <span>echo</span> <span>$GATEWAY_IP</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ol> 
> <li>Open MCP Inspector to connect to the MCP Server. 
> </li> 
> </ol> 
>  
> <div> 
> <pre><code><span>npx</span> <span>modelcontextprotocol</span><span>/</span><span>inspector</span><span>#</span><span>0.16</span><span>.</span><span>2</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>The URL to put into MCP Inspector is: <code>http://YOUR_ALB_LB_IP:8080/mcp</code> or if you’re running locally, <code>http://localhost:8080/mcp</code></p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvc0yf8ylrnhjeny31q8z.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvc0yf8ylrnhjeny31q8z.png" alt="" width="800" height="351"></a></p> 
>  
> <p>You’re now connected to the MCP Server via Inspector (the MCP client), but as you can see, it’s fully open. There’s no security at all. In the next section, that’ll be fixed.</p> 
>  
> <h2> 
>    
>    
>   Secure MCP Server Auth 
> </h2> 
>  
> <p>With a properly deployed MCP Server and agentgateway in front of it, let’s begin the journey of securing MCP Server connectivity. The first step is to enable token-based authentication. In this case, you’ll use a JWT token.</p> 
>  
> <ol> 
> <li>Add in a traffic policy for auth based on a JWT token. 
> </li> 
> </ol> 
>  
> <div> 
> <pre><code><span>kubectl</span> <span>apply</span> <span>-</span><span>f</span><span>-</span> <span>&lt;</span><span>&lt;</span><span>EOF</span> 
> <span>apiVersion</span><span>:</span> <span>gloo</span><span>.</span><span>solo</span><span>.</span><span>io</span><span>/</span><span>v1alpha1</span> 
> <span>kind</span><span>:</span> <span>GlooTrafficPolicy</span> 
> <span>metadata</span><span>:</span> 
>   <span>name</span><span>:</span> <span>jwt</span> 
>   <span>namespace</span><span>:</span> <span>gloo-system</span> 
> <span>spec</span><span>:</span> 
>   <span>targetRefs</span><span>:</span> 
>     <span>-</span> <span>group</span><span>:</span> <span>gateway</span><span>.</span><span>networking</span><span>.</span><span>k8s</span><span>.</span><span>io</span> 
>       <span>kind</span><span>:</span> <span>Gateway</span> 
>       <span>name</span><span>:</span> <span>agentgateway</span> 
>   <span>glooJWT</span><span>:</span> 
>     <span>beforeExtAuth</span><span>:</span> 
>       <span>providers</span><span>:</span> 
>         <span>selfminted</span><span>:</span> 
>           <span>issuer</span><span>:</span> <span>solo</span><span>.</span><span>io</span> 
>           <span>jwks</span><span>:</span> 
>             <span>local</span><span>:</span> 
>               <span>key</span><span>:</span> <span>'{"keys":[{"kty":"RSA","kid":"solo-public-key-001","use":"sig","alg":"RS256","n":"AOfIaJMUm7564sWWNHaXt_hS8H0O1Ew59-nRqruMQosfQqa7tWne5lL3m9sMAkfa3Twx0LMN_7QqRDoztvV3Wa_JwbMzb9afWE-IfKIuDqkvog6s-xGIFNhtDGBTuL8YAQYtwCF7l49SMv-GqyLe-nO9yJW-6wIGoOqImZrCxjxXFzF6mTMOBpIODFj0LUZ54QQuDcD1Nue2LMLsUvGa7V1ZHsYuGvUqzvXFBXMmMS2OzGir9ckpUhrUeHDCGFpEM4IQnu-9U8TbAJxKE5Zp8Nikefr2ISIG2Hk1K2rBAc_HwoPeWAcAWUAR5tWHAxx-UXClSZQ9TMFK850gQGenUp8","e":"AQAB"}]}'</span> 
> <span>EOF</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ol> 
> <li>Save the token for auth via MCP Inspector. 
> </li> 
> </ol> 
>  
> <div> 
> <pre><code><span>eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InNvbG8tcHVibGljLWtleS0wMDEifQ</span><span>.</span><span>eyJpc3MiOiJzb2xvLmlvIiwib3JnIjoic29sby5pbyIsInN1YiI6ImJvYiIsInRlYW0iOiJvcHMiLCJleHAiOjIwNzQyNzQ5NTQsImxsbXMiOnsibWlzdHJhbGFpIjpbIm1pc3RyYWwtbGFyZ2UtbGF0ZXN0Il19fQ</span><span>.</span><span>GF_uyLpZSTT1DIvJeO_eish1WDjMaS4BQSifGQhqPRLjzu3nXtPkaBRjceAmJi9gKZYAzkT25MIrT42ZIe3bHilrd1yqittTPWrrM4sWDDeldnGsfU07DWJHyboNapYR</span><span>-</span><span>KZGImSmOYshJlzm1tT_Bjt3</span><span>-</span><span>RK3OBzYi90_wl0dyAl9D7wwDCzOD4MRGFpoMrws_OgVrcZQKcadvIsH8figPwN4mK1U_1mxuL08RWTu92xBcezEO4CdBaFTUbkYN66Y2vKSTyPCxg3fLtg1mvlzU1</span><span>-</span><span>Wgm2xZIiPiarQHt6Uq7v9ftgzwdUBQM1AYLvUVhCN6XkkR9OU3p0OXiqEDjAxcg</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ol> 
> <li>Try to reconnect to the MCP Server and you’ll see an error similar to the below: 
> </li> 
> </ol> 
>  
> <div> 
> <pre><code><span>Connection</span> <span>Error</span> <span>-</span> <span>Check</span> <span>if</span> <span>your</span> <span>MCP</span> <span>server</span> <span>is</span> <span>running</span> <span>and</span> <span>proxy</span> <span>token</span> <span>is</span> <span>correct</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ol> 
> <li>Within MCP Inspector, click on <strong>Authentication</strong> and add in the following:</li> 
> <li>Header Name: <strong>Authorization</strong> 
> </li> 
> <li>Bearer Token: Bobs Token from step 2</li> 
> </ol> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ffus385h83g5xo9n07hfg.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ffus385h83g5xo9n07hfg.png" alt="" width="384" height="644"></a></p> 
>  
> <p>You should now be able to connect to the MCP Server successfully.</p> 
>  
> <p>With proper auth set up, you now know that not just anyone can use your agentgateway to connect to an MCP Server. This allows you to ensure that the traffic you’re observing from an AuthN/Z perspective is valid in comparison to the “anyone can do whatever they want” nature of MCP Servers without agentgateway in place.</p> 
>  
> <h2> 
>    
>    
>   Locking Down MCP Tool Lists 
> </h2> 
>  
> <p>The final step is to specify what MCP Tools are available. One of the main issues for organizations is they want to use Tools within an MCP Server, but not all Tools. For example, maybe a person or an AI Agent connecting to an MCP Server only needs the ability to view/list/get resources (like a readonly Agent), but with the current architecture out of the box available for MCP, that’s not doable.</p> 
>  
> <p>However, with traffic policies via agentgateway, it is.</p> 
>  
> <ol> 
> <li>Create a policy that specifies no tools available for use. This will help in testing the ability to lock down tools via the Gateway. 
> </li> 
> </ol> 
>  
> <div> 
> <pre><code><span>kubectl</span> <span>apply</span> <span>-</span><span>f</span><span>-</span> <span>&lt;</span><span>&lt;</span><span>EOF</span> 
> <span>apiVersion</span><span>:</span> <span>gateway</span><span>.</span><span>kgateway</span><span>.</span><span>dev</span><span>/</span><span>v1alpha1</span> 
> <span>kind</span><span>:</span> <span>TrafficPolicy</span> 
> <span>metadata</span><span>:</span> 
>   <span>name</span><span>:</span> <span>jwt-rbac</span> 
>   <span>namespace</span><span>:</span> <span>gloo-system</span> 
> <span>spec</span><span>:</span> 
>   <span>targetRefs</span><span>:</span> 
>     <span>-</span> <span>group</span><span>:</span> <span>gateway</span><span>.</span><span>kgateway</span><span>.</span><span>dev</span> 
>       <span>kind</span><span>:</span> <span>Backend</span> 
>       <span>name</span><span>:</span> <span>mcp-backend</span> 
>   <span>rbac</span><span>:</span> 
>     <span>policy</span><span>:</span> 
>       <span>matchExpressions</span><span>:</span> 
>         <span>-</span> <span>'mcp.tool.name == ""'</span> 
> <span>EOF</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ol> 
> <li>Disconnect and reconnect via the MCP Inspector and you should see no tools available.</li> 
> </ol> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxf7ay60kp3tgiijb26j1.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxf7ay60kp3tgiijb26j1.png" alt="" width="800" height="457"></a></p> 
>  
> <ol> 
> <li>Update the policy to include the <strong>fetch</strong> tool. 
> </li> 
> </ol> 
>  
> <div> 
> <pre><code><span>kubectl</span> <span>apply</span> <span>-</span><span>f</span><span>-</span> <span>&lt;</span><span>&lt;</span><span>EOF</span> 
> <span>apiVersion</span><span>:</span> <span>gateway</span><span>.</span><span>kgateway</span><span>.</span><span>dev</span><span>/</span><span>v1alpha1</span> 
> <span>kind</span><span>:</span> <span>TrafficPolicy</span> 
> <span>metadata</span><span>:</span> 
>   <span>name</span><span>:</span> <span>jwt-rbac</span> 
>   <span>namespace</span><span>:</span> <span>gloo-system</span> 
> <span>spec</span><span>:</span> 
>   <span>targetRefs</span><span>:</span> 
>     <span>-</span> <span>group</span><span>:</span> <span>gateway</span><span>.</span><span>kgateway</span><span>.</span><span>dev</span> 
>       <span>kind</span><span>:</span> <span>Backend</span> 
>       <span>name</span><span>:</span> <span>mcp-backend</span> 
>   <span>rbac</span><span>:</span> 
>     <span>policy</span><span>:</span> 
>       <span>matchExpressions</span><span>:</span> 
>         <span>-</span> <span>'mcp.tool.name == "fetch"'</span> 
> <span>EOF</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ol> 
> <li>Reconnect to the MCP Server via Inspector and you’ll now see the tool available.</li> 
> </ol> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F1bjmw9ea661dyxwo1hb8.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F1bjmw9ea661dyxwo1hb8.png" alt="" width="800" height="351"></a></p> 
>  
> <h2> 
>    
>    
>   Conclusion 
> </h2> 
>  
> <p>With all of the security concerns around MCP Servers, it reminds us of a very important aspect of cyber security - it’s not about trying to block all bad actors, it’s about mitigating as much risk as possible. That should be the goal for every organization and with these implementations, your MCP security posture should be in a much better place.</p>

---

## [5/10] ‘The biggest decision yet’: Jared Kaplan on allowing AI to train itself - The Guardian
**Source:** "Anthropic" - Google News | **Date:** 2025-12-02T12:37:00.000Z
**URL:** https://news.google.com/rss/articles/CBMitgFBVV95cUxQR3htZDNHeEE0aWl3cGMtbU1mVnV4TDY4QjllanlEODAwSFdfYWZybVFXdjFWVmxpeGZvVGlwUENwd1JjU1VXcW5tMVYxRGVTclB1MnFKcnMzVDRxWTNHRnBnT1RFNEl2UjJYRkdpcU1hb2JFNkt1ZWF2a1hNLThYNVRmU1I4eTRrVTFSSTlMTDFxRnNZUE5PNVZsM01QeFo5TDYwb0dOb3JYdkEtZlRoWFB4aDdLQQ?oc=5
**Reasoning:** Discusses AI self-training, relevant to general AI news and trends.

**Content/Abstract:**
> <a href="https://news.google.com/rss/articles/CBMitgFBVV95cUxQR3htZDNHeEE0aWl3cGMtbU1mVnV4TDY4QjllanlEODAwSFdfYWZybVFXdjFWVmxpeGZvVGlwUENwd1JjU1VXcW5tMVYxRGVTclB1MnFKcnMzVDRxWTNHRnBnT1RFNEl2UjJYRkdpcU1hb2JFNkt1ZWF2a1hNLThYNVRmU1I4eTRrVTFSSTlMTDFxRnNZUE5PNVZsM01QeFo5TDYwb0dOb3JYdkEtZlRoWFB4aDdLQQ?oc=5">‘The biggest decision yet’: Jared Kaplan on allowing AI to train itself</a>  The Guardian

---

## [5/10] Why Anthropic’s Claude Is the Co-Founder of the Year - Inc.com
**Source:** "Anthropic" - Google News | **Date:** 2025-12-02T12:07:25.000Z
**URL:** https://news.google.com/rss/articles/CBMilgFBVV95cUxORWVsaTJZZzQ1NkZsVEEtUlk0WFVnQXZjcHM2Z0hlTlN6QXF2NzJYdU12RWpRRlRKVWdSdXNLbjVXSWM5TGRxVEp6MHBHUGxCeXNHZV9CMU1kRHBpOXZIN0FIbExtTV84dTBqaGRnUEZuQnhHOEp1SncxOVZNMlNPdHVYaXVMOHZuaGdXVWY4UjlYN2ZYSnc?oc=5
**Reasoning:** Discusses a major figure in AI, relevant to general AI news.

**Content/Abstract:**
> <a href="https://news.google.com/rss/articles/CBMilgFBVV95cUxORWVsaTJZZzQ1NkZsVEEtUlk0WFVnQXZjcHM2Z0hlTlN6QXF2NzJYdU12RWpRRlRKVWdSdXNLbjVXSWM5TGRxVEp6MHBHUGxCeXNHZV9CMU1kRHBpOXZIN0FIbExtTV84dTBqaGRnUEZuQnhHOEp1SncxOVZNMlNPdHVYaXVMOHZuaGdXVWY4UjlYN2ZYSnc?oc=5">Why Anthropic’s Claude Is the Co-Founder of the Year</a>  Inc.com

---

## [5/10] AI agents pose immediate threat to smart contract security, Anthropic says - theblock.co
**Source:** "Anthropic" - Google News | **Date:** 2025-12-02T08:37:49.000Z
**URL:** https://news.google.com/rss/articles/CBMieEFVX3lxTE50Y3NQS05qaW5nUExHVlR0RWxoY2htcHVvNUpESUt3b0xGWEEwOWRyZzk3U1hvYi1oMEJNTDNTbjlQQTE4Njk5RlJ5cHAxU3VfdU5udGxma203ZWllUXNVS0dZVzNWQkNSQlNkdkplVmZjeDVHdjU5NtIBfkFVX3lxTFBlZEljLUpJampUZU42VV9iRlBJWllFNnJUR0tLbFM1M3FmaEJCM0RVR05pNFBEM3RtREg4OERzWDRoQnRsVFhobEpGUVlJNE1jS3RZZDZpWl9mY2JhZUdwZldvaXR2Y2VrcWJYeWRpcWkxUU5zVThwcXFGVURWZw?oc=5
**Reasoning:** Discusses AI agents' impact on security, relevant to AI agent security concerns.

**Content/Abstract:**
> <a href="https://news.google.com/rss/articles/CBMieEFVX3lxTE50Y3NQS05qaW5nUExHVlR0RWxoY2htcHVvNUpESUt3b0xGWEEwOWRyZzk3U1hvYi1oMEJNTDNTbjlQQTE4Njk5RlJ5cHAxU3VfdU5udGxma203ZWllUXNVS0dZVzNWQkNSQlNkdkplVmZjeDVHdjU5NtIBfkFVX3lxTFBlZEljLUpJampUZU42VV9iRlBJWllFNnJUR0tLbFM1M3FmaEJCM0RVR05pNFBEM3RtREg4OERzWDRoQnRsVFhobEpGUVlJNE1jS3RZZDZpWl9mY2JhZUdwZldvaXR2Y2VrcWJYeWRpcWkxUU5zVThwcXFGVURWZw?oc=5">AI agents pose immediate threat to smart contract security, Anthropic says</a>  theblock.co

---

## [5/10] From Vibe Coding to Spec-Driven Development that Slashed Dev Time
**Source:** The Practical Developer | **Date:** 2025-12-02T11:16:54.000Z
**URL:** https://dev.to/amarpreetbhatia/from-vibe-coding-to-spec-driven-development-that-slashed-dev-time-3f1f
**Reasoning:** The article discusses spec-driven development, which is somewhat related to engineering culture and trends.
**Authors:** amarpreetbhatia

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fk9wstkp8j5vg86wfh6dr.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><h2>  
>     
>     
>   🚀 From 'Vibe' to Victory: How Spec-Driven Development in Kiro IDE Slashed My Dev Time by 44%  
> </h2>  
>   
> <p><strong>A deep dive into building a full-featured Agile Planning app with zero WebSocket bugs.</strong></p>  
>   
>   
>   
>   
> <p>When I first used Kiro a few months ago, I was focused entirely on <strong>vibe coding</strong>. It was fun, but chaotic. Now, after experimenting with <strong>spec-driven development (SDD)</strong>, I'm completely blown away! I strongly suspect Kiro pioneered this structured approach, which was then adopted by other coding agents.</p>  
>   
> <p>This time, I built a full-featured Agile Planning application with real-time collaboration, GitHub integration, and a collaborative whiteboard. The result?</p>  
>   
> <blockquote>  
> <p>Complex features like authentication, WebSocket management, and drag-and-drop became <strong>surprisingly straightforward</strong>.</p>  
> </blockquote>  
>   
> <p>Here's exactly why the structured <strong>"Plan, Review, Execute"</strong> model beats "vibe coding" every single time, backed by concrete numbers from the project.</p>  
>   
>   
>   
>   
> <h3>  
>     
>     
>   🤯 The Two Worlds of Coding  
> </h3>  
>   
> <h4>  
>     
>     
>   What is "Vibe Coding"?  
> </h4>  
>   
> <p><em>You know the drill:</em></p>  
>   
> <ol>  
> <li> Open your editor.</li>  
> <li> Start typing code.</li>  
> <li> "I'll figure it out as I go."</li>  
> <li> Realize you forgot crucial edge cases.</li>  
> <li> Refactor everything (or leave it messy).</li>  
> <li> Repeat.</li>  
> </ol>  
>   
> <p>It <strong>feels</strong> productive, but it's chaos in disguise. It's building a skyscraper without a blueprint.</p>  
>   
> <h4>  
>     
>     
>   Enter: Spec-Driven Development (SDD)  
> </h4>  
>   
> <p>SDD follows a rigorous, structured approach built directly into the <strong>Kiro IDE</strong>:</p>  
>   
> <ol>  
> <li> <strong>Plan:</strong> Write detailed requirements and acceptance criteria.</li>  
> <li> <strong>Review:</strong> Create a comprehensive design document (architecture, data models, properties).</li>  
> <li> <strong>Execute:</strong> Implement with clear tasks and continuous validation against the spec.</li>  
> </ol>  
>   
> <p>I used Kiro IDE, which has built-in support for this workflow, and the difference was <strong>night and day.</strong></p>  
>   
>   
>   
>   
> <h3>  
>     
>     
>   🎯 The Project: Agile Planning Tool  
> </h3>  
>   
> <p>I built a real-time collaborative planning poker application.</p>  
>   
> <p><strong><a href="https://github.com/amarpreetbhatia/agile-planning-tool">View the Code: Agile Planning Tool on GitHub</a></strong></p>  
>   
> <div><table>  
> <thead>  
> <tr>  
> <th>Core Features Implemented</th>  
> <th>Tech Stack</th>  
> </tr>  
> </thead>  
> <tbody>  
> <tr>  
> <td>  
> <strong>Real-time</strong> Planning Poker</td>  
> <td>  
> <strong>Next.js 14</strong> (App Router)</td>  
> </tr>  
> <tr>  
> <td>  
> <strong>GitHub</strong> OAuth Authentication</td>  
> <td><strong>TypeScript</strong></td>  
> </tr>  
> <tr>  
> <td>  
> <strong>WebSocket</strong> Collaboration</td>  
> <td>  
> <strong>MongoDB</strong> + Mongoose</td>  
> </tr>  
> <tr>  
> <td>Drag-and-drop Story Management</td>  
> <td><strong>Socket.IO</strong></td>  
> </tr>  
> <tr>  
> <td><strong>Collaborative Whiteboard</strong></td>  
> <td>  
> <strong>Playwright</strong> (E2E Tests)</td>  
> </tr>  
> <tr>  
> <td>External Tool Embedding (Miro, Figma)</td>  
> <td><strong>Shadcn UI</strong></td>  
> </tr>  
> </tbody>  
> </table></div>  
>   
>   
>   
>   
> <h3>  
>     
>     
>   💡 1. Why Spec-Driven Development Won: Complex Features Became Simple  
> </h3>  
>   
> <p>SDD forces you to think through the entire system <em>before</em> writing the first line of code.</p>  
>   
> <h4>  
>     
>     
>   A. Authentication (GitHub OAuth)  
> </h4>  
>   
> <div><table>  
> <thead>  
> <tr>  
> <th>Vibe Coding</th>  
> <th>Spec-Driven Development</th>  
> </tr>  
> </thead>  
> <tbody>  
> <tr>  
> <td>  
> <strong>Approach:</strong> "I'll just add GitHub login... wait, how do I handle tokens? What about session management? Encryption?"</td>  
> <td>  
> <strong>Approach:</strong> The spec demands defining security up front.</td>  
> </tr>  
> <tr>  
> <td>  
> <strong>Result:</strong> 2 days of debugging and insecure token handling.</td>  
> <td>  
> <strong>Result:</strong> Implementation took <strong>2 hours</strong> instead of 2 days of debugging.</td>  
> </tr>  
> </tbody>  
> </table></div>  
>   
> <p><strong>Key Requirement: Authentication</strong></p>  
>   
> <blockquote>  
> <p>Acceptance Criteria:<br>  
> 3. THE System <strong>SHALL</strong> store <strong>encrypted</strong> access tokens in the database.</p>  
> </blockquote>  
>   
> <h4>  
>     
>     
>   B. Real-Time Collaboration  
> </h4>  
>   
> <div><table>  
> <thead>  
> <tr>  
> <th>Vibe Coding</th>  
> <th>Spec-Driven Development</th>  
> </tr>  
> </thead>  
> <tbody>  
> <tr>  
> <td>  
> <strong>Approach:</strong> "Let's add Socket.IO... oh wait, how do I handle disconnections? What about race conditions?"</td>  
> <td>  
> <strong>Approach:</strong> The spec forces planning for high-availability.</td>  
> </tr>  
> <tr>  
> <td>  
> <strong>Result:</strong> Production bugs related to race conditions and dropped connections.</td>  
> <td>  
> <strong>Result:</strong> <strong>Zero production bugs</strong> related to WebSocket management.</td>  
> </tr>  
> </tbody>  
> </table></div>  
>   
> <p><strong>Key Requirement: Real-Time Updates</strong></p>  
>   
> <blockquote>  
> <p>Acceptance Criteria:<br>  
> 3. WHEN connection drops, THE System <strong>SHALL</strong> attempt reconnection with <strong>exponential backoff</strong>.</p>  
> </blockquote>  
>   
> <h4>  
>     
>     
>   C. Drag-and-Drop Story Management  
> </h4>  
>   
> <div><table>  
> <thead>  
> <tr>  
> <th>Vibe Coding</th>  
> <th>Spec-Driven Development</th>  
> </tr>  
> </thead>  
> <tbody>  
> <tr>  
> <td>  
> <strong>Approach:</strong> "I'll use a library... which one? How do I persist order? What about conflicts?"</td>  
> <td>  
> <strong>Approach:</strong> The spec explicitly defines conflict resolution.</td>  
> </tr>  
> <tr>  
> <td>  
> <strong>Result:</strong> Janky UX and data integrity issues upon concurrent edits.</td>  
> <td>  
> <strong>Result:</strong> Smooth drag-and-drop with <strong>zero conflicts</strong>.</td>  
> </tr>  
> </tbody>  
> </table></div>  
>   
> <p><strong>Key Requirement: Story Backlog</strong></p>  
>   
> <blockquote>  
> <p>Acceptance Criteria:<br>  
> 4. THE System <strong>SHALL</strong> handle concurrent reordering conflicts gracefully.</p>  
> </blockquote>  
>   
>   
>   
>   
> <h3>  
>     
>     
>   📊 2. The Numbers: A Quantitative Victory  
> </h3>  
>   
> <p>The biggest win was the dramatic reduction in debugging and refactoring time. <strong>Upfront planning saves time, period.</strong></p>  
>   
> <h4>  
>     
>     
>   Development Time Comparison  
> </h4>  
>   
> <div><table>  
> <thead>  
> <tr>  
> <th>Approach</th>  
> <th>Planning</th>  
> <th>Coding</th>  
> <th>Debugging</th>  
> <th>Refactoring</th>  
> <th><strong>Total</strong></th>  
> </tr>  
> </thead>  
> <tbody>  
> <tr>  
> <td><strong>Vibe Coding</strong></td>  
> <td>0h</td>  
> <td>40h</td>  
> <td>30h</td>  
> <td>20h</td>  
> <td><strong>90h</strong></td>  
> </tr>  
> <tr>  
> <td><strong>Spec-Driven</strong></td>  
> <td>7h</td>  
> <td>35h</td>  
> <td><strong>5h</strong></td>  
> <td><strong>3h</strong></td>  
> <td><strong>50h</strong></td>  
> </tr>  
> <tr>  
> <td><strong>Time Saved</strong></td>  
> <td></td>  
> <td></td>  
> <td></td>  
> <td></td>  
> <td><strong>40 hours (44%)</strong></td>  
> </tr>  
> </tbody>  
> </table></div>  
>   
> <h4>  
>     
>     
>   Code Quality Metrics  
> </h4>  
>   
> <div><table>  
> <thead>  
> <tr>  
> <th>Metric</th>  
> <th>Vibe Coding (Hypothetical)</th>  
> <th>Spec-Driven (Actual Project)</th>  
> </tr>  
> </thead>  
> <tbody>  
> <tr>  
> <td>Production Bugs</td>  
> <td>15+</td>  
> <td><strong>2</strong></td>  
> </tr>  
> <tr>  
> <td>Major Refactors Needed</td>  
> <td>5 major</td>  
> <td><strong>0 major</strong></td>  
> </tr>  
> <tr>  
> <td>Test Coverage</td>  
> <td>40%</td>  
> <td><strong>85%</strong></td>  
> </tr>  
> <tr>  
> <td>Documentation</td>  
> <td>Sparse and outdated</td>  
> <td>Comprehensive (The Spec IS the documentation)</td>  
> </tr>  
> </tbody>  
> </table></div>  
>   
>   
>   
>   
> <h3>  
>     
>     
>   ✍️ 3. Testing Became Natural  
> </h3>  
>   
> <p>Because every major feature had pre-written <strong>Acceptance Criteria</strong> (AC), writing E2E tests was a mere translation process.</p>  
>   
> <p>An AC like: <em>“WHEN GitHub OAuth succeeds, THE System SHALL create a user session with GitHub profile data.”</em> directly translates into a test block.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>test</span><span>(</span><span>'</span><span>should complete full estimation workflow</span><span>'</span><span>,</span> <span>async </span><span>({</span> <span>page</span> <span>})</span> <span>=&gt;</span> <span>{</span>  
>   <span>// Requirement 2.1: Create session</span>  
>   <span>await</span> <span>page</span><span>.</span><span>click</span><span>(</span><span>'</span><span>button:has-text("New Session")</span><span>'</span><span>);</span>  
>   
>   <span>// Requirement 6.3: Cast vote</span>  
>   <span>await</span> <span>page</span><span>.</span><span>click</span><span>(</span><span>'</span><span>[data-testid="poker-card-5"]</span><span>'</span><span>);</span>  
>   
>   <span>// Verify: Average calculated correctly</span>  
>   <span>await</span> <span>expect</span><span>(</span><span>page</span><span>.</span><span>locator</span><span>(</span><span>'</span><span>[data-testid="average"]</span><span>'</span><span>))</span>  
>     <span>.</span><span>toContainText</span><span>(</span><span>'</span><span>5</span><span>'</span><span>);</span>  
> <span>});</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p><strong>Result:</strong> 20+ E2E tests written with Playwright, providing high confidence and 100% passing smoke tests before deployment.</p>  
>   
>   
>   
>   
> <h3>  
>     
>     
>   🛠️ Tools That Made It Possible  
> </h3>  
>   
> <p>The built-in SDD support in <strong>Kiro IDE</strong> was the catalyst:</p>  
>   
> <ul>  
> <li>  
> <strong>Structured Spec Creation:</strong> Used pre-built templates for requirements and design documents.</li>  
> <li>  
> <strong>Requirement Linking:</strong> Ability to link code commits and tasks directly back to the specific requirement they fulfill.</li>  
> <li>  
> <strong>Test Integration:</strong> Running tests directly from the spec view to validate acceptance criteria.</li>  
> </ul>  
>   
>   
>   
>   
> <h3>  
>     
>     
>   🛑 Common Objections Answered  
> </h3>  
>   
> <div><table>  
> <thead>  
> <tr>  
> <th>Objection</th>  
> <th>Reality</th>  
> </tr>  
> </thead>  
> <tbody>  
> <tr>  
> <td><em>"Specs take too long to write."</em></td>  
> <td>  
> <strong>False:</strong> 7 hours of planning saved 40+ hours of debugging and rework. That's a huge ROI.</td>  
> </tr>  
> <tr>  
> <td><em>"Requirements change anyway."</em></td>  
> <td>  
> <strong>False:</strong> Specs make changes <em>easier</em>. You know exactly which linked components need updating.</td>  
> </tr>  
> <tr>  
> <td><em>"This only works for big projects."</em></td>  
> <td>  
> <strong>False:</strong> Even a quick, 30-minute spec for a small feature saves hours of rework and edge-case fixing.</td>  
> </tr>  
> </tbody>  
> </table></div>  
>   
>   
>   
>   
> <h3>  
>     
>     
>   ✅ Final Thoughts  
> </h3>  
>   
> <p>After building this project, I can't imagine going back to "vibe coding" for anything serious. The structure, clarity, and confidence that spec-driven development provides is invaluable.</p>  
>   
> <p>Vibe coding <strong>feels fast</strong> but is ultimately <strong>slow</strong>.<br>  
> Spec-driven development <strong>feels slow</strong> but is ultimately <strong>fast</strong>.</p>  
>   
> <p><strong>The Bottom Line:</strong> Give the "Plan, Review, Execute" model a try on your next project. Your future self will thank you for the robust architecture, reduced bug count, and the sheer predictability of development.</p>  
>   
>   
>   
>   
> <p><em>What's your experience with structured development approaches? Have you tried spec-driven development? Let me know in the comments!</em></p>  
>   
> <p>#agile #typescript #testing #productivity #webdev #nextjs #react #mongodb #playwright #softwaredevelopment</p>

---

## [5/10] EKS Standard vs. EKS Auto Mode: The Evolutionary Leap in Kubernetes Operations
**Source:** The Practical Developer | **Date:** 2025-12-02T11:09:34.000Z
**URL:** https://dev.to/mechcloud_academy/eks-standard-vs-eks-auto-mode-the-evolutionary-leap-in-kubernetes-operations-287g
**Reasoning:** The article discusses Kubernetes operations, which is somewhat relevant to infrastructure trends.
**Authors:** Akash

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F3q5eilympx9819derull.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>For years, Amazon Elastic Kubernetes Service (EKS) has been the gold standard for running containerized workloads on AWS. But let’s be honest: while EKS managed the Control Plane beautifully, the <strong>Data Plane</strong> (the worker nodes) remained a significant operational burden.</p>  
>   
> <p>As Platform Engineers and SREs, we’ve spent countless hours tuning Managed Node Groups (MNGs), debugging CNI plugin versions, wrestling with IAM Roles for Service Accounts (IRSA), and fine-tuning Karpenter to get our bin-packing logic just right.</p>  
>   
> <p>With the release of <strong>EKS Auto Mode</strong>, AWS has fundamentally shifted the Shared Responsibility Model.</p>  
>   
> <p>This isn't just a minor feature update; it is a fork in the road for how we architect clusters. This guide will dissect the architectural differences between EKS Standard and Auto Mode, analyze the "under-the-hood" mechanics, and help you decide which path to take.</p>  
>   
> <h2>  
>     
>     
>   The "Standard" Way: Maximum Control, Maximum Toil  
> </h2>  
>   
> <p>In what we now call <strong>EKS Standard</strong>, the division of labor is clear but uneven. AWS ensures the API server is up, but the moment a packet leaves the control plane, it’s your problem.</p>  
>   
> <h3>  
>     
>     
>   The Standard Architecture  
> </h3>  
>   
> <p>In a standard cluster, you are the architect of the infrastructure layer:</p>  
>   
> <ol>  
> <li> <strong>Compute:</strong> You define Auto Scaling Groups (ASGs) or Managed Node Groups. You select the instance families (<code>m5.large</code>, <code>c6g.xlarge</code>). You decide on Spot vs. On-Demand ratios.</li>  
> <li> <strong>Scaling:</strong> You install the Cluster Autoscaler or, more likely, <strong>Karpenter</strong>. You manage the provisioner CRDs to ensure nodes spin up when pods go pending.</li>  
> <li> <strong>Operations:</strong> You are responsible for the "Add-on Lifecycle." When you upgrade Kubernetes from 1.29 to 1.30, you must manually ensure the VPC CNI, CoreDNS, and Kube-proxy are compatible.</li>  
> <li> <strong>Storage &amp; Networking:</strong> You manually install the EBS CSI driver and the AWS Load Balancer Controller (LBC) via Helm.</li>  
> </ol>  
>   
> <p><strong>The Pain Point:</strong> The "Undifferentiated Heavy Lifting."<br>  
> Every hour you spend fixing a conflict between the VPC CNI and a new node kernel is an hour you aren't spending on application reliability. Standard mode is powerful, but it requires a dedicated Platform Team to maintain the plumbing.</p>  
> <h2>  
>     
>     
>   Enter EKS Auto Mode: The "Serverless" Node Experience  
> </h2>  
>   
> <p>EKS Auto Mode is AWS’s answer to the operational overhead of Kubernetes. It is distinct from Fargate (which had severe limitations regarding DaemonSets and caching) because it still runs on EC2 instances—<strong>you just don't manage them.</strong></p>  
>   
> <p>When you enable Auto Mode, EKS takes ownership of the <strong>Compute, Storage, and Networking</strong> lifecycle within the cluster.</p>  
> <h3>  
>     
>     
>   1. Compute: The "Invisible" Karpenter  
> </h3>  
>   
> <p>In Auto Mode, the concept of a "Node Group" essentially vanishes. You don't create ASGs. You don't pick instance types.</p>  
>   
> <p>Instead, EKS uses <strong>Automated Node Pools</strong>.</p>  
>   
> <ul>  
> <li>  <strong>How it works:</strong> EKS analyzes pending pods. If a pod requests 4 vCPUs and 16GB RAM, EKS automatically provisions an EC2 instance that fits that workload and joins it to the cluster.</li>  
> <li>  <strong>Under the hood:</strong> It behaves remarkably like Karpenter is built directly into the Control Plane. It handles bin-packing, consolidation, and spot instance interruption handling automatically.</li>  
> <li>  <strong>Maintenance:</strong> AWS handles the OS patching. When a node needs a security update, EKS seamlessly drains the node and replaces it, adhering to your Pod Disruption Budgets (PDBs).</li>  
> </ul>  
> <h3>  
>     
>     
>   2. Networking: Native Load Balancing  
> </h3>  
>   
> <p>In Standard mode, exposing a service via an Application Load Balancer (ALB) meant installing the AWS Load Balancer Controller, setting up IAM roles, and managing CRDs.</p>  
>   
> <p>In Auto Mode, this is native.</p>  
>   
> <ul>  
> <li>  <strong>The Change:</strong> When you create a Service of <code>type: LoadBalancer</code>, EKS talks directly to the AWS networking APIs to provision a Network Load Balancer (NLB).</li>  
> <li>  <strong>Ingress:</strong> Similarly, creating an Ingress resource automatically triggers ALB creation without requiring a third-party controller running in your cluster.</li>  
> </ul>  
> <h3>  
>     
>     
>   3. Storage: Built-in CSI  
> </h3>  
>   
> <p>Stateful workloads in Standard mode often break during upgrades because the EBS CSI driver version falls behind the cluster version. In Auto Mode, the <strong>EBS CSI functionality is embedded</strong>. You simply request a Persistent Volume Claim (PVC), and the storage appears.</p>  
> <h2>  
>     
>     
>   Security: The Paradigm Shift  
> </h2>  
>   
> <p>This is perhaps the most controversial change for old-school Ops teams: <strong>EKS Auto Mode locks down the nodes.</strong></p>  
> <h3>  
>     
>     
>   No SSH, No SSM  
> </h3>  
>   
> <p>In Auto Mode, you cannot SSH into the worker nodes. You cannot use AWS Systems Manager (SSM) Session Manager to jump into a node and run <code>htop</code>.</p>  
>   
> <ul>  
> <li>  <strong>Why?</strong> The nodes are treated as ephemeral resources managed by AWS.</li>  
> <li>  <strong>The Benefit:</strong> This enforces an immutable infrastructure pattern. If a node is "acting weird," you don't fix it; you delete the pod, and EKS replaces the node.</li>  
> </ul>  
> <h3>  
>     
>     
>   EKS Pod Identity  
> </h3>  
>   
> <p>Auto Mode moves away from the complex IRSA (IAM Roles for Service Accounts) OIDC setup. It defaults to <strong>EKS Pod Identity</strong>.<br>  
> This creates a local agent on the nodes that intercepts AWS API calls from your pods and exchanges a token for temporary AWS credentials. It is significantly easier to set up in Terraform/CloudFormation than the OIDC provider method.</p>  
> <h2>  
>     
>     
>   Comparison Matrix: Standard vs. Auto  
> </h2>  
>   
> <div><table>  
> <thead>  
> <tr>  
> <th>Feature</th>  
> <th>EKS Standard</th>  
> <th>EKS Auto Mode</th>  
> </tr>  
> </thead>  
> <tbody>  
> <tr>  
> <td><strong>Node Management</strong></td>  
> <td>  
> <strong>Manual</strong> (Node Groups / Karpenter)</td>  
> <td>  
> <strong>Automatic</strong> (Managed Node Pools)</td>  
> </tr>  
> <tr>  
> <td><strong>OS Patching</strong></td>  
> <td>You trigger rollouts / AMI updates</td>  
> <td>Fully Automated by AWS</td>  
> </tr>  
> <tr>  
> <td><strong>Instance Selection</strong></td>  
> <td>You define classes (e.g., <code>t3</code>, <code>m5</code>)</td>  
> <td>EKS selects based on Pod Spec</td>  
> </tr>  
> <tr>  
> <td><strong>Load Balancing</strong></td>  
> <td>Install AWS LBC Helm Chart</td>  
> <td>Native / Built-in</td>  
> </tr>  
> <tr>  
> <td><strong>EBS Storage</strong></td>  
> <td>Install EBS CSI Driver</td>  
> <td>Native / Built-in</td>  
> </tr>  
> <tr>  
> <td><strong>Node Access</strong></td>  
> <td>SSH / SSM enabled</td>  
> <td><strong>Strictly Prohibited</strong></td>  
> </tr>  
> <tr>  
> <td><strong>Custom User Data</strong></td>  
> <td>Allowed (Custom Scripts)</td>  
> <td>Not Supported</td>  
> </tr>  
> <tr>  
> <td><strong>Cost</strong></td>  
> <td>EC2 + Control Plane ($0.10/hr)</td>  
> <td>EC2 + Control Plane + Flat fee</td>  
> </tr>  
> <tr>  
> <td><strong>Supported OS</strong></td>  
> <td>AL2, AL2023, Bottlerocket, Windows, Ubuntu</td>  
> <td>EKS Auto-optimized OS (AL2023 based)</td>  
> </tr>  
> </tbody>  
> </table></div>  
> <h2>  
>     
>     
>   Infrastructure as Code: The Difference  
> </h2>  
>   
> <p>The reduction in Terraform code required for Auto Mode is staggering.</p>  
>   
> <p><strong>The "Standard" Way (Simplified):</strong><br>  
> You need to define the cluster, the node groups, the IAM roles for nodes, and the Helm releases for necessary controllers.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>module</span> <span>"eks"</span> <span>{</span>  
>   <span>source</span>  <span>=</span> <span>"terraform-aws-modules/eks/aws"</span>  
>   
>   <span># You define the hardware</span>  
>   <span>eks_managed_node_groups</span> <span>=</span> <span>{</span>  
>     <span>app_nodes</span> <span>=</span> <span>{</span>  
>       <span>instance_types</span> <span>=</span> <span>[</span><span>"m5.large"</span><span>]</span>  
>       <span>min_size</span>     <span>=</span> <span>2</span>  
>       <span>max_size</span>     <span>=</span> <span>10</span>  
>     <span>}</span>  
>   <span>}</span>  
> <span>}</span>  
>   
> <span># Then you must maintain this separately</span>  
> <span>resource</span> <span>"helm_release"</span> <span>"aws_load_balancer_controller"</span> <span>{</span>  
>   <span>name</span>       <span>=</span> <span>"aws-load-balancer-controller"</span>  
>   <span>repository</span> <span>=</span> <span>"https://aws.github.io/eks-charts"</span>  
>   <span>chart</span>      <span>=</span> <span>"aws-load-balancer-controller"</span>  
>   <span># ... extensive configuration ...</span>  
> <span>}</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p><strong>The "Auto" Way:</strong><br>  
> You simply enable the capability flags.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>resource</span> <span>"aws_eks_cluster"</span> <span>"auto"</span> <span>{</span>  
>   <span>name</span> <span>=</span> <span>"production-auto"</span>  
>   
>   <span># The "Easy Button"</span>  
>   <span>compute_config</span> <span>{</span>  
>     <span>enabled</span>       <span>=</span> <span>true</span>   
>     <span>node_pools</span>    <span>=</span> <span>[</span><span>"general-purpose"</span><span>,</span> <span>"system"</span><span>]</span>   
>     <span>node_role_arn</span> <span>=</span> <span>aws_iam_role</span><span>.</span><span>auto_node_role</span><span>.</span><span>arn</span>  
>   <span>}</span>  
>   
>   <span># Native Networking</span>  
>   <span>kubernetes_network_config</span> <span>{</span>  
>     <span>elastic_load_balancing</span> <span>{</span>  
>       <span>enabled</span> <span>=</span> <span>true</span>   
>     <span>}</span>  
>   <span>}</span>  
>   
>   <span># Native Storage</span>  
>   <span>storage_config</span> <span>{</span>  
>     <span>block_storage</span> <span>{</span>  
>       <span>enabled</span> <span>=</span> <span>true</span>   
>     <span>}</span>  
>   <span>}</span>  
> <span>}</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p><em>No Node Groups to define. No Helm charts to manage for basic infrastructure.</em></p>  
>   
> <h2>  
>     
>     
>   When should you use which?  
> </h2>  
>   
> <h3>  
>     
>     
>   Case for EKS Auto Mode  
> </h3>  
>   
> <ol>  
> <li> <strong>Platform Efficiency:</strong> If your team spends more time upgrading clusters than building internal developer platforms (IDPs), switch to Auto. It drastically reduces "Day 2" operations.</li>  
> <li> <strong>Dynamic Workloads:</strong> If you run AI/ML training jobs, CI/CD runners, or batch processing, Auto Mode's ability to seamlessly scale from 0 to 100 nodes (and back) without configuring Karpenter is a huge win.</li>  
> <li> <strong>Greenfield Projects:</strong> Start here. Don't build technical debt (custom node groups) unless you prove you need them.</li>  
> </ol>  
>   
> <h3>  
>     
>     
>   Case for EKS Standard  
> </h3>  
>   
> <ol>  
> <li> <strong>Custom Kernel Requirements:</strong> If you need to load proprietary kernel modules, modify <code>sysctl</code> parameters that require root node access, or use a custom hardened AMI (like CIS benchmarks that deviate from AWS standards), you need Standard.</li>  
> <li> <strong>Legacy "Pet" Applications:</strong> If you have apps that require specific host-level configurations or mounting local instance store NVMe drives in a specific way that the CSI driver doesn't support yet.</li>  
> <li> <strong>Strict Compliance:</strong> If your compliance framework requires you to have SSH access to nodes for forensic analysis (though this is arguably an anti-pattern in cloud-native), Auto Mode's locked-down nature might be a blocker.</li>  
> </ol>  
>   
> <h2>  
>     
>     
>   Conclusion  
> </h2>  
>   
> <p>EKS Auto Mode is not just a wrapper; it is the maturation of Kubernetes on AWS. It acknowledges that for 90% of users, <strong>the node is just a utility</strong>.</p>  
>   
> <p>By abstracting the Data Plane, AWS allows Platform Engineers to move up the stack. Instead of being "Server Mechanics" fixing broken drivers and patching OS kernels, we can finally become "Platform Architects," focusing on reliability, observability, and developer experience.</p>  
>   
> <p>If you are starting a new cluster today, <strong>start with Auto Mode</strong>. If you are on Standard, look at your backlog of maintenance tasks—if it's full of upgrades and patching, it might be time to plan your migration.</p>  
>   
> <p><em>Have you tried EKS Auto Mode yet? Did the lack of SSH access break your workflow? Let’s discuss in the comments below!</em></p>

---

## [5/10] Meet Bumblebee: Agentic AI Flagging Risky Merchants in Under 90 Seconds
**Source:** The Practical Developer | **Date:** 2025-12-02T10:59:50.000Z
**URL:** https://dev.to/razorpaytech/meet-bumblebee-agentic-ai-flagging-risky-merchants-in-under-90-seconds-2nlf
**Reasoning:** The article discusses AI agents in risk management, which is somewhat relevant to AI agents in software engineering.
**Authors:** Ankur

**Content/Abstract:**
> <p><strong>contributors: <a href="https://dev.to/parin-k">@parin-k</a>, <a href="https://dev.to/sumit12dec">@sumit12dec</a></strong></p> 
>  
> <p>If you're familiar with a payments company, you know the drill. Risk agents manually review thousands of merchant websites every month, checking for red flags: sketchy privacy policies, misaligned pricing, questionable social media presence, suspicious domain registration patterns. </p> 
>  
> <p>At Razorpay, our risk operations team was conducting 10,000 to 12,000 manual website reviews monthly, each taking roughly four minutes of human attention. That's 700 to 800 human hours consumed every month, and the quality was inconsistent because different agents would interpret the same signals differently.</p> 
>  
> <p>The traditional approach to fraud detection involves throwing bodies at the problem or building rigid rule engines that break the moment fraudsters adapt their tactics. We needed something better, something that could scale with our transaction volume while actually getting smarter over time. </p> 
>  
> <p>That's why we built what we're calling <strong>Agentic Risk</strong>, a multi-agent AI system that automates merchant website evaluation from end to end while maintaining the nuanced judgment that used to require human expertise.</p> 
>  
> <p>Here's what makes this interesting: we didn't just replace humans with AI and call it done. We went through three distinct architectural iterations, each one teaching us hard lessons about what works and what doesn't when you're building AI agents for production fraud detection. </p> 
>  
> <p>The journey from our initial n8n prototype through an AI agent to our current multi-agent architecture reveals fundamental truths about building reliable AI systems at scale.</p> 
>  
> <p><iframe allowfullscreen="allowfullscreen" width="710" height="399" src="https://www.inoreader.com/yt-embed/?v=819bPUF0of8" referrerpolicy="strict-origin-when-cross-origin" style="width:100%;aspect-ratio:16/9;height:auto;display:block;border:0;"> 
> </iframe> 
> </p> 
>  
> <h2> 
>    
>    
>   <strong>The Business Problem: When Manual Review Can't Keep Up</strong> 
> </h2> 
>  
> <p>Let me paint the picture of what risk operations looked like before automation. When a new merchant signs up for Razorpay or when our fraud detection system flags an existing merchant, a case lands in our Risk Case Manager system. A human agent picks up that case and begins the investigation dance. </p> 
>  
> <p>This process takes four minutes when everything goes smoothly, but that's rarely the case. Websites are structured differently, policy pages are hidden in weird places, domain information services have different interfaces, and social media handles aren't always obvious. The worst part isn't the time; it's the inconsistency. One agent might flag a merchant for having a generic privacy policy while another agent considers the same policy acceptable. </p> 
>  
> <p>We were also paying thousands of dollars monthly for a third-party explicit content screening service, and it was generating about 50 alerts per month with less than 10% precision. Moreover, this service only caught one specific type of risk while ignoring dozens of other fraud indicators we cared about.</p> 
>  
> <p>The fundamental issue was that we had excellent observability tools, structured data systems, and experienced risk analysts, but the connective tissue between all these components was human labor. Scaling meant hiring more agents, which meant more inconsistency, higher cost, and no improvement in detection speed or accuracy.</p> 
>  
> <h2> 
>    
>    
>   <strong>Phase 1: The n8n Prototype - When Visual Orchestration Hits Its Limits</strong> 
> </h2> 
>  
> <p>We started with n8n, a visual workflow automation platform, to quickly prototype and validate our hypothesis. Within weeks, we had a working proof-of-concept integrating webhook ingestion, merchant metadata fetching, website content review via multimodal AI, domain lookups, GST enrichment, fraud metrics, and LLM-based risk analysis.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgqjfn2uho4pyhh2pn9we.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgqjfn2uho4pyhh2pn9we.png" alt="bumblebee-n8n-workflow" width="800" height="47"></a></p> 
>  
> <p>The prototype validated that automation was feasible and helped us identify the complete set of data points needed. However, n8n quickly revealed fundamental limitations: <strong>branch explosion</strong> (handling edge cases created unmaintainable 40-node workflows with duplicated logic), <strong>observability gaps</strong> (debugging failed nodes was painful with coarse logs), and <strong>platform instability</strong> (non-deterministic behavior in HTTP and merge operations). The n8n prototype taught us that production-grade risk automation would require a code-first approach with proper observability and the ability to use Python libraries directly.</p> 
>  
> <h2> 
>    
>    
>   <strong>Phase 2: Python + ReAct Agent - Better Control, New Bottlenecks</strong> 
> </h2> 
>  
> <p>We rebuilt as a Python web application with an API frontend and task workers. This immediately solved several Phase 1 problems: native Python libraries, structured logging with trace IDs, proper exception handling with retry logic, and complex NLP preprocessing capabilities.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F0g0sjs9puoe27ldedilz.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F0g0sjs9puoe27ldedilz.png" alt="bumblebee-sequence-diagram" width="800" height="733"></a></p> 
>  
> <p>The core was a single ReAct-style agent that iteratively reasoned about which tools to call, executed them, and incorporated results until producing a structured risk assessment. Phase 2 brought full observability, easy tool addition, and dynamic behavior that replaced brittle conditional logic.</p> 
>  
> <p>However, new bottlenecks emerged. <strong>Token bloat</strong> became critical as the agent accumulated 50KB+ of HTML content, domain data, and fraud metrics in its context window, regularly hitting token limits. <strong>Sequential execution</strong> meant tool invocations happened one after another even when they had no dependencies, scaling linearly with tool count. <strong>Temperature conflation</strong> forced a compromise setting that was suboptimal for both exploration (tool selection) and exploitation (final scoring). Phase 2 proved agentic orchestration was right, but single-agent architecture couldn't scale to thousands of concurrent evaluations.</p> 
>  
> <h2> 
>    
>    
>   <strong>Phase 3: Multi-Agent Architecture - When Specialization Wins</strong> 
> </h2> 
>  
> <p>The breakthrough came when we stopped treating fraud detection as a single AI task and started building a <strong>multi-agent collaboration system</strong>. Rather than one agent doing everything, we split responsibilities across specialized agents optimized for specific roles: Planner, Fetchers, and Analyzer.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F74bokg78giaw38ehj9ml.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F74bokg78giaw38ehj9ml.png" alt="bumblebee-multiagent-arch" width="800" height="503"></a></p> 
>  
> <p><strong>The Planner Agent</strong> receives the merchant case, examines available tools, checks system health and API quotas, and generates an execution plan. This isn't a rigid script; it's a structured specification of what information to gather, with priorities, timeouts, token budgets, and expected schemas. The Planner enforces business rules deterministically. Skip GST validation for non-Indian merchants. Deprioritize social media checks for B2B merchants where social presence matters less. This reduces unnecessary API calls and focuses resources on high-signal checks.</p> 
>  
> <p><strong>Data Fetcher Agents</strong> execute in parallel, each owning one data source or tool. Website scraping, WHOIS lookups, fraud database queries, social media metrics, pricing comparisons, policy verification. Here's the critical insight: fetchers don't just retrieve raw data. They perform <strong>local data pruning</strong> before returning results.</p> 
>  
> <p>The website content reviewer doesn't send back 50KB of HTML. It extracts only relevant sections: privacy policies, contact information, pricing tables, product descriptions. Using keyword matching or lightweight NLP models, it returns a compact JSON payload with structured snippets, confidence scores, and provenance links. This solves the token bloat problem. Instead of accumulating full raw outputs, the system maintains small, information-dense summaries.</p> 
>  
> <p>Each fetcher compresses its domain's data into a format optimized for downstream analysis. Fetchers also implement caching for data that doesn't change frequently. WHOIS information and domain reputation scores get cached with appropriate TTLs, reducing redundant external API calls and improving throughput during traffic spikes.</p> 
>  
> <p><strong>The Analyzer Agent</strong> consumes these structured payloads and produces the final risk assessment. It runs <strong>deterministic rules first</strong>: hard thresholds for fraud metrics, blacklist checks, compliance violations. These rules are fast, explainable, and don't require LLM inference.</p> 
>  
> <p>Only after deterministic rules does the Analyzer invoke the LLM for interpretive tasks: generating human-readable summaries, explaining why certain indicators triggered, identifying nuanced patterns that don't fit simple rules. Because fetchers already pruned and structured the data, the Analyzer's LLM calls work with minimal context, avoiding token limit issues entirely.</p> 
>  
> <p>Different agents use different temperature settings tuned for their roles. The Planner runs at medium temperature for flexible tool selection. The Analyzer uses very low temperature for deterministic risk scoring and higher temperature when generating business narratives where creative expression improves readability. This per-agent temperature control eliminates the compromises from Phase 2.</p> 
>  
> <p><strong>The execution model</strong> leverages Celery for orchestration. When a case arrives, the API enqueues a planning job. The Planner generates the execution plan and enqueues multiple fetcher jobs in parallel. As fetchers complete, their results stream into a shared state store. The Analyzer subscribes to fetcher completion events and begins processing as soon as enough data is available, not waiting for every fetcher if some are slow or failing.</p> 
>  
> <p>If a fetcher fails entirely (website unreachable, API rate-limiting), the Planner degrades gracefully. The Analyzer proceeds with available data and flags the missing information for manual review rather than blocking the entire evaluation. This resilience was impossible in Phase 2's sequential architecture.</p> 
>  
> <h2> 
>    
>    
>   <strong>The Results: When Architecture Meets Reality</strong> 
> </h2> 
>  
> <p>The shift to multi-agent architecture produced measurable improvements across every dimension. <strong>Token usage dropped 60%</strong> through fetcher-level pruning and elimination of full raw data in LLM context. <strong>End-to-end latency fell from 35 seconds to 8-12 seconds</strong> via parallel fetcher execution and focused LLM calls. <strong>Success rate rose from 88% to 99%+</strong>, measured as cases completing without token limits or LLM failures.</p> 
>  
> <p><strong>Cost per evaluation decreased</strong> despite adding sophisticated analysis. Smaller context windows meant cheaper LLM calls. Caching at the fetcher level reduced external API expenses. The system now handles thousands of concurrent evaluations without bottlenecking, scaling horizontally by adding task workers rather than vertically with bigger servers.</p> 
>  
> <p>The most important improvement is <strong>maintainability and extensibility</strong>. Adding a new risk signal requires writing a new fetcher agent with its pruning logic and output schema. The Planner automatically incorporates new tools once registered. The Analyzer adapts to new data sources without modification. This composability enables continuous fraud detection improvement by adding signals incrementally rather than requiring architectural rewrites.</p> 
>  
> <p>The multi-agent approach provides <strong>observability impossible in earlier phases</strong>. Each agent logs trace IDs, tokens consumed, latency, confidence scores, and reasoning. When a case produces unexpected results, we replay the exact sequence of fetcher outputs, examine what the Analyzer saw, and understand why it reached that conclusion. This audit trail is critical for debugging, regulatory compliance, and explaining decisions to merchants who dispute risk assessments.</p> 
>  
> <h2> 
>    
>    
>   <strong>What We Learned: Principles for Building Production AI Agents</strong> 
> </h2> 
>  
> <p>Our journey from n8n through ReAct to multi-agent orchestration taught us several lessons that apply broadly to anyone building AI systems for production use cases.</p> 
>  
> <p><strong>Start simple, evolve deliberately</strong>. N8n was the right choice for Phase 1 even though we knew it wouldn't scale. Rapid prototyping and stakeholder validation matter more than architectural purity in early stages. What's critical is recognizing when you've outgrown your current approach and having the discipline to rebuild rather than patch over fundamental limitations.</p> 
>  
> <p><strong>Token budgets are real constraints</strong>. Many blog posts about AI agents gloss over token management, but in production systems with large, messy real-world data, token limits are where architectures break. Design explicitly for token efficiency: prune early, prune often, and never pass raw, unstructured data to LLMs when you can send structured summaries instead.</p> 
>  
> <p><strong>Specialization beats generalization at scale</strong>. A single agent trying to handle planning, data fetching, and analysis will hit walls that can't be solved with better prompts or bigger models. Splitting responsibilities across specialized agents with clear interfaces between them produces systems that are faster, more reliable, and easier to understand.</p> 
>  
> <p><strong>Temperature is not a hyperparameter you tune once</strong>. Different tasks need different temperature settings, and trying to find a compromise temperature for a single agent produces mediocre results everywhere. Per-agent temperature control is a fundamental architectural requirement, not an optimization detail.</p> 
>  
> <p><strong>Parallelism matters more than model size</strong>. Running multiple smaller, focused agents in parallel often outperforms running one large agent sequentially, both in terms of latency and cost. This runs counter to the instinct to throw the biggest model available at every problem.</p> 
>  
> <p><strong>Observability is not optional</strong>. Without structured logging, trace IDs, and the ability to replay decision sequences, debugging production AI systems is nearly impossible. Invest in observability infrastructure early, ideally before you have a production incident that requires it.</p> 
>  
> <h2> 
>    
>    
>   <strong>The Path Forward: Continuous Improvement by Design</strong> 
> </h2> 
>  
> <p>What makes the multi-agent architecture particularly powerful is that it's designed for continuous improvement. As we accumulate more cases, we can identify patterns where the Analyzer produces low-confidence results or where human reviewers frequently override AI decisions. These cases become training data for improving fetcher pruning heuristics, refining Planner rules, and tuning Analyzer prompts.</p> 
>  
> <p>We're exploring several extensions. Fine-tuning small, specialized models for specific fetchers rather than relying entirely on general-purpose LLMs. This could further reduce cost and latency while improving accuracy for domain-specific tasks like policy compliance checking. Implementing feedback loops where human overrides automatically update Planner rules or Analyzer thresholds, creating a self-improving system that gets smarter as risk operators correct its mistakes.</p> 
>  
> <p>Another direction is adding <strong>predictive agents</strong> that don't just evaluate merchant risk at onboarding but continuously monitor for behavioral changes that might indicate fraud. Imagine fetchers running periodically in the background, detecting when a merchant's website content changes significantly, when pricing diverges from competitors, or when social media presence suddenly evaporates. The same multi-agent architecture that handles point-in-time evaluation can drive continuous risk monitoring with minimal modification.</p> 
>  
> <h2> 
>    
>    
>   <strong>Why This Matters Beyond Fraud Detection</strong> 
> </h2> 
>  
> <p>I've been talking about merchant risk evaluation specifically, but the architectural patterns we discovered apply broadly to any domain where AI agents need to process large amounts of heterogeneous data, make complex decisions, and produce explainable results. Financial services, healthcare, supply chain management, cybersecurity, and legal research all have similar characteristics: multiple data sources with different formats and latencies, domain expertise encoded in rules and models, and requirements for auditability and compliance.</p> 
>  
> <p>The lesson isn't "use multi-agent architecture for everything." The lesson is that as AI systems scale from demos to production, the architecture that got you to the first prototype often becomes the main thing preventing you from scaling further. Having the discipline to recognize when you've hit architectural limits, the willingness to rebuild from first principles, and the engineering rigor to measure improvements objectively separates successful production AI from expensive science projects.</p> 
>  
> <p>At Razorpay, we've taken fraud detection from a manual, inconsistent process consuming 800 agent hours monthly to an automated system that evaluates merchants in seconds with higher accuracy and comprehensive audit trails. We've reduced our per-review time by 75%, improved detection consistency, and freed up risk operators to focus on genuinely complex cases that require human judgment. And we've done it with an architecture that gets better over time rather than more fragile.</p> 
>  
> <p>If you're building AI agents for production use cases, the technology is ready. The LLMs are capable, the orchestration frameworks exist, and the integration tools work. The hard part is designing systems that handle real-world messiness, scale with your business, and maintain reliability when things inevitably break. That's where architecture matters, and that's what we learned the hard way through three iterations of building Agentic Risk.</p> 
>  
> <p><em>editor: <a href="https://dev.to/paaarth96">@paaarth96</a></em> </p>

---

## [5/10] Inside Mirakl’s Agent Commerce Vision
**Source:** OpenAI News | **Date:** 2025-12-01T22:00:00.000Z
**URL:** https://openai.com/index/mirakl
**Reasoning:** The article discusses AI agents in commerce, which is somewhat related to AI agents in software engineering.

**Content/Abstract:**
> Mirakl is redefining commerce through AI agents and ChatGPT Enterprise—achieving faster documentation, smarter customer support, and building toward agent-native commerce with Mirakl Nexus.

---

## [4/10] Discourse-Aware Scientific Paper Recommendation via QA-Style Summarization and Multi-Level Contrastive Learning
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251103330W/abstract
**Reasoning:** The paper focuses on scientific paper recommendation, which is not directly related to code intelligence or context engineering.
**Authors:** Wang, Shenghua, Yin, Zhen

**Content/Abstract:**
> The rapid growth of open-access (OA) publications has intensified the challenge of identifying relevant scientific papers. Due to privacy constraints and limited access to user interaction data, recent efforts have shifted toward content-based recommendation, which relies solely on textual information. However, existing models typically treat papers as unstructured text, neglecting their discourse organization and thereby limiting semantic completeness and interpretability. To address these limitations, we propose OMRC-MR, a hierarchical framework that integrates QA-style OMRC (Objective, Method, Result, Conclusion) summarization, multi-level contrastive learning, and structure-aware re-ranking for scholarly recommendation. The QA-style summarization module converts raw papers into structured and discourse-consistent representations, while multi-level contrastive objectives align semantic representations across metadata, section, and document levels. The final re-ranking stage further refines retrieval precision through contextual similarity calibration. Experiments on DBLP, S2ORC, and the newly constructed Sci-OMRC dataset demonstrate that OMRC-MR consistently surpasses state-of-the-art baselines, achieving up to 7.2% and 3.8% improvements in Precision@10 and Recall@10, respectively. Additional evaluations confirm that QA-style summarization produces more coherent and factually complete representations. Overall, OMRC-MR provides a unified and interpretable content-based paradigm for scientific paper recommendation, advancing trustworthy and privacy-aware scholarly information retrieval.

---

## [4/10] Hard vs. Noise: Resolving Hard-Noisy Sample Confusion in Recommender Systems via Large Language Models
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251107295S/abstract
**Reasoning:** The study addresses recommender systems using LLMs, which is not directly relevant to codebase indexing or context management.
**Authors:** Song, Tianrui, Chao, Wen-Shuo, Liu, Hao

**Content/Abstract:**
> Implicit feedback, employed in training recommender systems, unavoidably confronts noise due to factors such as misclicks and position bias. Previous studies have attempted to identify noisy samples through their diverged data patterns, such as higher loss values, and mitigate their influence through sample dropping or reweighting. However, we observed that noisy samples and hard samples display similar patterns, leading to hard-noisy confusion issue. Such confusion is problematic as hard samples are vital for modeling user preferences. To solve this problem, we propose LLMHNI framework, leveraging two auxiliary user-item relevance signals generated by Large Language Models (LLMs) to differentiate hard and noisy samples. LLMHNI obtains user-item semantic relevance from LLM-encoded embeddings, which is used in negative sampling to select hard negatives while filtering out noisy false negatives. An objective alignment strategy is proposed to project LLM-encoded embeddings, originally for general language tasks, into a representation space optimized for user-item relevance modeling. LLMHNI also exploits LLM-inferred logical relevance within user-item interactions to identify hard and noisy samples. These LLM-inferred interactions are integrated into the interaction graph and guide denoising with cross-graph contrastive alignment. To eliminate the impact of unreliable interactions induced by LLM hallucination, we propose a graph contrastive learning strategy that aligns representations from randomly edge-dropped views to suppress unreliable edges. Empirical results demonstrate that LLMHNI significantly improves denoising and recommendation performance.

---

## [4/10] Planning in Branch-and-Bound: Model-Based Reinforcement Learning for Exact Combinatorial Optimization
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251109219S/abstract
**Reasoning:** The paper discusses reinforcement learning for combinatorial optimization, which is not directly related to our primary interests.
**Authors:** Strang, Paul, Alès, Zacharie, Bissuel, Côme, Kedad-Sidhoum, Safia, Rachelson, Emmanuel

**Content/Abstract:**
> Mixed-Integer Linear Programming (MILP) lies at the core of many real-world combinatorial optimization (CO) problems, traditionally solved by branch-and-bound (B&amp;B). A key driver influencing B&amp;B solvers efficiency is the variable selection heuristic that guides branching decisions. Looking to move beyond static, hand-crafted heuristics, recent work has explored adapting traditional reinforcement learning (RL) algorithms to the B&amp;B setting, aiming to learn branching strategies tailored to specific MILP distributions. In parallel, RL agents have achieved remarkable success in board games, a very specific type of combinatorial problems, by leveraging environment simulators to plan via Monte Carlo Tree Search (MCTS). Building on these developments, we introduce Plan-and-Branch-and-Bound (PlanB&amp;B), a model-based reinforcement learning (MBRL) agent that leverages a learned internal model of the B&amp;B dynamics to discover improved branching strategies. Computational experiments empirically validate our approach, with our MBRL branching agent outperforming previous state-of-the-art RL methods across four standard MILP benchmarks.

---

## [4/10] Don't Just Search, Understand: Semantic Path Planning Agent for Spherical Tensegrity Robots in Unknown Environments
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251101236Z/abstract
**Reasoning:** The focus on path planning for robots using LLMs is not directly relevant to code intelligence or context engineering.
**Authors:** Zhang, Junwen, Liu, Changyue, Fu, Pengqi, Guo, Xiang, Shi, Ye, Liang, Xudong, Wang, Zhijian, Ma, Hanzhi

**Content/Abstract:**
> Endowed with inherent dynamical properties that grant them remarkable ruggedness and adaptability, spherical tensegrity robots stand as prototypical examples of hybrid softrigid designs and excellent mobile platforms. However, path planning for these robots in unknown environments presents a significant challenge, requiring a delicate balance between efficient exploration and robust planning. Traditional path planners, which treat the environment as a geometric grid, often suffer from redundant searches and are prone to failure in complex scenarios due to their lack of semantic understanding. To overcome these limitations, we reframe path planning in unknown environments as a semantic reasoning task. We introduce a Semantic Agent for Tensegrity robots (SATPlanner) driven by a Large Language Model (LLM). SATPlanner leverages high-level environmental comprehension to generate efficient and reliable planning strategies.At the core of SATPlanner is an Adaptive Observation Window mechanism, inspired by the "fast" and "slow" thinking paradigms of LLMs. This mechanism dynamically adjusts the perceptual field of the agent: it narrows for rapid traversal of open spaces and expands to reason about complex obstacle configurations. This allows the agent to construct a semantic belief of the environment, enabling the search space to grow only linearly with the path length (O(L)) while maintaining path quality. We extensively evaluate SATPlanner in 1,000 simulation trials, where it achieves a 100% success rate, outperforming other real-time planning algorithms. Critically, SATPlanner reduces the search space by 37.2% compared to the A* algorithm while achieving comparable, near-optimal path lengths. Finally, the practical feasibility of SATPlanner is validated on a physical spherical tensegrity robot prototype.

---

## [4/10] CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in Short-Video Search
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251115443X/abstract
**Reasoning:** The paper addresses retrieval in short-video platforms, which is not directly related to codebase indexing or context management.
**Authors:** Xie, Ao, Chen, Jiahui, Zhu, Quanzhi, Jiang, Xiaoze, Qin, Zhiheng, Yu, Enyun, Li, Han

**Content/Abstract:**
> Dense retrieval has become a foundational paradigm in modern search systems, especially on short-video platforms. However, most industrial systems adopt a self-reinforcing training pipeline that relies on historically exposed user interactions for supervision. This paradigm inevitably leads to a filter bubble effect, where potentially relevant but previously unseen content is excluded from the training signal, biasing the model toward narrow and conservative retrieval. In this paper, we present CroPS (Cross-Perspective Positive Samples), a novel retrieval data engine designed to alleviate this problem by introducing diverse and semantically meaningful positive examples from multiple perspectives. CroPS enhances training with positive signals derived from user query reformulation behavior (query-level), engagement data in recommendation streams (system-level), and world knowledge synthesized by large language models (knowledge-level). To effectively utilize these heterogeneous signals, we introduce a Hierarchical Label Assignment (HLA) strategy and a corresponding H-InfoNCE loss that together enable fine-grained, relevance-aware optimization. Extensive experiments conducted on Kuaishou Search, a large-scale commercial short-video search platform, demonstrate that CroPS significantly outperforms strong baselines both offline and in live A/B tests, achieving superior retrieval performance and reducing query reformulation rates. CroPS is now fully deployed in Kuaishou Search, serving hundreds of millions of users daily.

---

## [4/10] Make It Long, Keep It Fast: End-to-End 10k-Sequence Modeling at Billion Scale on Douyin
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251106077G/abstract
**Reasoning:** The focus on sequence modeling for video recommendations is not directly related to code intelligence or context engineering.
**Authors:** Guan, Lin, Yang, Jia-Qi, Zhao, Zhishan, Zhang, Beichuan, Sun, Bo, Luo, Xuanyuan, Ni, Jinan, Li, Xiaowen, Qi, Yuhang, Fan, Zhifang, Wang, Hangyu, Chen, Qiwei, Cheng, Yi, Zhang, Feng, Yang, Xiao

**Content/Abstract:**
> Short-video recommenders such as Douyin must exploit extremely long user histories without breaking latency or cost budgets. We present an end-to-end system that scales long-sequence modeling to 10k-length histories in production. First, we introduce Stacked Target-to-History Cross Attention (STCA), which replaces history self-attention with stacked cross-attention from the target to the history, reducing complexity from quadratic to linear in sequence length and enabling efficient end-to-end training. Second, we propose Request Level Batching (RLB), a user-centric batching scheme that aggregates multiple targets for the same user/request to share the user-side encoding, substantially lowering sequence-related storage, communication, and compute without changing the learning objective. Third, we design a length-extrapolative training strategy -- train on shorter windows, infer on much longer ones -- so the model generalizes to 10k histories without additional training cost. Across offline and online experiments, we observe predictable, monotonic gains as we scale history length and model capacity, mirroring the scaling law behavior observed in large language models. Deployed at full traffic on Douyin, our system delivers significant improvements on key engagement metrics while meeting production latency, demonstrating a practical path to scaling end-to-end long-sequence recommendation to the 10k regime.

---

## [4/10] MindRec: A Diffusion-driven Coarse-to-Fine Paradigm for Generative Recommendation
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251112597G/abstract
**Reasoning:** The paper discusses generative recommendation systems, which is not directly related to code intelligence or context engineering.
**Authors:** Gao, Mengyao, Gao, Chongming, Liu, Haoyan, Cai, Qingpeng, Jiang, Peng, Chen, Jiajia, Yuan, Shuai, He, Xiangnan

**Content/Abstract:**
> Recent advancements in large language model-based recommendation systems often represent items as text or semantic IDs and generate recommendations in an auto-regressive manner. However, due to the left-to-right greedy decoding strategy and the unidirectional logical flow, such methods often fail to produce globally optimal recommendations. In contrast, human reasoning does not follow a rigid left-to-right sequence. Instead, it often begins with keywords or intuitive insights, which are then refined and expanded. Inspired by this fact, we propose MindRec, a diffusion-driven coarse-to-fine generative paradigm that emulates human thought processes. Built upon a diffusion language model, MindRec departs from auto-regressive generation by leveraging a masked diffusion process to reconstruct items in a flexible, non-sequential manner. Particularly, our method first generates key tokens that reflect user preferences, and then expands them into the complete item, enabling adaptive and human-like generation. To further emulate the structured nature of human decision-making, we organize items into a hierarchical category tree. This structure guides the model to first produce the coarse-grained category and then progressively refine its selection through finer-grained subcategories before generating the specific item. To mitigate the local optimum problem inherent in greedy decoding, we design a novel beam search algorithm, Diffusion Beam Search, tailored for our mind-inspired generation paradigm. Experimental results demonstrate that MindRec yields a 9.5\% average improvement in top-1 accuracy over state-of-the-art methods, highlighting its potential to enhance recommendation performance. The implementation is available via https://github.com/Mr-Peach0301/MindRec.

---

## [4/10] Music Recommendation with Large Language Models: Challenges, Opportunities, and Evaluation
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251116478E/abstract
**Reasoning:** The paper focuses on music recommendation systems, which is not directly relevant to code intelligence or context engineering.
**Authors:** Epure, Elena V., Deldjoo, Yashar, Sguerra, Bruno, Schedl, Markus, Moussallam, Manuel

**Content/Abstract:**
> Music Recommender Systems (MRS) have long relied on an information-retrieval framing, where progress is measured mainly through accuracy on retrieval-oriented subtasks. While effective, this reductionist paradigm struggles to address the deeper question of what makes a good recommendation, and attempts to broaden evaluation, through user studies or fairness analyses, have had limited impact. The emergence of Large Language Models (LLMs) disrupts this framework: LLMs are generative rather than ranking-based, making standard accuracy metrics questionable. They also introduce challenges such as hallucinations, knowledge cutoffs, non-determinism, and opaque training data, rendering traditional train/test protocols difficult to interpret. At the same time, LLMs create new opportunities, enabling natural-language interaction and even allowing models to act as evaluators. This work argues that the shift toward LLM-driven MRS requires rethinking evaluation. We first review how LLMs reshape user modeling, item modeling, and natural-language recommendation in music. We then examine evaluation practices from NLP, highlighting methodologies and open challenges relevant to MRS. Finally, we synthesize insights-focusing on how LLM prompting applies to MRS, to outline a structured set of success and risk dimensions. Our goal is to provide the MRS community with an updated, pedagogical, and cross-disciplinary perspective on evaluation.

---

## [4/10] Denoised Recommendation Model with Collaborative Signal Decoupling
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251104237L/abstract
**Reasoning:** The focus on denoising in collaborative filtering is not directly related to code intelligence or context engineering.
**Authors:** Li, Zefeng, Yang, Ning

**Content/Abstract:**
> Although the collaborative filtering (CF) algorithm has achieved remarkable performance in recommendation systems, it suffers from suboptimal recommendation performance due to noise in the user-item interaction matrix. Numerous noise-removal studies have improved recommendation models, but most existing approaches conduct denoising on a single graph. This may cause attenuation of collaborative signals: removing edges between two nodes can interrupt paths between other nodes, weakening path-dependent collaborative information. To address these limitations, this study proposes a novel GNN-based CF model called DRCSD for denoising unstable interactions. DRCSD includes two core modules: a collaborative signal decoupling module (decomposes signals into distinct orders by structural characteristics) and an order-wise denoising module (performs targeted denoising on each order). Additionally, the information aggregation mechanism of traditional GNN-based CF models is modified to avoid cross-order signal interference until the final pooling operation. Extensive experiments on three public real-world datasets show that DRCSD has superior robustness against unstable interactions and achieves statistically significant performance improvements in recommendation accuracy metrics compared to state-of-the-art baseline models.

---

## [4/10] Query Generation Pipeline with Enhanced Answerability Assessment for Financial Information Retrieval
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251105000K/abstract
**Reasoning:** The paper discusses financial information retrieval, which is not directly relevant to code intelligence or context engineering.
**Authors:** Kim, Hyunkyu, Yoo, Yeeun, Kwak, Youngjun

**Content/Abstract:**
> As financial applications of large language models (LLMs) gain attention, accurate Information Retrieval (IR) remains crucial for reliable AI services. However, existing benchmarks fail to capture the complex and domain-specific information needs of real-world banking scenarios. Building domain-specific IR benchmarks is costly and constrained by legal restrictions on using real customer data. To address these challenges, we propose a systematic methodology for constructing domain-specific IR benchmarks through LLM-based query generation. As a concrete implementation of this methodology, our pipeline combines single and multi-document query generation with an enhanced and reasoning-augmented answerability assessment method, achieving stronger alignment with human judgments than prior approaches. Using this methodology, we construct KoBankIR, comprising 815 queries derived from 204 official banking documents. Our experiments show that existing retrieval models struggle with the complex multi-document queries in KoBankIR, demonstrating the value of our systematic approach for domain-specific benchmark construction and underscoring the need for improved retrieval techniques in financial domains.

---

## [4/10] BeautyGuard: Designing a Multi-Agent Roundtable System for Proactive Beauty Tech Compliance through Stakeholder Collaboration
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251112645L/abstract
**Reasoning:** The focus is on compliance in beauty tech, which is not directly related to our primary or secondary interests.
**Authors:** Li, Junwei, Wang, Wenqing, Mao, Huiliu, Ni, Jiazhe, Xiong, Zeyu

**Content/Abstract:**
> As generative AI enters enterprise workflows, ensuring compliance with legal, ethical, and reputational standards becomes a pressing challenge. In beauty tech, where biometric and personal data are central, traditional reviews are often manual, fragmented, and reactive. To examine these challenges, we conducted a formative study with six experts (four IT managers, two legal managers) at a multinational beauty company. The study revealed pain points in rule checking, precedent use, and the lack of proactive guidance. Motivated by these findings, we designed a multi-agent "roundtable" system powered by a large language model. The system assigns role-specialized agents for legal interpretation, checklist review, precedent search, and risk mitigation, synthesizing their perspectives into structured compliance advice. We evaluated the prototype with the same experts using System Usability Scale(SUS), The Official NASA Task Load Index(NASA-TLX), and interviews. Results show exceptional usability (SUS: 77.5/100) and minimal cognitive workload, with three key findings: (1) multi-agent systems can preserve tacit knowledge into standardized workflows, (2) information augmentation achieves higher acceptance than decision automation, and (3) successful enterprise AI should mirror organizational structures. This work contributes design principles for human-AI collaboration in compliance review, with broader implications for regulated industries beyond beauty tech.

---

## [4/10] Can We Predict the Next Question? A Collaborative Filtering Approach to Modeling User Behavior
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251112949F/abstract
**Reasoning:** The paper discusses user behavior modeling, which is not directly relevant to our primary interests in code intelligence or context engineering.
**Authors:** Fu, Bokang, Wang, Jiahao, Liu, Xiaojing, Liu, Yuli

**Content/Abstract:**
> In recent years, large language models (LLMs) have excelled in language understanding and generation, powering advanced dialogue and recommendation systems. However, a significant limitation persists: these systems often model user preferences statically, failing to capture the dynamic and sequential nature of interactive behaviors. The sequence of a user's historical questions provides a rich, implicit signal of evolving interests and cognitive patterns, yet leveraging this temporal data for predictive tasks remains challenging due to the inherent disconnect between language modeling and behavioral sequence modeling. To bridge this gap, we propose a Collaborative Filtering-enhanced Question Prediction (CFQP) framework. CFQP dynamically models evolving user-question interactions by integrating personalized memory modules with graph-based preference propagation. This dual mechanism allows the system to adaptively learn from user-specific histories while refining predictions through collaborative signals from similar users. Experimental results demonstrate that our approach effectively generates agents that mimic real-user questioning patterns, highlighting its potential for building proactive and adaptive dialogue systems.

---

## [4/10] Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251105919F/abstract
**Reasoning:** The paper discusses adversarial attacks on LLMs, which is not directly relevant to our primary interests in code intelligence or context engineering.
**Authors:** Fastowski, Alina, Prenkaj, Bardh, Li, Yuxiao, Kasneci, Gjergji

**Content/Abstract:**
> LLMs are now an integral part of information retrieval. As such, their role as question answering chatbots raises significant concerns due to their shown vulnerability to adversarial man-in-the-middle (MitM) attacks. Here, we propose the first principled attack evaluation on LLM factual memory under prompt injection via Xmera, our novel, theory-grounded MitM framework. By perturbing the input given to "victim" LLMs in three closed-book and fact-based QA settings, we undermine the correctness of the responses and assess the uncertainty of their generation process. Surprisingly, trivial instruction-based attacks report the highest success rate (up to ~85.3%) while simultaneously having a high uncertainty for incorrectly answered questions. To provide a simple defense mechanism against Xmera, we train Random Forest classifiers on the response uncertainty levels to distinguish between attacked and unattacked queries (average AUC of up to ~96%). We believe that signaling users to be cautious about the answers they receive from black-box and potentially corrupt LLMs is a first checkpoint toward user cyberspace safety.

---

## [4/10] Textual understanding boost in the WikiRace
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251110585E/abstract
**Reasoning:** The focus is on navigation strategies in Wikipedia, which is not directly relevant to our primary interests in code intelligence or context engineering.
**Authors:** Ebrahimi, Raman, Fuhrman, Sean, Nguyen, Kendrick, Gurusankar, Harini, Franceschetti, Massimo

**Content/Abstract:**
> The WikiRace game, where players navigate between Wikipedia articles using only hyperlinks, serves as a compelling benchmark for goal-directed search in complex information networks. This paper presents a systematic evaluation of navigation strategies for this task, comparing agents guided by graph-theoretic structure (betweenness centrality), semantic meaning (language model embeddings), and hybrid approaches. Through rigorous benchmarking on a large Wikipedia subgraph, we demonstrate that a purely greedy agent guided by the semantic similarity of article titles is overwhelmingly effective. This strategy, when combined with a simple loop-avoidance mechanism, achieved a perfect success rate and navigated the network with an efficiency an order of magnitude better than structural or hybrid methods. Our findings highlight the critical limitations of purely structural heuristics for goal-directed search and underscore the transformative potential of large language models to act as powerful, zero-shot semantic navigators in complex information spaces.

---

## [4/10] Jasper-Token-Compression-600M Technical Report
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251114405Z/abstract
**Reasoning:** The focus on token compression and model efficiency is not directly related to our primary interests in context engineering or codebase indexing.
**Authors:** Zhang, Dun, Zeng, Ziyang, Zhou, Yudong, Lu, Shuyang

**Content/Abstract:**
> This technical report presents the training methodology and evaluation results of the open-source Jasper-Token-Compression-600M model, released in November 2025. Building on previous distillation-based recipes from the English Stella and Jasper models, we successfully extend this approach to a bilingual (English and Chinese) domain, further enhancing model performance through the incorporation of contrastive learning. A key innovation of our model is the introduction of a one-dimensional convolution-based token compression module. We dynamically adjust the compression rate during training, enabling the model to learn more robust and efficient compressed text representations. By combining knowledge distillation with token compression techniques, we achieve significant improvements in both embedding quality and inference efficiency. Our model performs with higher efficiency than a traditional 0.6B model while achieving performance comparable to that of an 8B model. For more information on the model release, visit: https://huggingface.co/infgrad/Jasper-Token-Compression-600M.

---

## [4/10] Personalized Federated Recommendation With Knowledge Guidance
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251112959L/abstract
**Reasoning:** The paper focuses on federated recommendation systems, which is outside our primary and secondary interests.
**Authors:** Lim, Jaehyung, Kweon, Wonbin, Kim, Woojoo, Kim, Junyoung, Kim, Dongha, Yu, Hwanjo

**Content/Abstract:**
> Federated Recommendation (FedRec) has emerged as a key paradigm for building privacy-preserving recommender systems. However, existing FedRec models face a critical dilemma: memory-efficient single-knowledge models suffer from a suboptimal knowledge replacement practice that discards valuable personalization, while high-performance dual-knowledge models are often too memory-intensive for practical on-device deployment. We propose Federated Recommendation with Knowledge Guidance (FedRKG), a model-agnostic framework that resolves this dilemma. The core principle, Knowledge Guidance, avoids full replacement and instead fuses global knowledge into preserved local embeddings, attaining the personalization benefits of dual-knowledge within a single-knowledge memory footprint. Furthermore, we introduce Adaptive Guidance, a fine-grained mechanism that dynamically modulates the intensity of this guidance for each user-item interaction, overcoming the limitations of static fusion methods. Extensive experiments on benchmark datasets demonstrate that FedRKG significantly outperforms state-of-the-art methods, validating the effectiveness of our approach. The code is available at https://github.com/Jaehyung-Lim/fedrkg.

---

## [4/10] Fine-Tuning Diffusion-Based Recommender Systems via Reinforcement Learning with Reward Function Optimization
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251106937H/abstract
**Reasoning:** The focus on diffusion-based recommender systems and reinforcement learning is not directly related to our primary interests.
**Authors:** Hou, Yu, Li, Hua, Kim, Ha Young, Shin, Won-Yong

**Content/Abstract:**
> Diffusion models recently emerged as a powerful paradigm for recommender systems, offering state-of-the-art performance by modeling the generative process of user-item interactions. However, training such models from scratch is both computationally expensive and yields diminishing returns once convergence is reached. To remedy these challenges, we propose ReFiT, a new framework that integrates Reinforcement learning (RL)-based Fine-Tuning into diffusion-based recommender systems. In contrast to prior RL approaches for diffusion models depending on external reward models, ReFiT adopts a task-aligned design: it formulates the denoising trajectory as a Markov decision process (MDP) and incorporates a collaborative signal-aware reward function that directly reflects recommendation quality. By tightly coupling the MDP structure with this reward signal, ReFiT empowers the RL agent to exploit high-order connectivity for fine-grained optimization, while avoiding the noisy or uninformative feedback common in naive reward designs. Leveraging policy gradient optimization, ReFiT maximizes exact log-likelihood of observed interactions, thereby enabling effective post hoc fine-tuning of diffusion recommenders. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed ReFiT framework (a) exhibits substantial performance gains over strong competitors (up to 36.3% on sequential recommendation), (b) demonstrates strong efficiency with linear complexity in the number of users or items, and (c) generalizes well across multiple diffusion-based recommendation scenarios. The source code and datasets are publicly available at https://anonymous.4open.science/r/ReFiT-4C60.

---

## [4/10] LLM-Aligned Geographic Item Tokenization for Local-Life Recommendation
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251114221J/abstract
**Reasoning:** The geographic item tokenization for recommendations is not aligned with our primary interests in code and context engineering.
**Authors:** Jiang, Hao, Wang, Guoquan, Zhou, Donglin, Yu, Sheng, Zeng, Yang, Zeng, Wencong, Gai, Kun, Zhou, Guorui

**Content/Abstract:**
> Recent advances in Large Language Models (LLMs) have enhanced text-based recommendation by enriching traditional ID-based methods with semantic generalization capabilities. Text-based methods typically encode item textual information via prompt design and generate discrete semantic IDs through item tokenization. However, in domain-specific tasks such as local-life services, simply injecting location information into prompts fails to capture fine-grained spatial characteristics and real-world distance awareness among items. To address this, we propose LGSID, an LLM-Aligned Geographic Item Tokenization Framework for Local-life Recommendation. This framework consists of two key components: (1) RL-based Geographic LLM Alignment, and (2) Hierarchical Geographic Item Tokenization. In the RL-based alignment module, we initially train a list-wise reward model to capture real-world spatial relationships among items. We then introduce a novel G-DPO algorithm that uses pre-trained reward model to inject generalized spatial knowledge and collaborative signals into LLMs while preserving their semantic understanding. Furthermore, we propose a hierarchical geographic item tokenization strategy, where primary tokens are derived from discrete spatial and content attributes, and residual tokens are refined using the aligned LLM's geographic representation vectors. Extensive experiments on real-world Kuaishou industry datasets show that LGSID consistently outperforms state-of-the-art discriminative and generative recommendation models. Ablation studies, visualizations, and case studies further validate its effectiveness.

---

## [4/10] Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251110492Z/abstract
**Reasoning:** The focus on human priors in recommender systems is not directly related to our interests in code intelligence or context engineering.
**Authors:** Zhang, Yunkai, Zhang, Qiang, Lin, Feng, Qiu, Ruizhong, Yu, Hanchao, Liu, Jiayi, Xia, Yinglong, Yu, Zhuoran, Zheng, Zeyu, Yang, Diji

**Content/Abstract:**
> Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner. Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.

---

## [4/10] Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251115061C/abstract
**Reasoning:** The focus is on genomic question answering using multi-agent systems, which is tangentially related to AI agents but not directly applicable to software engineering.
**Authors:** Chen, Haodong, Zuccon, Guido, Leelanupab, Teerapong

**Content/Abstract:**
> Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization. In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution. OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

---

## [4/10] Learned-Rule-Augmented Large Language Model Evaluators
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T18:08:45.000Z
**URL:** https://arxiv.org/abs/2512.01958v1
**Reasoning:** The focus on LLM evaluators for NLG tasks is not directly related to code intelligence or context engineering.
**Authors:** Jie Meng

**Content/Abstract:**
> Large language models (LLMs) are predominantly used as evaluators for natural language generation (NLG) tasks, but their application to broader evaluation scenarios remains limited. In this work, we explore the potential of LLMs as general evaluators across diverse tasks. Although LLM-based evaluators have made progress in different areas, existing methods struggle to generalize due to their reliance on costly, human-designed evaluation principles, which are often misaligned with both annotated data and LLMs' understanding.To address these challenges, we propose a rule-augmented evaluation paradigm. First, we introduce a rule distillation method that automatically extracts scoring rules from data using an LLM-assisted Monte Carlo Tree Search (MCTS), alleviating scalability issues and improving alignment with data. Second, to enable LLMs to effectively apply the learned rules, we propose two strategies: (1) Chain-of-Rule (CoR), which guides LLM to follow distilled rules, and (2) training a rule-augmented LLM evaluator (RuAE) via reinforcement learning, further bridging the gap between rules and LLMs' reasoning. Extensive experiments on diverse tasks demonstrate the effectiveness and generalizability of our approach across various evaluation scenarios.

---

## [4/10] Rectifying LLM Thought from Lens of Optimization
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T17:41:08.000Z
**URL:** https://arxiv.org/abs/2512.01925v1
**Reasoning:** The analysis of LLM reasoning through optimization is not directly related to code intelligence or context engineering.
**Authors:** Junnan Liu

**Content/Abstract:**
> Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.

---

## [4/10] Latent Debate: A Surrogate Framework for Interpreting LLM Thinking
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T17:27:31.000Z
**URL:** https://arxiv.org/abs/2512.01909v1
**Reasoning:** The framework for interpreting LLM thinking is not directly related to code intelligence or context engineering.
**Authors:** Lihu Chen

**Content/Abstract:**
> Understanding the internal thinking process of Large Language Models (LLMs) and the cause of hallucinations remains a key challenge. To this end, we introduce latent debate, a novel framework for interpreting model predictions through the lens of implicit internal arguments. Unlike the current work of self-consistency and multi-agent debate, which relies on explicit debates among multiple answers or multiple models, latent debate captures the hidden supporting and attacking signals that arise within a single model during a single inference. We first present a model- and task-agnostic conceptual framework, and then instantiate it symbolically to approximate the thinking process of LLMs on True/False prediction tasks. Empirical studies demonstrate that latent debate is a faithful structured surrogate model that has highly consistent predictions with the original LLM. Beyond interpretability, we demonstrate that latent debate provides a strong baseline for hallucination detection. Further analysis reveals strong correlations between hallucinations and debate patterns, such as a high degree of latent debates in the middle layers is linked to a higher risk of hallucinations. These findings position latent debate as a potential framework for understanding internal mechanisms of LLMs, especially for scenarios where internal (dis)agreements appear during the inference steps.

---

## [4/10] OPOR-Bench: Evaluating Large Language Models on Online Public Opinion Report Generation
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T17:18:02.000Z
**URL:** https://arxiv.org/abs/2512.01896v1
**Reasoning:** The evaluation of LLMs for public opinion report generation is not directly related to code intelligence or context engineering.
**Authors:** Jinzheng Yu

**Content/Abstract:**
> Online Public Opinion Reports consolidate news and social media for timely crisis management by governments and enterprises. While large language models have made automated report generation technically feasible, systematic research in this specific area remains notably absent, particularly lacking formal task definitions and corresponding benchmarks. To bridge this gap, we define the Automated Online Public Opinion Report Generation (OPOR-GEN) task and construct OPOR-BENCH, an event-centric dataset covering 463 crisis events with their corresponding news articles, social media posts, and a reference summary. To evaluate report quality, we propose OPOR-EVAL, a novel agent-based framework that simulates human expert evaluation by analyzing generated reports in context. Experiments with frontier models demonstrate that our framework achieves high correlation with human judgments. Our comprehensive task definition, benchmark dataset, and evaluation framework provide a solid foundation for future research in this critical domain.

---

## [4/10] Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation Systems
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T15:26:20.000Z
**URL:** https://arxiv.org/abs/2512.01786v1
**Reasoning:** The focus on LLM evaluation systems is not directly related to code intelligence or context engineering.
**Authors:** Xiaochuan Li

**Content/Abstract:**
> As Large Language Models (LLMs) become integrated into high-stakes domains, there is a growing need for evaluation methods that are both scalable for real-time deployment and reliable for critical decision-making. While human evaluation is reliable, it is slow and costly. Single LLM judges are biased, and static juries lack adaptability. To overcome these limitations, we propose LLM Jury-on-Demand - a dynamic, learning-based framework for scalable and context-aware evaluation. Our method trains a set of reliability predictors to assess when LLM judges will agree with human experts, leveraging token distributions, embeddings, and structural input features. This enables a fully adaptive evaluation where, for each data point, an optimal jury of the most reliable judges is dynamically selected, and their scores are aggregated using their reliability as weights. Experiments on summarization and RAG benchmarks show that our dynamic jury system achieves significantly higher correlation with human judgment than both single-judge and static-jury baselines. These results highlight the promise of adaptive, learning-based juries for building scalable, more reliable and trustworthy evaluation systems for modern LLMs in high-stakes domains.

---

## [4/10] Design Patterns for Data Engineers: Cleaner ETL with the Builder Pattern.
**Source:** The Practical Developer | **Date:** 2025-12-02T14:33:55.000Z
**URL:** https://dev.to/cristianbergamo/design-patterns-for-data-engineers-cleaner-etl-with-the-builder-pattern-4bi7
**Reasoning:** The article on ETL design patterns is not directly related to code intelligence or context engineering.
**Authors:** Cristian Bergamo

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fdyv5buvt1pcvx9w1952i.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>In my job, I often end up writing long ETL pipelines in Python and PySpark. The usual story: you read a bunch of tables, join them together, run several preprocessing steps where the output of one step becomes the input of another, and meanwhile, requirements keep changing --&gt; new rules, new table names, renamed columns, and so on.</p>  
>   
> <p>Recently, I had to refactor a client function that was doing exactly this. It received several Spark DataFrames as input and was supposed to return one “clean” transactions DataFrame, ready to be written to the database. Inside, it was basically just calling a chain of helper functions in a fixed order. Each helper returned a DataFrame that either contributed to the final result or was used as input for some other step further down the pipeline.</p>  
>   
> <p>The problem was that everything was tightly coupled. Each helper depended (sometimes implicitly) on what the previous helpers were doing. Changing a single line in one of them meant double-checking a lot of other functions to make sure nothing broke. It was fragile, and refactoring was painful.</p>  
>   
> <p>In situations like this, the Builder design pattern turned out to be a lifesaver for me. It’s very handy when the object you want to build is the result of several processing steps, not just a few fields assigned inside an <code>__init__</code>. If you want a proper deep dive into the pattern itself, I recommend this page: <a href="https://refactoring.guru/design-patterns/builder">https://refactoring.guru/design-patterns/builder</a>. I first studied it in the book <em>Python Design Patterns</em> by Ayeva and Kasampalis, which I’d also recommend.</p>  
>   
> <p>In the example below, we’ll define:</p>  
>   
> <ul>  
> <li>a <strong>Transactions</strong> class – with very little responsibility: it just defines the attributes that make up our final object (raw inputs, intermediate tables, final output);</li>  
> <li>a <strong>TransactionsBuilderType1</strong> class – which receives the input DataFrames, creates a Transactions instance, and exposes the methods that progressively build each attribute;</li>  
> <li>a <strong>Director</strong> class – which knows the builder interface, has one main method that runs the builder steps in the right order, and exposes a method/property to return the final Transactions object once everything is done.  
> </li>  
> </ul>  
>   
> <div>  
> <pre><code>  
> class Transactions:  
>     def __init__(self, input_table_1, input_table_2):  
>         self.raw_input_table_1 = input_table_1  
>         self.raw_input_table_2 = input_table_2  
>   
>         self.preprocessed_table_1 = None  
>         self.preprocessed_table_2 = None  
>   
>         # Final ETL output table (e.g. cleaned, enriched transactions)  
>         self.preprocessed_transactions = None  
>   
>   
> class TransactionsBuilderType1:  
>     def __init__(self, input_table_1, input_table_2):  
>         self.transactions = Transactions(  
>             input_table_1=input_table_1,  
>             input_table_2=input_table_2,  
>         )  
>         ... # other builder-related attributes (configs, parameters, etc.)  
>   
>     def preprocess_table_1(self):  
>         # Business logic to preprocess input_table_1  
>         self.transactions.preprocessed_table_1 = ...  
>   
>     def preprocess_table_2(self):  
>         # Business logic to preprocess input_table_2  
>         self.transactions.preprocessed_table_2 = ...  
>   
>     def compute_final_table(self):  
>         # Use preprocessed tables to compute the final transactions table   
>         # (like self.preprocess_table_1.join(self.preprocess_table_2) etc.)  
>         self.transactions.preprocessed_transactions = ...   
>   
>   
> class Director:  
>     def __init__(self):  
>         self.builder = None  
>   
>     def construct_transactions(self, builder):  
>         self.builder = builder  
>         steps = [  
>             self.builder.preprocess_table_1,  
>             self.builder.preprocess_table_2,  
>             self.builder.compute_final_table,  
>         ]  
>   
>         for step in steps:  
>             step()  
>   
>     @property  
>     def transactions(self):  
>         # Return the fully built Transactions object  
>         return self.builder.transactions  
>   
>   
> # client code:  
>   
> input_table_1 = spark.table(...)  
> input_table_2 = spark.table(...)  
> builder = TransactionsBuilderType1(input_table_1, input_table_2)  
> director = Director()  
> director.construct_transactions(builder)  
>   
> preprocessed_transactions = director.transactions.preprocessed_transactions  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Let me know your thoughts!<br>  
> Thanks.</p>  
>   
> <p>Cristian Bergamo</p>

---

## [4/10] How to Use Vanna.ai to Query Your Database with Open-Source Language Models
**Source:** The Practical Developer | **Date:** 2025-12-02T14:15:40.000Z
**URL:** https://dev.to/aairom/how-to-use-vannaai-to-query-your-database-with-open-source-language-models-5ado
**Reasoning:** The article is about Text2SQL, which is not directly related to our primary or secondary interests.
**Authors:** Alain Airom

**Content/Abstract:**
> <p>Yet another Text2SQL exercise with Vanna.ai, Ollama, Granite and gpt-oss.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxg1nylxj8dvtsw4u2g04.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxg1nylxj8dvtsw4u2g04.png" alt="" width="800" height="800"></a></p> 
>  
> <h2> 
>    
>    
>   Introduction - What is Vanna.ai? 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F28kfmzxv996blqpaaxjn.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F28kfmzxv996blqpaaxjn.png" alt="" width="800" height="420"></a></p> 
>  
> <p>In the rapidly evolving world of data analytics, the search for the perfect Text-to-SQL generation tool often feels like a quest for the ‘Holy Grail’. I recently stumbled upon Vanna.ai, and it immediately stood out. At its core, <strong>Vanna.ai</strong> is a powerful agent designed to turn natural language questions into data insights. It achieves this by acting as a translator: taking a question from a user, converting it into valid SQL, executing that query against your database, and delivering the answer back in a rich format. More than just a simple query generator, Vanna 2.0 seems to be built for production, offering features like user-aware security, streaming responses (including interactive data tables and charts), and seamless integration with virtually any LLM (including open-source models like Granite and gpt-oss (among others…) and (almost) any database.</p> 
>  
> <p>As I mentioned, Vanna.ai seems to be fit for more than Text2SQL… but I stick to this part for my tests. You can refer to their site provided in “Links” to discover more. <strong>Last but not least; I have no affiliation with them!</strong></p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmlwlw0oqao67wk73p06h.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmlwlw0oqao67wk73p06h.png" alt="" width="800" height="467"></a></p> 
>  
> <h2> 
>    
>    
>   Implementation and Tests 
> </h2> 
>  
> <p>The magic of Vanna’s solution resides in their powerful, agent-based package, which handles everything from authenticating the user to selecting the right tools — like the <code>RunSqlTool</code>—to safely query the database and generate the final visualization. This robust architecture is the foundation of the sample application we'll be exploring from their site. So, let's jump into the test and coding steps!</p> 
>  
> <p><strong>To</strong> properly test Vanna’s capabilities and demonstrate its Text-to-SQL functionality, we first need a working environment with a database containing meaningful data. For maximum simplicity and portability, I chose to create and populate a SQLite database. As a file-based SQL engine, SQLite eliminates the need for a separate server setup, making it the fastest and easiest way to get our Users and Commands tables ready for the Vanna agent to query.</p> 
>  
> <p>So let’s jump into coding… 🪂</p> 
>  
> <h2> 
>    
>    
>   Prepare a / your Database 
> </h2> 
>  
> <ul> 
> <li>Preparing the environment 🧑‍🍳 
> </li> 
> </ul> 
>  
> <div> 
> <pre><code>python3 <span>-m</span> venv venv 
> <span>source </span>venv/bin/activate 
>  
> pip <span>install</span> <span>--upgrade</span> pip 
>  
> <span># to populate and generate data this package is nice!</span> 
> pip <span>install </span>faker 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ul> 
> <li>Create your tables in SQLite 
> </li> 
> </ul> 
>  
> <div> 
> <pre><code><span># setup_database.py 
> </span><span>import</span> <span>sqlite3</span> 
> <span>import</span> <span>os</span> 
>  
> <span># Define the database file name 
> </span><span>DB_NAME</span> <span>=</span> <span>'</span><span>users_commands.db</span><span>'</span> 
>  
> <span>def</span> <span>setup_database</span><span>():</span> 
>     <span>"""</span><span> 
>     Creates the SQLite database file and defines the Users and Commands tables. 
>     </span><span>"""</span> 
>     <span>if</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>exists</span><span>(</span><span>DB_NAME</span><span>):</span> 
>         <span>os</span><span>.</span><span>remove</span><span>(</span><span>DB_NAME</span><span>)</span> 
>  
>     <span>conn</span> <span>=</span> <span>sqlite3</span><span>.</span><span>connect</span><span>(</span><span>DB_NAME</span><span>)</span> 
>     <span>cursor</span> <span>=</span> <span>conn</span><span>.</span><span>cursor</span><span>()</span> 
>  
>     <span>cursor</span><span>.</span><span>execute</span><span>(</span><span>'''</span><span> 
>         CREATE TABLE Users ( 
>             ID INTEGER PRIMARY KEY, 
>             FirstName TEXT NOT NULL, 
>             LastName TEXT NOT NULL, 
>             DateOfBirth TEXT NOT NULL,  
>             Address TEXT, 
>             CommandList TEXT UNIQUE NOT NULL 
>         ) 
>     </span><span>'''</span><span>)</span> 
>  
>     <span>cursor</span><span>.</span><span>execute</span><span>(</span><span>'''</span><span> 
>         CREATE TABLE Commands ( 
>             ProductID TEXT PRIMARY KEY, 
>             ProductName TEXT NOT NULL, 
>             CommandNumber TEXT NOT NULL, 
>             FOREIGN KEY (CommandNumber) REFERENCES Users(CommandList) 
>         ) 
>     </span><span>'''</span><span>)</span> 
>  
>     <span>conn</span><span>.</span><span>commit</span><span>()</span> 
>     <span>conn</span><span>.</span><span>close</span><span>()</span> 
>     <span>print</span><span>(</span><span>f</span><span>"</span><span>✅ Successfully created database </span><span>'</span><span>{</span><span>DB_NAME</span><span>}</span><span>'</span><span> and tables </span><span>'</span><span>Users</span><span>'</span><span> and </span><span>'</span><span>Commands</span><span>'</span><span>.</span><span>"</span><span>)</span> 
>  
> <span>if</span> <span>__name__</span> <span>==</span> <span>"</span><span>__main__</span><span>"</span><span>:</span> 
>     <span>setup_database</span><span>()</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ul> 
> <li>Insert some data in the DB! 
> </li> 
> </ul> 
>  
> <div> 
> <pre><code><span># insert_data.py 
> </span><span>import</span> <span>sqlite3</span> 
> <span>from</span> <span>faker</span> <span>import</span> <span>Faker</span> 
> <span>import</span> <span>random</span> 
> <span>import</span> <span>uuid</span> 
>  
> <span>DB_NAME</span> <span>=</span> <span>'</span><span>users_commands.db</span><span>'</span> 
>  
> <span>def</span> <span>insert_random_data</span><span>(</span><span>num_records</span><span>=</span><span>10</span><span>):</span> 
>     <span>"""</span><span> 
>     Generates and inserts random user and command data into the database. 
>     </span><span>"""</span> 
>     <span>try</span><span>:</span> 
>         <span>conn</span> <span>=</span> <span>sqlite3</span><span>.</span><span>connect</span><span>(</span><span>DB_NAME</span><span>)</span> 
>         <span>cursor</span> <span>=</span> <span>conn</span><span>.</span><span>cursor</span><span>()</span> 
>         <span>fake</span> <span>=</span> <span>Faker</span><span>()</span> 
>  
>         <span>user_data</span> <span>=</span> <span>[]</span> 
>         <span>command_data</span> <span>=</span> <span>[]</span> 
>  
>         <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>num_records</span><span>):</span> 
>             <span>command_ref</span> <span>=</span> <span>str</span><span>(</span><span>uuid</span><span>.</span><span>uuid4</span><span>())</span> 
>  
>             <span>user_data</span><span>.</span><span>append</span><span>((</span> 
>                 <span>fake</span><span>.</span><span>first_name</span><span>(),</span> 
>                 <span>fake</span><span>.</span><span>last_name</span><span>(),</span> 
>                 <span>fake</span><span>.</span><span>date_of_birth</span><span>(</span><span>minimum_age</span><span>=</span><span>18</span><span>,</span> <span>maximum_age</span><span>=</span><span>65</span><span>).</span><span>strftime</span><span>(</span><span>'</span><span>%Y-%m-%d</span><span>'</span><span>),</span> 
>                 <span>fake</span><span>.</span><span>address</span><span>().</span><span>replace</span><span>(</span><span>'</span><span>\n</span><span>'</span><span>,</span> <span>'</span><span>, </span><span>'</span><span>),</span>  
>                 <span>command_ref</span> <span># The link field 
> </span>            <span>))</span> 
>  
>             <span>product_id</span> <span>=</span> <span>f</span><span>"</span><span>PROD-</span><span>{</span><span>random</span><span>.</span><span>randint</span><span>(</span><span>1000</span><span>,</span> <span>9999</span><span>)</span><span>}</span><span>"</span>  
>             <span>product_name</span> <span>=</span> <span>fake</span><span>.</span><span>word</span><span>().</span><span>capitalize</span><span>()</span> <span>+</span> <span>"</span><span> </span><span>"</span> <span>+</span> <span>fake</span><span>.</span><span>word</span><span>().</span><span>capitalize</span><span>()</span> 
>  
>             <span>command_data</span><span>.</span><span>append</span><span>((</span> 
>                 <span>product_id</span><span>,</span> 
>                 <span>product_name</span><span>,</span> 
>                 <span>command_ref</span>  
>             <span>))</span> 
>  
>         <span>cursor</span><span>.</span><span>executemany</span><span>(</span><span>'''</span><span> 
>             INSERT INTO Users (FirstName, LastName, DateOfBirth, Address, CommandList) 
>             VALUES (?, ?, ?, ?, ?) 
>         </span><span>'''</span><span>,</span> <span>user_data</span><span>)</span> 
>  
>         <span>cursor</span><span>.</span><span>executemany</span><span>(</span><span>'''</span><span> 
>             INSERT INTO Commands (ProductID, ProductName, CommandNumber) 
>             VALUES (?, ?, ?) 
>         </span><span>'''</span><span>,</span> <span>command_data</span><span>)</span> 
>  
>         <span>conn</span><span>.</span><span>commit</span><span>()</span> 
>         <span>print</span><span>(</span><span>f</span><span>"</span><span>✅ Successfully inserted </span><span>{</span><span>len</span><span>(</span><span>user_data</span><span>)</span><span>}</span><span> random records into **</span><span>'</span><span>Users</span><span>'</span><span>** and </span><span>'</span><span>Commands</span><span>'</span><span>.</span><span>"</span><span>)</span> 
>  
>     <span>except</span> <span>sqlite3</span><span>.</span><span>Error</span> <span>as</span> <span>e</span><span>:</span> 
>         <span>print</span><span>(</span><span>f</span><span>"</span><span>An error occurred: </span><span>{</span><span>e</span><span>}</span><span>"</span><span>)</span> 
>     <span>finally</span><span>:</span> 
>         <span>if</span> <span>conn</span><span>:</span> 
>             <span>conn</span><span>.</span><span>close</span><span>()</span> 
>  
> <span>if</span> <span>__name__</span> <span>==</span> <span>"</span><span>__main__</span><span>"</span><span>:</span> 
>     <span>insert_random_data</span><span>(</span><span>10</span><span>)</span> <span># Insert 10 lines of data!!! 
> </span></code></pre> 
>  
> </div> 
>  
>  
>  
> <ul> 
> <li>Test and Query the Database 🧪 
> </li> 
> </ul> 
>  
> <div> 
> <pre><code><span># query_database.py 
> </span><span>import</span> <span>sqlite3</span> 
> <span>import</span> <span>os</span> 
> <span>from</span> <span>typing</span> <span>import</span> <span>List</span><span>,</span> <span>Tuple</span> 
>  
> <span>DB_NAME</span> <span>=</span> <span>'</span><span>users_commands.db</span><span>'</span> 
> <span>OUTPUT_DIR</span> <span>=</span> <span>'</span><span>output</span><span>'</span> 
> <span>REPORT_PATH</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>OUTPUT_DIR</span><span>,</span> <span>'</span><span>database_report.md</span><span>'</span><span>)</span> 
>  
>  
> <span>def</span> <span>format_table_data</span><span>(</span><span>title</span><span>:</span> <span>str</span><span>,</span> <span>headers</span><span>:</span> <span>List</span><span>[</span><span>str</span><span>],</span> <span>data</span><span>:</span> <span>List</span><span>[</span><span>Tuple</span><span>],</span> <span>is_markdown</span><span>:</span> <span>bool</span><span>,</span> <span>col_widths</span><span>:</span> <span>List</span><span>[</span><span>int</span><span>]</span> <span>=</span> <span>None</span><span>)</span> <span>-&gt;</span> <span>str</span><span>:</span> 
>     <span>"""</span><span>Formats data into a printable string, either for console or Markdown.</span><span>"""</span> 
>     <span>output</span> <span>=</span> <span>""</span> 
>  
>     <span>if</span> <span>is_markdown</span><span>:</span> 
>         <span>output</span> <span>+=</span> <span>f</span><span>"</span><span>## </span><span>{</span><span>title</span><span>}</span><span>\n\n</span><span>"</span> 
>  
>         <span># Markdown Header Row 
> </span>        <span>output</span> <span>+=</span> <span>"</span><span>| </span><span>"</span> <span>+</span> <span>"</span><span> | </span><span>"</span><span>.</span><span>join</span><span>(</span><span>headers</span><span>)</span> <span>+</span> <span>"</span><span> |</span><span>\n</span><span>"</span> 
>         <span># Markdown Separator Row 
> </span>        <span>output</span> <span>+=</span> <span>"</span><span>|-</span><span>"</span> <span>+</span> <span>"</span><span>-|-</span><span>"</span><span>.</span><span>join</span><span>([</span><span>'</span><span>-</span><span>'</span> <span>*</span> <span>len</span><span>(</span><span>h</span><span>)</span> <span>for</span> <span>h</span> <span>in</span> <span>headers</span><span>])</span> <span>+</span> <span>"</span><span>-|</span><span>\n</span><span>"</span> 
>  
>         <span># Markdown Data Rows 
> </span>        <span>for</span> <span>row</span> <span>in</span> <span>data</span><span>:</span> 
>             <span># Ensure all elements are strings for joining 
> </span>            <span>str_row</span> <span>=</span> <span>[</span><span>str</span><span>(</span><span>col</span><span>)</span> <span>for</span> <span>col</span> <span>in</span> <span>row</span><span>]</span> 
>             <span>output</span> <span>+=</span> <span>"</span><span>| </span><span>"</span> <span>+</span> <span>"</span><span> | </span><span>"</span><span>.</span><span>join</span><span>(</span><span>str_row</span><span>)</span> <span>+</span> <span>"</span><span> |</span><span>\n</span><span>"</span> 
>         <span>output</span> <span>+=</span> <span>"</span><span>\n</span><span>"</span> 
>  
>     <span>else</span><span>:</span> <span># Console Formatting 
> </span>        <span>output</span> <span>+=</span> <span>"</span><span>-</span><span>"</span> <span>*</span> <span>70</span> <span>+</span> <span>"</span><span>\n</span><span>"</span> 
>         <span>output</span> <span>+=</span> <span>f</span><span>"</span><span>--- </span><span>{</span><span>title</span><span>}</span><span> ---</span><span>\n</span><span>"</span> 
>         <span>output</span> <span>+=</span> <span>"</span><span>-</span><span>"</span> <span>*</span> <span>70</span> <span>+</span> <span>"</span><span>\n</span><span>"</span> 
>  
>         <span>if</span> <span>not</span> <span>data</span><span>:</span> 
>             <span>output</span> <span>+=</span> <span>"</span><span>The table is empty.</span><span>\n</span><span>"</span> 
>             <span>return</span> <span>output</span> 
>  
>         <span>if</span> <span>not</span> <span>col_widths</span> <span>or</span> <span>len</span><span>(</span><span>col_widths</span><span>)</span> <span>!=</span> <span>len</span><span>(</span><span>headers</span><span>):</span> 
>             <span>col_widths</span> <span>=</span> <span>[</span><span>len</span><span>(</span><span>h</span><span>)</span> <span>for</span> <span>h</span> <span>in</span> <span>headers</span><span>]</span> <span># Simple fallback 
> </span> 
>         <span>header_row</span> <span>=</span> <span>""</span> 
>         <span>for</span> <span>i</span><span>,</span> <span>header</span> <span>in</span> <span>enumerate</span><span>(</span><span>headers</span><span>):</span> 
>             <span>width</span> <span>=</span> <span>[</span><span>4</span><span>,</span> <span>15</span><span>,</span> <span>15</span><span>,</span> <span>12</span><span>,</span> <span>25</span><span>,</span> <span>36</span><span>][</span><span>i</span><span>]</span> <span>if</span> <span>len</span><span>(</span><span>headers</span><span>)</span> <span>==</span> <span>6</span> <span>else</span> <span>[</span><span>15</span><span>,</span> <span>15</span><span>,</span> <span>25</span><span>,</span> <span>15</span><span>][</span><span>i</span><span>]</span> 
>             <span>header_row</span> <span>+=</span> <span>f</span><span>"</span><span>{</span><span>header</span><span>:</span><span>&lt;</span><span>{</span><span>width</span><span>}}</span><span> | </span><span>"</span> 
>         <span>output</span> <span>+=</span> <span>header_row</span><span>.</span><span>strip</span><span>()</span> <span>+</span> <span>"</span><span>\n</span><span>"</span> 
>         <span>output</span> <span>+=</span> <span>"</span><span>-</span><span>"</span> <span>*</span> <span>(</span><span>len</span><span>(</span><span>header_row</span><span>)</span> <span>+</span> <span>5</span><span>)</span> <span>+</span> <span>"</span><span>\n</span><span>"</span> 
>  
>         <span>for</span> <span>row</span> <span>in</span> <span>data</span><span>:</span> 
>             <span>data_row</span> <span>=</span> <span>""</span> 
>             <span>for</span> <span>i</span><span>,</span> <span>item</span> <span>in</span> <span>enumerate</span><span>(</span><span>row</span><span>):</span> 
>                 <span>width</span> <span>=</span> <span>[</span><span>4</span><span>,</span> <span>15</span><span>,</span> <span>15</span><span>,</span> <span>12</span><span>,</span> <span>25</span><span>,</span> <span>36</span><span>][</span><span>i</span><span>]</span> <span>if</span> <span>len</span><span>(</span><span>headers</span><span>)</span> <span>==</span> <span>6</span> <span>else</span> <span>[</span><span>15</span><span>,</span> <span>15</span><span>,</span> <span>25</span><span>,</span> <span>15</span><span>][</span><span>i</span><span>]</span> 
>  
>                 <span>display_item</span> <span>=</span> <span>str</span><span>(</span><span>item</span><span>)</span> 
>                 <span>if</span> <span>i</span> <span>==</span> <span>4</span> <span>and</span> <span>len</span><span>(</span><span>headers</span><span>)</span> <span>==</span> <span>6</span><span>:</span> <span># Address column in Users table 
> </span>                    <span>display_item</span> <span>=</span> <span>display_item</span><span>[:</span><span>width</span> <span>-</span> <span>3</span><span>]</span> <span>+</span> <span>'</span><span>...</span><span>'</span> <span>if</span> <span>len</span><span>(</span><span>display_item</span><span>)</span> <span>&gt;</span> <span>width</span> <span>else</span> <span>display_item</span> 
>  
>                 <span>data_row</span> <span>+=</span> <span>f</span><span>"</span><span>{</span><span>display_item</span><span>:</span><span>&lt;</span><span>{</span><span>width</span><span>}}</span><span> | </span><span>"</span> 
>  
>             <span>output</span> <span>+=</span> <span>data_row</span><span>.</span><span>strip</span><span>()</span> <span>+</span> <span>"</span><span>\n</span><span>"</span> 
>         <span>output</span> <span>+=</span> <span>"</span><span>\n</span><span>"</span> 
>  
>     <span>return</span> <span>output</span> 
>  
> <span>def</span> <span>query_and_display_data</span><span>():</span> 
>     <span>"""</span><span> 
>     Connects to the database, queries the data, prints to console,  
>     and saves the results to a Markdown file. 
>     </span><span>"""</span> 
>     <span>conn</span> <span>=</span> <span>None</span> 
>     <span>try</span><span>:</span> 
>         <span>conn</span> <span>=</span> <span>sqlite3</span><span>.</span><span>connect</span><span>(</span><span>DB_NAME</span><span>)</span> 
>         <span>cursor</span> <span>=</span> <span>conn</span><span>.</span><span>cursor</span><span>()</span> 
>  
>         <span>cursor</span><span>.</span><span>execute</span><span>(</span><span>"</span><span>SELECT ID, FirstName, LastName, DateOfBirth, Address, CommandList FROM Users</span><span>"</span><span>)</span> 
>         <span>users_data</span> <span>=</span> <span>cursor</span><span>.</span><span>fetchall</span><span>()</span> 
>         <span>users_headers</span> <span>=</span> <span>[</span><span>'</span><span>ID</span><span>'</span><span>,</span> <span>'</span><span>First Name</span><span>'</span><span>,</span> <span>'</span><span>Last Name</span><span>'</span><span>,</span> <span>'</span><span>DOB</span><span>'</span><span>,</span> <span>'</span><span>Address</span><span>'</span><span>,</span> <span>'</span><span>CommandList Ref</span><span>'</span><span>]</span> 
>  
>         <span>if</span> <span>not</span> <span>users_data</span><span>:</span> 
>             <span>console_output</span> <span>=</span> <span>"</span><span>The Users table is empty. Please ensure </span><span>'</span><span>insert_data.py</span><span>'</span><span> was run.</span><span>"</span> 
>             <span>markdown_content</span> <span>=</span> <span>"</span><span># Database Report</span><span>\n\n</span><span>## Users and Commands Data</span><span>\n\n</span><span>"</span> <span>+</span> <span>console_output</span> 
>         <span>else</span><span>:</span> 
>             <span>join_query</span> <span>=</span> <span>"""</span><span> 
>                 SELECT 
>                     U.FirstName, 
>                     U.LastName, 
>                     C.ProductName, 
>                     C.ProductID 
>                 FROM 
>                     Users U 
>                 INNER JOIN 
>                     Commands C ON U.CommandList = C.CommandNumber 
>                 ORDER BY 
>                     U.ID; 
>             </span><span>"""</span> 
>             <span>cursor</span><span>.</span><span>execute</span><span>(</span><span>join_query</span><span>)</span> 
>             <span>joined_data</span> <span>=</span> <span>cursor</span><span>.</span><span>fetchall</span><span>()</span> 
>             <span>joined_headers</span> <span>=</span> <span>[</span><span>'</span><span>First Name</span><span>'</span><span>,</span> <span>'</span><span>Last Name</span><span>'</span><span>,</span> <span>'</span><span>Product Name</span><span>'</span><span>,</span> <span>'</span><span>Product ID</span><span>'</span><span>]</span> 
>  
>             <span>console_output</span> <span>=</span> <span>format_table_data</span><span>(</span><span>"</span><span>1. Listing All Records from the </span><span>'</span><span>Users</span><span>'</span><span> Table</span><span>"</span><span>,</span> <span>users_headers</span><span>,</span> <span>users_data</span><span>,</span> <span>is_markdown</span><span>=</span><span>False</span><span>)</span> 
>             <span>console_output</span> <span>+=</span> <span>"</span><span>\n</span><span>"</span> <span>*</span> <span>2</span> 
>             <span>console_output</span> <span>+=</span> <span>format_table_data</span><span>(</span><span>"</span><span>2. Listing Joined Data (</span><span>'</span><span>User</span><span>'</span><span> and </span><span>'</span><span>Command</span><span>'</span><span>) using INNER JOIN</span><span>"</span><span>,</span> <span>joined_headers</span><span>,</span> <span>joined_data</span><span>,</span> <span>is_markdown</span><span>=</span><span>False</span><span>)</span> 
>  
>             <span>markdown_content</span> <span>=</span> <span>"</span><span># Database Query Report</span><span>\n\n</span><span>"</span> 
>             <span>markdown_content</span> <span>+=</span> <span>format_table_data</span><span>(</span><span>"</span><span>User Records (Users Table)</span><span>"</span><span>,</span> <span>users_headers</span><span>,</span> <span>users_data</span><span>,</span> <span>is_markdown</span><span>=</span><span>True</span><span>)</span> 
>             <span>markdown_content</span> <span>+=</span> <span>"</span><span>\n</span><span>---</span><span>\n\n</span><span>"</span> 
>             <span>markdown_content</span> <span>+=</span> <span>format_table_data</span><span>(</span><span>"</span><span>User Commands (Joined Data)</span><span>"</span><span>,</span> <span>joined_headers</span><span>,</span> <span>joined_data</span><span>,</span> <span>is_markdown</span><span>=</span><span>True</span><span>)</span> 
>  
>  
>         <span># Console Display 
> </span>        <span>print</span><span>(</span><span>console_output</span><span>)</span> 
>  
>         <span># File Save 
> </span>        <span>os</span><span>.</span><span>makedirs</span><span>(</span><span>OUTPUT_DIR</span><span>,</span> <span>exist_ok</span><span>=</span><span>True</span><span>)</span> 
>  
>         <span>with</span> <span>open</span><span>(</span><span>REPORT_PATH</span><span>,</span> <span>'</span><span>w</span><span>'</span><span>)</span> <span>as</span> <span>f</span><span>:</span> 
>             <span>f</span><span>.</span><span>write</span><span>(</span><span>markdown_content</span><span>)</span> 
>  
>         <span>print</span><span>(</span><span>f</span><span>"</span><span>\n</span><span>✅ Successfully saved database report to: </span><span>{</span><span>REPORT_PATH</span><span>}</span><span>"</span><span>)</span> 
>  
>     <span>except</span> <span>sqlite3</span><span>.</span><span>Error</span> <span>as</span> <span>e</span><span>:</span> 
>         <span>print</span><span>(</span><span>f</span><span>"</span><span>\n</span><span>❌ Database Error: </span><span>{</span><span>e</span><span>}</span><span>"</span><span>)</span> 
>         <span>print</span><span>(</span><span>f</span><span>"</span><span>Please ensure the database file </span><span>'</span><span>{</span><span>DB_NAME</span><span>}</span><span>'</span><span> exists and the tables were correctly set up.</span><span>"</span><span>)</span> 
>     <span>except</span> <span>Exception</span> <span>as</span> <span>e</span><span>:</span> 
>         <span>print</span><span>(</span><span>f</span><span>"</span><span>\n</span><span>❌ An unexpected error occurred: </span><span>{</span><span>e</span><span>}</span><span>"</span><span>)</span> 
>     <span>finally</span><span>:</span> 
>         <span>if</span> <span>conn</span><span>:</span> 
>             <span>conn</span><span>.</span><span>close</span><span>()</span> 
>  
> <span>if</span> <span>__name__</span> <span>==</span> <span>"</span><span>__main__</span><span>"</span><span>:</span> 
>     <span>query_and_display_data</span><span>()</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ul> 
> <li>Populate and build your database. 
> </li> 
> </ul> 
>  
> <div> 
> <pre><code>python setup_database.py 
> python insert_data.py 
> python query_database.py 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ul> 
> <li>This will give you both a console output and a markdown file. 
> </li> 
> </ul> 
>  
> <div> 
> <pre><code><span>&gt; python query_database.py 
> ---------------------------------------------------------------------- 
> --- 1. Listing All Records from the 'Users' Table --- 
> ---------------------------------------------------------------------- 
> ID   | First Name      | Last Name       | DOB          | Address                   | CommandList Ref                      | 
> ---------------------------------------------------------------------------------------------------------------------------------- 
> </span>1    | David           | Frank           | 1983-03-22   | 3973 Carmen Gateway Su... | 173e986d-77b0-4c6e-9314-52813419f108 | 
> 2    | Sydney          | Middleton       | 1981-09-24   | 4080 Regina Lake Apt. ... | 5bd1cf9a-2cfc-46e5-b150-77c368efaccb | 
> 3    | Joshua          | Holder          | 2000-06-19   | 0072 Sanchez Hollow, P... | 68009684-34a9-4be0-8f67-ef26155d6cfd | 
> 4    | Robert          | Walker          | 1968-01-10   | USS Lee, FPO AP 42065     | dc60fd66-f59b-486b-8f04-4162bdf9c99e | 
> 5    | Douglas         | Johnson         | 1999-08-18   | 51189 Jacqueline Shore... | 80c13c6f-c5f7-4df7-abfe-f91ced3e1237 | 
> 6    | James           | Hernandez       | 1976-10-28   | Unit 7218 Box 1614, DP... | 9b4dee77-37ad-4fa4-8672-a7c053e71429 | 
> 7    | Diana           | Thomas          | 2004-02-29   | 84300 Vaughn Crossroad... | 7062b085-18b4-40c3-8e4e-c3bd6c214e08 | 
> 8    | Jamie           | Torres          | 1961-06-22   | 857 Pennington Flats S... | 31725920-e0c3-4fac-bd42-31d113e6e107 | 
> 9    | Renee           | Rice            | 2006-09-04   | 963 Marvin Underpass, ... | 8536b45b-3502-480a-a9be-89189e68478e | 
> 10   | Henry           | Wallace         | 1962-04-18   | 79093 Danielle Haven S... | 83c466e4-239f-4844-8a79-6286be0edb30 |<span> 
>  
>  
>  
> </span><span>----------------------------------------------------------------------</span> 
> <span>--- 2. Listing Joined Data ('User' and 'Command') using INNER JOIN --- 
> ---------------------------------------------------------------------- 
> First Name      | Last Name       | Product Name              | Product ID      | 
> --------------------------------------------------------------------------------------- 
> </span>David           | Frank           | Wife Choose               | PROD-3937       | 
> Sydney          | Middleton       | Produce Fall              | PROD-5279       | 
> Joshua          | Holder          | Plan Sell                 | PROD-5104       | 
> Robert          | Walker          | Interesting Guy           | PROD-3216       | 
> Douglas         | Johnson         | Time At                   | PROD-8672       | 
> James           | Hernandez       | Someone Mouth             | PROD-4010       | 
> Diana           | Thomas          | Pressure Move             | PROD-3587       | 
> Jamie           | Torres          | It Every                  | PROD-8192       | 
> Renee           | Rice            | Writer Still              | PROD-7510       | 
> Henry           | Wallace         | Professional Scene        | PROD-7318       |<span> 
>  
>  
>  
> </span>✅ Successfully saved database report to: output/database_report.md 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ul> 
> <li>Also, I made a GUI interface to see my database and the data (pretty hard-coded stuff, but we get the idea) 🈁 
> </li> 
> </ul> 
>  
> <div> 
> <pre><code><span>&lt;!DOCTYPE html&gt;</span> 
> <span>&lt;html</span> <span>lang=</span><span>"en"</span><span>&gt;</span> 
> <span>&lt;head&gt;</span> 
>     <span>&lt;meta</span> <span>charset=</span><span>"UTF-8"</span><span>&gt;</span> 
>     <span>&lt;meta</span> <span>name=</span><span>"viewport"</span> <span>content=</span><span>"width=device-width, initial-scale=1.0"</span><span>&gt;</span> 
>     <span>&lt;title&gt;</span>SQLite Database Browser (Users <span>&amp;</span> Commands)<span>&lt;/title&gt;</span> 
>     <span>&lt;script </span><span>src=</span><span>"https://cdn.tailwindcss.com"</span><span>&gt;&lt;/script&gt;</span> 
>     <span>&lt;script </span><span>src=</span><span>"https://cdnjs.cloudflare.com/ajax/libs/sql.js/1.10.3/sql-wasm.js"</span><span>&gt;&lt;/script&gt;</span> 
>     <span>&lt;style&gt;</span> 
>         <span>body</span> <span>{</span> <span>font-family</span><span>:</span> <span>'Inter'</span><span>,</span> <span>sans-serif</span><span>;</span> <span>background-color</span><span>:</span> <span>#f7f9fb</span><span>;</span> <span>}</span> 
>         <span>.container</span> <span>{</span> <span>max-width</span><span>:</span> <span>1200px</span><span>;</span> <span>}</span> 
>         <span>.table-container</span> <span>{</span> <span>max-height</span><span>:</span> <span>400px</span><span>;</span> <span>overflow-y</span><span>:</span> <span>auto</span><span>;</span> <span>background-color</span><span>:</span> <span>white</span><span>;</span> <span>border-radius</span><span>:</span> <span>0.5rem</span><span>;</span> <span>}</span> 
>         <span>.sql-textarea</span> <span>{</span> <span>font-family</span><span>:</span> <span>monospace</span><span>;</span> <span>}</span> 
>         <span>table</span> <span>{</span> <span>width</span><span>:</span> <span>100%</span><span>;</span> <span>border-collapse</span><span>:</span> <span>collapse</span><span>;</span> <span>}</span> 
>         <span>th</span><span>,</span> <span>td</span> <span>{</span> <span>padding</span><span>:</span> <span>8px</span> <span>12px</span><span>;</span> <span>text-align</span><span>:</span> <span>left</span><span>;</span> <span>border-bottom</span><span>:</span> <span>1px</span> <span>solid</span> <span>#e5e7eb</span><span>;</span> <span>}</span> 
>         <span>th</span> <span>{</span> <span>background-color</span><span>:</span> <span>#1e40af</span><span>;</span> <span>color</span><span>:</span> <span>white</span><span>;</span> <span>position</span><span>:</span> <span>sticky</span><span>;</span> <span>top</span><span>:</span> <span>0</span><span>;</span> <span>}</span> 
>         <span>.btn</span> <span>{</span> <span>transition</span><span>:</span> <span>background-color</span> <span>0.2s</span><span>;</span> <span>}</span> 
>         <span>.btn</span><span>:hover</span> <span>{</span> <span>filter</span><span>:</span> <span>brightness</span><span>(</span><span>1.1</span><span>);</span> <span>}</span> 
>     <span>&lt;/style&gt;</span> 
> <span>&lt;/head&gt;</span> 
> <span>&lt;body</span> <span>class=</span><span>"p-6"</span><span>&gt;</span> 
>  
>     <span>&lt;div</span> <span>id=</span><span>"app"</span> <span>class=</span><span>"container mx-auto space-y-8"</span><span>&gt;</span> 
>         <span>&lt;h1</span> <span>class=</span><span>"text-4xl font-bold text-gray-800 border-b-4 border-indigo-600 pb-2"</span><span>&gt;</span>Database Viewer Interface<span>&lt;/h1&gt;</span> 
>  
>         <span>&lt;div</span> <span>class=</span><span>"bg-white p-6 rounded-xl shadow-lg flex flex-col md:flex-row justify-between items-start md:items-center space-y-4 md:space-y-0"</span><span>&gt;</span> 
>             <span>&lt;div</span> <span>id=</span><span>"status"</span> <span>class=</span><span>"text-lg font-semibold text-green-700"</span><span>&gt;</span>Database initializing...<span>&lt;/div&gt;</span> 
>             <span>&lt;button</span> <span>id=</span><span>"resetDbBtn"</span> <span>class=</span><span>"btn bg-red-500 hover:bg-red-600 text-white font-bold py-2 px-4 rounded-lg shadow-md"</span> <span>onclick=</span><span>"initDb(true)"</span><span>&gt;</span> 
>                 Reset Database <span>&amp;</span> Reload Data 
>             <span>&lt;/button&gt;</span> 
>         <span>&lt;/div&gt;</span> 
>  
>         <span>&lt;!-- Predefined Queries --&gt;</span> 
>         <span>&lt;div</span> <span>class=</span><span>"bg-white p-6 rounded-xl shadow-lg space-y-4"</span><span>&gt;</span> 
>             <span>&lt;h2</span> <span>class=</span><span>"text-2xl font-semibold text-gray-700"</span><span>&gt;</span>Predefined Queries<span>&lt;/h2&gt;</span> 
>             <span>&lt;div</span> <span>class=</span><span>"flex flex-wrap gap-4"</span><span>&gt;</span> 
>                 <span>&lt;button</span> <span>class=</span><span>"btn bg-indigo-500 hover:bg-indigo-600 text-white font-semibold py-2 px-4 rounded-lg shadow-md"</span> <span>onclick=</span><span>"displayUsers()"</span><span>&gt;</span> 
>                     Show All Users 
>                 <span>&lt;/button&gt;</span> 
>                 <span>&lt;button</span> <span>class=</span><span>"btn bg-indigo-500 hover:bg-indigo-600 text-white font-semibold py-2 px-4 rounded-lg shadow-md"</span> <span>onclick=</span><span>"displayJoinedData()"</span><span>&gt;</span> 
>                     Show Users <span>&amp;</span> Commands (JOIN) 
>                 <span>&lt;/button&gt;</span> 
>             <span>&lt;/div&gt;</span> 
>         <span>&lt;/div&gt;</span> 
>  
>         <span>&lt;div</span> <span>class=</span><span>"bg-white p-6 rounded-xl shadow-lg space-y-4"</span><span>&gt;</span> 
>             <span>&lt;h2</span> <span>class=</span><span>"text-2xl font-semibold text-gray-700"</span><span>&gt;</span>Custom SQL Query<span>&lt;/h2&gt;</span> 
>             <span>&lt;textarea</span> <span>id=</span><span>"sqlInput"</span> <span>class=</span><span>"sql-textarea w-full p-3 border-2 border-gray-300 rounded-lg focus:ring-indigo-500 focus:border-indigo-500"</span> <span>rows=</span><span>"4"</span> <span>placeholder=</span><span>"e.g., SELECT * FROM Users WHERE FirstName = 'Alex'"</span><span>&gt;&lt;/textarea&gt;</span> 
>             <span>&lt;button</span> <span>class=</span><span>"btn bg-blue-600 hover:bg-blue-700 text-white font-semibold py-2 px-4 rounded-lg shadow-md"</span> <span>onclick=</span><span>"executeCustomQuery()"</span><span>&gt;</span> 
>                 Execute Query 
>             <span>&lt;/button&gt;</span> 
>         <span>&lt;/div&gt;</span> 
>  
>         <span>&lt;div</span> <span>class=</span><span>"space-y-4"</span><span>&gt;</span> 
>             <span>&lt;h2</span> <span>id=</span><span>"resultTitle"</span> <span>class=</span><span>"text-3xl font-bold text-gray-800"</span><span>&gt;</span>Query Results<span>&lt;/h2&gt;</span> 
>             <span>&lt;div</span> <span>id=</span><span>"results"</span> <span>class=</span><span>"table-container border-2 border-gray-200"</span><span>&gt;</span> 
>                 <span>&lt;p</span> <span>class=</span><span>"p-4 text-gray-500"</span><span>&gt;</span>Run a query above to see results.<span>&lt;/p&gt;</span> 
>             <span>&lt;/div&gt;</span> 
>         <span>&lt;/div&gt;</span> 
>  
>     <span>&lt;/div&gt;</span> 
>  
>     <span>&lt;script&gt;</span> 
>         <span>let</span> <span>db</span> <span>=</span> <span>null</span><span>;</span> 
>         <span>let</span> <span>SQL</span> <span>=</span> <span>null</span><span>;</span> 
>         <span>const</span> <span>DB_NAME</span> <span>=</span> <span>'</span><span>users_commands.db</span><span>'</span><span>;</span> 
>         <span>const</span> <span>NUM_RECORDS</span> <span>=</span> <span>10</span><span>;</span> 
>         <span>const</span> <span>resultDiv</span> <span>=</span> <span>document</span><span>.</span><span>getElementById</span><span>(</span><span>'</span><span>results</span><span>'</span><span>);</span> 
>         <span>const</span> <span>resultTitle</span> <span>=</span> <span>document</span><span>.</span><span>getElementById</span><span>(</span><span>'</span><span>resultTitle</span><span>'</span><span>);</span> 
>         <span>const</span> <span>statusDiv</span> <span>=</span> <span>document</span><span>.</span><span>getElementById</span><span>(</span><span>'</span><span>status</span><span>'</span><span>);</span> 
>  
>         <span>const</span> <span>MOCK_DATA</span> <span>=</span> <span>{</span> 
>             <span>firstNames</span><span>:</span> <span>[</span><span>"</span><span>Alex</span><span>"</span><span>,</span> <span>"</span><span>Bella</span><span>"</span><span>,</span> <span>"</span><span>Chris</span><span>"</span><span>,</span> <span>"</span><span>Dana</span><span>"</span><span>,</span> <span>"</span><span>Ethan</span><span>"</span><span>,</span> <span>"</span><span>Fiona</span><span>"</span><span>,</span> <span>"</span><span>George</span><span>"</span><span>,</span> <span>"</span><span>Hannah</span><span>"</span><span>,</span> <span>"</span><span>Ivan</span><span>"</span><span>,</span> <span>"</span><span>Jasmine</span><span>"</span><span>],</span> 
>             <span>lastNames</span><span>:</span> <span>[</span><span>"</span><span>Smith</span><span>"</span><span>,</span> <span>"</span><span>Jones</span><span>"</span><span>,</span> <span>"</span><span>Williams</span><span>"</span><span>,</span> <span>"</span><span>Brown</span><span>"</span><span>,</span> <span>"</span><span>Davis</span><span>"</span><span>,</span> <span>"</span><span>Miller</span><span>"</span><span>,</span> <span>"</span><span>Wilson</span><span>"</span><span>,</span> <span>"</span><span>Moore</span><span>"</span><span>,</span> <span>"</span><span>Taylor</span><span>"</span><span>,</span> <span>"</span><span>Anderson</span><span>"</span><span>],</span> 
>             <span>addresses</span><span>:</span> <span>[</span> 
>                 <span>"</span><span>123 Main St, Anytown</span><span>"</span><span>,</span> <span>"</span><span>45 Oak Ave, Smallville</span><span>"</span><span>,</span> <span>"</span><span>78 Pine Ln, Big City</span><span>"</span><span>,</span> 
>                 <span>"</span><span>90 Maple Rd, Suburbia</span><span>"</span><span>,</span> <span>"</span><span>11 Elm Dr, Metropolis</span><span>"</span><span>,</span> <span>"</span><span>22 Birch Blvd, Village</span><span>"</span><span>,</span> 
>                 <span>"</span><span>33 Cedar Ct, Hamlet</span><span>"</span><span>,</span> <span>"</span><span>44 Spruce Sq, Town</span><span>"</span><span>,</span> <span>"</span><span>55 Willow Wy, County</span><span>"</span><span>,</span> 
>                 <span>"</span><span>66 Poplar Pk, District</span><span>"</span> 
>             <span>],</span> 
>             <span>productNames</span><span>:</span> <span>[</span><span>"</span><span>Laptop</span><span>"</span><span>,</span> <span>"</span><span>Monitor</span><span>"</span><span>,</span> <span>"</span><span>Keyboard</span><span>"</span><span>,</span> <span>"</span><span>Mouse</span><span>"</span><span>,</span> <span>"</span><span>Webcam</span><span>"</span><span>,</span> <span>"</span><span>Headset</span><span>"</span><span>,</span> <span>"</span><span>Router</span><span>"</span><span>,</span> <span>"</span><span>Speaker</span><span>"</span><span>,</span> <span>"</span><span>Printer</span><span>"</span><span>,</span> <span>"</span><span>Scanner</span><span>"</span><span>]</span> 
>         <span>};</span> 
>  
>         <span>function</span> <span>generateRandomData</span><span>()</span> <span>{</span> 
>             <span>const</span> <span>data</span> <span>=</span> <span>[];</span> 
>             <span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>NUM_RECORDS</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span> 
>                 <span>const</span> <span>commandRef</span> <span>=</span> <span>crypto</span><span>.</span><span>randomUUID</span><span>();</span> <span>// Unique link ID</span> 
>  
>                 <span>const</span> <span>year</span> <span>=</span> <span>1960</span> <span>+</span> <span>Math</span><span>.</span><span>floor</span><span>(</span><span>Math</span><span>.</span><span>random</span><span>()</span> <span>*</span> <span>30</span><span>);</span> 
>                 <span>const</span> <span>month</span> <span>=</span> <span>String</span><span>(</span><span>Math</span><span>.</span><span>floor</span><span>(</span><span>Math</span><span>.</span><span>random</span><span>()</span> <span>*</span> <span>12</span><span>)</span> <span>+</span> <span>1</span><span>).</span><span>padStart</span><span>(</span><span>2</span><span>,</span> <span>'</span><span>0</span><span>'</span><span>);</span> 
>                 <span>const</span> <span>day</span> <span>=</span> <span>String</span><span>(</span><span>Math</span><span>.</span><span>floor</span><span>(</span><span>Math</span><span>.</span><span>random</span><span>()</span> <span>*</span> <span>28</span><span>)</span> <span>+</span> <span>1</span><span>).</span><span>padStart</span><span>(</span><span>2</span><span>,</span> <span>'</span><span>0</span><span>'</span><span>);</span> 
>                 <span>const</span> <span>dob</span> <span>=</span> <span>`</span><span>${</span><span>year</span><span>}</span><span>-</span><span>${</span><span>month</span><span>}</span><span>-</span><span>${</span><span>day</span><span>}</span><span>`</span><span>;</span> 
>  
>                 <span>data</span><span>.</span><span>push</span><span>({</span> 
>                     <span>firstName</span><span>:</span> <span>MOCK_DATA</span><span>.</span><span>firstNames</span><span>[</span><span>i</span><span>],</span> 
>                     <span>lastName</span><span>:</span> <span>MOCK_DATA</span><span>.</span><span>lastNames</span><span>[</span><span>i</span><span>],</span> 
>                     <span>dob</span><span>:</span> <span>dob</span><span>,</span> 
>                     <span>address</span><span>:</span> <span>MOCK_DATA</span><span>.</span><span>addresses</span><span>[</span><span>i</span><span>],</span> 
>                     <span>commandList</span><span>:</span> <span>commandRef</span><span>,</span> 
>                     <span>// Command data</span> 
>                     <span>productID</span><span>:</span> <span>`PROD-</span><span>${</span><span>Math</span><span>.</span><span>floor</span><span>(</span><span>Math</span><span>.</span><span>random</span><span>()</span> <span>*</span> <span>9000</span><span>)</span> <span>+</span> <span>1000</span><span>}</span><span>`</span><span>,</span> 
>                     <span>productName</span><span>:</span> <span>MOCK_DATA</span><span>.</span><span>productNames</span><span>[</span><span>i</span><span>]</span> <span>+</span> <span>'</span><span> Pro</span><span>'</span><span>,</span> 
>                     <span>commandNumber</span><span>:</span> <span>commandRef</span> 
>                 <span>});</span> 
>             <span>}</span> 
>             <span>return</span> <span>data</span><span>;</span> 
>         <span>}</span> 
>  
>         <span>async</span> <span>function</span> <span>initDb</span><span>(</span><span>reset</span> <span>=</span> <span>false</span><span>)</span> <span>{</span> 
>             <span>statusDiv</span><span>.</span><span>textContent</span> <span>=</span> <span>"</span><span>Loading WebAssembly SQLite engine...</span><span>"</span><span>;</span> 
>             <span>try</span> <span>{</span> 
>                 <span>if </span><span>(</span><span>!</span><span>SQL</span><span>)</span> <span>{</span> 
>                     <span>SQL</span> <span>=</span> <span>await</span> <span>initSqlJs</span><span>({</span> <span>locateFile</span><span>:</span> <span>file</span> <span>=&gt;</span> <span>`https://cdnjs.cloudflare.com/ajax/libs/sql.js/1.10.3/</span><span>${</span><span>file</span><span>}</span><span>`</span> <span>});</span> 
>                 <span>}</span> 
>  
>                 <span>if </span><span>(</span><span>db</span><span>)</span> <span>{</span> 
>                     <span>db</span><span>.</span><span>close</span><span>();</span> 
>                 <span>}</span> 
>  
>                 <span>db</span> <span>=</span> <span>new</span> <span>SQL</span><span>.</span><span>Database</span><span>();</span> 
>  
>                 <span>// --- A. Create Users Table ---</span> 
>                 <span>db</span><span>.</span><span>run</span><span>(</span><span>` 
>                     CREATE TABLE Users ( 
>                         ID INTEGER PRIMARY KEY, 
>                         FirstName TEXT NOT NULL, 
>                         LastName TEXT NOT NULL, 
>                         DateOfBirth TEXT NOT NULL,  
>                         Address TEXT, 
>                         CommandList TEXT UNIQUE NOT NULL 
>                     ); 
>                 `</span><span>);</span> 
>  
>                 <span>db</span><span>.</span><span>run</span><span>(</span><span>` 
>                     CREATE TABLE Commands ( 
>                         ProductID TEXT PRIMARY KEY, 
>                         ProductName TEXT NOT NULL, 
>                         CommandNumber TEXT NOT NULL, 
>                         FOREIGN KEY (CommandNumber) REFERENCES Users(CommandList) 
>                     ); 
>                 `</span><span>);</span> 
>  
>                 <span>const</span> <span>mockData</span> <span>=</span> <span>generateRandomData</span><span>();</span> 
>                 <span>mockData</span><span>.</span><span>forEach</span><span>(</span><span>item</span> <span>=&gt;</span> <span>{</span> 
>                     <span>db</span><span>.</span><span>run</span><span>(</span><span>"</span><span>INSERT INTO Users (FirstName, LastName, DateOfBirth, Address, CommandList) VALUES (?, ?, ?, ?, ?)</span><span>"</span><span>,</span> 
>                         <span>[</span><span>item</span><span>.</span><span>firstName</span><span>,</span> <span>item</span><span>.</span><span>lastName</span><span>,</span> <span>item</span><span>.</span><span>dob</span><span>,</span> <span>item</span><span>.</span><span>address</span><span>,</span> <span>item</span><span>.</span><span>commandList</span><span>]);</span> 
>  
>                     <span>db</span><span>.</span><span>run</span><span>(</span><span>"</span><span>INSERT INTO Commands (ProductID, ProductName, CommandNumber) VALUES (?, ?, ?)</span><span>"</span><span>,</span> 
>                         <span>[</span><span>item</span><span>.</span><span>productID</span><span>,</span> <span>item</span><span>.</span><span>productName</span><span>,</span> <span>item</span><span>.</span><span>commandNumber</span><span>]);</span> 
>                 <span>});</span> 
>  
>                 <span>statusDiv</span><span>.</span><span>textContent</span> <span>=</span> <span>reset</span>  
>                     <span>?</span> <span>`✅ Database Reset &amp; </span><span>${</span><span>NUM_RECORDS</span><span>}</span><span> records inserted successfully!`</span> 
>                     <span>:</span> <span>`✅ Database initialized with </span><span>${</span><span>NUM_RECORDS</span><span>}</span><span> records.`</span><span>;</span> 
>                 <span>statusDiv</span><span>.</span><span>classList</span><span>.</span><span>remove</span><span>(</span><span>'</span><span>text-red-700</span><span>'</span><span>);</span> 
>                 <span>statusDiv</span><span>.</span><span>classList</span><span>.</span><span>add</span><span>(</span><span>'</span><span>text-green-700</span><span>'</span><span>);</span> 
>  
>                 <span>displayUsers</span><span>();</span> 
>  
>             <span>}</span> <span>catch </span><span>(</span><span>error</span><span>)</span> <span>{</span> 
>                 <span>statusDiv</span><span>.</span><span>textContent</span> <span>=</span> <span>`❌ Error initializing database: </span><span>${</span><span>error</span><span>.</span><span>message</span><span>}</span><span>`</span><span>;</span> 
>                 <span>statusDiv</span><span>.</span><span>classList</span><span>.</span><span>remove</span><span>(</span><span>'</span><span>text-green-700</span><span>'</span><span>);</span> 
>                 <span>statusDiv</span><span>.</span><span>classList</span><span>.</span><span>add</span><span>(</span><span>'</span><span>text-red-700</span><span>'</span><span>);</span> 
>                 <span>console</span><span>.</span><span>error</span><span>(</span><span>"</span><span>DB Initialization Error:</span><span>"</span><span>,</span> <span>error</span><span>);</span> 
>             <span>}</span> 
>         <span>}</span> 
>  
>  
>         <span>function</span> <span>renderResults</span><span>(</span><span>results</span><span>)</span> <span>{</span> 
>             <span>resultDiv</span><span>.</span><span>innerHTML</span> <span>=</span> <span>''</span><span>;</span> 
>  
>             <span>if </span><span>(</span><span>!</span><span>results</span> <span>||</span> <span>results</span><span>.</span><span>length</span> <span>===</span> <span>0</span><span>)</span> <span>{</span> 
>                 <span>resultDiv</span><span>.</span><span>innerHTML</span> <span>=</span> <span>'</span><span>&lt;p class="p-4 text-orange-500"&gt;Query executed successfully, but returned no rows.&lt;/p&gt;</span><span>'</span><span>;</span> 
>                 <span>return</span><span>;</span> 
>             <span>}</span> 
>  
>             <span>const</span> <span>table</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>table</span><span>'</span><span>);</span> 
>             <span>table</span><span>.</span><span>classList</span><span>.</span><span>add</span><span>(</span><span>'</span><span>min-w-full</span><span>'</span><span>,</span> <span>'</span><span>divide-y</span><span>'</span><span>,</span> <span>'</span><span>divide-gray-200</span><span>'</span><span>);</span> 
>  
>             <span>const</span> <span>thead</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>thead</span><span>'</span><span>);</span> 
>             <span>const</span> <span>headerRow</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>tr</span><span>'</span><span>);</span> 
>             <span>results</span><span>[</span><span>0</span><span>].</span><span>columns</span><span>.</span><span>forEach</span><span>(</span><span>col</span> <span>=&gt;</span> <span>{</span> 
>                 <span>const</span> <span>th</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>th</span><span>'</span><span>);</span> 
>                 <span>th</span><span>.</span><span>textContent</span> <span>=</span> <span>col</span><span>;</span> 
>                 <span>headerRow</span><span>.</span><span>appendChild</span><span>(</span><span>th</span><span>);</span> 
>             <span>});</span> 
>             <span>thead</span><span>.</span><span>appendChild</span><span>(</span><span>headerRow</span><span>);</span> 
>             <span>table</span><span>.</span><span>appendChild</span><span>(</span><span>thead</span><span>);</span> 
>  
>            <span>const</span> <span>tbody</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>tbody</span><span>'</span><span>);</span> 
>             <span>results</span><span>[</span><span>0</span><span>].</span><span>values</span><span>.</span><span>forEach</span><span>(</span><span>row</span> <span>=&gt;</span> <span>{</span> 
>                 <span>const</span> <span>tr</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>tr</span><span>'</span><span>);</span> 
>                 <span>row</span><span>.</span><span>forEach</span><span>(</span><span>cell</span> <span>=&gt;</span> <span>{</span> 
>                     <span>const</span> <span>td</span> <span>=</span> <span>document</span><span>.</span><span>createElement</span><span>(</span><span>'</span><span>td</span><span>'</span><span>);</span> 
>                     <span>td</span><span>.</span><span>textContent</span> <span>=</span> <span>cell</span> <span>===</span> <span>null</span> <span>?</span> <span>'</span><span>NULL</span><span>'</span> <span>:</span> <span>cell</span><span>;</span> 
>                     <span>tr</span><span>.</span><span>appendChild</span><span>(</span><span>td</span><span>);</span> 
>                 <span>});</span> 
>                 <span>tbody</span><span>.</span><span>appendChild</span><span>(</span><span>tr</span><span>);</span> 
>             <span>});</span> 
>             <span>table</span><span>.</span><span>appendChild</span><span>(</span><span>tbody</span><span>);</span> 
>  
>             <span>resultDiv</span><span>.</span><span>appendChild</span><span>(</span><span>table</span><span>);</span> 
>         <span>}</span> 
>  
>         <span>function</span> <span>runQuery</span><span>(</span><span>sql</span><span>,</span> <span>title</span><span>)</span> <span>{</span> 
>             <span>resultTitle</span><span>.</span><span>textContent</span> <span>=</span> <span>title</span><span>;</span> 
>             <span>try</span> <span>{</span> 
>                 <span>if </span><span>(</span><span>!</span><span>db</span><span>)</span> <span>{</span> 
>                     <span>resultDiv</span><span>.</span><span>innerHTML</span> <span>=</span> <span>'</span><span>&lt;p class="p-4 text-red-500"&gt;Database not initialized. Please click "Reset Database &amp; Reload Data".&lt;/p&gt;</span><span>'</span><span>;</span> 
>                     <span>return</span><span>;</span> 
>                 <span>}</span> 
>                 <span>const</span> <span>results</span> <span>=</span> <span>db</span><span>.</span><span>exec</span><span>(</span><span>sql</span><span>);</span> 
>                 <span>renderResults</span><span>(</span><span>results</span><span>);</span> 
>             <span>}</span> <span>catch </span><span>(</span><span>error</span><span>)</span> <span>{</span> 
>                 <span>resultDiv</span><span>.</span><span>innerHTML</span> <span>=</span> <span>`&lt;div class="p-4 bg-red-100 text-red-700 rounded-lg"&gt;❌ SQL Error: </span><span>${</span><span>error</span><span>.</span><span>message</span><span>}</span><span>&lt;/div&gt;`</span><span>;</span> 
>             <span>}</span> 
>         <span>}</span> 
>  
>  
>         <span>function</span> <span>displayUsers</span><span>()</span> <span>{</span> 
>             <span>const</span> <span>sql</span> <span>=</span> <span>"</span><span>SELECT ID, FirstName, LastName, DateOfBirth, Address, CommandList FROM Users</span><span>"</span><span>;</span> 
>             <span>runQuery</span><span>(</span><span>sql</span><span>,</span> <span>"</span><span>Table: Users (All Fields)</span><span>"</span><span>);</span> 
>         <span>}</span> 
>  
>         <span>function</span> <span>displayJoinedData</span><span>()</span> <span>{</span> 
>             <span>const</span> <span>sql</span> <span>=</span> <span>` 
>                 SELECT 
>                     U.ID, 
>                     U.FirstName, 
>                     U.LastName, 
>                     C.ProductName, 
>                     C.ProductID, 
>                     U.CommandList as CommandRef 
>                 FROM 
>                     Users U 
>                 INNER JOIN 
>                     Commands C ON U.CommandList = C.CommandNumber 
>                 ORDER BY 
>                     U.ID; 
>             `</span><span>;</span> 
>             <span>runQuery</span><span>(</span><span>sql</span><span>,</span> <span>"</span><span>Joined Data: Users linked to Commands</span><span>"</span><span>);</span> 
>         <span>}</span> 
>  
>         <span>function</span> <span>executeCustomQuery</span><span>()</span> <span>{</span> 
>             <span>const</span> <span>sql</span> <span>=</span> <span>document</span><span>.</span><span>getElementById</span><span>(</span><span>'</span><span>sqlInput</span><span>'</span><span>).</span><span>value</span><span>.</span><span>trim</span><span>();</span> 
>             <span>if </span><span>(</span><span>!</span><span>sql</span><span>)</span> <span>{</span> 
>                 <span>resultDiv</span><span>.</span><span>innerHTML</span> <span>=</span> <span>'</span><span>&lt;p class="p-4 text-orange-500"&gt;Please enter an SQL query to execute.&lt;/p&gt;</span><span>'</span><span>;</span> 
>                 <span>return</span><span>;</span> 
>             <span>}</span> 
>             <span>runQuery</span><span>(</span><span>sql</span><span>,</span> <span>"</span><span>Custom Query Results</span><span>"</span><span>);</span> 
>         <span>}</span> 
>  
>         <span>window</span><span>.</span><span>onload</span> <span>=</span> <span>()</span> <span>=&gt;</span> <span>{</span> 
>             <span>initDb</span><span>();</span> 
>         <span>};</span> 
>  
>     <span>&lt;/script&gt;</span> 
> <span>&lt;/body&gt;</span> 
> <span>&lt;/html&gt;</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Feeegkrl4o8pk42yjhty6.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Feeegkrl4o8pk42yjhty6.png" alt="" width="800" height="478"></a></p> 
>  
> <h2> 
>    
>    
>   Text-2-SQL Tests 
> </h2> 
>  
> <p>Now that the database and our data are readyn we will start using the solution.</p> 
>  
> <ul> 
> <li>What is extremely convenient is the way Vanna.ai supports setup: depending on the specific LLM provider (like Ollama) and database type (like SQLite) you select, their documentation provides the exact installation requirements and configuration snippets you need, making the initial setup frictionless. 
> </li> 
> </ul> 
>  
> <div> 
> <pre><code>pip <span>install</span> <span>'vanna[fastapi,httpx,ollama]'</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F7x9lbmcwamylgyjchj8i.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F7x9lbmcwamylgyjchj8i.png" alt="" width="800" height="284"></a></p> 
>  
> <ul> 
> <li>The site provides the full sample code to build an application, just copy/paste it! 
> </li> 
> </ul> 
>  
> <div> 
> <pre><code><span># vanna-ai-app.py 
> # All imports at the top 
> </span><span>from</span> <span>vanna</span> <span>import</span> <span>Agent</span> 
> <span>from</span> <span>vanna.core.registry</span> <span>import</span> <span>ToolRegistry</span> 
> <span>from</span> <span>vanna.core.user</span> <span>import</span> <span>UserResolver</span><span>,</span> <span>User</span><span>,</span> <span>RequestContext</span> 
> <span>from</span> <span>vanna.tools</span> <span>import</span> <span>RunSqlTool</span><span>,</span> <span>VisualizeDataTool</span> 
> <span>from</span> <span>vanna.tools.agent_memory</span> <span>import</span> <span>SaveQuestionToolArgsTool</span><span>,</span> <span>SearchSavedCorrectToolUsesTool</span><span>,</span> <span>SaveTextMemoryTool</span> 
> <span>from</span> <span>vanna.servers.fastapi</span> <span>import</span> <span>VannaFastAPIServer</span> 
> <span>from</span> <span>vanna.integrations.ollama</span> <span>import</span> <span>OllamaLlmService</span> 
> <span>from</span> <span>vanna.integrations.sqlite</span> <span>import</span> <span>SqliteRunner</span> 
> <span>from</span> <span>vanna.integrations.local.agent_memory</span> <span>import</span> <span>DemoAgentMemory</span> 
>  
> <span># Configure your LLM 
> </span><span>llm</span> <span>=</span> <span>OllamaLlmService</span><span>(</span> 
>     <span>model</span><span>=</span><span>"</span><span>gpt-oss:latest</span><span>"</span><span>,</span> 
>     <span>host</span><span>=</span><span>"</span><span>http://localhost:11434</span><span>"</span> 
> <span>)</span> 
>  
> <span># Configure your database 
> </span><span>db_tool</span> <span>=</span> <span>RunSqlTool</span><span>(</span> 
>     <span>sql_runner</span><span>=</span><span>SqliteRunner</span><span>(</span><span>database_path</span><span>=</span><span>"</span><span>./users_commands.db</span><span>"</span><span>)</span> 
> <span>)</span> 
>  
> <span># Configure your agent memory 
> </span><span>agent_memory</span> <span>=</span> <span>DemoAgentMemory</span><span>(</span><span>max_items</span><span>=</span><span>1000</span><span>)</span> 
>  
> <span># Configure user authentication 
> </span><span>class</span> <span>SimpleUserResolver</span><span>(</span><span>UserResolver</span><span>):</span> 
>     <span>async</span> <span>def</span> <span>resolve_user</span><span>(</span><span>self</span><span>,</span> <span>request_context</span><span>:</span> <span>RequestContext</span><span>)</span> <span>-&gt;</span> <span>User</span><span>:</span> 
>         <span>user_email</span> <span>=</span> <span>request_context</span><span>.</span><span>get_cookie</span><span>(</span><span>'</span><span>vanna_email</span><span>'</span><span>)</span> <span>or</span> <span>'</span><span>guest@example.com</span><span>'</span> 
>         <span>group</span> <span>=</span> <span>'</span><span>admin</span><span>'</span> <span>if</span> <span>user_email</span> <span>==</span> <span>'</span><span>admin@example.com</span><span>'</span> <span>else</span> <span>'</span><span>user</span><span>'</span> 
>         <span>return</span> <span>User</span><span>(</span><span>id</span><span>=</span><span>user_email</span><span>,</span> <span>email</span><span>=</span><span>user_email</span><span>,</span> <span>group_memberships</span><span>=</span><span>[</span><span>group</span><span>])</span> 
>  
> <span>user_resolver</span> <span>=</span> <span>SimpleUserResolver</span><span>()</span> 
>  
> <span># Create your agent 
> </span><span>tools</span> <span>=</span> <span>ToolRegistry</span><span>()</span> 
> <span>tools</span><span>.</span><span>register_local_tool</span><span>(</span><span>db_tool</span><span>,</span> <span>access_groups</span><span>=</span><span>[</span><span>'</span><span>admin</span><span>'</span><span>,</span> <span>'</span><span>user</span><span>'</span><span>])</span> 
> <span>tools</span><span>.</span><span>register_local_tool</span><span>(</span><span>SaveQuestionToolArgsTool</span><span>(),</span> <span>access_groups</span><span>=</span><span>[</span><span>'</span><span>admin</span><span>'</span><span>])</span> 
> <span>tools</span><span>.</span><span>register_local_tool</span><span>(</span><span>SearchSavedCorrectToolUsesTool</span><span>(),</span> <span>access_groups</span><span>=</span><span>[</span><span>'</span><span>admin</span><span>'</span><span>,</span> <span>'</span><span>user</span><span>'</span><span>])</span> 
> <span>tools</span><span>.</span><span>register_local_tool</span><span>(</span><span>SaveTextMemoryTool</span><span>(),</span> <span>access_groups</span><span>=</span><span>[</span><span>'</span><span>admin</span><span>'</span><span>,</span> <span>'</span><span>user</span><span>'</span><span>])</span> 
> <span>tools</span><span>.</span><span>register_local_tool</span><span>(</span><span>VisualizeDataTool</span><span>(),</span> <span>access_groups</span><span>=</span><span>[</span><span>'</span><span>admin</span><span>'</span><span>,</span> <span>'</span><span>user</span><span>'</span><span>])</span> 
>  
> <span>agent</span> <span>=</span> <span>Agent</span><span>(</span> 
>     <span>llm_service</span><span>=</span><span>llm</span><span>,</span> 
>     <span>tool_registry</span><span>=</span><span>tools</span><span>,</span> 
>     <span>user_resolver</span><span>=</span><span>user_resolver</span><span>,</span> 
>     <span>agent_memory</span><span>=</span><span>agent_memory</span> 
> <span>)</span> 
>  
> <span># Run the server 
> </span><span>server</span> <span>=</span> <span>VannaFastAPIServer</span><span>(</span><span>agent</span><span>)</span> 
> <span>server</span><span>.</span><span>run</span><span>()</span>  <span># Access at http://localhost:8000 
> </span></code></pre> 
>  
> </div> 
>  
>  
>  
> <ul> 
> <li>The sample application from the site uses “gpt-oss” LLM, but you can put whatever LLM you choose. I tested both “granite” and “gpt-oss”. 
> </li> 
> </ul> 
>  
> <div> 
> <pre><code><span># Configure your LLM 
> </span><span>llm</span> <span>=</span> <span>OllamaLlmService</span><span>(</span> 
>     <span>model</span><span>=</span><span>"</span><span>granite4:latest</span><span>"</span><span>,</span> 
>     <span>host</span><span>=</span><span>"</span><span>http://localhost:11434</span><span>"</span> 
> <span>)</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ul> 
> <li>As the interface is generated by “FastAPI” the defaut port number is “<a href="http://localhost:8000">http://localhost:8000”</a> but you can adapt it to your needs and for instance run different instance by changing the port number. 
> </li> 
> </ul> 
>  
> <div> 
> <pre><code><span># Run the server 
> </span><span>server</span> <span>=</span> <span>VannaFastAPIServer</span><span>(</span><span>agent</span><span>)</span> 
>  
> <span># Setting port=9000  
> </span><span>server</span><span>.</span><span>run</span><span>(</span><span>port</span><span>=</span><span>9000</span><span>)</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ul> 
> <li>During my initial testing, I found that running queries using both the Granite and GPT-OSS models consistently yielded accurate answers and correctly generated SQL queries. While I wasn’t aiming to conduct rigorous, side-by-side benchmarking, this functionality test confirms the core power of using Vanna.ai with these locally hosted open-source models. The primary goal was to validate the integration and core functionality, and in that regard, the results were highly successful. We leave it to the user community to dive deeper into performance comparisons, extended tests, and complex benchmarking scenarios. The first screens come from “gpt-oss”.</li> 
> </ul> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fc7569ep9sglm6ka2cy2l.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fc7569ep9sglm6ka2cy2l.png" alt="" width="800" height="430"></a></p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5llw33l0x7j5m06av2eb.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5llw33l0x7j5m06av2eb.png" alt="" width="800" height="519"></a></p> 
>  
> <ul> 
> <li>The following are generated queries and outputs from granite 🪨</li> 
> </ul> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9eafeagt82hqknrt370t.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9eafeagt82hqknrt370t.png" alt="" width="800" height="119"></a></p> 
>  
> <ul> 
> <li>The output ⬇️</li> 
> </ul> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft9dxzipxx3mcxgdltch9.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft9dxzipxx3mcxgdltch9.png" alt="" width="800" height="322"></a></p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmbunwb2sb1insa9r87hb.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmbunwb2sb1insa9r87hb.png" alt="" width="800" height="383"></a></p> 
>  
> <ul> 
> <li>A very handy feature is that every query generates a CSV file, meaning the user not only gets the beautifully presented output on the UI but also receives a tangible, exportable data file as a direct result. We leave it to the user community to dive deeper into performance comparisons, extended tests, and complex benchmarking scenarios. 
> </li> 
> </ul> 
>  
> <div> 
> <pre><code>ID,FirstName,LastName,DateOfBirth,Address,CommandList 
> 10,Henry,Wallace,1962-04-18,"79093 Danielle Haven Suite 823, New Kevinton, PR 23752",83c466e4-239f-4844-8a79-6286be0edb30 
> </code></pre> 
>  
> </div> 
>  
>  
>  
>  
>  
> <div> 
> <pre><code>ID,FirstName,LastName,DateOfBirth,Address,CommandList 
> 1,David,Frank,1983-03-22,"3973 Carmen Gateway Suite 371, Timothytown, NY 51959",173e986d-77b0-4c6e-9314-52813419f108 
> 2,Sydney,Middleton,1981-09-24,"4080 Regina Lake Apt. 187, Kathrynchester, MN 66872",5bd1cf9a-2cfc-46e5-b150-77c368efaccb 
> 3,Joshua,Holder,2000-06-19,"0072 Sanchez Hollow, Port Jackfurt, NC 88639",68009684-34a9-4be0-8f67-ef26155d6cfd 
> 4,Robert,Walker,1968-01-10,"USS Lee, FPO AP 42065",dc60fd66-f59b-486b-8f04-4162bdf9c99e 
> 5,Douglas,Johnson,1999-08-18,"51189 Jacqueline Shores, East Denise, ME 11103",80c13c6f-c5f7-4df7-abfe-f91ced3e1237 
> 6,James,Hernandez,1976-10-28,"Unit 7218 Box 1614, DPO AP 28470",9b4dee77-37ad-4fa4-8672-a7c053e71429 
> 7,Diana,Thomas,2004-02-29,"84300 Vaughn Crossroad, Port Sharon, VA 67416",7062b085-18b4-40c3-8e4e-c3bd6c214e08 
> 8,Jamie,Torres,1961-06-22,"857 Pennington Flats Suite 672, Rushville, DC 37245",31725920-e0c3-4fac-bd42-31d113e6e107 
> 9,Renee,Rice,2006-09-04,"963 Marvin Underpass, Rebeccachester, KS 45025",8536b45b-3502-480a-a9be-89189e68478e 
> 10,Henry,Wallace,1962-04-18,"79093 Danielle Haven Suite 823, New Kevinton, PR 23752",83c466e4-239f-4844-8a79-6286be0edb30 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>That’s a wrap! ✌️</p> 
>  
> <h2> 
>    
>    
>   Conclusion 
> </h2> 
>  
> <p>In conclusion, I guess these tests validated Vanna.ai’s strength as a comprehensive Text-to-SQL solution, moving beyond simple query generation. This entire setup, running within a custom-ported FastAPI server, demonstrated core functionalities: accurate SQL generation from natural language questions, seamless execution against a database tables, rich visual output on the UI, and the practical utility of immediate CSV export. The ease of configuration, coupled with the accuracy achieved using either local or cloud based models with a variety of database engines. I really give a “thumb-up” to Venna.ai’s solution.</p> 
>  
> <p>Thanks for reading! 🤗</p> 
>  
> <h2> 
>    
>    
>   Links 
> </h2> 
>  
> <ul> 
> <li>Vanna.ai GitHub: <a href="https://github.com/vanna-ai/vanna">https://github.com/vanna-ai/vanna</a> 
> </li> 
> <li>Vanna.ai site: <a href="https://vanna.ai/">https://vanna.ai/</a> 
> </li> 
> </ul>

---

## [4/10] How to Prompt AI for Consistent JSON Responses
**Source:** The Practical Developer | **Date:** 2025-12-02T14:03:05.000Z
**URL:** https://dev.to/cucoleadan/how-to-prompt-ai-for-consistent-json-responses-1gbm
**Reasoning:** The article is about prompting AI for consistent JSON, which is not directly related to our primary or secondary interests.
**Authors:** cucoleadan

**Content/Abstract:**
> <p><em>This was originally posted on the <a href="https://vibestacklab.substack.com/p/how-to-prompt-ai-for-consistent-json">VSL Substack publication</a>.</em></p> 
>  
> <p>You just used AI to generate your API integration code, tested it locally, and deployed it to production because it seemingly worked perfectly. Two hours later, your logs are filled with parsing failures and users cannot complete actions because "Invalid JSON" errors are popping up everywhere. The AI gave you working code, but the problem is that it only worked in your controlled test environment with your specific input.</p> 
>  
> <p>It happened to me so many times I lost count, so I’m going to share one of the best ways to get your AI to generate valid json, every single time.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Flt9g6nxaevfcah1x6hkb.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Flt9g6nxaevfcah1x6hkb.png" alt="Consistent JSON Output System" width="800" height="446"></a></p> 
>  
> <h2> 
>    
>    
>   Bad JSON Means High Costs 
> </h2> 
>  
> <p>When you are building features that depend on structured data like API integrations, database operations, or configuration files, JSON consistency directly determines whether your app works or breaks. A single malformed response can quickly cascade into hundreds of failed user actions, corrupt database writes, and support tickets flooding your inbox.</p> 
>  
> <p>While AI generates JSON that looks correct during development, production environments expose inconsistencies like missing commas, trailing commas in strict parsers, unescaped quotes in user-generated content, and Unicode characters from international users. These small formatting errors crash entire features, so you need AI to output valid and production-ready JSON consistently instead of just succeeding in your local tests.</p> 
>  
> <h2> 
>    
>    
>   The Airport Security Model 
> </h2> 
>  
> <p>Airport security examines your documents multiple times at check-in, security, and the gate because mistakes are expensive to fix later. JSON validation works the same way because AI generates structurally sound JSON in controlled conditions but often breaks under real-world variance. That’s why you need to build multiple validation checkpoints to catch these errors.</p> 
>  
> <p>Most builders stop at the prompt, but production-ready apps require five distinct checkpoints to ensure reliability.</p> 
>  
> <h2> 
>    
>    
>   The Most Common Ways AI Breaks Your Data 
> </h2> 
>  
> <p><strong>Syntax Errors:</strong> Missing or extra commas, brackets, or quotes break <code>JSON.parse()</code> immediately on your server. For example, AI often adds a trailing comma after the last object property or forgets to close a nested object properly. While some lenient parsers might handle this, standard Node.js or Python backends will throw an exception and crash the request before your logic even runs.</p> 
>  
> <p><strong>Escaped Character Traps:</strong> One of the most insidious errors occurs when AI double-escapes characters inside the JSON string itself (e.g., returning <code>\\"</code> instead of <code>\"</code>). This technically creates a valid string but invalid JSON content for your parser, causing it to fail when processing fields that contain quotes, backslashes, or special characters.</p> 
>  
> <p><strong>Structure Mismatch:</strong> This happens when you get valid JSON in the wrong shape, such as receiving <code>{"title": "My Article"}</code> when you actually need <code>{"article": {"title": "My Article"}}</code>. Your backend code expects nested objects but receives flat structures, which causes undefined property errors that can take down your entire API endpoint.</p> 
>  
> <p><strong>Type Confusion:</strong> AI sometimes returns strings when you need numbers or arrays when you need objects. This results in valid JSON with broken logic. An example is when a count field comes back as <code>"25"</code> (string) instead of <code>25</code> (number), which breaks database schema validation and any math operations you perform on that data.</p> 
>  
> <p><strong>Truncated Responses:</strong> When you request a large dataset, some models will hit their output token limit and cut off the JSON mid-stream (e.g., ending with <code>...</code> or just stopping). This leaves you with an incomplete, unparsable string. The reliable fix is to request data in smaller chunks—generating multiple subsets and concatenating the results in your code rather than asking for one massive payload.</p> 
>  
> <h2> 
>    
>    
>   The 5-Checkpoint Framework for Bulletproof JSON 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fozkxat662nvsirl28qtd.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fozkxat662nvsirl28qtd.png" alt="The Validation Checkpoint System" width="800" height="446"></a></p> 
>  
> <h2> 
>    
>    
>   1. Be Explicit in Your Prompt 
> </h2> 
>  
> <p><strong>Don't say:</strong> "Return the SEO data as JSON"</p> 
>  
> <p><strong>Say:</strong> "Return ONLY valid JSON with no additional text. Use this exact structure: <code>{"title": string, "description": string}</code>"</p> 
>  
> <h2> 
>    
>    
>   2. Provide a JSON Schema 
> </h2> 
>  
> <p><strong>The most reliable way to get consistent JSON structure is to give AI an exact schema to follow.</strong></p> 
>  
> <p>JSON Schema defines the structure, types, and requirements for your data. Instead of describing what you want in natural language, you provide a formal specification that eliminates ambiguity.</p> 
>  
> <p><strong>Add this to your prompt:</strong></p> 
>  
> <p>"Return JSON matching this exact schema. Do not add any fields not in the schema. All required fields must be present:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>{</span> 
>   <span>"</span><span>type</span><span>"</span><span>:</span> <span>"</span><span>object</span><span>"</span><span>,</span> 
>   <span>"</span><span>properties</span><span>"</span><span>:</span> <span>{</span> 
>     <span>"</span><span>title</span><span>"</span><span>:</span> <span>{</span> 
>       <span>"</span><span>type</span><span>"</span><span>:</span> <span>"</span><span>string</span><span>"</span><span>,</span> 
>       <span>"</span><span>maxLength</span><span>"</span><span>:</span> <span>70</span><span>,</span> 
>       <span>"</span><span>description</span><span>"</span><span>:</span> <span>"</span><span>An engaging, SEO-friendly article title</span><span>"</span> 
>     <span>},</span> 
>     <span>"</span><span>description</span><span>"</span><span>:</span> <span>{</span> 
>       <span>"</span><span>type</span><span>"</span><span>:</span> <span>"</span><span>string</span><span>"</span><span>,</span> 
>       <span>"</span><span>maxLength</span><span>"</span><span>:</span> <span>160</span><span>,</span> 
>       <span>"</span><span>description</span><span>"</span><span>:</span> <span>"</span><span>A concise summary of the article content</span><span>"</span> 
>     <span>}</span> 
>   <span>},</span> 
>   <span>"</span><span>required</span><span>"</span><span>:</span> <span>[</span> 
>     <span>"</span><span>title</span><span>"</span><span>,</span> 
>     <span>"</span><span>description</span><span>"</span> 
>   <span>],</span> 
>   <span>"</span><span>additionalProperties</span><span>"</span><span>:</span> <span>false</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>Why this works:</strong> Schemas eliminate type confusion and structure mismatch because the model validates its output against the schema pattern during generation. The <code>additionalProperties: false</code> flag is particularly powerful because it strictly forbids the AI from hallucinating extra fields you didn't ask for. This catches errors before they reach your code. Learn more about schema properties at the <a href="https://spec.openapis.org/oas/v3.0.3#schema">OpenAPI Schema specification</a>.</p> 
>  
> <h2> 
>    
>    
>   3. Request Code Fences 
> </h2> 
>  
> <p>Add to your prompt: "Wrap the JSON in markdown code fences with json syntax highlighting"</p> 
>  
> <p><strong>This prevents AI from adding explanatory text before or after the JSON because extra text breaks parsing when you extract the response.</strong></p> 
>  
> <h2> 
>    
>    
>   4. Validate Before Using 
> </h2> 
>  
> <p>Never assume AI output is valid. Always wrap in try-catch:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>try</span> <span>{</span> 
>   <span>const</span> <span>data</span> <span>=</span> <span>JSON</span><span>.</span><span>parse</span><span>(</span><span>aiResponse</span><span>);</span> 
>   <span>*</span><span>// Use data here*</span> 
> <span>}</span> <span>catch </span><span>(</span><span>error</span><span>)</span> <span>{</span> 
>   <span>console</span><span>.</span><span>error</span><span>(</span><span>'</span><span>Invalid JSON from AI:</span><span>'</span><span>,</span> <span>error</span><span>);</span> 
>   <span>*</span><span>// Handle the error gracefully*</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h2> 
>    
>    
>   5. Check Structure After Parsing 
> </h2> 
>  
> <p>Valid JSON doesn't guarantee correct structure:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>function</span> <span>validateSeoData</span><span>(</span><span>data</span><span>)</span> <span>{</span> 
>   <span>if </span><span>(</span><span>!</span><span>data</span><span>.</span><span>title</span><span>)</span> <span>return</span> <span>false</span><span>;</span> 
>   <span>if </span><span>(</span><span>typeof</span> <span>data</span><span>.</span><span>title</span> <span>!==</span> <span>'</span><span>string</span><span>'</span><span>)</span> <span>return</span> <span>false</span><span>;</span> 
>   <span>if </span><span>(</span><span>typeof</span> <span>data</span><span>.</span><span>description</span> <span>!==</span> <span>'</span><span>string</span><span>'</span><span>)</span> <span>return</span> <span>false</span><span>;</span> 
>   <span>return</span> <span>true</span><span>;</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F92i04czxtnjcqv4t59av.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F92i04czxtnjcqv4t59av.png" alt="5-stage validation" width="800" height="446"></a></p> 
>  
> <h2> 
>    
>    
>   Red Flags: What to Watch For 
> </h2> 
>  
> <p><strong>🚩 AI adds explanation text before or after JSON:</strong> This breaks parsing. AI models are trained to be conversational and often add context before JSON blocks, so you must always request "ONLY valid JSON, no additional text."</p> 
>  
> <p><strong>🚩 Nested objects are inconsistent:</strong> AI formats the first object correctly but makes errors in deeply nested structures. The model's attention weakens as JSON depth increases, which leads to malformed brackets and missing commas three or four levels deep.</p> 
>  
> <p><strong>🚩 Numbers as strings:</strong> AI often wraps numbers in quotes because it sees numbers as text tokens rather than numeric types. You need to be explicit that "count should be a number, not a string."</p> 
>  
> <p><strong>🚩 Double-escaped characters in strings:</strong> AI returns <code>\\"</code> instead of <code>\"</code> for quotes inside strings because the model treats escape sequences as text patterns rather than functional syntax.</p> 
>  
> <h2> 
>    
>    
>   Watch Out For Escaped Character 
> </h2> 
>  
> <p><strong>One of the most frustrating JSON issues occurs when AI returns responses with escaped characters inside the JSON itself.</strong></p> 
>  
> <p><strong>Example of what goes wrong:</strong></p> 
>  
> <ul> 
> <li> 
> <strong>What you expect:</strong> <code>{"title": "The \"Best\" Guide"}</code> 
> </li> 
> <li> 
> <strong>What AI sometimes returns:</strong> <code>{"title": "The \\\"Best\\\" Guide"}</code> 
> </li> 
> </ul> 
>  
> <p><strong>The AI double-escapes quotes, backslashes, and special characters.</strong> When you parse this JSON, you get literal backslash characters in your strings instead of properly escaped quotes. This breaks things when you try to display the data to users or pass it to other systems that expect clean strings.</p> 
>  
> <p><strong>How to prevent it:</strong></p> 
>  
> <p>Add to your prompt: <strong>"Use proper JSON escaping. Quotes inside strings should use single backslash escape ("), not double backslash (\")."</strong></p> 
>  
> <h2> 
>    
>    
>   Hidden Edge Cases 
> </h2> 
>  
> <p><strong>Trailing commas:</strong> AI adds commas after the last item in objects or arrays. Strict JSON parsers reject this immediately even though JavaScript often accepts it, so this breaks in production environments using different parsers.</p> 
>  
> <p><strong>Unescaped special characters:</strong> If your data contains quotes or backslashes, AI often forgets to escape them. A title like "O'Brien's Guide" will break your JSON when AI returns <code>"title": "O'Brien's Guide"</code> instead of <code>"title": "O\'Brien's Guide"</code> if your parser is strict about apostrophes.</p> 
>  
> <p><strong>Inconsistent null handling:</strong> AI randomly switches between <code>null</code>, <code>"null"</code>, empty string, or omitting the field entirely. Your application logic assumes one pattern but receives variations, which causes conditional checks to fail unpredictably.</p> 
>  
> <h2> 
>    
>    
>   My Production Safety Net 
> </h2> 
>  
> <p>You have likely spent an hour debugging a critical failure only to discover the culprit was a single missing comma. In production, that one missing comma is not just a syntax error. It represents hundreds of failed user requests and corrupted data that piled up before you even noticed the issue.</p> 
>  
> <p><strong>Here's the technique that changed everything for me:</strong></p> 
>  
> <p><strong>Use <code>jsonrepair</code> to automatically fix malformed JSON.</strong><br> 
> </p> 
>  
> <div> 
> <pre><code><span>import</span> <span>{</span> <span>jsonrepair</span> <span>}</span> <span>from</span> <span>'</span><span>jsonrepair</span><span>'</span><span>;</span> 
>  
> <span>try</span> <span>{</span> 
>   <span>*</span><span>// Try parsing normally first*</span> 
>   <span>const</span> <span>data</span> <span>=</span> <span>JSON</span><span>.</span><span>parse</span><span>(</span><span>aiResponse</span><span>);</span> 
> <span>}</span> <span>catch </span><span>(</span><span>error</span><span>)</span> <span>{</span> 
>   <span>*</span><span>// If parsing fails, attempt repair*</span> 
>   <span>try</span> <span>{</span> 
>     <span>const</span> <span>repairedJson</span> <span>=</span> <span>jsonrepair</span><span>(</span><span>aiResponse</span><span>);</span> 
>     <span>const</span> <span>data</span> <span>=</span> <span>JSON</span><span>.</span><span>parse</span><span>(</span><span>repairedJson</span><span>);</span> 
>     <span>console</span><span>.</span><span>log</span><span>(</span><span>'</span><span>JSON repaired successfully</span><span>'</span><span>);</span> 
>   <span>}</span> <span>catch </span><span>(</span><span>repairError</span><span>)</span> <span>{</span> 
>     <span>console</span><span>.</span><span>error</span><span>(</span><span>'</span><span>Could not repair JSON:</span><span>'</span><span>,</span> <span>repairError</span><span>);</span> 
>     <span>*</span><span>// Handle the error*</span> 
>   <span>}</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>The <code>jsonrepair</code> library automatically fixes common JSON formatting issues like adding missing commas, removing trailing commas, fixing unescaped quotes, and repairing truncated JSON.</p> 
>  
> <p><strong>Think of it as insurance. You hope you never need it, but when production breaks at 2 AM, you will be grateful it is there. And don’t ask me how I know that…</strong></p> 
>  
> <h2> 
>    
>    
>   Tool of the Week: Zod 
> </h2> 
>  
> <p><a href="https://zod.dev/">Zod - TypeScript-first schema validation</a></p> 
>  
> <p>Once you have valid JSON from AI, you still need to validate that the structure matches what your app expects. Zod lets you define schemas and validate data in one line. TypeScript validates types at compile time, while Zod validates data at runtime.</p> 
>  
> <p>Example:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>import</span> <span>{</span> <span>z</span> <span>}</span> <span>from</span> <span>'</span><span>zod</span><span>'</span><span>;</span> 
>  
> <span>const</span> <span>SeoSchema</span> <span>=</span> <span>z</span><span>.</span><span>object</span><span>({</span> 
>   <span>title</span><span>:</span> <span>z</span><span>.</span><span>string</span><span>().</span><span>max</span><span>(</span><span>70</span><span>),</span> 
>   <span>description</span><span>:</span> <span>z</span><span>.</span><span>string</span><span>().</span><span>max</span><span>(</span><span>160</span><span>)</span> 
> <span>});</span> 
>  
> <span>*</span><span>// Validate AI output*</span> 
> <span>const</span> <span>result</span> <span>=</span> <span>SeoSchema</span><span>.</span><span>safeParse</span><span>(</span><span>aiData</span><span>);</span> 
> <span>if </span><span>(</span><span>!</span><span>result</span><span>.</span><span>success</span><span>)</span> <span>{</span> 
>   <span>console</span><span>.</span><span>error</span><span>(</span><span>'</span><span>Invalid structure:</span><span>'</span><span>,</span> <span>result</span><span>.</span><span>error</span><span>);</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p><strong>It catches structure mismatches and type confusion instantly, so pairing it with <code>jsonrepair</code> gives you bulletproof AI output handling.</strong></p> 
>  
> <h2> 
>    
>    
>   Final Thoughts 
> </h2> 
>  
> <p>Valid JSON is boring until it breaks production at the worst possible time. Build the checkpoints now and thank yourself later.</p> 
>  
> <p>Let's Connect: What's your strategy for handling AI-generated structured data? Do you validate everything, or trust AI until it breaks? Reply and tell me about the weirdest JSON error you've debugged.</p> 
>  
> <p><em>This was originally posted on the <a href="https://vibestacklab.substack.com/p/how-to-prompt-ai-for-consistent-json">VSL Substack publication</a>.</em></p>

---

## [4/10] Generating REST API Tests With Descriptive Names
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T13:58:06.000Z
**URL:** https://arxiv.org/abs/2512.01690v1
**Reasoning:** The article is about generating REST API tests, which is not directly related to our primary or secondary interests.
**Authors:** Philip Garrett

**Content/Abstract:**
> Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5. 
>   To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability. 
>   These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation.

---

## [4/10] ICAD-LLM: One-for-All Anomaly Detection via In-Context Learning with Large Language Models
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T13:41:30.000Z
**URL:** https://arxiv.org/abs/2512.01672v1
**Reasoning:** The article is about anomaly detection, which is not directly related to our primary or secondary interests.
**Authors:** Zhongyuan Wu

**Content/Abstract:**
> Anomaly detection (AD) is a fundamental task of critical importance across numerous domains. Current systems increasingly operate in rapidly evolving environments that generate diverse yet interconnected data modalities -- such as time series, system logs, and tabular records -- as exemplified by modern IT systems. Effective AD methods in such environments must therefore possess two critical capabilities: (1) the ability to handle heterogeneous data formats within a unified framework, allowing the model to process and detect multiple modalities in a consistent manner during anomalous events; (2) a strong generalization ability to quickly adapt to new scenarios without extensive retraining. However, most existing methods fall short of these requirements, as they typically focus on single modalities and lack the flexibility to generalize across domains. To address this gap, we introduce a novel paradigm: In-Context Anomaly Detection (ICAD), where anomalies are defined by their dissimilarity to a relevant reference set of normal samples. Under this paradigm, we propose ICAD-LLM, a unified AD framework leveraging Large Language Models' in-context learning abilities to process heterogeneous data within a single model. Extensive experiments demonstrate that ICAD-LLM achieves competitive performance with task-specific AD methods and exhibits strong generalization to previously unseen tasks, which substantially reduces deployment costs and enables rapid adaptation to new environments. To the best of our knowledge, ICAD-LLM is the first model capable of handling anomaly detection tasks across diverse domains and modalities.

---

## [4/10] Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T13:32:59.000Z
**URL:** https://arxiv.org/abs/2512.01661v1
**Reasoning:** The focus is on LLM reliability and unsolvable problem detection, which is not directly related to code intelligence or context engineering.
**Authors:** Dengyun Peng

**Content/Abstract:**
> Ensuring LLM reliability requires not only solving complex problems but also recognizing when a problem is unsolvable. Current models often struggle to distinguish objective unsolvability (inherent contradictions in the problem) from subjective capability limitations (problems beyond the model's competence), which leads to hallucinations and overconfidence. To address this, we propose UnsolvableQA and UnsolvableRL to solve feasible problems, detect inherent contradictions, and prudently refuse tasks beyond capability. Specifically, we construct UnsolvableQA, a dataset of paired solvable and unsolvable instances derived via a dual-track methodology: programmatic generation for logic puzzles and a novel "Reverse Construction" method that injects contradictions into valid reasoning chains for mathematics. Building on this dataset, we introduce UnsolvableRL, a reinforcement learning framework with three reward components jointly accounting for accuracy, unsolvability, and difficulty. Empirical results show that our approach achieves near-perfect unsolvability detection while also improving accuracy on solvable tasks. Crucially, we identify Capability Collapse, demonstrating that explicit exposure to unsolvable data is indispensable for preventing models from becoming systematically overconfident. Our code and data are available at https://github.com/sfasfaffa/unsolvableQA.

---

## [4/10] Package Dashboard: A Cross-Ecosystem Framework for Dual-Perspective Analysis of Software Packages
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T12:52:03.000Z
**URL:** https://arxiv.org/abs/2512.01630v1
**Reasoning:** The focus is on software supply chain analysis, which is not directly related to code intelligence or context engineering.
**Authors:** Ziheng Liu

**Content/Abstract:**
> Software supply chain attacks have revealed blind spots in existing SCA tools, which are often limited to a single ecosystem and assess either software artifacts or community activity in isolation. This fragmentation across tools and ecosystems forces developers to manually reconcile scattered data, undermining risk assessments. We present Package Dashboard, a cross-ecosystem framework that provides a unified platform for supply chain analysis, enabling a holistic, dual-perspective risk assessment by integrating package metadata, vulnerability information, and upstream community health metrics. By combining dependency resolution with repository analysis, it reduces cognitive load and improves traceability. Demonstrating the framework's versatility, a large-scale study of 374,000 packages across five Linux distributions shows its ability to uncover not only conventional vulnerabilities and license conflicts but also overlooked risks such as archived or inaccessible repositories. Ultimately, Package Dashboard provides a unified view of risk, equipping developers and DevSecOps engineers with actionable insights to strengthen the transparency, trustworthiness, and traceability of open-source ecosystems. Package Dashboard is publicly available at https://github.com/n19htfall/PackageDashboard, and a demonstration video can be found at https://youtu.be/y9ncftP8KPQ. Besides, the online version is available at https://pkgdash.osslab-pku.org.

---

## [4/10] When High-Performance Computing Meets Software Testing: Distributed Fuzzing using MPI
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T12:38:20.000Z
**URL:** https://arxiv.org/abs/2512.01617v1
**Reasoning:** The topic is about distributed fuzzing and HPC, which is not directly related to our primary interests.
**Authors:** Pierciro Caliandro

**Content/Abstract:**
> This paper explores the integration of MPI-based synchronization techniques into distributed fuzzing frameworks, highlighting possible substantial performance improvements compared to traditional filesystem-based synchronization methods. By employing lightweight MPI primitives, reductions in communication latency are achieved, facilitating more efficient data exchanges across distributed fuzzing nodes. Experimental results obtained over standard benchmarks demonstrate enhanced coverage progression from the early stages of the fuzzing process, which could be beneficial if fuzzing is employed in CI/CD pipelines at any stage of software development. Furthermore, the coordinated exchange of input corpora among clusters of fuzzers effectively addresses coverage stagnation, enabling a sustained exploration of complex and deep execution paths. Overall, the adoption of MPI-based synchronization approaches shows promising potential for significantly enhancing the scalability and efficacy of distributed fuzz testing.

---

## [4/10] Agent-Kernel: A MicroKernel Multi-Agent System Framework for Adaptive Social Simulation Powered by LLMs
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T12:30:58.000Z
**URL:** https://arxiv.org/abs/2512.01610v1
**Reasoning:** The article discusses multi-agent systems and social simulations, which is not directly related to code intelligence or context engineering.
**Authors:** Yuren Mao

**Content/Abstract:**
> Multi-Agent System (MAS) developing frameworks serve as the foundational infrastructure for social simulations powered by Large Language Models (LLMs). However, existing frameworks fail to adequately support large-scale simulation development due to inherent limitations in adaptability, configurability, reliability, and code reusability. For example, they cannot simulate a society where the agent population and profiles change over time. To fill this gap, we propose Agent-Kernel, a framework built upon a novel society-centric modular microkernel architecture. It decouples core system functions from simulation logic and separates cognitive processes from physical environments and action execution. Consequently, Agent-Kernel achieves superior adaptability, configurability, reliability, and reusability. We validate the framework's superiority through two distinct applications: a simulation of the Universe 25 (Mouse Utopia) experiment, which demonstrates the handling of rapid population dynamics from birth to death; and a large-scale simulation of the Zhejiang University Campus Life, successfully coordinating 10,000 heterogeneous agents, including students and faculty.

---

## [4/10] GPTrace: Effective Crash Deduplication Using LLM Embeddings
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T12:30:30.000Z
**URL:** https://arxiv.org/abs/2512.01609v1
**Reasoning:** The topic is about crash deduplication using LLM embeddings, which is not directly related to our primary interests.
**Authors:** Patrick Herter

**Content/Abstract:**
> Fuzzing is a highly effective method for uncovering software vulnerabilities, but analyzing the resulting data typically requires substantial manual effort. This is amplified by the fact that fuzzing campaigns often find a large number of crashing inputs, many of which share the same underlying bug. Crash deduplication is the task of finding such duplicate crashing inputs and thereby reducing the data that needs to be examined. Many existing deduplication approaches rely on comparing stack traces or other information that is collected when a program crashes. Although various metrics for measuring the similarity of such pieces of information have been proposed, many do not yield satisfactory deduplication results. In this work, we present GPTrace, a deduplication workflow that leverages a large language model to evaluate the similarity of various data sources associated with crashes by computing embedding vectors and supplying those as input to a clustering algorithm. We evaluate our approach on over 300 000 crashing inputs belonging to 50 ground truth labels from 14 different targets. The deduplication results produced by GPTrace show a noticeable improvement over hand-crafted stack trace comparison methods and even more complex state-of-the-art approaches that are less flexible.

---

## [4/10] OpenDORS: A dataset of openly referenced open research software
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T11:45:50.000Z
**URL:** https://arxiv.org/abs/2512.01570v1
**Reasoning:** The dataset is related to research software but not directly related to codebase indexing or retrieval.
**Authors:** Stephan Druskat

**Content/Abstract:**
> In many academic disciplines, software is created during the research process or for a research purpose. The crucial role of software for research is increasingly acknowledged. The application of software engineering to research software has been formalized as research software engineering, to create better software that enables better research. Despite this, large-scale studies of research software and its development are still lacking. To enable such studies, we present a dataset of 134,352 unique open research software projects and 134,154 source code repositories referenced in open access literature. Each dataset record identifies the referencing publication and lists source code repositories of the software project. For 122,425 source code repositories, the dataset provides metadata on latest versions, license information, programming languages and descriptive metadata files. We summarize the distributions of these features in the dataset and describe additional software metadata that extends the dataset in future work. Finally, we suggest examples of research that could use the dataset to develop a better understanding of research software practice in RSE research.

---

## [4/10] Estimating the prevalence of LLM-assisted text in scholarly writing
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T11:34:15.000Z
**URL:** https://arxiv.org/abs/2512.01560v1
**Reasoning:** The article discusses LLM usage in scholarly writing, which is tangentially related to AI but not directly relevant to our core interests.
**Authors:** Andrew Gray

**Content/Abstract:**
> The use of large language models (LLMs) in scholarly publications has grown dramatically since the launch of ChatGPT in late 2022. This usage is often undisclosed, and it can be challenging for readers and reviewers to identify human written but LLM-revised or translated text, or predominantly LLM-generated text. Given the known quality and reliability issues connected with LLM-generated text, their potential growth poses an increasing problem for research integrity, and for public trust in research. 
>   This study presents a simple and easily reproducible methodology to show the growth in the full text of published papers, across the full range of research, as indexed in the Dimensions database. It uses this to demonstrate that LLM tools are likely to have been involved in the production of more than 10% of all published papers in 2024, based on disproportionate use of specific indicative words, and draws together earlier studies to confirm that this is a plausible overall estimate. 
>   It then discusses the implications of this for the integrity of scholarly publishing, highlighting evidence that use of LLMs for text generation is still being concealed or downplayed by authors, and presents an argument that more comprehensive disclosure requirements are urgently required to address this.

---

## [4/10] LEC: Linear Expectation Constraints for False-Discovery Control in Selective Prediction and Routing Systems
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T11:27:09.000Z
**URL:** https://arxiv.org/abs/2512.01556v1
**Reasoning:** The article discusses false discovery control in LLMs, which is not directly related to context engineering or code retrieval.
**Authors:** Zhiyuan Wang

**Content/Abstract:**
> Large language models (LLMs) often generate unreliable answers, while heuristic uncertainty methods fail to fully distinguish correct from incorrect predictions, causing users to accept erroneous answers without statistical guarantees. We address this issue through the lens of false discovery rate (FDR) control, ensuring that among all accepted predictions, the proportion of errors does not exceed a target risk level. To achieve this in a principled way, we propose LEC, which reinterprets selective prediction as a constrained decision problem by enforcing a Linear Expectation Constraint over selection and error indicators. Then, we establish a finite-sample sufficient condition, which relies only on a held-out set of exchangeable calibration samples, to compute an FDR-constrained, coverage-maximizing threshold. Furthermore, we extend LEC to a two-model routing mechanism: given a prompt, if the current model's uncertainty exceeds its calibrated threshold, we delegate it to a stronger model, while maintaining a unified FDR guarantee. Evaluations on closed-ended and open-ended question-answering (QA) datasets show that LEC achieves tighter FDR control and substantially improves sample retention over prior methods. Moreover, the two-model routing mechanism achieves lower risk levels while accepting more correct samples than each individual model.

---

## [4/10] Teaching an Online Multi-Institutional Research Level Software Engineering Course with Industry - an Experience Report
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T10:46:43.000Z
**URL:** https://arxiv.org/abs/2512.01523v1
**Reasoning:** The article is about teaching a software engineering course, which is tangentially related to engineering culture.
**Authors:** Pankaj Jalote

**Content/Abstract:**
> Covid has made online teaching and learning acceptable and students, faculty, and industry professionals are all comfortable with this mode. This comfort can be leveraged to offer an online multi-institutional research-level course in an area where individual institutions may not have the requisite faculty to teach and/or research students to enroll. If the subject is of interest to industry, online offering also allows industry experts to contribute and participate with ease. Advanced topics in Software Engineering are ideally suited for experimenting with this approach as industry, which is often looking to incorporate advances in software engineering in their practices, is likely to agree to contribute and participate. In this paper we describe an experiment in teaching a course titled "AI in Software Engineering" jointly between two institutions with active industry participation, and share our and student's experience. We believe this collaborative teaching approach can be used for offering research level courses in any applied area of computer science by institutions who are small and find it difficult to offer research level courses on their own.

---

## [4/10] DuckDB on xNVMe
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T10:12:37.000Z
**URL:** https://arxiv.org/abs/2512.01490v1
**Reasoning:** The discussion on DuckDB and NVMe SSDs is more about database and storage hardware, not directly relevant to our focus areas.
**Authors:** Marius Ottosen

**Content/Abstract:**
> DuckDB is designed for portability. It is also designed to run anywhere, and possibly in contexts where it can be specialized for performance, e.g., as a cloud service or on a smart device. In this paper, we consider the way DuckDB interacts with local storage. Our long term research question is whether and how SSDs could be co-designed with DuckDB. As a first step towards vertical integration of DuckDB and programmable SSDs, we consider whether and how DuckDB can access NVMe SSDs directly. By default, DuckDB relies on the POSIX file interface. In contrast, we rely on the xNVMe library and explore how it can be leveraged in DuckDB. We leverage the block-based nature of the DuckDB buffer manager to bypass the synchronous POSIX I/O interface, the file system and the block manager. Instead, we directly issue asynchronous I/Os against the SSD logical block address space. Our preliminary experimental study compares different ways to manage asynchronous I/Os atop xNVMe. The speed-up we observe over the DuckDB baseline is significant, even for the simplest scan query over a TPC-H table. As expected, the speed-up increases with the scale factor, and the Linux NVMe passthru improves performance. Future work includes a more thorough experimental study, a flexible solution that combines raw NVMe access and legacy POSIX file interface as well the co-design of DuckDB and SSDs.

---

## [4/10] Multi-Path Collaborative Reasoning via Reinforcement Learning
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T10:05:46.000Z
**URL:** https://arxiv.org/abs/2512.01485v1
**Reasoning:** The article on multi-path collaborative reasoning via reinforcement learning is not directly related to code intelligence or context engineering.
**Authors:** Jindi Lv

**Content/Abstract:**
> Chain-of-Thought (CoT) reasoning has significantly advanced the problem-solving capabilities of Large Language Models (LLMs), yet conventional CoT often exhibits internal determinism during decoding, limiting exploration of plausible alternatives. Recent methods attempt to address this by generating soft abstract tokens to enable reasoning in a continuous semantic space. However, we find that such approaches remain constrained by the greedy nature of autoregressive decoding, which fundamentally isolates the model from alternative reasoning possibilities. In this work, we propose Multi-Path Perception Policy Optimization (M3PO), a novel reinforcement learning framework that explicitly injects collective insights into the reasoning process. M3PO leverages parallel policy rollouts as naturally diverse reasoning sources and integrates cross-path interactions into policy updates through a lightweight collaborative mechanism. This design allows each trajectory to refine its reasoning with peer feedback, thereby cultivating more reliable multi-step reasoning patterns. Empirical results show that M3PO achieves state-of-the-art performance on both knowledge- and reasoning-intensive benchmarks. Models trained with M3PO maintain interpretability and inference efficiency, underscoring the promise of multi-path collaborative learning for robust reasoning.

---

## [4/10] OpenTelemetry Filelog Receiver: A Guide to Ingesting Log Files
**Source:** The Practical Developer | **Date:** 2025-12-02T13:38:46.000Z
**URL:** https://dev.to/dash0/opentelemetry-filelog-receiver-a-guide-to-ingesting-log-files-38m6
**Reasoning:** The guide on OpenTelemetry Filelog Receiver is not directly related to our primary interests but touches on observability.
**Authors:** Ayooluwa Isaiah

**Content/Abstract:**
> <p>Even in the age of cloud-native apps and distributed tracing, plain old log files remain one of the richest sources of truth in any system. From legacy business applications and batch jobs to <a href="https://www.dash0.com/guides/nginx-logs">NGINX</a>, databases, and on-prem infrastructure, critical diagnostics still end up written to disk.</p> 
>  
> <p><a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver">The OpenTelemetry Collector filelog receiver</a> gives you a way to bring those logs into a <a href="https://www.dash0.com/guides/opentelemetry-collector">modern observability pipeline</a>. It continuously tails files, parses their contents, and converts raw text into <a href="https://www.dash0.com/knowledge/opentelemetry-logging-explained">structured OpenTelemetry LogRecords</a>.</p> 
>  
> <p>This guide shows you how to put that power to work, from the basics of reading a file to building a production-ready pipeline that handles rotation, recovers from restarts, and never loses a single line. You'll learn how to structure, enrich, and standardize log file entries so they become first-class observability data.</p> 
>  
> <p>Let's begin!</p> 
>  
> <h2> 
>    
>    
>   How the Filelog receiver works 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg30rpueploqdrubf9zil.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg30rpueploqdrubf9zil.png" alt="An illustration of how filelogreceiver works in OpenTelemetry" width="800" height="335"></a></p> 
>  
> <p>Before we get into configuration details, it helps to picture how the receiver handles a log file throughout its lifecycle. You can think of it as a simple repeating four-step loop:</p> 
>  
> <ol> 
> <li><p><strong>Discover:</strong> The receiver scans the filesystem at regular intervals, using the <code>include</code> and <code>exclude</code> patterns you've set, to figure out which log files it should pay attention to.</p></li> 
> <li><p><strong>Read:</strong> Once a file is picked up, the receiver opens it and begins following along as new lines are written. The <code>start_at</code> setting decides whether it begins from <code>beginning</code> or just tails new content from the <code>end</code>.</p></li> 
> <li><p><strong>Parse:</strong> Each line (or block of lines, if multiline parsing is used) runs through a series of <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/README.md">Stanza operators</a> (if configured). These operators parse the raw text, pull out key attributes, assign timestamps and severity levels, and ultimately structure the log data.</p></li> 
> <li><p><strong>Emit:</strong> Finally, the structured log records are passed into the Collector's pipeline, where they can be <a href="https://www.dash0.com/guides/opentelemetry-filter-processor">filtered</a>, transformed further, or exported to your backend.</p></li> 
> </ol> 
>  
> <p>This <code>Discover -&gt; Read -&gt; Parse -&gt; Emit</code> loop forms the foundation of everything the receiver does.</p> 
>  
> <h2> 
>    
>    
>   Quick Start: tailing a log file 
> </h2> 
>  
> <p>One of the most common use cases is when your application is already writing logs in JSON format to a file. For example, imagine you have a service writing JSON logs to <code>/var/log/myapp/app.log</code>:<br> 
> </p> 
>  
> <div> 
> <pre><code><span>{</span><span>"time"</span><span>:</span><span>"2025-09-28 20:15:12"</span><span>,</span><span>"level"</span><span>:</span><span>"INFO"</span><span>,</span><span>"message"</span><span>:</span><span>"User logged in successfully"</span><span>,</span><span>"user_id"</span><span>:</span><span>"u-123"</span><span>,</span><span>"source_ip"</span><span>:</span><span>"192.168.1.100"</span><span>}</span><span> 
> </span><span>{</span><span>"time"</span><span>:</span><span>"2025-09-28 20:15:45"</span><span>,</span><span>"level"</span><span>:</span><span>"WARN"</span><span>,</span><span>"message"</span><span>:</span><span>"Password nearing expiration"</span><span>,</span><span>"user_id"</span><span>:</span><span>"u-123"</span><span>}</span><span> 
> </span></code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Here's a minimal <code>filelog</code> receiver example to read and ingest such logs into an OpenTelemetry pipeline:<br> 
> </p> 
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>receivers</span><span>:</span> 
>   <span>filelog</span><span>:</span> 
>     <span># 1. DISCOVER all .log files in /var/log/myapp/</span> 
>     <span>include</span><span>:</span> <span>[</span><span>/var/log/myapp/*.log</span><span>]</span> 
>     <span># 2. READ from the beginning of new files</span> 
>     <span>start_at</span><span>:</span> <span>beginning</span> 
>     <span># 3. PARSE using the json_parser operator</span> 
>     <span>operators</span><span>:</span> 
>       <span>-</span> <span>type</span><span>:</span> <span>json_parser</span> 
>         <span># Tell the parser where to find the timestamp and how it's formatted</span> 
>         <span>timestamp</span><span>:</span> 
>           <span>parse_from</span><span>:</span> <span>attributes.time</span> 
>           <span>layout</span><span>:</span> <span>"</span><span>%Y-%m-%d</span><span> </span><span>%H:%M:%S"</span> 
>         <span># Tell the parser which field contains the severity</span> 
>         <span>severity</span><span>:</span> 
>           <span>parse_from</span><span>:</span> <span>attributes.level</span> 
>  
> <span>exporters</span><span>:</span> 
>   <span>debug</span><span>:</span> 
>     <span>verbosity</span><span>:</span> <span>detailed</span> 
>  
> <span>service</span><span>:</span> 
>   <span>pipelines</span><span>:</span> 
>     <span>logs</span><span>:</span> 
>       <span>receivers</span><span>:</span> <span>[</span><span>filelog</span><span>]</span> 
>       <span>exporters</span><span>:</span> <span>[</span><span>debug</span><span>]</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Here's a breakdown of the above configuration:</p> 
>  
> <ul> 
> <li> 
> <code>include</code>: Points the receiver to all <code>.log</code> files in <code>/var/log/myapp/</code>.</li> 
> <li> 
> <code>start_at: beginning</code>: Ensures the receiver processes the entire file the first time it sees it. By default (<code>end</code>), it would only capture new lines written after the Collector starts.</li> 
> <li> 
> <code>operators</code>: In this case, there's just one: the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/json_parser.md">json_parser</a>. Its job is to take each log line, interpret it as JSON, and then promote selected fields into the log record's core metadata.</li> 
> <li> 
> <code>timestamp</code> and <code>severity</code>: Within the <code>json_parser</code>, we're pulling the <code>time</code> and <code>level</code> fields out of the JSON and promoting them to the OpenTelemetry's top-level <code>Timestamp</code> and <code>Severity*</code> fields for each log record.</li> 
> </ul> 
>  
> <p><a href="https://www.dash0.com/guides/opentelemetry-debug-exporter">With the debug exporter</a>, you'll see the parsed and structured output. Instead of just raw JSON, each field is now properly represented inside each log record:<br> 
> </p> 
>  
> <div> 
> <pre><code>LogRecord #0 
> ObservedTimestamp: 2025-09-28 20:48:36.728437503 +0000 UTC 
> Timestamp: 2025-09-28 20:15:12 +0000 UTC 
> SeverityText: INFO 
> SeverityNumber: Info(9) 
> Body: Str({"time":"2025-09-28 20:15:12","level":"INFO","message":"User logged in successfully","user_id":"u-123","source_ip":"192.168.1.100"}) 
> Attributes: 
>      -&gt; user_id: Str(u-123) 
>      -&gt; source_ip: Str(192.168.1.100) 
>      -&gt; log.file.name: Str(myapp.log) 
>      -&gt; time: Str(2025-09-28 20:15:12) 
>      -&gt; level: Str(INFO) 
>      -&gt; message: Str(User logged in successfully) 
> Trace ID: 
> Span ID: 
> Flags: 0 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>The raw JSON logs have now been converted into OpenTelemetry's unified log data format, ensuring a consistent foundation for cross-system observability.</p> 
>  
> <p>The <code>log.file.name</code> attribute is automatically added by the receiver by default, and you can also enable <code>include_file_path</code> to capture the full file path as well:<br> 
> </p> 
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>receivers</span><span>:</span> 
>   <span>filelog</span><span>:</span> 
>     <span>include</span><span>:</span> <span>[</span><span>/var/log/myapp/*.log</span><span>]</span> 
>     <span>include_file_path</span><span>:</span> <span>true</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>This allows you to easily filter or query logs based on their exact source path:<br> 
> </p> 
>  
> <div> 
> <pre><code>Attributes: 
>      -&gt; log.file.path: Str(/var/log/myapp/app.log) 
>      -&gt; log.file.name: Str(app.log) 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>You can find more enrichment options in the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/filelogreceiver/README.md">official OpenTelemetry Filelog receiver documentation</a>.</p> 
>  
> <h2> 
>    
>    
>   Filtering and managing log files 
> </h2> 
>  
> <p>The most fundamental step in configuring the <code>filelog</code> receiver is telling it which files to monitor. This is controlled using <code>include</code> and <code>exclude</code> glob patterns.</p> 
>  
> <p>The receiver first uses <code>include</code> to generate a list of all potential files, then it applies the <code>exclude</code> patterns to remove any unwanted files from that list.</p> 
>  
> <p>Here's an example:<br> 
> </p> 
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>receivers</span><span>:</span> 
>   <span>filelog</span><span>:</span> 
>     <span>include</span><span>:</span> <span>[</span><span>/var/log/apps/**/*.log</span><span>]</span> 
>     <span>exclude</span><span>:</span> 
>       <span>-</span> <span>/var/log/apps/**/debug.log</span> 
>       <span>-</span> <span>/var/log/apps/**/*.tmp</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>In this scenario, the receiver will collect every <code>.log</code> file under <code>/var/log/apps/</code>, including subdirectories, but it will skip any file named <code>debug.log</code> and any file ending with <code>.tmp</code>.</p> 
>  
> <h3> 
>    
>    
>   Excluding files by modification age 
> </h3> 
>  
> <p>If the log directory you're reading contains many existing log files, you can instruct the receiver to ignore files that have not been modified within a given time window with <code>exclude_older_than</code>:<br> 
> </p> 
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>receivers</span><span>:</span> 
>   <span>filelog</span><span>:</span> 
>     <span>include</span><span>:</span> <span>[</span><span>/var/log/myapp/*.log</span><span>]</span> 
>     <span>exclude_older_than</span><span>:</span> <span>24h</span> 
>     <span>start_at</span><span>:</span> <span>beginning</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>In this example, even if <code>app-2025-07-15.log</code> matches the pattern, it will be skipped if it hasn't been updated in the past 24 hours.</p> 
>  
> <h2> 
>    
>    
>   Parsing unstructured text with regular expressions 
> </h2> 
>  
> <p>Most infrastructure logs don't come neatly packaged as JSON. More often, they're plain text strings that follow a loose pattern, such as web server access logs, database query logs, or operating system messages. These logs are human-readable but difficult for machines to analyze until they're given some structure.</p> 
>  
> <p>The Collector addresses this with the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/regex_parser.md">regex_parser operator</a>. Using regular expressions with named capture groups, you can break a raw log line into meaningful fields and promote them into structured attributes.</p> 
>  
> <p>For example, consider an <a href="https://www.dash0.com/guides/nginx-logs">NGINX access log</a> in the <a href="https://en.wikipedia.org/wiki/Common_Log_Format">Common Log Format</a>:<br> 
> </p> 
>  
> <div> 
> <pre><code>127.0.0.1 - - [28/Sep/2025:20:30:00 +0000] "GET /api/v1/users HTTP/1.1" 200 512 
> 127.0.0.1 - - [28/Sep/2025:20:30:05 +0000] "POST /api/v1/login HTTP/1.1" 401 128 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>You can configure the <code>regex_parser</code> like this to parse them into structured attributes:<br> 
> </p> 
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>receivers</span><span>:</span> 
>   <span>filelog</span><span>:</span> 
>     <span>include</span><span>:</span> <span>[</span><span>/var/log/nginx/access.log</span><span>]</span> 
>     <span>start_at</span><span>:</span> <span>beginning</span> 
>     <span>operators</span><span>:</span> 
>       <span>-</span> <span>type</span><span>:</span> <span>regex_parser</span> 
>         <span># Use named capture groups to extract data</span> 
>         <span>regex</span><span>:</span> 
>           <span>'</span><span>^(?P&lt;client_ip&gt;[^</span><span> </span><span>]+)</span><span> </span><span>-</span><span> </span><span>-</span><span> </span><span>\[(?P&lt;timestamp&gt;[^\]]+)\]</span> 
>           <span>"(?P&lt;http_method&gt;[A-Z]+)</span><span> </span><span>(?P&lt;http_path&gt;[^</span><span> </span><span>"]+)[^"]*"</span> 
>           <span>(?P&lt;status_code&gt;\d{3})</span><span> </span><span>(?P&lt;response_size&gt;\d+)$'</span> 
>         <span># Parse the extracted timestamp</span> 
>         <span>timestamp</span><span>:</span> 
>           <span>parse_from</span><span>:</span> <span>attributes.timestamp</span> 
>           <span>layout</span><span>:</span> <span>"</span><span>%d/%b/%Y:%H:%M:%S</span><span> </span><span>%z"</span> 
>         <span># Map status codes to severities</span> 
>         <span>severity</span><span>:</span> 
>           <span>parse_from</span><span>:</span> <span>attributes.status_code</span> 
>           <span>mapping</span><span>:</span> 
>             <span>info</span><span>:</span> 
>               <span>-</span> <span>min</span><span>:</span> <span>200</span> 
>                 <span>max</span><span>:</span> <span>399</span> 
>             <span>warn</span><span>:</span> <span>4xx</span> 
>             <span>error</span><span>:</span> <span>5xx</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>The core of this setup is the <code>regex</code> expression with named capture groups. Each group labels a slice of the line so the parser can turn it into an attribute: <code>client_ip</code> grabs the remote address, <code>timestamp</code> captures the bracketed time string, <code>http_method</code> and <code>http_path</code> pull the request pieces, <code>status_code</code> picks up the three-digit response code, and <code>response_size</code> records the byte count.</p> 
>  
> <p>Once those attributes exist, the <code>timestamp</code> field parses the <code>timestamp</code> string into a proper datetime value, and the <code>severity</code> block translates status codes into meaningful severity levels using an explicit <code>mapping</code>: 2xx and 3xx responses as <code>INFO</code>, 4xx as <code>WARN</code>, and 5xx as <code>ERROR</code>.</p> 
>  
> <p>Once access logs are ingested with this configuration, you'll see a structured log record with all the important pieces extracted out as attributes:<br> 
> </p> 
>  
> <div> 
> <pre><code>LogRecord #0 
> ObservedTimestamp: 2025-09-28 21:17:42.31729069 +0000 UTC 
> Timestamp: 2025-09-28 20:30:00 +0000 UTC 
> SeverityText: 200 
> SeverityNumber: Info(9) 
> Body: Str(127.0.0.1 - - [28/Sep/2025:20:30:00 +0000] "GET /api/v1/users HTTP/1.1" 200 512) 
> Attributes: 
>      -&gt; status_code: Str(200) 
>      -&gt; response_size: Str(512) 
>      -&gt; log.file.name: Str(myapp.log) 
>      -&gt; client_ip: Str(127.0.0.1) 
>      -&gt; timestamp: Str(28/Sep/2025:20:30:00 +0000) 
>      -&gt; http_method: Str(GET) 
>      -&gt; http_path: Str(/api/v1/users) 
> Trace ID: 
> Span ID: 
> Flags: 0 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>With a single expression and a couple of parsing steps, a flat NGINX access log is transformed into structured OpenTelemetry data. A natural next step is aligning the captured attributes with the <a href="https://opentelemetry.io/docs/specs/semconv/http/">HTTP semantic conventions</a> through the <a href="https://www.dash0.com/guides/opentelemetry-attributes-processor">attributes processor</a> or <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md">transform processor</a>.</p> 
>  
> <h2> 
>    
>    
>   Handling multiple log formats 
> </h2> 
>  
> <p>Log files rarely come in just one flavor. For example, you might be ingesting NGINX logs, database logs, and application logs, each with their own format.</p> 
>  
> <p>The cleanest way to handle this is to define a separate <code>filelog</code> receiver for each file type. Each receiver has its own parsing rules and runs independently, which keeps your setup organized and easy to debug.</p> 
>  
> <p>This is the best approach when the log formats are completely different and share nothing in common.<br> 
> </p> 
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>receivers</span><span>:</span> 
>   <span># NGINX access logs</span> 
>   <span>filelog/nginx_access</span><span>:</span> 
>     <span>include</span><span>:</span> <span>[</span><span>/var/log/nginx/access.log</span><span>]</span> 
>     <span>operators</span><span>:</span> 
>       <span>-</span> <span>type</span><span>:</span> <span>regex_parser</span> 
>         <span># ... NGINX access log parsing rules</span> 
>  
>   <span># NGINX error logs</span> 
>   <span>filelog/nginx_error</span><span>:</span> 
>     <span>include</span><span>:</span> <span>[</span><span>/var/log/nginx/error.log</span><span>]</span> 
>     <span>operators</span><span>:</span> 
>       <span>-</span> <span>type</span><span>:</span> <span>regex_parser</span> 
>         <span># ... NGINX error log parsing rules</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Sometimes, though, variation happens within a single file.</p> 
>  
> <p>Maybe most lines are simple messages, but others add extra fields like a <code>trace_id</code>:<br> 
> </p> 
>  
> <div> 
> <pre><code>INFO: Application started successfully. 
> DEBUG: Processing request for trace_id=12345 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Instead of writing one massive regex to cover every case, you can use conditional operators with <code>if</code>:<br> 
> </p> 
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>receivers</span><span>:</span> 
>   <span>filelog</span><span>:</span> 
>     <span>include</span><span>:</span> <span>[</span><span>/var/log/app.log</span><span>]</span> 
>     <span>operators</span><span>:</span> 
>       <span># Parse the basic structure of every line</span> 
>       <span>-</span> <span>type</span><span>:</span> <span>regex_parser</span> 
>         <span>id</span><span>:</span> <span>base_parser</span> <span># a unique ID is required when multiple operators of the same type is being used</span> 
>         <span>regex</span><span>:</span> <span>'</span><span>^(?P&lt;severity&gt;\w+):</span><span> </span><span>(?P&lt;message&gt;.*)$'</span> 
>  
>       <span># Only run this parser when "trace_id" appears</span> 
>       <span>-</span> <span>type</span><span>:</span> <span>regex_parser</span> 
>         <span>id</span><span>:</span> <span>trace_parser</span> 
>         <span>if</span><span>:</span> <span>'</span><span>attributes["message"]</span><span> </span><span>matches</span><span> </span><span>"trace_id"'</span> 
>         <span>parse_from</span><span>:</span> <span>attributes.message</span> 
>         <span>regex</span><span>:</span> <span>'</span><span>.*trace_id=(?P&lt;trace_id&gt;\w+).*'</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Here's what happens:</p> 
>  
> <ul> 
> <li>The first parser runs on every log line and extracts <code>severity</code> and <code>message</code>.</li> 
> <li>The second parser runs only when the message contains <code>trace_id</code>, enriching the log with that extra field.</li> 
> </ul> 
>  
> <p>By combining these two approaches, multiple receivers for unrelated formats and conditional parsing for minor variations, you can handle almost any kind of log your systems produce without creating unreadable or brittle configurations.</p> 
>  
> <h2> 
>    
>    
>   Handling stack traces and multiline logs 
> </h2> 
>  
> <p>Not all log entries fit neatly on a single line. A stack trace is a classic example:<br> 
> </p> 
>  
> <div> 
> <pre><code>2025-09-28 21:05:42 [ERROR] Unhandled exception: Cannot read property 'foo' of undefined 
> TypeError: Cannot read property 'foo' of undefined 
>     at Object.&lt;anonymous&gt; (/usr/src/app/index.js:15:18) 
>     at Module._compile (node:internal/modules/cjs/loader:1254:14) 
>     at Module._extensions..js (node:internal/modules/cjs/loader:1308:10) 
>     at Module.load (node:internal/modules/cjs/loader:1117:32) 
>     at Module._load (node:internal/modules/cjs/loader:958:12) 
>     at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:81:12) 
>     at node:internal/main/run_main_module:17:47 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>If you send this directly to the Collector, the filelog receiver will treat each line as a separate log record. That's not what you want, since the error message and every stack frame belong together.</p> 
>  
> <p>The fix is to use the <code>multiline</code> configuration, which tells the receiver how to group lines into a single entry:<br> 
> </p> 
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>receivers</span><span>:</span> 
>   <span>filelog</span><span>:</span> 
>     <span>include</span><span>:</span> <span>[</span><span>/var/log/myapp/*.log</span><span>]</span> 
>     <span>start_at</span><span>:</span> <span>beginning</span> 
>  
>     <span>multiline</span><span>:</span> 
>       <span># New entry starts when a line begins with "YYYY-MM-DD HH:MM:SS"</span> 
>       <span>line_start_pattern</span><span>:</span> <span>^\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}</span> 
>  
>     <span>operators</span><span>:</span> 
>       <span>-</span> <span>type</span><span>:</span> <span>regex_parser</span> 
>         <span>regex</span><span>:</span> <span>(?P&lt;timestamp&gt;\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})\s+\[(?P&lt;severity&gt;[A-Za-z]+)\]\s+(?P&lt;message&gt;.+)</span> 
>  
>         <span>timestamp</span><span>:</span> 
>           <span>parse_from</span><span>:</span> <span>attributes.timestamp</span> 
>           <span>layout</span><span>:</span> <span>"</span><span>%Y-%m-%d</span><span> </span><span>%H:%M:%S"</span> 
>  
>         <span>severity</span><span>:</span> 
>           <span>parse_from</span><span>:</span> <span>attributes.severity</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Here, the <code>line_start_pattern</code> acts as the anchor. A new log entry begins only when a line starts with a date in the form <code>YYYY-MM-DD HH:MM:SS</code>, and any line that doesn't match is appended to the previous one.</p> 
>  
> <p>The result is that the entire stack trace, from the error message down through each <code>at ...</code> frame, gets captured as one structured log record. This preserves full context, making it far easier to analyze and troubleshoot errors.<br> 
> </p> 
>  
> <div> 
> <pre><code>LogRecord #0 
> ObservedTimestamp: 2025-10-07 12:04:26.963143642 +0000 UTC 
> Timestamp: 2025-09-28 21:05:42 +0000 UTC 
> SeverityText: ERROR 
> SeverityNumber: Error(17) 
> Body: Str(2025-09-28 21:05:42 [ERROR] Unhandled exception: Cannot read property 'foo' of undefined 
> TypeError: Cannot read property 'foo' of undefined 
>     at Object.&lt;anonymous&gt; (/usr/src/app/index.js:15:18) 
>     at Module._compile (node:internal/modules/cjs/loader:1254:14) 
>     at Module._extensions..js (node:internal/modules/cjs/loader:1308:10) 
>     at Module.load (node:internal/modules/cjs/loader:1117:32) 
>     at Module._load (node:internal/modules/cjs/loader:958:12) 
>     at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:81:12) 
>     at node:internal/main/run_main_module:17:47) 
> Attributes: 
>      -&gt; log.file.name: Str(/var/log/myapp/app.log) 
>      -&gt; message: Str(Unhandled exception: Cannot read property 'foo' of undefined) 
>      -&gt; timestamp: Str(2025-09-28 21:05:42) 
>      -&gt; severity: Str(ERROR) 
> Trace ID: 
> Span ID: 
> Flags: 0 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h2> 
>    
>    
>   Parsing metadata from file headers 
> </h2> 
>  
> <p>Some log files don't just contain log entries. They begin with a header section that holds important metadata about the entire file. Without that context, the individual log lines can be hard to interpret.</p> 
>  
> <p>This pattern is common with batch jobs and export processes. For example, a nightly billing run might write a fresh log file for each execution. At the top of that file you might see something like this:<br> 
> </p> 
>  
> <div> 
> <pre><code># Job-ID: job-d8e8fca2 
> # Job-Type: nightly-billing-run 
> # Executed-By: scheduler-prod-1 
> # Records-To-Process: 1500 
> 2025-10-08T08:20:00Z INFO: Starting billing run. 
> 2025-10-08T08:21:15Z INFO: Processed account #1. 
> 2025-10-08T08:21:16Z WARN: Account #2 has a negative balance. 
> . . . 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Those first lines tell you exactly which job produced the logs that follow. If you ignore them, you lose that crucial context. The <code>header</code> feature solves this by parsing metadata from the top of the file and stamping it onto every subsequent log record.</p> 
>  
> <p>It defines a small, dedicated pipeline that runs only on the initial block of lines. You need to specify a regex to match which lines belong to the header. The <code>metadata_operators</code> then parse those lines into attributes which are automatically added to every log entry that follows.</p> 
>  
> <p>To use this feature, you need to do three things:</p> 
>  
> <ol> 
> <li> Enable the <code>filelog.allowHeaderMetadataParsing</code> feature gate: 
> </li> 
> </ol> 
>  
> <div> 
> <pre><code><span># docker-compose.yml</span> 
> <span>services</span><span>:</span> 
>   <span>otelcol</span><span>:</span> 
>     <span>command</span><span>:</span> 
>       <span>[</span> 
>         <span>--config=/etc/otelcol-contrib/config.yaml</span><span>,</span> 
>         <span>--feature-gates=filelog.allowHeaderMetadataParsing</span><span>,</span> 
>       <span>]</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <ol> 
> <li> Set <code>start_at: beginning</code> since the header has to be read from the top.</li> 
> <li> Configure both the <code>header</code> rules and the main <code>operators</code> pipeline.</li> 
> </ol> 
>  
> <p>Here's the configuration to parse the headers in the sample log file:<br> 
> </p> 
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>receivers</span><span>:</span> 
>   <span>filelog</span><span>:</span> 
>     <span>include</span><span>:</span> <span>[</span><span>/var/log/jobs/*.log</span><span>]</span> 
>     <span>start_at</span><span>:</span> <span>beginning</span> <span># required</span> 
>     <span>header</span><span>:</span> 
>       <span>pattern</span><span>:</span> <span>^#</span> 
>       <span>metadata_operators</span><span>:</span> 
>         <span>-</span> <span>type</span><span>:</span> <span>key_value_parser</span> 
>           <span>delimiter</span><span>:</span> <span>"</span><span>:</span><span> </span><span>"</span> 
>           <span>pair_delimiter</span><span>:</span> <span>"</span><span>#</span><span> </span><span>"</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Here's what's happening:</p> 
>  
> <ul> 
> <li><p><code>pattern: ^#</code> says that any line starting with <code>#</code> belongs to the header. Those header lines are then passed through the pipeline of <code>metadata_operators</code>.</p></li> 
> <li><p>The <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/key_value_parser.md">key_value_parser</a> operator splits each header line into a key and value using <code>:</code> as the separator, while <code>#</code> denotes the beginning of a new key/value pair.</p></li> 
> </ul> 
>  
> <p>These results in the following attributes on every log entry that follows in that file:<br> 
> </p> 
>  
> <div> 
> <pre><code>Attributes: 
>      -&gt; Job-ID: Str(job-d8e8fca2) 
>      -&gt; Job-Type: Str(nightly-billing-run) 
>      -&gt; Executed-By: Str(scheduler-prod-1) 
>      -&gt; Records-To-Process: Str(1500) 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>As you can see, the <code>Job-ID</code> and other header fields are now attached to the log record, providing invaluable context that would have otherwise been lost.</p> 
>  
> <p>From here, you can process them further by promoting the header fields to <a href="https://www.dash0.com/knowledge/what-are-opentelemetry-resources">resource attributes</a> and aligning with <a href="https://www.dash0.com/knowledge/otel-semantic-conventions-explainer">OpenTelemetry semantic conventions</a>.</p> 
>  
> <h2> 
>    
>    
>   How to avoid lost or duplicate logs 
> </h2> 
>  
> <p>When the Collector restarts, log ingestion can easily go wrong if state is not preserved as you risk either re-ingesting old data or skipping over new logs. If you use <code>start_at: beginning</code>, the receiver will reread all your log files and create massive duplication. With <code>start_at: end</code>, you might miss any entries written while the Collector was down.</p> 
>  
> <p>The way to solve this is with <strong>checkpointing</strong>. By configuring a storage extension, you instruct the <code>filelog</code> receiver to save its position in each file (the last read offset) to disk and pick up exactly where it left off.</p> 
>  
> <p>A conventional approach is using the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/extension/storage/filestorage">file_storage extension</a> for this purpose:<br> 
> </p> 
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>extensions</span><span>:</span> 
>   <span>file_storage</span><span>:</span> 
>     <span>directory</span><span>:</span> <span>/var/otelcol/storage</span> 
>  
> <span>receivers</span><span>:</span> 
>   <span>filelog</span><span>:</span> 
>     <span>include</span><span>:</span> <span>[</span><span>/var/log/myapp/*.log</span><span>]</span> 
>     <span>start_at</span><span>:</span> <span>beginning</span> 
>     <span># Link the receiver to the storage extension</span> 
>     <span>storage</span><span>:</span> <span>file_storage</span> 
>  
> <span># ... processors, exporters</span> 
>  
> <span>service</span><span>:</span> 
>   <span># The extension must be enabled in the service section</span> 
>   <span>extensions</span><span>:</span> <span>[</span><span>file_storage</span><span>]</span> 
>   <span>pipelines</span><span>:</span> 
>     <span>logs</span><span>:</span> 
>       <span>receivers</span><span>:</span> <span>[</span><span>filelog</span><span>]</span> 
>       <span># ...</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>With the <code>storage</code> extension enabled, the receiver will:</p> 
>  
> <ol> 
> <li> On startup, check the <code>/var/otelcol/storage</code> directory for saved offsets.</li> 
> <li> Resume reading from the saved offset for any file it was tracking, ensuring no data is lost or duplicated.</li> 
> <li> Periodically update the storage with its latest progress.</li> 
> </ol> 
>  
> <p>Checkpointing ensures that log collection is resilient to restarts, upgrades, and even crashes. It is a critical best practice for reliable log ingestion.</p> 
>  
> <h3> 
>    
>    
>   Handling log delivery failures gracefully 
> </h3> 
>  
> <p>Checkpointing with a storage extension protects you during Collector restarts, but another common failure mode is when the receiver reads a batch successfully but fails to hand it off to the next stage.</p> 
>  
> <p>This can happen if an exporter can't reach its endpoint, or the <a href="https://www.dash0.com/guides/opentelemetry-memory-limiter-processor">memory limiter</a> is refusing data. By default, the receiver will drop that batch of logs and move on to the next, causing silent data loss.</p> 
>  
> <p>To prevent this, the receiver has a built-in mechanism to retry sending failed batches. When <code>retry_on_failure</code> is enabled, the receiver will pause, wait for a configured interval, and attempt to resend the exact same batch of logs. This process repeats with an <a href="https://en.wikipedia.org/wiki/Exponential_backoff">exponential backoff</a> until the batch is sent successfully or the <code>max_elapsed_time</code> is reached:<br> 
> </p> 
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>receivers</span><span>:</span> 
>   <span>filelog</span><span>:</span> 
>     <span>retry_on_failure</span><span>:</span> 
>       <span>enabled</span><span>:</span> <span>true</span> 
>       <span># Wait 5 seconds after the first failure before the first retry.</span> 
>       <span>initial_interval</span><span>:</span> <span>5s</span> 
>       <span># The longest the receiver will wait between retries is 30 seconds.</span> 
>       <span>max_interval</span><span>:</span> <span>30s</span> 
>       <span># Give up trying to send a batch after 10 minutes.</span> 
>       <span>max_elapsed_time</span><span>:</span> <span>10m</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>By combining checkpointing with a robust retry policy , you'll create a highly resilient log file ingestion pipeline that can withstand both Collector restarts and temporary downstream outages or throttling.</p> 
>  
> <h3> 
>    
>    
>   Deleting log files after processing 
> </h3> 
>  
> <p>Some workflows call for processing a file once and then removing it to save space and avoid reprocessing. You can enable this with <code>delete_after_read</code>, which requires <code>start_at: beginning</code>:<br> 
> </p> 
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>receivers</span><span>:</span> 
>   <span>filelog</span><span>:</span> 
>     <span>include</span><span>:</span> <span>[</span><span>/var/log/archives/*.gz</span><span>]</span> 
>     <span>start_at</span><span>:</span> <span>beginning</span> 
>     <span>delete_after_read</span><span>:</span> <span>true</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>You must need also enable the <code>filelog.allowFileDeletion</code> feature gate for this to work:<br> 
> </p> 
>  
> <div> 
> <pre><code><span># docker-compose.yml</span> 
> <span>services</span><span>:</span> 
>   <span>otelcol</span><span>:</span> 
>     <span>command</span><span>:</span> 
>       <span>[</span> 
>         <span>--config=/etc/otelcol-contrib/config.yaml</span><span>,</span> 
>         <span>--feature-gates=filelog.allowFileDeletion</span><span>,</span> 
>       <span>]</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Finally, ensure that the files are configured to be deletable and that the Collector service has enough permissions to delete the file. If permissions are insufficient, you will see a "could not delete" log record:<br> 
> </p> 
>  
> <div> 
> <pre><code>2025-10-08T06:42:03.973Z        error   reader/reader.go:278    could not delete        {"resource": {"service.instance.id": "7c0daf0e-e625-4da8-9577-072606dce057", "service.name": "otelcol-contrib", "service.version": "0.136.0"}, "otelcol.component.id": "filelog", "otelcol.component.kind": "receiver", "otelcol.signal": "logs", "component": "fileconsumer", "path": "/var/log/myapp/app.log", "filename": "/var/log/myapp/app.log"} 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Just be careful when enabling this setting as it deletes the files from disk permanently.</p> 
>  
> <h2> 
>    
>    
>   Handling log rotation seamlessly 
> </h2> 
>  
> <p>Log files don't grow indefinitely. <a href="https://www.dash0.com/guides/log-rotation-linux-logrotate">Eventually, they'll get rotated</a> (or at least they should). The <code>filelog</code> receiver is built to handle common rotation patterns, such as <code>app.log</code> to <code>app.log.1</code> automatically and without losing data.</p> 
>  
> <p>Instead of relying on filenames alone, the receiver tracks each file using a unique fingerprint derived from the first few kilobytes of content. When rotation occurs, it recognizes that the original file has been renamed, finishes reading it, and then starts fresh from the beginning of the new <code>app.log</code>.</p> 
>  
> <p>This behavior requires no additional configuration; it works out of the box, giving you reliable log ingestion even in environments with frequent rotations.</p> 
>  
> <h3> 
>    
>    
>   Reading compressed files 
> </h3> 
>  
> <p>Many log rotation tools compress old logs to save disk space, producing files like <code>access.log.1.gz</code>. The <code>filelog</code> receiver can handle these seamlessly by decompressing them on the fly.</p> 
>  
> <p>To make this work, you use the <code>compression</code> setting. This tells the receiver that some or all of the files it discovers may be compressed and need to be decompressed before parsing.</p> 
>  
> <p>You have two main choices for the <code>compression</code> setting:</p> 
>  
> <ul> 
> <li> 
> <code>gzip</code>: Treats all matched files as gzip-compressed.</li> 
> <li> 
> <code>auto</code>: Automatically detects compression based on file extension (currently <code>.gz</code>). This is the best option when a directory contains a mix of active, uncompressed logs and older, compressed ones.</li> 
> </ul> 
>  
> <p>For example, if your directory has both <code>app.log</code> (active) and <code>app.log.1.gz</code> (rotated and compressed), you can configure the receiver like this:<br> 
> </p> 
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>receivers</span><span>:</span> 
>   <span>filelog</span><span>:</span> 
>     <span>include</span><span>:</span> <span>[</span><span>/var/log/myapp/*</span><span>]</span> 
>     <span>start_at</span><span>:</span> <span>beginning</span> 
>     <span># Automatically detect and decompress .gz files</span> 
>     <span>compression</span><span>:</span> <span>auto</span> 
>     <span>operators</span><span>:</span> 
>       <span>-</span> <span>type</span><span>:</span> <span>regex_parser</span> 
>         <span># ... your parsing rules</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>When working with compressed logs, there are two main things to keep in mind.</p> 
>  
> <p>First, the receiver assumes that compressed files can only grow by appending new data. If a file is completely rewritten, for example by taking the original content and recompressing it together with new lines, the receiver may not handle it correctly.</p> 
>  
> <p>Second, there's the question of fingerprinting. By default, the receiver identifies files based on their compressed bytes. This works fine in most cases, but if files are renamed or moved it can cause confusion. To make identification more reliable, you can enable the <code>filelog.decompressFingerprint</code> feature gate. With this enabled, the fingerprint is calculated from the decompressed content.<br> 
> </p> 
>  
> <div> 
> <pre><code><span># docker-compose.yml</span> 
> <span>services</span><span>:</span> 
>   <span>otelcol</span><span>:</span> 
>     <span>command</span><span>:</span> 
>       <span>[</span> 
>         <span>--config=/etc/otelcol-contrib/config.yaml</span><span>,</span> 
>         <span>--feature-gates=filelog.decompressFingerprint</span><span>,</span> 
>       <span>]</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>One caution: if you turn this feature on in an existing setup, the fingerprints will change. That means compressed files that were already read may be ingested again.</p> 
>  
> <h2> 
>    
>    
>   Performance tuning for high-volume environments 
> </h2> 
>  
> <p>The OTel collector <code>filelog</code> receiver's default settings are optimized for general use, but in production environments with hundreds of log files or very high throughput, you'll likely need to tune its performance.</p> 
>  
> <p>By default, the receiver tries to read from every matched file at once. On a system producing thousands of files, this can hog the CPU and quickly hit file handle limits.</p> 
>  
> <p>The <code>max_concurrent_files</code> setting puts a cap on how many files are read at the same time. The default is <code>1024</code>, but lowering this can keep your system from getting overwhelmed.</p> 
>  
> <p>Another key setting is <code>poll_interval</code>, which controls how often the receiver checks for new files and new log lines. The default is 200ms which means logs show up almost immediately, but CPU use goes up because the filesystem is scanned more often.</p> 
>  
> <p>For less critical logs or resource-constrained environments, bumping this to <code>1s</code> or even 5s can be a good trade-off as it'll reduce the polling overhead with only a negligible impact on observability for most use cases.</p> 
>  
> <p>Finally, unusually large log entries are guarded against through the <code>max_log_size</code> setting. It defines the largest allowed log entry, so that anything bigger gets truncated. The default is 1MiB, which is a sensible default for most workloads.<br> 
> </p> 
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>receivers</span><span>:</span> 
>   <span>filelog/k8s_pods</span><span>:</span> 
>     <span>include</span><span>:</span> <span>[</span><span>/var/log/pods/*/*/*.log</span><span>]</span> 
>     <span>max_concurrent_files</span><span>:</span> <span>200</span> 
>     <span>poll_interval</span><span>:</span> <span>1s</span> 
>     <span>max_log_size</span><span>:</span> <span>2MiB</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h2> 
>    
>    
>   Enforcing log file order 
> </h2> 
>  
> <p>Most of the time, the order in which log files are ingested doesn't matter. But some systems produce logs as a series of sequential files where processing order is critical.</p> 
>  
> <p>By default, the <code>filelog</code> receiver reads all matching files concurrently, which means you could end up processing them out of sequence. The <code>ordering_criteria</code> setting solves this by enforcing a strict order when reading files.</p> 
>  
> <p>For example, given a set of log files with the following conventions:<br> 
> </p> 
>  
> <div> 
> <pre><code>batch-run-001.log 
> batch-run-002.log 
> batch-run-003.log 
> </code></pre> 
>  
> </div> 
>  
>  
>  
>  
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>receivers</span><span>:</span> 
>   <span>filelog/batch_logs</span><span>:</span> 
>     <span>include</span><span>:</span> <span>[</span><span>/var/log/batch-runs/batch-run-*.log</span><span>]</span> 
>     <span>start_at</span><span>:</span> <span>beginning</span> 
>     <span>ordering_criteria</span><span>:</span> 
>       <span>top_n</span><span>:</span> <span>1</span> 
>       <span># Extract the sequence number from the filename</span> 
>       <span>regex</span><span>:</span> <span>batch-run-(?P&lt;seq_num&gt;\d+)\.log</span> 
>       <span># Sort files by the sequence number as a number, not a string</span> 
>       <span>sort_by</span><span>:</span> 
>         <span>-</span> <span>regex_key</span><span>:</span> <span>seq_num</span> 
>           <span>sort_type</span><span>:</span> <span>numeric</span> 
>           <span>ascending</span><span>:</span> <span>true</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>With this setup, the receiver will discover all files matching <code>batch-run-*.log</code>, extract the sequence number from each filename, and sort the files numerically in ascending order by that sequence.</p> 
>  
> <p>The <code>top_n</code> property determines how many files will be tracked after applying the ordering criteria. With <code>top_n: 1</code>, only the first file (<code>batch-run-001.log</code>) will be tracked and ingested into the pipeline.</p> 
>  
> <h2> 
>    
>    
>   Filelog receiver tips and best practices 
> </h2> 
>  
> <p>When troubleshooting the <code>filelog</code> receiver, a few issues come up again and again. Here's how to diagnose and fix them quickly:</p> 
>  
> <h3> 
>    
>    
>   Log files are not being watched 
> </h3> 
>  
> <p>When the Collector starts watching a file for log entries, you'll see a log like this in its output:<br> 
> </p> 
>  
> <div> 
> <pre><code>2025-10-09T08:47:05.574Z        info    fileconsumer/file.go:261        Started watching file   {...} 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>If you don't see this message, or if you see the log below, it means that the receiver hasn't picked up any files yet:<br> 
> </p> 
>  
> <div> 
> <pre><code>2025-10-09T09:25:20.280Z        warn    fileconsumer/file.go:49 finding files   {..., "error": "no files match the configured criteria"} 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Start by double-checking your <code>include</code>, <code>exclude</code>, and <code>exclude_older_than</code> settings to make sure your file patterns actually match the files you expect.</p> 
>  
> <p>Next, verify that the Collector process has permission to access both the files and their parent directories. Missing directory-level permissions are one of the most common reasons files aren't discovered or watched.</p> 
>  
> <h3> 
>    
>    
>   Files are watched but no log lines are read 
> </h3> 
>  
> <p>If you can see "Started watching file" messages but no logs are being collected, the most common cause is the <code>start_at</code> setting. By default, it's set to <code>end</code>, which tells the receiver to start reading only new lines appended after the Collector starts.</p> 
>  
> <p>When you're testing with an existing file that isn't actively being written to, this means nothing will appear. To read the entire file from the start, set <code>start_at</code> to <code>beginning</code>:<br> 
> </p> 
>  
> <div> 
> <pre><code><span># otelcol.yaml</span> 
> <span>receivers</span><span>:</span> 
>   <span>filelog</span><span>:</span> 
>     <span>start_at</span><span>:</span> <span>beginning</span> 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>This ensures the receiver processes all existing content the first time the file is discovered.</p> 
>  
> <h3> 
>    
>    
>   Regular expression doesn't match log lines 
> </h3> 
>  
> <p>If your logs aren't being parsed correctly, the issue is usually with your regular expression. When this happens, the Collector often logs an error like:<br> 
> </p> 
>  
> <div> 
> <pre><code>2025-10-09T09:32:14.949Z        error   helper/transformer.go:154       Failed to process entry {"resource": {"service.instance.id": "f8ec2efd-16e9-44ad-9ed2-9f406e46719f", "service.name": "otelcol-contrib", "service.version": "0.136.0"}, "otelcol.component.id": "filelog", "otelcol.component.kind": "receiver", "otelcol.signal": "logs", "operator_id": "regex_parser", "operator_type": "regex_parser", "error": "regex pattern does not match", "action": "send", "entry.timestamp": "0001-01-01T00:00:00.000Z", "log.file.name": "batch-run-001.log"} 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Before adjusting your Collector config, test the regex outside of it using a tool like <a href="https://regex101.com/">Regex101</a>. Make sure to select the <strong>Golang</strong> flavor so it behaves the same way as the Collector's regex engine.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmppgv21pumkd0xnc46h8.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmppgv21pumkd0xnc46h8.png" alt="Regex101 being used to test OpenTelemetry Collector regular expression in Golang flavor" width="800" height="436"></a></p> 
>  
> <p>If you're not seeing this error but your regex still isn't working, check whether the <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/on_error.md"><code>on_error</code> parameter</a> is set to one of the <code>_quiet</code> modes. Those values suppress operator errors unless the Collector log level is set to <code>DEBUG</code>.</p> 
>  
> <p>Common causes of regex mismatches include invisible spaces or tabs, missing anchors (<code>^</code> or <code>$</code>), incorrect escaping, or small format differences between your log and your pattern. Double-check these details before investigating further.</p> 
>  
> <h3> 
>    
>    
>   Logs are duplicated after restart 
> </h3> 
>  
> <p>If you notice duplicate logs appearing after the Collector restarts, it usually means the receiver isn't remembering where it left off. To fix this, enable a <code>storage</code> extension so the <code>filelog</code> receiver can checkpoint its position in each file.</p> 
>  
> <p>This allows the receiver to resume reading exactly where it stopped, preventing both data loss and duplication. Without it, the receiver will reread entire files from the start after every restart.</p> 
>  
> <h2> 
>    
>    
>   Final thoughts 
> </h2> 
>  
> <p>The <code>filelog</code> receiver in OpenTelemetry is an essential bridge between traditional file-based logging (often with unstructured data) and the world of modern, structured observability.</p> 
>  
> <p>By mastering its core concepts of discovery, parsing with operators, and checkpointing, you can build a reliable log ingestion pipeline for any service that writes its logs to a file.</p> 
>  
> <p>Once you've transformed your raw text logs into well-structured OpenTelemetry data, the full observability ecosystem opens up. You can enrich, filter, and route them to any backend that speaks <a href="https://www.dash0.com/knowledge/opentelemetry-protocol-otlp">OTLP</a>.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fi6og47d2te9vq6goemzj.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fi6og47d2te9vq6goemzj.png" alt="Sending log data to Dash0" width="800" height="456"></a></p> 
>  
> <p>For a faster path from collecting telemetry to insight, consider using <a href="https://www.dash0.com/">Dash0</a>, an observability platform purpose-built for OpenTelemetry data. <a href="https://www.dash0.com/sign-up">Try it out today with a free 14-day trial</a>.</p>

---

## [4/10] 🇺🇸 SAM ALTMAN JUST HIT “CODE RED” - AND BIG TECH FINALLY SMELLS BLOOD OpenAI just threw the emergency lever. Sam Altman - the man who spent the last 2 years looking untouchable - told his staff this week that the AI crown is no longer guaran - x.com
**Source:** "Anthropic" - Google News | **Date:** 2025-12-02T12:41:16.000Z
**URL:** https://news.google.com/rss/articles/CBMiYkFVX3lxTE13bjBjREtnbElSb2pDb2JrcHlmQzJWbG1xUTJWc1dpamR5cXN5cnV3LTNYVTJwSl9tWHBkNDRObmFCSG9ENkstTzkwUlRpX3JPdlExdVJRaFhhTS12RGFMWUVR?oc=5
**Reasoning:** The article is about competitive dynamics in AI, but lacks specific relevance to our primary or secondary interests.

**Content/Abstract:**
> <a href="https://news.google.com/rss/articles/CBMiYkFVX3lxTE13bjBjREtnbElSb2pDb2JrcHlmQzJWbG1xUTJWc1dpamR5cXN5cnV3LTNYVTJwSl9tWHBkNDRObmFCSG9ENkstTzkwUlRpX3JPdlExdVJRaFhhTS12RGFMWUVR?oc=5">🇺🇸 SAM ALTMAN JUST HIT “CODE RED” - AND BIG TECH FINALLY SMELLS BLOOD OpenAI just threw the emergency lever. Sam Altman - the man who spent the last 2 years looking untouchable - told his staff this week that the AI crown is no longer guaran</a>  x.com

---

## [4/10] Improve your Prompt Engineering with the help of a little Mermaid
**Source:** The Practical Developer | **Date:** 2025-12-02T13:18:00.000Z
**URL:** https://dev.to/grimch/improve-your-prompt-engineering-with-the-help-of-a-little-mermaid-2j60
**Reasoning:** The article discusses Mermaid diagrams for LLM prompting, which is tangentially related to context engineering.
**Authors:** Christoph Grimm

**Content/Abstract:**
> <p><strong>Spoiler Alert:</strong> This article is about <strong>using Mermaid diagrams as a method of re-enforcement in LLM prompting</strong> and <strong>not</strong> about the fary tail <em>The Little Mermaid</em> written by Danish author Hans Christian Anderson.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Summary 
> </h2> 
>  
> <p>You probably have seen a lot of <a href="https://mermaid.js.org/"><strong>Mermaid Diagrams</strong></a> already without even realizing that what is powering them is a formal description language which is very well understood by LLMs.<br> 
> <br><strong>Mermaid syntax</strong> is very good for describing formal, structural, or temporal information like "A happens, then B, and if X, then C".<br> It forces clarity and guarantees that the visualization precisely reflects the text definition which makes it a perfect choice for a <strong>prompt as code approach</strong>.</p> 
>  
> <p>Complementing your text with additional Mermaid code in your prompt will provide the LLM so called "<strong>Dual-Path Reinforcement</strong>":</p> 
>  
> <ul> 
> <li> 
> <strong>Text</strong> provides the context, intent, nuance, and soft constraints. 
>  
> <ul> 
> <li>It explains the goal of the sequence and the purpose of the participants, </li> 
> <li>clarifies ambiguities present in the structural diagram</li> 
> <li>and guides the overall interpretation.</li> 
> </ul> 
> </li> 
> <li> 
> <strong>Complementary Mermaid Code</strong> provides the strict, non-negotiable logic to ground the text. 
>  
> <ul> 
> <li>It explicitly defines the flow, participants, timing, and conditions (loops, branches) in a way that is less prone to misinterpretation</li> 
> <li>and confirms the exact, step-by-step logic, preventing the LLM from "hallucinating" or misinterpreting the flow based on ambiguous human language.</li> 
> </ul> 
> </li> 
> </ul> 
>  
> <p>Once a diagram is defined, you as the prompt engineer will simply <strong>refer to the visualization</strong>, removing the need to re-read the verbose Mermaid code for logical flow. This visual clarity is a significant advantage over other <em>prompt as code</em> approaches like JSON or TypeScript.</p> 
>  
>  
> <h2> 
>    
>    
>   Why Mermaid Syntax Helps an LLM 
> </h2> 
>  
> <p>For a Large Language Model (LLM) Mermaid syntax offers several critical advantages when processing structured information:</p> 
>  
> <ul> 
> <li>Natural language descriptions of complex systems ("A uses B, which calls C unless D is present") are inherently <strong>ambiguous</strong>. the LLM needs to interpret the context and resolve linguistic relationships.</li> 
> <li>Mermaid syntax is a Domain-Specific Language (DSL) that is highly structured and unambiguous. A --&gt; B means A connects to B. This structure immediately tells the LLM the entities and the directed relationship between them, converting a natural languageinterpretation task into a simple, efficient parsing task.</li> 
> <li>Mermaid allows the LLMto map the text directly to a well-defined conceptual structure (a graph, a tree, a timeline). This is analogous to how a human can immediately internalize a flowchart versus reading a dense text description.</li> 
> </ul> 
>  
>  
> <h2> 
>    
>    
>   Example: Describing a Purchase Order Process 
> </h2> 
>  
> <p>To demonstrate the efficiency and structural clarity of adding complementary Mermaid syntax to your LLM prompt, we will analyze a common scenario: a user purchasing an item in an e-commerce system, which involves interaction between three services - <code>Frontend</code>, <code>OrderService</code>, and <code>InventoryService</code>:</p> 
>  
> <ol> 
> <li> User clicks 'Buy' on the <strong>Frontend</strong>.</li> 
> <li> <strong>Frontend</strong> sends a request to the <strong>OrderService</strong> to initiate the order.</li> 
> <li> <strong>OrderService</strong> checks the stock by asking the <strong>InventoryService</strong>.</li> 
> <li> <strong>InventoryService</strong> returns the stock level.</li> 
> <li> If stock is available, <strong>OrderService</strong> deducts the stock from <strong>InventoryService</strong>.</li> 
> <li> <strong>OrderService</strong> confirms the order back to the <strong>Frontend</strong>.</li> 
> <li> <strong>Frontend</strong> shows a success message.</li> 
> </ol> 
> <h3> 
>    
>    
>   Textual Representation 
> </h3> 
>  
> <blockquote> 
> <p>The purchase process begins when the Frontend sends a request to the OrderService, specifying the Product ID and the desired quantity.<br><br> 
> The OrderService then takes on the responsibility of verifying inventory. It issues a call to the InventoryService to check the current stock levels for that specific product ID.<br><br> 
> After this check, the InventoryService responds back to the OrderService with the current quantity available.<br><br> 
> Importantly, the process includes a critical conditional branch: If the stock is available, the OrderService sends another message back to the InventoryService to officially deduct the purchased quantity. Following this, the OrderService sends an Order Confirmation, including the new Order ID, back to the Frontend.<br><br> 
> Conversely, if the stock is unavailable during the initial check, the OrderService terminates that branch and sends an immediate 'Error: Out of Stock' message back to the Frontend.<br><br> 
> Regardless of the outcome of the order, the final step involves the Frontend internally displaying an appropriate success or failure message to the user.</p> 
> </blockquote> 
> <h3> 
>    
>    
>   Complementary Mermaid Sequence Diagram (DSL) 
> </h3> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fqbnidz4ckz3kvnxfj1oo.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fqbnidz4ckz3kvnxfj1oo.png" alt="Purchase Order Sequence Diagram" width="800" height="503"></a></p> 
>  
> <p>What the LLM actually <em>sees</em> is not the above image but this Mermaid code:<br> 
> </p> 
>  
> <div> 
> <pre><code>sequenceDiagram 
>     participant F as Frontend 
>     participant O as OrderService 
>     participant I as InventoryService 
>  
>     F-&gt;&gt;O: Initiate Purchase (Product ID, Qty) 
>     O-&gt;&gt;I: Check Stock (Product ID) 
>     I--&gt;&gt;O: Stock Available (Qty) 
>     alt Stock Available 
>         O-&gt;&gt;I: Deduct Stock (Qty) 
>         O--&gt;&gt;F: Order Confirmation (ID) 
>     else Stock Unavailable 
>         O--&gt;&gt;F: Error: Out of Stock 
>     end 
>     F-&gt;&gt;F: Display Success Message 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <h3> 
>    
>    
>   The combined input compensates for the core weaknesses of the LLM: 
> </h3> 
>  
> <ul> 
> <li> 
> <strong>Ambiguity in Text:</strong> Mermaid provides the strict, non-negotiable logic to ground the text.</li> 
> <li> 
> <strong>Hallucination/Creativity:</strong> Mermaid forces the model into a rigid structure (like code), constraining its creative freedom to invent steps that aren't there.</li> 
> <li> 
> <strong>Losing Track in Long Context:</strong> The diagram acts as an easy-to-parse summary. The model can cross-reference the text against the visual logic flow, improving its long-term attention to the process.</li> 
> </ul> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Appendix: Mermaid Diagram Types you should consider for your Prompts 
> </h2> 
>  
> <div><table> 
> <thead> 
> <tr> 
> <th>Diagram Type</th> 
> <th>Suitable For Knowledge Representation Aspect</th> 
> <th>Example Use Case in Code/System Analysis</th> 
> <th>Key Benefit for the LLM</th> 
> </tr> 
> </thead> 
> <tbody> 
> <tr> 
> <td> 
> <strong>Flowchart</strong> (<code>graph TD</code>, <code>LR</code>, etc.)</td> 
> <td><strong>Temporal Flow, Process Logic, Decision Trees.</strong></td> 
> <td>Mapping the execution path of a complex function, describing a deployment pipeline, or outlining a user sign-up workflow.</td> 
> <td>Easily extracts <strong>process order</strong> and <strong>conditional logic</strong> (e.g., <em>if</em> statement paths) due to explicit node-to-node connections.</td> 
> </tr> 
> <tr> 
> <td> 
> <strong>Sequence Diagram</strong> (<code>sequenceDiagram</code>)</td> 
> <td><strong>Temporal Interaction, API Calls, Message Passing.</strong></td> 
> <td>Documenting the step-by-step interaction between microservices, a client-server authentication handshake, or the order of event emission.</td> 
> <td>Clearly defines the <strong>order of events</strong> over time and the <strong>participating entities</strong> (lifelines), which is excellent for tracing bugs.</td> 
> </tr> 
> <tr> 
> <td> 
> <strong>Class Diagram</strong> (<code>classDiagram</code>)</td> 
> <td><strong>Structure, Hierarchy, Relationships between Entities (OOP).</strong></td> 
> <td>Defining the structure of a new module, documenting inheritance, or showing the composition of objects within a codebase.</td> 
> <td>Unambiguously captures <strong>Object-Oriented Programming (OOP) concepts</strong> like inheritance, composition, and public/private methods.</td> 
> </tr> 
> <tr> 
> <td> 
> <strong>Entity Relationship Diagram (ERD)</strong> (<code>erDiagram</code>)</td> 
> <td><strong>Data Structure, Relationship between Data Models.</strong></td> 
> <td>Describing the schema of a database, defining the relationship between data entities (e.g., User, Order, Product) in a system.</td> 
> <td>Provides a structured map of <strong>data entities</strong> and their <strong>cardinality</strong> (one-to-many, etc.), essential for data-driven applications.</td> 
> </tr> 
> <tr> 
> <td> 
> <strong>State Diagram</strong> (<code>stateDiagram-v2</code>)</td> 
> <td><strong>State Transitions, Finite State Machines.</strong></td> 
> <td>Documenting the lifecycle of an object (e.g., Order status: Draft $\rightarrow$ Pending $\rightarrow$ Shipped $\rightarrow$ Delivered) or a UI component's behavior.</td> 
> <td>Offers clear logic on <strong>valid transitions</strong> and the <strong>events</strong> that trigger them, perfect for analyzing complex object lifecycles.</td> 
> </tr> 
> </tbody> 
> </table></div> 
>  
>  
>  
>  
> <p><strong>Note that this article is NOT the single response of a prompt that I gave to an LLM.</strong><br><br> 
> It was written by combining and structuring aspects  of various interactions I had with Gemini about<br> 
> using Mermaids in prompting, defining initial context in GEMINI.md, and also about guard railing an LLM in the MCP Service I am working on.<br><br> 
> <strong>My motivation to share this approach</strong> simply is because I think it is something genuine and new and might help you in your daily work with LLMs.</p>

---

## [4/10] Enterprise DAL Final: Automated User Auditing and Architectural Retrospective
**Source:** The Practical Developer | **Date:** 2025-12-02T13:00:00.000Z
**URL:** https://dev.to/gigaherz/enterprise-dal-final-automated-user-auditing-and-architectural-retrospective-3jpc
**Reasoning:** Discusses data access layers, not directly relevant to context engineering or code retrieval.
**Authors:** GigAHerZ

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2ealhjh6a6j2d9zpbhwt.webp" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>This post marks the end of our <a href="https://byteaether.github.io/series/enterprise-dal/">series</a> on constructing an enterprise-grade Data Access Layer (DAL) in C# with Linq2Db. Our goal was to create a DAL that is secure and resilient by automatically handling crucial cross-cutting concerns.</p>  
>   
> <h2>  
>     
>     
>   Implementing Automated User Auditing  
> </h2>  
>   
> <p>To achieve compliance and enhance debugging, we implement <strong>Automated User Auditing</strong>. This feature automatically populates <code>CreatedByUserId</code> and <code>ModifiedByUserId</code> fields with the current user's unique identifier (<code>Ulid</code>).</p>  
>   
> <p>The implementation is integrated into our existing interface-driven architecture:</p>  
>   
> <ol>  
> <li> <strong>Contracts:</strong> New interfaces (<code>IUserCreatable</code>, <code>IUserModifiable</code>) define the required properties.</li>  
> <li> <strong>Automation:</strong> Our custom scaffolding ensures that any entity table with <code>created_by_user_id</code>/<code>modified_by_user_id</code> columns automatically implements these interfaces.</li>  
> <li> <strong>Injection Logic:</strong> We update our <code>CrudExtensions</code> to inspect the entity on create/modify operations. If the entity implements the auditing interface, the current <code>UserId</code> from the request-scoped context is injected before the database operation executes.</li>  
> </ol>  
>   
> <p><strong>Crucial detail for Architects:</strong> We detail a necessary, advanced technique using <code>(LinqToDB.Internal.Linq).Internals.GetDataContext(source)</code> to correctly inject the <code>ModifiedByUserId</code> user ID during <strong>fluent batch UPDATE</strong> operations (<code>IUpdatable&lt;T&gt;</code>). This ensures auditing integrity across all modification types.</p>  
>   
> <h2>  
>     
>     
>   Verdict: 100% Goal Achievement  
> </h2>  
>   
> <p>The final architecture successfully automated every requirement initially set for a perfect enterprise DAL:</p>  
>   
> <ul>  
> <li>  
> <strong>Ubiquitous Filtering:</strong> Soft-Delete, Multi-Tenancy, and Row-Level Security are enforced by a highly composable global query filter system, transparently generating the correct SQL joins and <code>WHERE</code> clauses for <em>all</em> read operations.</li>  
> <li>  
> <strong>Projected Security:</strong> The use of Linq2Db’s projection capabilities allows for complex security and tenancy rules to be resolved contextually from related entities, a key differentiator.</li>  
> <li>  
> <strong>Auditing:</strong> Timestamp and User auditing are handled automatically on creation and modification.</li>  
> </ul>  
>   
> <p>This robust DAL abstracts away security and compliance boilerplate, letting developers focus purely on business logic.</p>  
>   
>   
>   
>   
> <p><strong>For the full implementation details, including source code and an in-depth explanation of the global query filter system,</strong> <a href="https://byteaether.github.io/2025/building-an-enterprise-data-access-layer-automated-user-auditing-and-series-wrap-up/">read the complete article on my blog</a>.</p>

---

## [4/10] Pixlore — A Web-to-Figma Engine That Bridges UI, Code, and Product Workflows
**Source:** The Practical Developer | **Date:** 2025-12-02T12:48:53.000Z
**URL:** https://dev.to/doong_yee/pixlore-a-web-to-figma-engine-that-bridges-ui-code-and-product-workflows-5a6h
**Reasoning:** Focuses on UI and design workflows, not directly relevant to code intelligence or context engineering.
**Authors:** Doong Yee

**Content/Abstract:**
> <p>As front-end engineers and product teams, we often need to understand, audit, or replicate UI patterns from existing web interfaces — and doing that manually in Figma can feel… inefficient.</p> 
>  
> <p>Rebuilding layout grids, reconstructing spacing, extracting semantics from CSS, mapping design tokens — none of this is <em>hard</em>, but it’s definitely <strong>repetitive</strong>.</p> 
>  
> <p><a href="https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202">Pixlore</a> is a Figma plugin we’ve been working on that aims to eliminate this repetitive layer.<br> 
> It converts <strong>live webpages or HTML</strong> into a clean, editable Figma structure that mirrors how modern front-end systems are built.</p> 
>  
> <p>Here’s why it’s become part of many dev/design workflows 👇</p> 
>  
> <h3> 
>    
>    
>   1. Structurally Accurate Reverse Engineering 
> </h3> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fl0810poulc6y5t9h4c0p.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fl0810poulc6y5t9h4c0p.png" alt="" width="800" height="450"></a></p> 
>  
> <p><a href="https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202">Pixlore</a> doesn’t just screenshot, it <strong>parses DOM → styles → layout → constraints</strong>, then reconstructs a Figma tree that matches:</p> 
>  
> <ul> 
> <li>Auto Layout structure</li> 
> <li>Flexbox / CSS grid relationships</li> 
> <li>Component grouping + layer hierarchy</li> 
> <li>Typography scale + color tokens</li> 
> <li>Spacing rules inferred from CSS</li> 
> </ul> 
>  
> <p>Developers like this because the resulting Figma file is:</p> 
>  
> <ul> 
> <li>predictable</li> 
> <li>debuggable</li> 
> <li>clean enough to hand over</li> 
> <li>consistent with how UI code is actually structured</li> 
> </ul> 
>  
> <p>It’s basically a <strong>DOM → Figma AST translator</strong>.</p> 
>  
> <h3> 
>    
>    
>   2. AI Editing: Modify Layouts Using Natural Language 
> </h3> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fs3n7kf65tlgyf1jy1qb7.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fs3n7kf65tlgyf1jy1qb7.png" alt="" width="800" height="450"></a></p> 
>  
> <p>This is where dev–design collaboration gets interesting.</p> 
>  
> <p><a href="https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202">Pixlore</a> includes a conversational AI that can modify generated layouts:</p> 
>  
> <ul> 
> <li>“Make the grid 4 columns with 24px gutter.”</li> 
> <li>“Replace all images with gray placeholders.”</li> 
> <li>“Normalize spacing to an 8px scale.”</li> 
> <li>“Apply a neutral color theme.”</li> 
> <li>“Convert this section into reusable components.”</li> 
> </ul> 
>  
> <p>Think of it as using ChatGPT &amp; Gemini3 — <em>but embedded directly inside Figma</em> and acting only on selected nodes.</p> 
>  
> <p>It reduces the designer’s manual work and gives developers more consistent layouts to work with.</p> 
>  
> <h3> 
>    
>    
>   3. Extension Capture for Dynamic / Authenticated Pages 
> </h3> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg7pfr76udh3hiuagiavk.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fg7pfr76udh3hiuagiavk.png" alt="" width="800" height="450"></a></p> 
>  
> <p>Developers often deal with:</p> 
>  
> <ul> 
> <li>Internal dashboards</li> 
> <li>Login-protected apps</li> 
> <li>Infinite scroll</li> 
> <li>Hydrated UI states</li> 
> </ul> 
>  
> <p><a href="https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202">Pixlore</a>’s browser extension lets you capture the rendered HTML/DOM from those states and port them into Figma.</p> 
>  
> <p>Great for:</p> 
>  
> <ul> 
> <li>UX teardown</li> 
> <li>Competitor audits</li> 
> <li>Rebuilding internal tools</li> 
> <li>Mapping legacy UI before refactoring</li> 
> </ul> 
>  
> <h3> 
>    
>    
>   4. Interaction Pattern Extraction 
> </h3> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxnfwegtkv8s268mjmmnn.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fxnfwegtkv8s268mjmmnn.png" alt="" width="800" height="450"></a></p> 
>  
> <p><a href="https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202">Pixlore</a> can annotate the exported design with detected interaction logic:</p> 
>  
> <ul> 
> <li>hover / active states</li> 
> <li>expandable sections</li> 
> <li>menu navigation flow</li> 
> <li>button behavior</li> 
> <li>scroll-triggered elements</li> 
> </ul> 
>  
> <p>This helps dev teams communicate functional requirements without manually documenting everything.</p> 
>  
>  
>  
>  
> <h3> 
>    
>    
>   Why Dev Teams Use It 
> </h3> 
>  
> <p>In real workflows, devs report using <a href="https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202">Pixlore</a> for:</p> 
>  
> <h4> 
>    
>    
>   <strong>UI audits</strong> 
> </h4> 
>  
> <p>Quickly understand the structure of a new interface before implementing a component library or refactor.</p> 
>  
> <h4> 
>    
>    
>   <strong>Design–Dev alignment</strong> 
> </h4> 
>  
> <p>Designers start with accurate structural drafts → developers get clearer specs.</p> 
>  
> <h4> 
>    
>    
>   <strong>Rapid prototyping</strong> 
> </h4> 
>  
> <p>Build a prototype or redesign using a real-world reference instead of starting from a blank frame.</p> 
>  
> <h4> 
>    
>    
>   <strong>Onboarding</strong> 
> </h4> 
>  
> <p>New teammates can learn product UI principles by reverse-engineering existing screens.</p> 
>  
>  
>  
>  
> <h3> 
>    
>    
>   Simple Pricing, Fast ROI 
> </h3> 
>  
> <p>Compared to other tools doing similar web-to-Figma extraction, <a href="https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202">Pixlore</a> aims to be:</p> 
>  
> <ul> 
> <li>⚡️ <strong>Faster</strong> 
> </li> 
> <li>📐 <strong>More accurate</strong> 
> </li> 
> <li>🪙 <strong>Significantly more affordable</strong> 
> </li> 
> </ul> 
>  
> <p>Teams use the free tier to validate the workflow, then upgrade after confirming it reduces manual rebuild time.</p> 
>  
>  
>  
>  
> <h3> 
>    
>    
>   🚀 <a href="https://pixlore.newportai.com/despilot-server/operation/trace/promotion/vf-ydd-20251202">Try It</a> (Free) 
> </h3> 
>  
> <p>If your dev/design workflow involves analyzing or replicating existing interfaces, this might save you hours.</p> 
>  
> <p>I’m also collecting feedback from developers:</p> 
>  
> <ul> 
> <li>Does the reconstructed layout match your expectations?</li> 
> <li>What would make it more useful in your workflow?</li> 
> <li>Should we export back to code in future versions?</li> 
> </ul> 
>  
> <p>Happy to chat in comments or DMs, always looking to improve the tool for real-world engineering use cases.</p>

---

## [4/10] Getting Started with Nop: How to Creatively Extend GraphQL
**Source:** The Practical Developer | **Date:** 2025-12-02T12:17:12.000Z
**URL:** https://dev.to/canonical/getting-started-with-nop-how-to-creatively-extend-graphql-5e7c
**Reasoning:** The article is about extending GraphQL, which is not directly related to our primary or secondary interests.
**Authors:** canonical

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fua8xqp8153db4qk3d1ul.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>The Nop platform does not use common GraphQL open-source libraries such as <code>graphql-java</code>; instead, it implements the NopGraphQL engine from scratch. The NopGraphQL engine introduces many novel implementation approaches that broaden GraphQL’s scope of application and enhance its practicality.</p>  
>   
> <p>For detailed documentation, see <a href="https://gitee.com/canonical-entropy/nop-entropy/blob/master/docs-en/dev-guide/graphql/index.md">graphql/index.md</a></p>  
>   
> <h2>  
>     
>     
>   I. Simplifying GraphQL Queries with Fragment Definitions  
> </h2>  
>   
> <p>GraphQL requires the frontend to specify the fields to be returned, which can be cumbersome when there are many fields. In such cases, you can use the Fragment feature of the GraphQL language to define common field sets and reference these fragments in queries to simplify them.</p>  
>   
> <h3>  
>     
>     
>   1.1 Add selection definitions in XMeta, prefixed with <code>F_</code>  
> </h3>  
>   
> <p>Each backend service object in the Nop platform has an associated XMeta metadata model file, where you can augment GraphQL types with additional metadata.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>&lt;meta&gt;</span>  
>   <span>&lt;selections&gt;</span>  
>     <span>&lt;selection</span> <span>id=</span><span>"F_defaults"</span><span>&gt;</span>  
>       userId, userName, status, relatedRoleList{ roleName}  
>     <span>&lt;/selection&gt;</span>  
>   <span>&lt;/selections&gt;</span>  
> <span>&lt;/meta&gt;</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <ul>  
> <li>By convention, only fragment definitions prefixed with <code>F_</code> are accessible from the frontend. Selections have other uses as well.</li>  
> <li>If <code>F_defaults</code> is not configured, it will be automatically inferred based on all non-lazy fields of the GraphQL type. If explicitly specified, the specified content is used.</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   1.2 Reference fragments in frontend queries  
> </h3>  
>   
> <p>When invoking backend services via GraphQL, you can use fragments:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>query</span><span>{</span><span>  
>    </span><span>NopAuthUser__findList</span><span>{</span><span>  
>      </span><span>...</span><span>F_defaults</span><span>,</span><span> </span><span>groupMappings</span><span>{...</span><span>F_defaults</span><span>}</span><span>  
>    </span><span>}</span><span>  
> </span><span>}</span><span>  
> </span></code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Alternatively, when calling backend services via REST, use the <code>@selection</code> parameter to reference fragments:<br>  
> </p>  
>   
> <div>  
> <pre><code>/r/NopAuthUser__findList?@selection=...F_defaults,groupMappings  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <ul>  
> <li>When using REST, if <code>@selection</code> is not provided, it is equivalent to returning <code>F_defaults</code>.</li>  
> </ul>  
>   
> <p><strong>Under REST, if the selection only specifies object-level fields, it will be automatically expanded to nested levels.</strong></p>  
>   
> <h2>  
>     
>     
>   II. Simplify Tree-structured Queries with the <code>@TreeChildren</code> Directive  
> </h2>  
>   
> <p>For retrieving tree structures such as organization trees or menu trees, NopGraphQL provides an extended syntax via the directive mechanism to directly express recursive data fetching, for example:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>query</span><span> </span><span>{</span><span>  
>     </span><span>NopAuthDept_findList</span><span>{</span><span>  
>         </span><span>value</span><span>:</span><span> </span><span>id</span><span>  
>         </span><span>label</span><span>:</span><span> </span><span>displayName</span><span>  
>         </span><span>children</span><span> </span><span>@TreeChildren</span><span>(</span><span>max</span><span>:</span><span>5</span><span>)</span><span>  
>     </span><span>}</span><span>  
> </span><span>}</span><span>  
> </span></code></pre>  
>   
> </div>  
>   
>   
>   
> <ul>  
> <li>  
> <code>@TreeChildren(max:5)</code> indicates that, following the current level’s structure, up to 5 levels will be nested.</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   III. Map Type  
> </h2>  
>   
> <p>GraphQL is a strongly typed framework that requires all data to have explicit type definitions, which can be inconvenient in certain dynamic scenarios. For instance, sometimes you may need to return an extensible collection to the frontend.</p>  
>   
> <p>NopGraphQL introduces a special scalar type: Map, which can be used to describe dynamic data structures. For example:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>type</span><span> </span><span>QueryBean</span><span>{</span><span>  
>     </span><span>filter</span><span>:</span><span> </span><span>Map</span><span>  
>     </span><span>orderBy</span><span>:</span><span> </span><span>[</span><span>OrderFieldBean</span><span>]</span><span>  
> </span><span>}</span><span>  
> </span></code></pre>  
>   
> </div>  
>   
>   
>   
> <h2>  
>     
>     
>   IV. XMeta Metadata Model  
> </h2>  
>   
> <p>The XMeta metadata model enables many features via configuration.</p>  
>   
> <h3>  
>     
>     
>   4.1 Map to existing properties via mapToProp  
> </h3>  
>   
>   
>   
> <div>  
> <pre><code><span>&lt;prop</span> <span>name=</span><span>"a"</span> <span>mapToProp=</span><span>"b.a"</span><span>&gt;</span>  
> <span>&lt;/prop&gt;</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <ul>  
> <li>The mapToProp attribute can specify an alias for an existing property. When the frontend accesses property a, it actually retrieves the a property on the associated object b.</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   4.2 Specify computed expressions directly via getter  
> </h3>  
>   
> <p>In NopGraphQL, you can introduce dynamically computed fields in a BizModel service class via the <code>@BizLoader</code> annotation.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>@BizModel</span><span>(</span><span>"LoginApi"</span><span>)</span>  
> <span>public</span> <span>class</span> <span>LoginApiBizModelDelta</span> <span>{</span>  
>     <span>@BizLoader</span><span>(</span><span>autoCreateField</span> <span>=</span> <span>true</span><span>,</span> <span>forType</span> <span>=</span> <span>LoginResult</span><span>.</span><span>class</span><span>)</span>  
>     <span>@LazyLoad</span>  
>     <span>public</span> <span>String</span> <span>location</span><span>(</span><span>@ContextSource</span> <span>LoginResult</span> <span>result</span><span>,</span>  
>                            <span>IServiceContext</span> <span>context</span><span>)</span> <span>{</span>  
>         <span>return</span> <span>"loc:"</span> <span>+</span> <span>result</span><span>.</span><span>getUserInfo</span><span>().</span><span>getUserId</span><span>();</span>  
>     <span>}</span>  
> <span>}</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>For lightweight computed expressions, defining service functions may be overly complex; in such cases, you can define them directly in XMeta via a getter expression:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>&lt;prop</span> <span>name=</span><span>"myValue"</span><span>&gt;</span>  
>   <span>&lt;getter&gt;</span>  
>     return entity.name + 'Ext'  
>   <span>&lt;/getter&gt;</span>  
> <span>&lt;/prop&gt;</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <h3>  
>     
>     
>   4.3 Field-level access control  
> </h3>  
>   
> <p>In the xmeta file, you can specify <code>auth</code> settings for a <code>prop</code>:<br>  
> </p>  
>   
> <div>  
> <pre><code>  
> <span>&lt;prop</span> <span>name=</span><span>"xx"</span><span>&gt;</span>  
>     <span>&lt;auth</span> <span>permissions=</span><span>"NopAuthUser:query"</span> <span>roles=</span><span>"admin"</span> <span>for=</span><span>"read"</span><span>/&gt;</span>  
>     <span>&lt;auth</span> <span>permissions=</span><span>"NopAuthUser:mutation"</span> <span>roles=</span><span>"hr"</span> <span>for=</span><span>"write"</span><span>/&gt;</span>  
> <span>&lt;/prop&gt;</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <ul>  
> <li>This configuration enables read/write access control at the field level. <code>for="read"</code> controls read access to the field, <code>for="write"</code> controls write access, and <code>for="all"</code> allows both read and write.</li>  
> <li>Before performing any actual actions, the NopGraphQL engine checks access permissions for each field in the selection set, ensuring you don't end up executing business operations only to discover that certain result fields are inaccessible.</li>  
> <li>For data permissions and filter criteria for related sub-tables, see <a href="https://dev.to/canonical/nop-getting-started-how-to-implement-complex-queries-l98">4-complex-query.md</a>.</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   4.4 Automatically generate data dictionary text fields  
> </h3>  
>   
> <p>In business development, a common requirement is to translate backend business field values into display text according to a data dictionary configuration. In the Nop platform, during the loading phase, XMeta model files use metaprogramming to dynamically determine whether a data dictionary is configured.<br>  
> If so, a dictionary translation field is automatically generated.<br>  
> </p>  
>   
> <div>  
> <pre><code>  
> <span>&lt;prop</span> <span>name=</span><span>"status"</span><span>&gt;</span>  
>     <span>&lt;schema</span> <span>type=</span><span>"Integer"</span> <span>dict=</span><span>"auth/user-status"</span><span>/&gt;</span>  
> <span>&lt;/prop&gt;</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>After metaprogramming transformation, the following field definitions are generated:<br>  
> </p>  
>   
> <div>  
> <pre><code>  
> <span>&lt;prop</span> <span>name=</span><span>"status"</span> <span>graphql:labelProp=</span><span>"status_label"</span><span>&gt;</span>  
>     <span>&lt;schema</span> <span>type=</span><span>"Integer"</span> <span>dict=</span><span>"auth/user-status"</span><span>/&gt;</span>  
> <span>&lt;/prop&gt;</span>  
> <span>&lt;prop</span> <span>name=</span><span>"status_label"</span> <span>internal=</span><span>"true"</span> <span>graphql:dictName=</span><span>"auth/user-status"</span>  
>       <span>graphql:dictValueProp=</span><span>"status"</span><span>&gt;</span>  
>     <span>&lt;schema</span> <span>type=</span><span>"String"</span><span>/&gt;</span>  
> <span>&lt;/prop&gt;</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <h3>  
>     
>     
>   4.5 Masked display  
> </h3>  
>   
> <p>For security reasons, some sensitive user information must not be printed to log files; when returned to the frontend for display, it also needs to be masked—only showing the first few and the last few characters,<br>  
> such as credit card numbers, user phone numbers, etc.</p>  
>   
> <p>You can specify a masking display pattern via <code>ui:maskPattern</code>, and when GraphQL returns field values it will automatically apply this pattern.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>&lt;prop</span> <span>name=</span><span>"email"</span> <span>ui:maskPattern=</span><span>"3*4"</span><span>&gt;</span>  
>   
> <span>&lt;/prop&gt;</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <ul>  
> <li>  
> <code>ui:maskPattern="3*4"</code> means keeping the first 3 and last 4 characters, with the rest replaced by <code>*</code>.</li>  
> </ul>

---

## [4/10] รัน Typhoon 2.5 บน Colab ฟรี: จาก 30B (ไม่ไหว) สู่ 4B "Sweet Spot"
**Source:** The Practical Developer | **Date:** 2025-12-02T12:12:47.000Z
**URL:** https://dev.to/ubinix_warun/ran-typhoon-25-bn-colab-frii-cchaak-30b-aimaihw-suu-4b-sweet-spot-2jdp
**Reasoning:** The article discusses running models on Colab, which is not directly related to our primary interests.
**Authors:** Warun C. ⚡

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftcser2z7dvqfaxrryx18.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>สวัสดีครับ! นี่คือบทความสรุปการเดินทางของเราในการพยายามรันโมเดล Typhoon 2.5 (ทั้ง 30B และ 4B) บน Google Colab Free Tier ครับ เราได้ลองผิดลองถูกมาหลายวิธี และนี่คือบทเรียนทั้งหมดที่เราพบ ตั้งแต่ความล้มเหลวไปจนถึง Config ที่ดีที่สุดครับ</p>  
>   
> <p><a href="https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb0e8bfbf2b81&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fubinix-warun&amp;collection=UBINIX+WARUN&amp;collectionId=49c3a90b9f1f&amp;newsletterV3=UBINIX+WARUN&amp;newsletterV3Id=b0e8bfbf2b81">อ่านฉบับเต็ม...</a></p>  
>   
> <h2>  
>     
>     
>   บทเรียนที่ 1: Typhoon 30B "ใหญ่เกินไป" สำหรับ Colab ฟรี  
> </h2>  
>   
> <p>ความฝันของเราคือการรันโมเดลเรือธง 30B แต่ความจริงก็คือ:</p>  
>   
> <ul>  
> <li><p>Ollama (บน T4 GPU): รอดแบบเฉียดฉิว! ใช้ VRAM 14.3 GB จาก 15 GB ที่มีให้ นี่คือการรัน "จนเต็มขีดจำกัด" ของ T4</p></li>  
> <li>  
> <p>Transformers (เขียนโค้ด Python): ล่มโดยสิ้นเชิง!</p>  
>   
> <ul>  
> <li>บน T4 (GPU): เจอปัญหา "Disk is almost full" (Disk 112GB ไม่พอโหลดโมเดล 60-70GB)</li>  
> <li>บน TPU (v5e-1): แก้ปัญหา Disk ได้ แต่เจอ "Time Out" (แค่โหลดโมเดลก็ 40 นาที จนค้าง)</li>  
> </ul>  
>   
>   
> </li>  
>   
> </ul>  
>   
> <p><a href="https://medium.com/super-ai-agents/%E0%B9%80%E0%B8%88%E0%B8%B2%E0%B8%B0%E0%B8%A5%E0%B8%B6%E0%B8%81-typhoon-2-5-part-1-%E0%B8%97%E0%B8%94%E0%B8%A5%E0%B8%AD%E0%B8%87%E0%B8%A3%E0%B8%B1%E0%B8%99-30b-%E0%B8%9A%E0%B8%99-colab-%E0%B8%9F%E0%B8%A3%E0%B8%B5-%E0%B8%A8%E0%B8%B6%E0%B8%81%E0%B8%9B%E0%B8%B0%E0%B8%97%E0%B8%B0-runtime-ollama-02402ff24871">สรุป Part 1: ถ้าอยากลอง 30B บน Colab ฟรี, Ollama คือทางเดียว (แต่ก็เสี่ยงมาก) ส่วน transformers นั้น "เป็นไปไม่ได้"</a></p>  
>   
> <h2>  
>     
>     
>   บทเรียนที่ 2: Typhoon 4B "คุณภาพ" คือคำตอบ เมื่อ 30B ไม่รอด เราจึงหันมาที่ "น้องเล็ก" 4B แต่คำถามคือ "จำเป็นต้องใช้ GPU T4 ไหม?"  
> </h2>  
>   
> <ul>  
> <li>Ollama (บน CPU Runtime): รันได้! ใช้ System RAM ~3.5GB และ CPU 100%</li>  
> </ul>  
>   
> <blockquote>  
> <p>...แต่ (จุดเปลี่ยน): เมื่อทดสอบ เราพบว่า "คุณภาพคำตอบ (Quality) ดร็อปอย่างชัดเจน" คำตอบมักจะสั้น, เหตุผลเพี้ยน, หรือไม่ดีเท่าที่ควร</p>  
> </blockquote>  
>   
> <p><a href="https://medium.com/super-ai-agents/%E0%B9%80%E0%B8%88%E0%B8%B2%E0%B8%B0%E0%B8%A5%E0%B8%B6%E0%B8%81-typhoon-2-5-part-2-%E0%B8%81%E0%B8%A5%E0%B8%B1%E0%B8%9A%E0%B8%AA%E0%B8%B9%E0%B9%88%E0%B8%9E%E0%B8%B7%E0%B9%89%E0%B8%99%E0%B8%90%E0%B8%B2%E0%B8%99-%E0%B8%A3%E0%B8%B1%E0%B8%99-4b-%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-4-bit-quantization-a6e0bb262cce">สรุป Part 2: การรันบน CPU ทำได้แค่ "ให้ติด" แต่ถ้าคุณต้องการ "คุณภาพ" ที่แท้จริง... คุณต้องใช้ T4 GPU และนี่คือวิธีที่เราทำ<br>  
> </a></p>  
> <h2>  
>     
>     
>   3. "พระเอก" ของงาน: 4-bit Quantization (โค้ด Transformers)  
> </h2>  
>   
> <p>นี่คือวิธีรัน 4B บน T4 GPU เพื่อให้ได้ "คุณภาพสูงสุด" โดยใช้ VRAM น้อยที่สุดครับ</p>  
>   
> <p>Quantization คืออะไร? คือการ "บีบอัด" โมเดล (เหมือนบีบไฟล์ RAW เป็น .JPG) เราใช้ 4-bit Quantization เพื่อลดขนาดโมเดลใน VRAM ลง 4 เท่า! (จาก 16-bit)</p>  
>   
> <p>นี่คือโค้ดหลักที่เราใช้:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>import</span> <span>torch</span>  
> <span>from</span> <span>transformers</span> <span>import</span> <span>AutoTokenizer</span><span>,</span> <span>AutoModelForCausalLM</span><span>,</span> <span>BitsAndBytesConfig</span>  
>   
> <span># 1. เลือกโมเดล 4B  
> </span><span>model_id</span> <span>=</span> <span>"</span><span>scb10x/typhoon2.5-qwen3-4b</span><span>"</span>  
>   
> <span># 2. นี่คือหัวใจสำคัญ! (เราจะเทียบ NF4 vs FP4)  
> </span><span>quantization_config</span> <span>=</span> <span>BitsAndBytesConfig</span><span>(</span>  
>     <span>load_in_4bit</span><span>=</span><span>True</span><span>,</span>  
>     <span>bnb_4bit_quant_type</span><span>=</span><span>"</span><span>nf4</span><span>"</span><span>,</span> <span># หรือ "fp4"  
> </span>    <span>bnb_4bit_compute_dtype</span><span>=</span><span>torch</span><span>.</span><span>bfloat16</span>  
> <span>)</span>  
>   
> <span># 3. โหลดโมเดล  
> </span><span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_id</span><span>)</span>  
> <span>model</span> <span>=</span> <span>AutoModelForCausalLM</span><span>.</span><span>from_pretrained</span><span>(</span>  
>     <span>model_id</span><span>,</span>  
>     <span>quantization_config</span><span>=</span><span>quantization_config</span><span>,</span>  
>     <span>torch_dtype</span><span>=</span><span>torch</span><span>.</span><span>bfloat16</span><span>,</span>  
>     <span>device_map</span><span>=</span><span>"</span><span>auto</span><span>"</span>  
> <span>)</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>วิธีนี้ ทำให้เราใช้ VRAM ไปเพียง ~4.0 GB (จากการทดลองใน Part 2) และได้คุณภาพคำตอบที่ดีเยี่ยม!</p>  
>   
> <h2>  
>     
>     
>   4. บทเรียนที่ 3: Benchmark 8-bit vs 4-bit... ใครชนะ?  
> </h2>  
>   
> <p>เรารู้ว่า 4-bit ดี แต่แบบไหนดีที่สุด? เราจึงทดสอบ 3 Configs บน T4 GPU (จากผลทดลองจริง):</p>  
>   
> <h2>  
>     
>     
>   🏆 บทสรุป (The Final Verdict): Config ที่ดีที่สุด  
> </h2>  
>   
> <p>4-bit Quantization ชนะ 8-bit ขาดลอย!</p>  
>   
> <p><a href="https://medium.com/super-ai-agents/%E0%B9%80%E0%B8%88%E0%B8%B2%E0%B8%B0%E0%B8%A5%E0%B8%B6%E0%B8%81-typhoon-2-5-part-3-benchmark-%E0%B9%80%E0%B8%97%E0%B8%B5%E0%B8%A2%E0%B8%9A-8-bit-vs-4-bit-nf4-vs-fp4-f33e3f61417d">สรุป Part 3: “Benchmark” เทียบ 8-bit vs 4-bit (NF4 vs FP4)</a></p>  
>   
> <ul>  
> <li>เร็วกว่า 2.5 เท่า! (12 Tok/s vs 4.6 Tok/s)</li>  
> <li>ประหยัด VRAM กว่า 35% (2.71 GB vs 4.19 GB)</li>  
> </ul>  
>   
> <p>ศึกชิงที่ 1: NF4 vs FP4</p>  
>   
> <ul>  
> <li>VRAM: ใช้เท่ากัน (2.71 GB)</li>  
> <li>Speed: FP4 เร็วกว่า NF4 เล็กน้อย (ประมาณ 3-4%)</li>  
> <li>Quality: NF4 (Normal Float) ถูกออกแบบมาเพื่อ "รักษาคุณภาพ" ได้ดีกว่า FP4 (Float Point) ที่เน้น "ความเร็ว"</li>  
> </ul>  
>   
> <p>คำแนะนำสุดท้าย: สำหรับ Google Colab T4:</p>  
>   
> <blockquote>  
> <p>4-bit NF4 (bnb_4bit_quant_type="nf4") คือ "จุดสมดุล" (The Sweet Spot) ที่ดีที่สุด</p>  
>   
> <p>คุณจะได้โมเดลที่ ประหยัด VRAM ที่สุด (2.71 GB), เร็วมาก (11.68 Tok/s), และ รักษาคุณภาพคำตอบ ไว้ได้ใกล้เคียงต้นฉบับที่สุดครับ!</p>  
> </blockquote>

---

## [4/10] Mock Elements: The Unsung Heroes of UI Design
**Source:** The Practical Developer | **Date:** 2025-12-02T12:05:15.000Z
**URL:** https://dev.to/zopdev/mock-elements-the-unsung-heroes-of-ui-design-ocd
**Reasoning:** The article is about UI design, which is not directly related to our primary interests.
**Authors:** Rocktim M

**Content/Abstract:**
> <p>When crafting user interfaces, designers and developers often need to present visuals, forms, and scripts before real data is available. Mock elements, placeholders and fictional samples, make this possible with both clarity and tradition. Below are some of the most famous placeholders and their backstories.</p> 
>  
> <h2> 
>    
>    
>   The Role of Mock Elements in UI Design 
> </h2> 
>  
> <p>Mock elements act as stand-ins for actual content, helping designers and clients focus on visual structure, usability, and layout, undistracted by real-world data or identities. They keep prototypes relevant, flexible, and safe for sharing and testing while enabling early feedback and iteration.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Lorem Ipsum 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F1tpcwp3o79ahuesfcc1e.jpeg"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F1tpcwp3o79ahuesfcc1e.jpeg" alt="Picture depicting Lorem Ipsum" width="605" height="958"></a></p> 
>  
> <p><strong>History:</strong> The “lorem ipsum” text originated from a scrambled section of Cicero’s 1st-century Latin treatise “de finibus bonorum et malorum.” Letraset further popularized it in the 1960s for type samples, and desktop publishing in the 1980s cemented its role.<br><br> 
> <strong>Significance &amp; Usage:</strong> Designers use lorem ipsum to fill text fields and paragraphs in UI mockups, allowing everyone to focus strictly on layout and typographic choices instead of content. Its meaningless Latin roots prevent storyline distraction—an effect called “greeking”—and help judge graphical hierarchy.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Alice and Bob 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Feyzo7wyb36egild22upc.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Feyzo7wyb36egild22upc.png" alt="Picture depicting Alice &amp; Bob" width="800" height="450"></a></p> 
>  
> <p><strong>History:</strong> “Alice and Bob” became canonical in cryptography with the seminal 1978 RSA paper. Previously, researchers used impersonal A and B, but the friendlier names made technical papers more accessible and memorable.<br><br> 
> <strong>Significance &amp; Usage:</strong> In UI scenarios, Alice and Bob typically represent communicating users, especially in chat apps, email forms, and privacy demos. The names are proxies for any generic pair, lending emotional resonance and clarity without revealing real identities.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   John Doe 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9olszuzlla5ng6huaf5x.jpeg"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9olszuzlla5ng6huaf5x.jpeg" alt="Picture depicting John Doe" width="800" height="466"></a></p> 
>  
> <p><strong>History:</strong> “John Doe” traces back to English common law and the 14th century, where it was used in fictional legal actions concerning land ownership. Over time, it became the go-to name for unknown, average, or anonymous male persons.<br><br> 
> <strong>Significance &amp; Usage:</strong> In web forms, legal demos, and healthcare mock interfaces, “John Doe” identifies hypothetical users without associating with actual people. It’s also vital in survey prototypes, error screens, and contact lists where anonymity is essential.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Acme Corp 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fur74bh19im1nrnmdf4q2.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fur74bh19im1nrnmdf4q2.png" alt="Picture depicting Acme Corp" width="200" height="200"></a></p> 
>  
> <p><strong>History:</strong> While some real companies bear the name, “Acme Corporation” gained fame as a fictional firm supplying comedic gadgets to Wile E. Coyote in Looney Tunes. Over decades, it’s become a generic brand for UI and legal mockups.<br><br> 
> <strong>Significance &amp; Usage:</strong> In interface design, Acme Corp is the archetypal placeholder company for demo dashboards, example invoices, and onboarding flows. It’s safely neutral and universally understood as a stand-in, avoiding legal risk or confusion.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   123 Main Street, Anytown, USA 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2ek930b9z03cq2hflmlq.jpeg"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2ek930b9z03cq2hflmlq.jpeg" alt="Picture depicting 123 Main Street, Anytown, USA" width="800" height="311"></a></p> 
>  
> <p><strong>History:</strong> By convention, “123 Main Street, Anytown, USA” is the default fake address for software demos, documentation, and forms. Its universal, harmless wording means everyone recognizes it as not real, helping avoid data privacy issues.<br><br> 
> <strong>Significance &amp; Usage:</strong> Sample address fields, test registrations, and validation scripts often use this structure to illustrate required formats or fill space during development.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   555-1234 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fezz9oxxklmagt6wvg76p.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fezz9oxxklmagt6wvg76p.png" alt="Picture depicting 555-1234" width="800" height="448"></a></p> 
>  
> <p><strong>History:</strong> The “555” prefix for phone numbers got standardized in the USA in the 1950s mostly for directory assistance, but was popularized as a movie/TV placeholder since real numbers could provoke prank calling. By the 1990s, the North American Numbering Plan designated 555-x prefixes for fiction, minimizing accidental real-world connections.<br><br> 
> <strong>Significance &amp; Usage:</strong> Designers use 555-1234 for form validation, contact demos, and UI prototypes to guarantee sample numbers are safe and never misroute calls or texts.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   <a href="mailto:example@example.com">example@example.com</a> 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fo3l4zzt7gucrs9fy9njd.jpeg"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fo3l4zzt7gucrs9fy9njd.jpeg" alt="Picture depicting exaple@example.com" width="672" height="254"></a></p> 
>  
> <p><strong>History:</strong> The “example.com” domain is reserved for illustrative and instructional purpose only, following IETF guidelines. Likewise, “<a href="mailto:example@example.com">example@example.com</a>” emerged as its logical and safe default for email placeholders.<br><br> 
> <strong>Significance &amp; Usage:</strong> By using <a href="mailto:example@example.com">example@example.com</a> in mockups and UI forms, developers avoid accidental emails to real users, ensure demo code is generic, and sidestep confidential data leaks.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Conclusion 
> </h2> 
>  
> <p>Mock elements are fundamental to safe, effective, and rapid UI design. They educate, protect, and streamline the creative process—and their legacy is deeply woven into every modern app, site, and system.</p> 
>  
> <p>👉 <a href="https://zop.dev/zopnight">Try ZopNight today</a><br><br> 
> 👉 <a href="https://bookings.zop.dev/#/discover-zopdev">Book a demo</a></p>

---

## [4/10] AI Regulation News Today: Global & U.S. Policy Updates
**Source:** The Practical Developer | **Date:** 2025-12-02T12:00:49.000Z
**URL:** https://dev.to/techdecodedly/ai-regulation-news-today-global-us-policy-updates-3dcf
**Reasoning:** The article is about AI regulation, which is not directly related to our primary or secondary interests.
**Authors:** Techdecodedly

**Content/Abstract:**
> <p>AI Regulation News Today shows that around the world, AI regulation is rapidly evolving as governments race to set standards for safe and responsible AI development. A 2025 survey notes that “at least 69 countries” (including the EU) have proposed or adopted AI laws and initiatives. The European Union leads with the landmark EU Artificial Intelligence Act (Regulation 2024/1689), adopted July 2024 and entering into force August 2024 (with most provisions effective in 2026). In Asia, China issued its first generative AI rules (“Interim Measures”) for content services, while India, Singapore and others are rolling out national AI strategies and sector-specific guidelines. International bodies reinforce these efforts: the UN recently encouraged countries to adopt AI rules for “safe, secure and trustworthy” systems, and organizations like the OECD have AI Principles promoting trustworthy AI globally. </p> 
>  
> <p><strong>Key highlights include:</strong><br> 
> • <strong>EU AI Act (2024):</strong> First-ever comprehensive AI law across the 27-member bloc. It takes a risk-based approach to AI systems and will impose fines up to 7% of global turnover for non-compliance.<br> 
> • C*<em>hina’s Interim Measures:</em>* New administrative rules govern generative AI service providers in China’s digital ecosystem.<br> 
> • <strong>International Frameworks:</strong> Bodies like the OECD and G7 emphasize AI ethics, and the UN’s AI resolutions call for member states to enact national AI regulations.</p> 
>  
> <p><strong>U.S. AI Regulation 2025: Trump’s Approach</strong><br> 
> <a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5vrn97ljlcjgps1b2vj0.jpg"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5vrn97ljlcjgps1b2vj0.jpg" alt="" width="800" height="433"></a><br> 
> The United States still has no single federal AI law, relying instead on a patchwork of laws and guidelines. In early 2025 the Trump administration took a markedly different tack from the previous (Biden) administration. President Trump’s January 2025 Executive Order “Removing Barriers to American Leadership in AI” revoked many of Biden’s AI directives. Trump’s order calls for all agencies to rescind policies seen as hindering U.S. AI dominance. <br> 
> In July 2025 the administration also released “America’s AI Action Plan”, outlining 90+ actions to boost AI innovation and leadership. This plan has a pro‑innovation, deregulatory bent – contrasting with the EU’s risk-based model and even some state AI laws (like Colorado’s AI Act) that focus on preventing bias. <br> 
> Meanwhile, Congress is considering various AI bills, most aiming to issue voluntary guidelines or create new agencies. In practice, U.S. companies navigate a maze of rules: current federal laws (e.g. consumer protection, aviation or defense statutes) apply in limited ways, and agencies like the FTC and FCC are adapting existing mandates to cover AI.</p> 
>  
> <p><strong>Key points in U.S. federal AI policy include:</strong><br> 
> <a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frjljznym6euwvnz9g3t4.jpg"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frjljznym6euwvnz9g3t4.jpg" alt="" width="800" height="433"></a></p> 
>  
> <ul> 
> <li> 
> <strong>Trump’s 2025 EO (“Removing Barriers”):</strong> Signaled a permissive, growth-focused stance. It rescinds Biden’s Oct 2023 AI EO (Safe, Secure &amp; Trustworthy AI) and directs agencies to withdraw any “obstacles” to AI development.</li> 
> <li> 
> <strong>American AI Action Plan (July 2025)</strong>: Lists over 90 federal initiatives to secure U.S. AI leadership. It emphasizes export of AI tech, infrastructure upgrades, and incentives for industry.</li> 
> <li> 
> <strong>No Comprehensive Law Yet:</strong> Developers still operate under existing statutes. As one legal update notes, without federal AI rules “developers and deployers of AI systems will operate in an increasing patchwork of state and local laws”. Federal lawmakers to date favor voluntary standards (for example, promoting AI safety research and transparency) rather than stringent mandates, to avoid stifling innovation.</li> 
> </ul> 
>  
> <p><strong>State AI Laws in the U.S.</strong><br> 
> At the state level, Ai legislative activity is surging. Dozens of states have introduced AI bills, leading to a fragmented state-by-state regulatory landscape.** For example:**<br> 
> • <strong>Colorado:</strong> In May 2024 Colorado passed the nation’s first AI Act, effective Feb 2026. It requires developers/deployers of “high-risk” AI systems to use reasonable care to protect consumers from “algorithmic discrimination” (unlawful bias) in areas like hiring, credit, healthcare, etc...<br> 
> • <strong>California:</strong> In 2024 California legislators drafted dozens of AI-related bills on topics like transparency of AI-generated content, rights of people depicted in AI media, data privacy, and banning deceptive deepfakes. These add to the U.S. regulatory patchwork. (A White &amp; Case analysis notes CA’s laws “aim to impose wide-ranging obligations” on AI developers, covering everything from safety reporting to content disclosures.)<br> 
> • <strong>Other States:</strong> Over 45 states considered AI measures in 2024, and 31 enacted related laws or resolutions. For instance, Utah created an AI Policy Act, New York and Illinois are moving data/biometric laws with AI provisions, and many states have task forces or guidelines.</p> 
>  
> <p><strong>Frequently Asked Questions</strong><br> 
> <strong>Q: How is the U.S. handling AI regulation in 2025?</strong><br> 
> <strong>A:</strong> As of 2025 the U.S. has no single AI law. The Trump administration has prioritized innovation over restriction. A January 2025 Executive Order (“Removing Barriers to American Leadership in AI”) rescinded many of the Biden administration’s AI safety directives. In July 2025 the White House released an AI Action Plan with 90+ measures to boost U.S. AI leadership. At the same time, Congress has debated AI bills (mostly setting guidelines), and agencies like the FTC continue to use existing laws (e.g. anti-discrimination rules) to police AI. In practice, companies must comply with a mix of existing laws and voluntary standards, and pay close attention to state regulations.<br> 
> <strong>Q: What is the EU AI Act?</strong><br> 
> <strong>A:</strong> The EU Artificial Intelligence Act is the world’s first comprehensive AI law. Published in the EU Official Journal on July 12, 2024, it creates a risk-based framework for AI in all member states. High-risk AI systems (e.g. in healthcare, transport, law enforcement) will face strict requirements, while prohibited AI uses (like undetectable manipulative techniques) are banned. The law took effect in August 2024, with most rules enforceable by August 2026. It also imposes penalties up to €35 million or 7% of global turnover for violations.<br> 
> <strong>Q: How many countries have AI regulations?</strong><br> 
> <strong>A:</strong> By early 2025, many nations are moving on AI governance. One analysis found “at least 69 countries have proposed over 1000 AI-related policy initiatives and legal frameworks”. This includes data protection rules adapted for AI, special AI ethics laws, and government strategies. So far, major economies (EU, China, U.S.) and dozens of others (India, Canada, Australia, Brazil, etc.) have some AI rules or guidelines in place or in the works.<br> 
> <strong>Q: Which U.S. states have their own AI laws?</strong><br> 
> <strong>A:</strong> Several states are active. Colorado passed a landmark AI Act in 2024, targeting bias: it requires impact assessments and care to avoid “algorithmic discrimination” by high-risk AI systems. California has introduced many AI bills (e.g. requiring disclosures on AI-generated content) and broader AI/transparency laws. Utah, New York, and Illinois, among others, have new laws or regulations affecting AI use (from autonomous vehicles to biometric data). In 2024 over 30 states enacted AI-related laws or resolutions, so companies should track the state landscape carefully.<br> 
> <strong>Q: What did President Trump’s 2025 AI executive order do?</strong><br> 
> <strong>A:</strong> President Trump’s Jan 2025 EO titled “Removing Barriers to American Leadership in Artificial Intelligence” reversed many Biden-era AI policies. It revokes Biden’s Oct 2023 AI EO and directs federal agencies to rescind any rules or guidance seen as stifling innovation. The order explicitly emphasizes maintaining U.S. global AI dominance and calls for a new AI Action Plan (published in July 2025). In practice, it signals a shift from the prior administration’s risk- and safety-focused approach toward a deregulation stance.</p> 
>  
> <p><strong>Conclusion: Why AI Regulation News Today Matters More Than Ever</strong><br> 
> The rise of AI regulation is no longer a distant policy discussion—it’s a pressing reality shaping our digital lives, economies, and futures. From u.s. ai regulation 2025 under the Trump administration’s innovation-first stance, to a growing network of state ai laws like those in Colorado and California, the legal landscape is shifting fast. Meanwhile, international frameworks and ai regulations around the world 2025—like the EU AI Act or China’s content rules—are setting powerful precedents.<br> 
> Whether you’re a business leader deploying AI tools, a developer writing algorithms, or a consumer curious about artificial intelligence laws and regulations, staying informed is critical. These laws affect how your data is used, how prices are set, and what kind of technologies get released (or banned). Understanding the regulatory direction can help you act responsibly, innovate ethically, and protect both user trust and long-term success.</p> 
>  
> <p>At TechDecodedly, we’re committed to delivering accurate, human-readable updates on AI regulations around the world, U.S. AI action plans, and the future of tech policy. Subscribe, follow, and keep reading—we’ll help you make sense of it all.</p>

---

## [4/10] The Great Equaliser
**Source:** The Practical Developer | **Date:** 2025-12-02T12:00:00.000Z
**URL:** https://dev.to/rawveg/the-great-equaliser-20em
**Reasoning:** The article is about AI democratization, which is not directly related to our primary interests.
**Authors:** Tim Green

**Content/Abstract:**
> <p>The corner shop that predicts your shopping habits better than Amazon. The local restaurant that automates its supply chain with the precision of McDonald's. The one-person consultancy that analyses data like McKinsey. These scenarios aren't science fiction—they're the emerging reality as artificial intelligence democratises tools once exclusive to corporate giants. But as small businesses gain access to enterprise-grade capabilities, a fundamental question emerges: will AI truly level the playing field, or simply redraw the battle lines in ways we're only beginning to understand?</p> 
>  
> <h2> 
>    
>    
>   The New Arsenal 
> </h2> 
>  
> <p>Walk into any high street business today and you'll likely encounter AI working behind the scenes. The local bakery uses machine learning to optimise flour orders. The independent bookshop employs natural language processing to personalise recommendations. The neighbourhood gym deploys computer vision to monitor equipment usage and predict maintenance needs. What was once the exclusive domain of Fortune 500 companies—sophisticated data analytics, predictive modelling, automated customer service—is now available as a monthly subscription.</p> 
>  
> <p>This transformation represents more than just technological advancement; it's a fundamental shift in the economic architecture. According to research from the Brookings Institution, AI functions as a "wide-ranging" technology that redefines how information is integrated, data is analysed, and decisions are made across every aspect of business operations. Unlike previous technological waves that primarily affected specific industries or functions, AI's impact cuts across all sectors simultaneously.</p> 
>  
> <p>The democratisation happens through cloud computing platforms that package complex AI capabilities into user-friendly interfaces. A small retailer can now access the same customer behaviour prediction algorithms that power major e-commerce platforms. A local manufacturer can implement quality control systems that rival those of industrial giants. The barriers to entry—massive computing infrastructure, teams of data scientists, years of algorithm development—have largely evaporated.</p> 
>  
> <p>Consider the transformation in customer relationship management. Where large corporations once held decisive advantages through expensive CRM systems and dedicated analytics teams, small businesses can now deploy AI-powered tools that automatically segment customers, predict purchasing behaviour, and personalise marketing messages. The playing field appears more level than ever before.</p> 
>  
> <p>Yet this apparent equalisation masks deeper complexities. Access to tools doesn't automatically translate to competitive advantage, and the same AI systems that empower small businesses also amplify the capabilities of their larger competitors. The question isn't whether AI will reshape local economies—it already is. The question is whether this reshaping will favour David or Goliath.</p> 
>  
> <h2> 
>    
>    
>   Local Economies in Flux 
> </h2> 
>  
> <p>Much like the corner shop discovering it can compete with retail giants through predictive analytics, local economies are experiencing transformations that challenge traditional assumptions about scale and proximity. The impact unfolds in unexpected ways. Traditional advantages—proximity to customers, personal relationships, intimate market knowledge—suddenly matter less when AI can predict consumer behaviour with precision. Simultaneously, new advantages emerge for businesses that can harness these tools effectively.</p> 
>  
> <p>Small businesses often possess inherent agility that larger corporations struggle to match. They can implement new AI systems faster, pivot strategies more quickly, and adapt to local market conditions with greater flexibility. A family-owned restaurant can adjust its menu based on AI-analysed customer preferences within days, while a chain restaurant might need months to implement similar changes across its corporate structure.</p> 
>  
> <p>The "tele-everything" environment accelerated by AI adoption fundamentally alters the value of physical presence. Local businesses that once relied primarily on foot traffic and geographical convenience must now compete with online-first enterprises that leverage AI to deliver personalised experiences regardless of location. This shift doesn't necessarily disadvantage local businesses, but it forces them to compete on new terms.</p> 
>  
> <p>Some local economies are experiencing a renaissance as AI enables small businesses to serve global markets. A craftsperson in rural Wales can now use AI-powered tools to identify international customers, optimise pricing strategies, and manage complex supply chains that were previously beyond their capabilities. The local becomes global, but the global also becomes intensely local as AI enables mass customisation and hyper-personalised services.</p> 
>  
> <p>The transformation extends beyond individual businesses to entire economic ecosystems. Local suppliers, service providers, and complementary businesses must all adapt to new AI-driven demands and capabilities. A local accounting firm might find its traditional bookkeeping services automated away, but discover new opportunities in helping businesses implement and optimise AI systems. The ripple effects create new interdependencies and collaborative possibilities that reshape entire commercial districts.</p> 
>  
> <h2> 
>    
>    
>   The Corporate Response 
> </h2> 
>  
> <p>Large corporations aren't passive observers in this transformation. They're simultaneously benefiting from the same AI democratisation while developing strategies to maintain their competitive advantages. The result is an arms race where both small businesses and corporations are rapidly adopting AI capabilities, but with vastly different resources and strategic approaches.</p> 
>  
> <p>Corporate advantages in the AI era often centre on data volume and variety. While small businesses can access sophisticated AI tools, large corporations possess vast datasets that can train more accurate and powerful models. A multinational retailer has purchase data from millions of customers across diverse markets, enabling AI insights that a local shop with hundreds of customers simply cannot match. This data advantage compounds over time, as larger datasets enable more sophisticated AI models, which generate better insights, which attract more customers, which generate more data.</p> 
>  
> <p>Scale also provides advantages in AI implementation. Corporations can afford dedicated AI teams, custom algorithm development, and integration across multiple business functions. They can experiment with cutting-edge technologies, absorb the costs of failed implementations, and iterate rapidly towards optimal solutions. Small businesses, despite having access to AI tools, often lack the resources for such comprehensive adoption.</p> 
>  
> <p>However, corporate size can also become a liability. Large organisations often struggle with legacy systems, bureaucratic decision-making processes, and resistance to change. A small business can implement a new AI-powered inventory management system in weeks, while a corporation might need years to navigate internal approvals, system integrations, and change management processes. The very complexity that enables corporate scale can inhibit the rapid adaptation that AI environments reward.</p> 
>  
> <p>The competitive dynamics become particularly complex in markets where corporations and small businesses serve similar customer needs. AI enables both to offer increasingly sophisticated services, but the nature of competition shifts from traditional factors like price and convenience to new dimensions like personalisation depth, prediction accuracy, and automated service quality. A local financial advisor equipped with AI-powered portfolio analysis tools might compete effectively with major investment firms, not on the breadth of services, but on the depth of personal attention combined with sophisticated analytical capabilities.</p> 
>  
> <h2> 
>    
>    
>   New Forms of Inequality 
> </h2> 
>  
> <p>The promise of AI democratisation comes with a darker counterpart: the emergence of new forms of inequality that may prove more entrenched than those they replace. While AI tools become more accessible, the skills, knowledge, and resources required to use them effectively remain unevenly distributed.</p> 
>  
> <p>Digital literacy emerges as a critical factor determining who benefits from AI democratisation. Small business owners who can understand and implement AI systems gain significant advantages over those who cannot. This creates a new divide not based on access to capital or technology, but on the ability to comprehend and leverage complex digital tools. The gap between AI-savvy and AI-naive businesses may prove wider than traditional competitive gaps.</p> 
>  
> <p>A significant portion of technology experts express concern about AI's societal impact. Research from the Pew Research Centre indicates that many experts believe the tech-driven future will worsen life for most people, specifically citing "greater inequality" as a major outcome. This pessimism stems partly from AI's potential to replace human workers while concentrating benefits among those who own and control AI systems.</p> 
>  
> <p>The productivity gains from AI create a paradox for small businesses. While these tools can dramatically increase efficiency and capability, they also reduce the need for human employees. A small business that once employed ten people might accomplish the same work with five people and sophisticated AI systems. The business becomes more competitive, but contributes less to local employment and economic circulation. This labour-saving potential of AI creates a fundamental tension between business efficiency and community economic health.</p> 
>  
> <p>Geographic inequality also intensifies as AI adoption varies significantly across regions. Areas with strong digital infrastructure, educated populations, and supportive business environments see rapid AI adoption among local businesses. Rural or economically disadvantaged areas lag behind, creating growing gaps in local economic competitiveness. The digital divide evolves into an AI divide with potentially more severe consequences.</p> 
>  
> <p>Access to data becomes another source of inequality. While AI tools are democratised, the data required to train them effectively often isn't. Businesses in data-rich environments—urban areas with dense customer interactions, regions with strong digital adoption, markets with sophisticated tracking systems—can leverage AI more effectively than those in data-poor environments. This creates a new form of resource inequality where information, rather than capital or labour, becomes the primary determinant of competitive advantage.</p> 
>  
> <p>The emergence of these inequalities is particularly concerning because they compound existing disadvantages. Businesses that already struggle with traditional competitive factors—limited capital, poor locations, outdated infrastructure—often find themselves least equipped to navigate AI adoption successfully. The democratisation of AI tools doesn't automatically democratise the benefits if the underlying capabilities to use them remain concentrated.</p> 
>  
> <h2> 
>    
>    
>   The Skills Revolution 
> </h2> 
>  
> <p>The AI transformation demands new skills that don't align neatly with traditional business education or experience. Small business owners must become part technologist, part data analyst, part strategic planner in ways that previous generations never required. This skills revolution creates opportunities for some while leaving others behind.</p> 
>  
> <p>Traditional business skills—relationship building, local market knowledge, operational efficiency—remain important but are no longer sufficient. Success increasingly requires understanding how to select appropriate AI tools, interpret outputs, and integrate digital systems with human processes. The learning curve is steep, and not everyone can climb it effectively. A successful restaurant owner with decades of experience in food service and customer relations might struggle to understand machine learning concepts or data analytics principles necessary to leverage AI-powered inventory management or customer prediction systems.</p> 
>  
> <p>Educational institutions struggle to keep pace with the rapidly evolving skill requirements. Business schools that taught traditional management principles find themselves scrambling to incorporate AI literacy into curricula. Vocational training programmes designed for traditional trades must now include digital components. The mismatch between educational offerings and business needs creates gaps that some entrepreneurs can bridge while others cannot.</p> 
>  
> <p>Generational differences compound the skills challenge. Younger business owners who grew up with digital technology often adapt more quickly to AI tools, while older entrepreneurs with decades of experience may find the transition more difficult. This creates potential for generational turnover in local business leadership as AI adoption becomes essential for competitiveness. However, the relationship isn't simply age-based—some older business owners embrace AI enthusiastically while some younger ones struggle with its complexity.</p> 
>  
> <p>The skills revolution also affects employees within small businesses. Workers must adapt to AI-augmented roles, learning to collaborate with systems rather than simply performing traditional tasks. Some thrive in this environment, developing hybrid human-AI capabilities that make them more valuable. Others struggle with the transition, potentially facing displacement or reduced relevance. A retail employee who learns to work with AI-powered inventory systems and customer analytics becomes more valuable, while one who resists such integration may find their role diminished.</p> 
>  
> <p>The pace of change in required skills creates ongoing challenges. AI capabilities evolve rapidly, meaning that skills learned today may become obsolete within years. This demands a culture of continuous learning that many small businesses struggle to maintain while managing day-to-day operations. The businesses that succeed are often those that can balance immediate operational needs with ongoing skill development.</p> 
>  
> <h2> 
>    
>    
>   Redefining Competition 
> </h2> 
>  
> <p>Just as the local restaurant now competes on supply chain optimisation rather than just food quality, AI doesn't just change the tools of competition; it fundamentally alters what businesses compete on. Traditional competitive factors like price, location, and product quality remain important, but new dimensions emerge that can overwhelm traditional advantages.</p> 
>  
> <p>Prediction capability becomes a key competitive differentiator. Businesses that can accurately forecast customer needs, market trends, and operational requirements gain significant advantages over those relying on intuition or historical patterns. A local retailer that predicts seasonal demand fluctuations can optimise inventory and pricing in ways that traditional competitors cannot match. This predictive capability extends beyond simple forecasting to understanding complex patterns in customer behaviour, market dynamics, and operational efficiency.</p> 
>  
> <p>Personalisation depth emerges as another competitive battlefield. AI enables small businesses to offer individually customised experiences that were previously impossible at their scale. A neighbourhood coffee shop can remember every customer's preferences, predict their likely orders, and adjust recommendations based on weather, time of day, and purchasing history. This level of personalisation can compete effectively with larger chains that offer consistency but less individual attention.</p> 
>  
> <p>Speed of adaptation becomes crucial as market conditions change rapidly. Businesses that can quickly adjust strategies, modify offerings, and respond to new opportunities gain advantages over slower competitors. AI systems that continuously monitor market conditions and automatically adjust business parameters enable small businesses to be more responsive than larger organisations with complex decision-making hierarchies. A small online retailer can adjust pricing in real-time based on competitor analysis and demand patterns, while a large corporation might need weeks to implement similar changes.</p> 
>  
> <p>Data quality and integration emerge as competitive moats. Businesses that collect clean, comprehensive data and integrate it effectively across all operations can leverage AI more powerfully than those with fragmented or poor-quality information. This creates incentives for better data management practices but also advantages businesses that start with superior data collection capabilities. A small business that systematically tracks customer interactions, inventory movements, and operational metrics can build AI capabilities that larger competitors with poor data practices cannot match.</p> 
>  
> <p>The redefinition of competition extends to entire business models. AI enables new forms of value creation that weren't previously possible at small business scale. A local service provider might develop AI-powered tools that become valuable products in their own right. A neighbourhood retailer might create data insights that benefit other local businesses. Competition evolves from zero-sum battles over market share to more complex ecosystems of value creation and exchange.</p> 
>  
> <p>Customer expectations also shift as AI capabilities become more common. Businesses that don't offer AI-enabled features—personalised recommendations, predictive service, automated support—may appear outdated compared to competitors that do. This creates pressure for AI adoption not just for operational efficiency, but for customer satisfaction and retention.</p> 
>  
> <h2> 
>    
>    
>   The Network Effect 
> </h2> 
>  
> <p>As AI adoption spreads across local economies, network effects emerge that can either amplify competitive advantages or create new forms of exclusion. Businesses that adopt AI early and effectively often find their advantages compound over time, while those that lag behind face increasingly difficult catch-up challenges.</p> 
>  
> <p>Data network effects prove particularly powerful. Businesses that collect more customer data can train better AI models, which provide superior service, which attracts more customers, which generates more data. This virtuous cycle can quickly separate AI-successful businesses from their competitors in ways that traditional competitive dynamics rarely achieved. A local delivery service that uses AI to optimise routes and predict demand can provide faster, more reliable service, attracting more customers and generating more data to further improve its AI systems.</p> 
>  
> <p>Partnership networks also evolve around AI capabilities. Small businesses that can effectively integrate AI systems often find new collaboration opportunities with other AI-enabled enterprises. They can share data insights, coordinate supply chains, and develop joint offerings that leverage combined AI capabilities. Businesses that cannot participate in these AI-enabled networks risk isolation from emerging collaborative opportunities.</p> 
>  
> <p>Platform effects emerge as AI tools become more sophisticated and interconnected. Businesses that adopt compatible AI systems can more easily integrate with suppliers, customers, and partners who use similar technologies. This creates pressure for standardisation around particular AI platforms, potentially disadvantaging businesses that choose different or incompatible systems. A small manufacturer that uses AI systems compatible with its suppliers' inventory management can achieve seamless coordination, while one using incompatible systems faces integration challenges.</p> 
>  
> <p>The network effects extend beyond individual businesses to entire local economic ecosystems. Regions where many businesses adopt AI capabilities can develop supportive infrastructure, shared expertise, and collaborative advantages that attract additional AI-enabled enterprises. Areas that lag in AI adoption may find themselves increasingly isolated from broader economic networks. Cities that develop strong AI business clusters can offer shared resources, talent pools, and collaborative opportunities that individual businesses in less developed areas cannot access.</p> 
>  
> <p>Knowledge networks become particularly important as AI implementation requires ongoing learning and adaptation. Businesses in areas with strong AI adoption can share experiences, learn from each other's successes and failures, and collectively develop expertise that benefits the entire local economy. This creates positive feedback loops where AI success breeds more AI success, but also means that areas that fall behind may find it increasingly difficult to catch up.</p> 
>  
> <h2> 
>    
>    
>   Global Reach, Local Impact 
> </h2> 
>  
> <p>AI democratisation enables small businesses to compete in global markets while simultaneously making global competition more intense at the local level. This paradox creates both opportunities and threats for local economies in ways that previous technological waves didn't achieve.</p> 
>  
> <p>A small manufacturer in Manchester can now use AI to identify customers in markets they never previously accessed, optimise international shipping routes, and manage currency fluctuations with sophisticated algorithms. The barriers to global commerce—language translation, market research, logistics coordination—diminish significantly when AI tools handle these complexities automatically. Machine learning systems can analyse global market trends, identify emerging opportunities, and even handle customer service in multiple languages, enabling small businesses to operate internationally with capabilities that previously required large multinational operations.</p> 
>  
> <p>However, this global reach works in both directions. Local businesses that once competed primarily with nearby enterprises now face competition from AI-enabled businesses anywhere in the world. A local graphic design firm competes not just with other local designers, but with AI-augmented freelancers from dozens of countries who can deliver similar services at potentially lower costs. The protective barriers of geography and local relationships diminish when AI enables remote competitors to offer personalised, efficient service regardless of physical location.</p> 
>  
> <p>The globalisation of competition through AI creates pressure for local businesses to find defensible advantages that global competitors cannot easily replicate. Physical presence, local relationships, and regulatory compliance become more valuable when other competitive factors can be matched by distant AI-enabled competitors. A local accountant might compete with global AI-powered tax preparation services by offering face-to-face consultation and deep knowledge of local regulations that remote competitors cannot match.</p> 
>  
> <p>Cultural and regulatory differences provide some protection for local businesses, but AI's ability to adapt to local preferences and navigate regulatory requirements reduces these natural barriers. A global e-commerce platform can use AI to automatically adjust its offerings for local tastes, comply with regional regulations, and even communicate in local dialects or cultural contexts. This erosion of natural competitive barriers forces local businesses to compete more directly on service quality, innovation, and efficiency rather than relying on geographic or cultural advantages.</p> 
>  
> <p>The global competition enabled by AI also creates opportunities for specialisation and niche market development. Small businesses can use AI to identify and serve highly specific customer segments globally, rather than trying to serve broad local markets. A craftsperson specialising in traditional techniques can use AI to find customers worldwide who value their specific skills, creating sustainable businesses around expertise that might not support a local market.</p> 
>  
> <p>International collaboration becomes more feasible as AI tools handle communication, coordination, and logistics challenges. Small businesses can participate in global supply chains, joint ventures, and collaborative projects that were previously accessible only to large corporations. This creates opportunities for local businesses to access global resources, expertise, and markets while maintaining their local identity and operations.</p> 
>  
> <h2> 
>    
>    
>   Policy and Regulatory Responses 
> </h2> 
>  
> <p>Governments and regulatory bodies are beginning to recognise the transformative potential of AI democratisation and its implications for local economies. Policy responses vary significantly across jurisdictions, creating a patchwork of approaches that may determine which regions benefit most from AI-enabled economic transformation.</p> 
>  
> <p>Some governments focus on ensuring broad access to AI tools and training, recognising that digital divides could become AI divides with severe economic consequences. Public funding for AI education, infrastructure development, and small business support programmes aims to prevent the emergence of AI-enabled inequality between different economic actors and regions. The European Union's Digital Single Market strategy includes provisions for supporting small business AI adoption, while countries like Singapore have developed comprehensive AI governance frameworks that include support for small and medium enterprises.</p> 
>  
> <p>Competition policy faces new challenges as AI blurs traditional boundaries between markets and competitive advantages. Regulators must determine whether AI democratisation genuinely increases competition or whether it creates new forms of market concentration that require intervention. The complexity of AI systems makes it difficult to assess competitive impacts using traditional regulatory frameworks. When a few large technology companies provide the AI platforms that most small businesses depend on, questions arise about whether this creates new forms of economic dependency that require regulatory attention.</p> 
>  
> <p>Data governance emerges as a critical policy area affecting small business competitiveness. Regulations that restrict data collection or sharing may inadvertently disadvantage small businesses that rely on AI tools requiring substantial data inputs. Conversely, policies that enable broader data access might help level the playing field between small businesses and large corporations with extensive proprietary datasets. The General Data Protection Regulation in Europe, for example, affects how small businesses can collect and use customer data for AI applications, potentially limiting their ability to compete with larger companies that have more resources for compliance.</p> 
>  
> <p>Privacy and security regulations create compliance burdens that affect small businesses differently than large corporations. While AI tools can help automate compliance processes, the underlying regulatory requirements may still favour businesses with dedicated legal and technical resources. Policy makers must balance privacy protection with the need to avoid creating insurmountable barriers for small business AI adoption.</p> 
>  
> <p>International coordination becomes increasingly important as AI-enabled businesses operate across borders more easily. Differences in AI regulation, data governance, and digital trade policies between countries can create competitive advantages or disadvantages for businesses in different jurisdictions. Small businesses with limited resources to navigate complex international regulatory environments may find themselves at a disadvantage compared to larger enterprises with dedicated compliance teams.</p> 
>  
> <p>The pace of AI development often outstrips regulatory responses, creating uncertainty for businesses trying to plan AI investments and implementations. Regulatory frameworks developed for traditional business models may not adequately address the unique challenges and opportunities created by AI adoption. This regulatory lag can create both opportunities for early adopters and risks for businesses that invest in AI capabilities that later face regulatory restrictions.</p> 
>  
> <h2> 
>    
>    
>   The Human Element 
> </h2> 
>  
> <p>Despite AI's growing capabilities, human factors remain crucial in determining which businesses succeed in the AI-enabled economy. The interaction between human creativity, judgement, and relationship-building skills with AI capabilities often determines competitive outcomes more than pure technological sophistication.</p> 
>  
> <p>Small businesses often possess advantages in human-AI collaboration that larger organisations struggle to match. The close relationships between owners, employees, and customers in small businesses enable more nuanced understanding of how AI tools should be deployed and customised. A local business owner who knows their customers personally can guide AI systems more effectively than distant corporate algorithms. This intimate knowledge allows for AI implementations that enhance rather than replace human insights and relationships.</p> 
>  
> <p>Trust and relationships become more valuable, not less, as AI capabilities proliferate. Customers who feel overwhelmed by purely digital interactions may gravitate towards businesses that combine AI efficiency with human warmth and understanding. Small businesses that successfully blend AI capabilities with personal service can differentiate themselves from purely digital competitors. A local bank that uses AI for fraud detection and risk assessment while maintaining personal relationships with customers can offer security and efficiency alongside human understanding and flexibility.</p> 
>  
> <p>The human element also affects AI implementation success within businesses. Small business owners who can effectively communicate AI benefits to employees, customers, and partners are more likely to achieve successful adoption than those who treat AI as a purely technical implementation. Change management skills become as important as technical capabilities in determining AI success. Employees who understand how AI tools enhance their work rather than threaten their jobs are more likely to use these tools effectively and contribute to successful implementation.</p> 
>  
> <p>Ethical considerations around AI use create opportunities for small businesses to differentiate themselves through more responsible AI deployment. While large corporations may face pressure to maximise AI efficiency regardless of broader impacts, small businesses with strong community ties may choose AI implementations that prioritise local employment, customer privacy, or social benefit alongside business objectives. This ethical positioning can become a competitive advantage in markets where customers value responsible business practices.</p> 
>  
> <p>The human element extends to customer experience design and service delivery. AI can handle routine tasks and provide data insights, but human creativity and empathy remain essential for understanding customer needs, designing meaningful experiences, and building lasting relationships. Small businesses that use AI to enhance human capabilities rather than replace them often achieve better customer satisfaction and loyalty than those that pursue purely automated solutions.</p> 
>  
> <p>Creativity and innovation in AI application often come from human insights about customer needs, market opportunities, and operational challenges. Small business owners who understand their operations intimately can identify AI applications that larger competitors might miss. This human insight into business operations and customer needs becomes a source of competitive advantage in AI implementation.</p> 
>  
> <h2> 
>    
>    
>   Future Trajectories 
> </h2> 
>  
> <p>The trajectory of AI democratisation and its impact on local economies remains uncertain, with multiple possible futures depending on technological development, policy choices, and market dynamics. Understanding these potential paths helps businesses and policymakers prepare for different scenarios.</p> 
>  
> <p>One trajectory leads towards genuine democratisation where AI tools become so accessible and easy to use that most small businesses can compete effectively with larger enterprises on AI-enabled capabilities. In this scenario, local economies flourish as small businesses leverage AI to serve global markets while maintaining local roots and relationships. The corner shop truly does compete with Amazon, not by matching its scale, but by offering superior personalisation and local relevance powered by AI insights.</p> 
>  
> <p>An alternative trajectory sees AI democratisation creating new forms of concentration where a few AI platform providers control the tools that all businesses depend on. Small businesses gain access to AI capabilities but become dependent on platforms controlled by large technology companies, potentially creating new forms of economic subjugation rather than liberation. In this scenario, the democratisation of AI tools masks a concentration of control over the underlying infrastructure and algorithms that determine business success.</p> 
>  
> <p>A third possibility involves fragmentation where AI adoption varies dramatically across regions, industries, and business types, creating a complex patchwork of AI-enabled and traditional businesses. This scenario might preserve diversity in business models and competitive approaches but could also create significant inequalities between different economic actors and regions. Some areas become AI-powered economic hubs while others remain trapped in traditional competitive dynamics.</p> 
>  
> <p>The speed of AI development affects all these trajectories. Rapid advancement might favour businesses and regions that can adapt quickly while leaving others behind. Slower, more gradual development might enable broader adoption and more equitable outcomes but could also delay beneficial transformations in productivity and capability. The current pace of AI development, particularly in generative AI capabilities, suggests that rapid change is more likely than gradual evolution.</p> 
>  
> <p>International competition adds another dimension to these trajectories. Countries that develop strong AI capabilities and supportive regulatory frameworks may see their local businesses gain advantages over those in less developed AI ecosystems. China's rapid advancement in AI innovation, as documented by the Information Technology and Innovation Foundation, demonstrates how national AI strategies can affect local business competitiveness on a global scale.</p> 
>  
> <p>The role of human-AI collaboration will likely determine which trajectory emerges. Research from the Pew Research Centre suggests that the most positive outcomes occur when AI enhances human capabilities rather than simply replacing them. Local economies that successfully integrate AI tools with human skills and relationships may achieve better outcomes than those that pursue purely technological solutions.</p> 
>  
> <h2> 
>    
>    
>   Preparing for Transformation 
> </h2> 
>  
> <p>The AI transformation of local economies is not a distant future possibility but a current reality that businesses, policymakers, and communities must navigate actively. Success in this environment requires understanding both the opportunities and risks while developing strategies that leverage AI capabilities while preserving human and community values.</p> 
>  
> <p>Small businesses must develop AI literacy not as a technical specialisation but as a core business capability. This means understanding what AI can and cannot do, how to select appropriate tools, and how to integrate AI systems with existing operations and relationships. The learning curve is steep, but the costs of falling behind may be steeper. Business owners need to invest time in understanding AI capabilities, experimenting with available tools, and developing strategies for gradual implementation that builds on their existing strengths.</p> 
>  
> <p>Local communities and policymakers must consider how to support AI adoption while preserving the diversity and character that make local economies valuable. This might involve public investment in digital infrastructure, education programmes, or support for businesses struggling with AI transition. The goal should be enabling beneficial transformation rather than simply accelerating technological adoption. Communities that proactively address AI adoption challenges are more likely to benefit from the opportunities while mitigating the risks.</p> 
>  
> <p>The democratisation of AI represents both the greatest opportunity and the greatest challenge facing local economies in generations. It promises to level competitive playing fields that have favoured large corporations for decades while threatening to create new forms of inequality that could be more entrenched than those they replace. The outcome will depend not on the technology itself, but on how wisely we deploy it in service of human and community flourishing.</p> 
>  
> <p>Collaboration between businesses, educational institutions, and government agencies becomes essential for successful AI adoption. Small businesses need access to training, technical support, and financial resources to implement AI effectively. Educational institutions must adapt curricula to include AI literacy alongside traditional business skills. Government agencies must develop policies that support beneficial AI adoption while preventing harmful concentration of power or exclusion of vulnerable businesses.</p> 
>  
> <p>The transformation requires balancing efficiency gains with social and economic values. While AI can dramatically improve business productivity and competitiveness, communities must consider the broader impacts on employment, social cohesion, and economic diversity. The most successful AI adoptions are likely to be those that enhance human capabilities and community strengths rather than simply replacing them with automated systems.</p> 
>  
> <p>As we stand at this inflection point, the choices made by individual businesses, local communities, and policymakers will determine whether AI democratisation fulfils its promise of economic empowerment or becomes another force for concentration and inequality. The technology provides the tools; wisdom in their application will determine the results.</p> 
>  
> <p>The corner shop that predicts your needs, the restaurant that optimises its operations, the consultancy that analyses like a giant—these are no longer future possibilities but present realities. The question is no longer whether AI will transform local economies, but whether that transformation will create the more equitable and prosperous future that its democratisation promises. The answer lies not in the algorithms themselves, but in the human choices that guide their deployment.</p> 
>  
> <p>Is AI levelling the field, or just redrawing the battle lines?</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   References and Further Information 
> </h2> 
>  
> <p><strong>Primary Sources:</strong></p> 
>  
> <p>Brookings Institution. "How artificial intelligence is transforming the world." Available at: <a href="http://www.brookings.edu">www.brookings.edu</a></p> 
>  
> <p>Pew Research Center. "Experts Say the 'New Normal' in 2025 Will Be Far More Tech-Driven." Available at: <a href="http://www.pewresearch.org">www.pewresearch.org</a></p> 
>  
> <p>Pew Research Center. "Improvements ahead: How humans and AI might evolve together in the next decade." Available at: <a href="http://www.pewresearch.org">www.pewresearch.org</a></p> 
>  
> <p>ScienceDirect. "Opinion Paper: 'So what if ChatGPT wrote it?' Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy." Available at: <a href="http://www.sciencedirect.com">www.sciencedirect.com</a></p> 
>  
> <p>ScienceDirect. "AI revolutionizing industries worldwide: A comprehensive overview of artificial intelligence applications across diverse sectors." Available at: <a href="http://www.sciencedirect.com">www.sciencedirect.com</a></p> 
>  
> <p>Information Technology and Innovation Foundation. "China Is Rapidly Becoming a Leading Innovator in Advanced Technologies." Available at: itif.org</p> 
>  
> <p>International Monetary Fund. "Technological Progress, Artificial Intelligence, and Inclusive Growth." Available at: <a href="http://www.elibrary.imf.org">www.elibrary.imf.org</a></p> 
>  
> <p><strong>Additional Reading:</strong></p> 
>  
> <p>For deeper exploration of AI's economic impacts, readers should consult academic journals focusing on technology economics, policy papers from major think tanks examining AI democratisation, and industry reports tracking small business AI adoption rates across different sectors and regions. The European Union's Digital Single Market strategy documents provide insight into policy approaches to AI adoption support, while Singapore's AI governance frameworks offer examples of comprehensive national AI strategies that include small business considerations.</p> 
>  
>  
>  
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fos7pdncawa0mgqcin0gf.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fos7pdncawa0mgqcin0gf.png" title="Tim Green" alt="Tim Green"></a></p> 
>  
> <p><strong>Tim Green</strong> <em>UK-based Systems Theorist &amp; Independent Technology Writer</em></p> 
>  
> <p>Tim explores the intersections of artificial intelligence, decentralised cognition, and posthuman ethics. His work, published at <a href="https://smarterarticles.co.uk/">smarterarticles.co.uk</a>, challenges dominant narratives of technological progress while proposing interdisciplinary frameworks for collective intelligence and digital stewardship.</p> 
>  
> <p>His writing has been featured on Ground News and shared by independent researchers across both academic and technological communities.</p> 
>  
> <p><strong>ORCID:</strong> <a href="https://orcid.org/0009-0002-0156-9795">0009-0002-0156-9795</a><br><br> 
> <strong>Email:</strong> <a href="mailto:tim@smarterarticles.co.uk">tim@smarterarticles.co.uk</a></p>

---

## [4/10] AI Adoption Surges While Governance Lags — Report Warns of Growing Shadow Identity Risk
**Source:** DevOps.com | **Date:** 2025-12-02T12:00:56.000Z
**URL:** https://devops.com/ai-adoption-surges-while-governance-lags-report-warns-of-growing-shadow-identity-risk/
**Reasoning:** The article is about AI governance, which is not directly related to our primary interests.
**Authors:** cybernewswire

**Content/Abstract:**
> <div><img width="1536" height="1024" src="https://devops.com/wp-content/uploads/2025/12/AI-Data-Security-Report-Image-2_1764604257jUacyOMyCu.jpeg" alt="" style="margin-bottom:0px;"></div><img width="150" height="150" src="https://devops.com/wp-content/uploads/2025/12/AI-Data-Security-Report-Image-2_1764604257jUacyOMyCu-150x150.jpeg" alt="">Baltimore, MD, 2nd December 2025, CyberNewsWire

---

## [4/10] OpenAI Language Translation: Pros & Cons for Enterprises
**Source:** The Practical Developer | **Date:** 2025-12-02T11:50:06.000Z
**URL:** https://dev.to/jennamitchell/openai-language-translation-pros-cons-for-enterprises-32e8
**Reasoning:** The article is about language translation using OpenAI, which is not directly related to our primary interests.
**Authors:** Jenna Mitchell

**Content/Abstract:**
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fr87yq3bmiaxy03meh6p6.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fr87yq3bmiaxy03meh6p6.png" alt="" width="800" height="499"></a></p> 
>  
> <p>Is your organization interested in leveraging OpenAI for language translation via ChatGPT or API? OpenAI and its popular generative pre-trained models offer many benefits, but there are also some potential drawbacks to consider.</p> 
>  
> <p>Regardless, OpenAI and it’s popular Large Language Model (LLM) ChatGPT are cutting-edge technologies that will revolutionize the language translation industry.</p> 
>  
> <p>In fact, it’s happening as you read this article.</p> 
>  
> <h2> 
>    
>    
>   Background and what you’ll learn 
> </h2> 
>  
> <p>As language industry veterans and software developers, we have first-hand experience in harnessing artificial intelligence for natural language processing. In fact, it’s our mission at Pairaphrase to make translation fast, smart and safe for multinational enterprise organizations.</p> 
>  
> <p>While Machine Translation has been around since the 80’s, the growing popularity of AI-powered translation services is now amplified with the buzz surrounding ChatGPT and Agentic Translation solutions. OpenAI's advanced language model offers powerful translation capabilities and productivity gains.</p> 
>  
> <p>However, there are pros and cons to using OpenAI for language translation. Oftentimes, the pros and cons depend on the content you intend to translate with OpenAI.</p> 
>  
> <p>Familiarize yourself with the following advantages and disadvantages of translating with ChatGPT/OpenAI. As a result, you can make an informed decision for your organization.</p> 
>  
> <h2> 
>    
>    
>   Pros of Using OpenAI for Language Translation 
> </h2> 
>  
> <h3> 
>    
>    
>   Accuracy 
> </h3> 
>  
> <p>While there have been some concerns about accuracy and hallucinations, OpenAI's language model will oftentimes produce accurate translations. This is due to its extensive training on vast amounts of pre-existing data.</p> 
>  
> <p>More time is needed to understand how accurate OpenAI translation is.</p> 
>  
> <p>There are multiple benefits to using a highly accurate translation tool to translate text. These include:</p> 
>  
> <p>Decent first-draft translation</p> 
>  
> <p>Lack of misspellings</p> 
>  
> <p>Less edits required</p> 
>  
> <h3> 
>    
>    
>   Broad Language Support 
> </h3> 
>  
> <p>If your organization needs to translate a wide range of languages and language pairs, it’s possible with OpenAI. It supports almost any language pair your organization requires for commercial use.</p> 
>  
> <h3> 
>    
>    
>   Speed &amp; Efficiency 
> </h3> 
>  
> <p>Its fast production of translations (compared to pure human translation) makes OpenAI suitable for real-time translation needs or situations that require high-volume translation.</p> 
>  
> <p>You can integrate the OpenAI API into your organization’s other software applications such as <a href="https://www.pairaphrase.com/blog/translation-management-systems">Translation Management Systems</a> (TMS) and Content Management Systems (CMS). All you need is the API key.</p> 
>  
> <p>Alternatively, you can use translation software that comes with ChatGPT built into the application.</p> 
>  
> <h3> 
>    
>    
>   Contextual Understanding 
> </h3> 
>  
> <p>Want to produce coherent and contextually accurate translations? This is possible due to OpenAI's ability to comprehend and interpret the context of the text being translated.</p> 
>  
> <p>While this is true in many instances, it’s important to read the “cons” list in this article to grasp the nuances of this.</p> 
>  
> <p>OpenAI uses large language models (LLMs). This language model makes predictions based on the large amounts of data on which it’s been pre-trained. Compare this with Natural Language Processing (NLP) which uses the immediate context of your text to generate a translation.</p> 
>  
> <h3> 
>    
>    
>   Continuous Improvement 
> </h3> 
>  
> <p>OpenAI's language models are continuously updated and refined, ensuring that translation quality can improve over time as the model learns from new data. Continuous improvement means your organization will need to spend less hours translating as time moves forward.</p> 
>  
> <h3> 
>    
>    
>   Ideal for Consumer-Facing Content 
> </h3> 
>  
> <p>When you use OpenAI for language translation, you can expect it to be less “dry” than word-for-word translation. This makes it great for consumer-facing text and entertainment content. In other words, it’s ideal for transcreation.</p> 
>  
> <p>Whether you’re translating fictional literature, video scripts or advertising copy, OpenAI is a good option.</p> 
>  
> <h2> 
>    
>    
>   Cons of Using OpenAI for Language Translation 
> </h2> 
>  
> <h3> 
>    
>    
>   OpenAI May Not Be Ideal for Technical Translation 
> </h3> 
>  
> <p>Need to develop translations for technical concepts or straight-forward texts? Don’t use OpenAI for that. Based on our professional experience with various machine translation engines, we recommend Google and Microsoft’s engines for those types of translation projects.</p> 
>  
> <p>Google and Microsoft stick to word-for-word translations as per traditional translation methodology. It shouldn’t be treated as a replacement for traditional translation methodology.</p> 
>  
> <p>OpenAI is a complementary Machine Translation option, as it’s more suited for consumer-facing texts, as mentioned in the “pros” list above.</p> 
>  
> <p>Tip: Familiarize yourself with the best translation engine to use by content type.</p> 
>  
> <h3> 
>    
>    
>   Non-English Translations Might Suffer 
> </h3> 
>  
> <p>If you have a lot of content with the source language as English, OpenAI will likely generate better translations for you. This is because it’s pre-trained on mostly English due to the existence of more English language content on the internet.</p> 
>  
> <p>Compare this to foreign language content, of which there is naturally less abundance online. This is because there’s less volume of published content in those languages.</p> 
>  
> <h3> 
>    
>    
>   Limited Subject Matter Expertise 
> </h3> 
>  
> <p>While it’s trained on large datasets, be cautious when using OpenAI for translation of specialized knowledge. It will likely present knowledge gaps in your texts.</p> 
>  
> <p>This applies for non-technical texts, too. For example, if you were to transcreate a science fiction novel, this would involve translating scientific concepts–albeit fictional. This means even a literary translation of a fictional technical concept could be negatively affected by relying on artificial intelligence alone.</p> 
>  
> <h3> 
>    
>    
>   Trouble with Ambiguous Text 
> </h3> 
>  
> <p>Pay careful attention to this one if your source text includes ambiguous text made complex by cultural references, nuances or idiomatic expressions or nuances. This applies to machine translation in general (not only when you translate using OpenAI).</p> 
>  
> <h3> 
>    
>    
>   Potential for Bias &amp; Inaccurate Data 
> </h3> 
>  
> <p>When you translate using OpenAI, it’s important to note that its translation quality relies on the quality of the data that was used to train it. Biased or inaccurate training data may affect the accuracy and reliability of OpenAI's translations.</p> 
>  
> <p>This is a well-known weakness of OpenAI, and it applies for more use cases than just translation.</p> 
>  
> <h3> 
>    
>    
>   Security &amp; Privacy Concerns 
> </h3> 
>  
> <p>When you use OpenAI/ChatGPT, it warns you not to enter confidential information into the chatbot.</p> 
>  
> <p>If you’re going to translate sensitive or confidential information, make sure you use OpenAI via a translation management system that doesn’t return data to machine translation engines. This means your data won’t be sent back to OpenAI.</p> 
>  
> <p>Try Pairaphrase for secure ChatGPT translations.</p> 
>  
> <h3> 
>    
>    
>   Can't Translate a Scanned PDF Document 
> </h3> 
>  
> <p>ChatGPT itself cannot translate a scanned document on its own without help from external tools. It doesn't have the capability to extract the text from the scanned document for translation.</p> 
>  
> <p>However, you can accomplish this with an AI PDF translator such as Pairaphrase.</p> 
>  
> <p>It is translation software integrated with ChatGPT AND it includes optical character recognition (OCR). This means you can extract the text using OCR and translate your scanned document, all without leaving the Pairaphrase application.</p> 
>  
> <h2> 
>    
>    
>   Key Takeaway 
> </h2> 
>  
> <p>AI-assisted translation can completely transform your organization’s productivity.</p> 
>  
> <p>However, it’s important to always use human oversight when using AI. If you decide to use OpenAI/ChatGPT for translation, make sure you fully understand your organization’s policies on data security and quality assurance.</p> 
>  
> <p>And remember, you might be better off using other translation engines for technical translations.</p> 
>  
> <p>Dive Deeper: Explore the 17 best AI translators for enterprise teams</p> 
>  
> <p>Now that you understand OpenAI's translation strengths and weaknesses, we'll recommend a solution below that gives you the best of both LLM translation and non-LLM translation in a secure environment.</p> 
>  
> <h2> 
>    
>    
>   Conclusion: 
> </h2> 
>  
> <p>OpenAI’s translation capabilities offer impressive speed, adaptability and creativity, making them a strong option for organizations seeking to enhance productivity and improve consumer-facing content. However, these benefits come with important considerations—ranging from accuracy limitations in technical domains to privacy concerns and variability in non-English performance. With the right safeguards, human oversight, and a clear understanding of when OpenAI is (and isn’t) the best tool for the job, organizations can harness AI translation effectively. Ultimately, the strongest approach pairs LLM-powered innovation with traditional machine translation and secure translation workflows to achieve the best balance of quality, reliability and data protection.</p> 
>  
> <p><em>This content was originally published <a href="https://www.pairaphrase.com/blog/openai-language-translation">here</a></em></p>

---

## [4/10] Converted all Behat WebAPIExtension step definitions to Node.js, packaged in Webship-JS
**Source:** The Practical Developer | **Date:** 2025-12-02T11:27:58.000Z
**URL:** https://dev.to/webshipco/converted-all-behat-webapiextension-step-definitions-to-nodejs-packaged-in-webship-js-5fh0
**Reasoning:** The article is about converting API testing steps to Node.js, which is not directly related to our primary or secondary interests.
**Authors:** webship.co

**Content/Abstract:**
> <p>Behat’s <a href="https://github.com/Behat/WebApiExtension">WebAPIExtension</a> was a simple and effective way to test JSON-based APIs using Gherkin steps. On July 14, 2025, the repository was archived by its owner and became read-only, meaning it’s no longer maintained, but not abandoned in purpose.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2mqtx2r007m8ec10y8oh.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2mqtx2r007m8ec10y8oh.png" alt="" width="800" height="481"></a></p> 
>  
> <p>At Webship.co, we saw great value in the extension’s structured API-testing approach. So instead of letting it fade away, we rebuilt all of its step definitions in Node.js and integrated them directly into Webship-JS.<br> 
> Now, teams can use the same clear BDD style, but with modern JavaScript tooling and active support.</p> 
>  
> <p><strong>Why WebAPIExtension was worth reviving</strong></p> 
>  
> <ul> 
> <li> It made API testing readable and easy to understand.</li> 
> <li>    It supported setting headers, sending JSON bodies, checking status codes, and validating responses.</li> 
> <li>    It helped teams describe API behavior in a simple Given/When/Then format.</li> 
> </ul> 
>  
> <p>We kept all these strengths, and enhanced them.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsnvbnqvf6z8671lhuxk4.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsnvbnqvf6z8671lhuxk4.png" alt="" width="800" height="550"></a></p> 
>  
> <p><strong>What’s new in Webship-js</strong></p> 
>  
> <p>Our rebuilt API steps now support:</p> 
>  
> <ul> 
> <li>Setting headers and request bodies</li> 
> <li>Sending all HTTP methods (GET, POST, PUT, DELETE…)</li> 
> <li>Validating status codes</li> 
> <li>Matching JSON responses, including nested fields</li> 
> <li>Checking response headers</li> 
> <li>Using matcher patterns (regex, array length, JWT, etc.)</li> 
> </ul> 
>  
> <p>*<em>Example: *</em><br> 
> Sends a request (POST, PUT, etc.) to the given endpoint, using the values listed in the table as the JSON request body.<br> 
> </p> 
>  
> <div> 
> <pre><code>When I send a POST request to "/users" with values: 
>             | name  | John Doe         | 
>             | email | john@example.com | 
>             | age   | 30               | 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Full documentation of the steps is available here:<br> 
> <a href="https://webship.co/docs/webship-js/1.0.x/api-step-definitions">https://webship.co/docs/webship-js/1.0.x/api-step-definitions</a></p> 
>  
> <p><strong>Conclusion</strong></p> 
>  
> <p>Although the original WebAPIExtension is now archived, its value continues.<br> 
> With our Node.js rebuild, Webship-JS brings it back — cleaner, faster, and actively maintained.<br> 
> The same familiar BDD API experience, but modern and ready for real projects today.</p>

---

## [4/10] Fundamentals of Large Language Models: Understanding LLM Architectures
**Source:** The Practical Developer | **Date:** 2025-12-02T11:15:31.000Z
**URL:** https://dev.to/derrickryangiggs/fundamentals-of-large-language-models-understanding-llm-architectures-446j
**Reasoning:** The article provides a basic understanding of LLM architectures, which is general AI knowledge not directly impacting coding.
**Authors:** Ryan Giggs

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F0liru52os1p4ivsw7cyr.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><h2>  
>     
>     
>   What is an LLM?  
> </h2>  
>   
> <p>An LLM (Large Language Model) is fundamentally a probabilistic model that predicts distributions over vocabulary tokens. At its core, an LLM understands a fixed set of words called a <strong>vocabulary</strong> and assigns probabilities to each word appearing in a given context. </p>  
>   
> <p>The "Large" in LLM refers to the <strong>number of parameters</strong> the model contains. These models can have billions or even trillions of parameters, allowing them to capture complex patterns in language. While there's no universally agreed-upon threshold for what constitutes "large," modern LLMs typically range from hundreds of millions to hundreds of billions of parameters.</p>  
>   
> <h3>  
>     
>     
>   Understanding Vocabulary Size  
> </h3>  
>   
> <p>Recent research has revealed that vocabulary size plays a crucial role in LLM performance, with optimal vocabulary sizes depending on the compute budget used for training. Most modern LLMs use vocabulary sizes ranging from 30,000 to 100,000 tokens, though this varies significantly:</p>  
>   
> <ul>  
> <li>  
> <strong>BERT</strong>: 30,000 tokens (WordPiece tokenization)</li>  
> <li>  
> <strong>GPT-3</strong>: ~50,000 tokens</li>  
> <li>  
> <strong>Llama 2</strong>: 32,000 tokens</li>  
> <li>  
> <strong>RoBERTa</strong>: 50,265 tokens (Byte-Pair Encoding)</li>  
> <li>  
> <strong>T5</strong>: 32,128 tokens</li>  
> </ul>  
>   
> <p>Research suggests that many existing LLMs actually use suboptimal vocabulary sizes - for example, Llama2-70B's optimal vocabulary size should have been at least 216,000 tokens, seven times larger than its actual 32,000-token vocabulary.</p>  
>   
> <h2>  
>     
>     
>   LLM Architectures: The Three Main Types  
> </h2>  
>   
> <p>Modern transformer models use one of three fundamental architectures: encoder-only, decoder-only, or encoder-decoder (sequence-to-sequence). Each architecture is optimized for different types of tasks.</p>  
>   
> <h3>  
>     
>     
>   1. Encoder-Only Models  
> </h3>  
>   
> <p>Encoders are designed to convert sequences of words into vector representations (embeddings) that can be used for various predictive modeling tasks such as classification. These models use <strong>bidirectional attention</strong>, meaning they can look at context from both directions simultaneously.</p>  
>   
> <p><strong>Key Characteristics:</strong></p>  
>   
> <ul>  
> <li>Use bidirectional attention to access all words in the input sentence</li>  
> <li>Specialized in understanding and analyzing text</li>  
> <li>Pretrained using masked language modeling objectives</li>  
> </ul>  
>   
> <p><strong>Popular Encoder Models:</strong></p>  
>   
> <ul>  
> <li><p><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: Introduced in October 2018 by Google researchers, BERT uses 12-24 layers (depending on the variant) and dramatically improved the state of the art for many NLP tasks. The base model has 110M parameters, while the large model has 340M parameters.</p></li>  
> <li><p><strong>RoBERTa (Robustly Optimized BERT Approach)</strong>: An improved version of BERT that modifies the training procedure by removing the next-sentence prediction task, using larger mini-batches, and training on 160GB of text (10x more than BERT). It has 355M parameters in its large version.</p></li>  
> <li><p><strong>DistilBERT</strong>: A distilled version that retains 95% of BERT's performance with only 60% of its parameters (66M).</p></li>  
> <li><p><strong>DeBERTa</strong>: Uses disentangled attention mechanisms for improved performance.</p></li>  
> <li><p><strong>ModernBERT</strong>: Released in 2024 as a state-of-the-art replacement for BERT, featuring an 8,192 token sequence length and significantly faster processing.</p></li>  
> </ul>  
>   
> <p><strong>Primary Use Cases:</strong></p>  
>   
> <ul>  
> <li>Text classification (sentiment analysis, topic classification)</li>  
> <li>Named entity recognition (NER)</li>  
> <li>Question answering</li>  
> <li>Semantic similarity tasks</li>  
> <li>Embedding generation for retrieval systems</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   2. Decoder-Only Models  
> </h3>  
>   
> <p>Decoders are designed to generate new text by predicting the next word in a sequence. They use masked (causal) self-attention, which only allows tokens to attend to previous tokens in the sequence, ensuring autoregressive generation.</p>  
>   
> <p><strong>Key Characteristics:</strong></p>  
>   
> <ul>  
> <li>Use unidirectional (causal) attention</li>  
> <li>Generate text one token at a time</li>  
> <li>Excel at creative text generation and completion</li>  
> </ul>  
>   
> <p><strong>Popular Decoder Models:</strong></p>  
>   
> <ul>  
> <li><p><strong>GPT Series (GPT, GPT-2, GPT-3, GPT-4)</strong>: The GPT series pioneered the decoder-only architecture, with models ranging from 117M parameters (GPT-2 small) to hundreds of billions in GPT-4. GPT models became state of the art in natural language generation starting in 2018.</p></li>  
> <li><p><strong>Llama (1, 2, 3)</strong>: Meta's open-source models that have become foundational for many smaller LLM projects, with the Llama 2 family ranging up to 70 billion parameters.</p></li>  
> <li><p><strong>Falcon</strong>: A series of open-source models trained on refined web data.</p></li>  
> <li><p><strong>BLOOM</strong>: A multilingual decoder-only model.</p></li>  
> <li><p><strong>Mistral</strong>: High-performance open-weight models with efficient architectures.</p></li>  
> </ul>  
>   
> <p><strong>Primary Use Cases:</strong></p>  
>   
> <ul>  
> <li>Text generation and completion</li>  
> <li>Conversational AI and chatbots</li>  
> <li>Creative writing assistance</li>  
> <li>Code generation</li>  
> <li>Question answering in chat format</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   3. Encoder-Decoder Models  
> </h3>  
>   
> <p>Encoder-decoder models combine both architectures, using an encoder to understand the input and a decoder to generate appropriate output text. This makes them perfect for tasks that transform one sequence into another, like translation or summarization.</p>  
>   
> <p><strong>Key Characteristics:</strong></p>  
>   
> <ul>  
> <li>Bidirectional understanding through the encoder</li>  
> <li>Autoregressive generation through the decoder</li>  
> <li>Connected via cross-attention mechanism</li>  
> <li>Ideal for sequence-to-sequence tasks</li>  
> </ul>  
>   
> <p><strong>Popular Encoder-Decoder Models:</strong></p>  
>   
> <ul>  
> <li><p><strong>T5 (Text-to-Text Transfer Transformer)</strong>: Developed by Google, T5 treats every NLP task as a text-to-text problem, ranging from 60M to 11B parameters. It uses task-specific prefixes (e.g., "translate English to German:", "summarize:") to handle different tasks with the same architecture.</p></li>  
> <li><p><strong>BART (Bidirectional and Auto-Regressive Transformers)</strong>: Developed by Facebook (Meta) in 2019, BART combines strengths of BERT and GPT. It's pretrained by corrupting text in various ways (deleting words, shuffling sentences, masking tokens) and learning to reconstruct the original.</p></li>  
> <li><p><strong>UL2</strong>: A unified framework for language understanding and generation.</p></li>  
> <li><p><strong>mT5</strong>: Multilingual variant of T5 supporting 100+ languages.</p></li>  
> </ul>  
>   
> <p><strong>Primary Use Cases:</strong></p>  
>   
> <ul>  
> <li>Machine translation</li>  
> <li>Text summarization</li>  
> <li>Question answering with context</li>  
> <li>Text simplification</li>  
> <li>Paraphrasing</li>  
> <li>Data-to-text generation</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   The Transformer Foundation  
> </h2>  
>   
> <p>All these architectures are built on the transformer architecture, which uses self-attention mechanisms to process input sequences. The original transformer was proposed in the 2017 paper "Attention Is All You Need" by Google researchers.</p>  
>   
> <p><strong>Key Components of Transformers:</strong></p>  
>   
> <ol>  
> <li><p><strong>Tokenization</strong>: Converting text into discrete tokens using algorithms like Byte-Pair Encoding (BPE), WordPiece, or SentencePiece.</p></li>  
> <li><p><strong>Embeddings</strong>: Converting tokens into dense vector representations.</p></li>  
> <li><p><strong>Self-Attention Layers</strong>: Mechanisms that allow each token to attend to other tokens in the sequence, capturing contextual relationships.</p></li>  
> <li><p><strong>Feed-Forward Networks</strong>: Processing representations through neural networks.</p></li>  
> <li><p><strong>Layer Normalization</strong>: Stabilizing training and improving convergence.</p></li>  
> <li><p><strong>Positional Encodings</strong>: Adding position information since attention has no inherent notion of sequence order.</p></li>  
> </ol>  
>   
> <h2>  
>     
>     
>   Choosing the Right Architecture  
> </h2>  
>   
> <p>When selecting an architecture for a specific task, consider whether you need bidirectional understanding (encoder), text generation (decoder), or sequence transformation (encoder-decoder).</p>  
>   
> <p><strong>Decision Framework:</strong></p>  
>   
> <ul>  
> <li>  
> <strong>Need deep understanding of text?</strong> → Use encoder-only models</li>  
> <li>  
> <strong>Need to generate creative or conversational text?</strong> → Use decoder-only models</li>  
> <li>  
> <strong>Need to transform one text form to another?</strong> → Use encoder-decoder models</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   Recent Trends and Future Directions  
> </h2>  
>   
> <p>Almost every major LLM since GPT-3 has adopted the decoder-only architecture due to its simplicity and effectiveness at scale. However, encoder models remain critical for tasks like retrieval-augmented generation (RAG), classification, and entity extraction, with billions of downloads per month.</p>  
>   
> <p>Recent developments include reasoning models like OpenAI's o1 and DeepSeek-R1, which generate step-by-step analysis before producing answers, achieving better performance on complex tasks.</p>  
>   
> <p>Alternative architectures like Mamba (state space models) are emerging as potential challengers to transformer dominance, offering linear-time processing for long sequences.</p>  
>   
> <p>Understanding the fundamental differences between encoder, decoder, and encoder-decoder architectures is essential for working effectively with LLMs. Each architecture serves distinct purposes:</p>  
>   
> <ul>  
> <li>  
> <strong>Encoders</strong> excel at understanding and analyzing text</li>  
> <li>  
> <strong>Decoders</strong> shine at generating creative and coherent text</li>  
> <li>  
> <strong>Encoder-decoders</strong> bridge both worlds for transformation tasks</li>  
> </ul>  
>   
> <p>As the field continues to evolve rapidly, these foundational concepts remain crucial for anyone working with or building upon large language models.</p>  
>   
> <p><em>Are you working with LLMs in your projects? Which architecture have you found most useful for your use case? Share your experiences in the comments below</em></p>

---

## [4/10] Lyft Taps AWS and Anthropic to Launch Agentic AI - eWeek
**Source:** "Anthropic" - Google News | **Date:** 2025-12-02T11:18:20.000Z
**URL:** https://news.google.com/rss/articles/CBMiVkFVX3lxTE1UWlhIMnIyY25wR1NSWnQ1TlhQWlRoYUpDbVVXY0pzUjFSVnF2anBaMDM5cGhlU2c5bkRLX3NqaDRjT2ZGa1JGSUVtSVlYTTZGam03RFJn?oc=5
**Reasoning:** The article mentions a collaboration to launch agentic AI, but lacks details on how it impacts software engineering or coding.

**Content/Abstract:**
> <a href="https://news.google.com/rss/articles/CBMiVkFVX3lxTE1UWlhIMnIyY25wR1NSWnQ1TlhQWlRoYUpDbVVXY0pzUjFSVnF2anBaMDM5cGhlU2c5bkRLX3NqaDRjT2ZGa1JGSUVtSVlYTTZGam03RFJn?oc=5">Lyft Taps AWS and Anthropic to Launch Agentic AI</a>  eWeek

---

## [4/10] Agentgateway Review: A Feature-Rich New AI Gateway
**Source:** The Practical Developer | **Date:** 2025-12-02T11:23:19.000Z
**URL:** https://dev.to/spacewander/agentgateway-review-a-feature-rich-new-ai-gateway-53lm
**Reasoning:** The review of Agentgateway lacks specific details on how it impacts code intelligence or context engineering.
**Authors:** spacewander

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2yg0naa6ipyn3y71oa8b.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><h2>  
>     
>     
>   Introduction  
> </h2>  
>   
> <p><a href="https://github.com/agentgateway/agentgateway">agentgateway</a> is a data plane developed by solo specifically for AI scenarios. The data plane is written in Rust and can be configured via xDS (a gRPC-based protocol) and YAML. Recently they decided to replace kgateway’s AI data plane from Envoy to agentgateway. I expect the enterprise version of Gloo will follow. Previously, most AI-related data-plane features were implemented in Envoy calling a Go sidecar via ext_proc, and I guess the real-world results were mediocre.</p>  
>   
> <p>This gateway supports four AI scenarios:</p>  
>   
> <ol>  
> <li>MCP  
> </li>  
> <li>A2A  
> </li>  
> <li>Proxying inference requests to LLM providers  
> </li>  
> <li>Load balancing for inference services</li>  
> </ol>  
>   
> <p>Below I explain each of these scenarios. Note I’m discussing the open-source agentgateway — some features may exist only in the enterprise edition and are outside the scope of this doc.</p>  
>   
> <h2>  
>     
>     
>   MCP  
> </h2>  
>   
> <p>agentgateway was originally started to address the difficulty of handling stateful MCP requests in existing Envoy data planes. So its MCP support is the most complete.</p>  
>   
> <p>By default, agentgateway treats MCP as a stateful protocol. It has a SessionManager struct responsible for session creation and maintenance (<a href="https://github.com/agentgateway/agentgateway/blob/1ca00e32d3f475539a20120f72c45c05aaf80377/crates/agentgateway/src/mcp/session.rs#L370">code link</a>). But this SessionManager is a local in-process store, which means if you run multiple agentgateway instances there’s no guarantee a client will hit the same SessionManager each time. If you want sticky sessions toward upstreams, it’s actually simpler to consistent-hash on the MCP-Session-ID header so the same session ID routes to the same backend even if requests land on different agentgateway instances. Extending SessionManager to use a remote store is another solution, but it’s more expensive. To me, making MCP stateful by default is a mistake. I’m glad they plan to make MCP a <a href="https://github.com/modelcontextprotocol/modelcontextprotocol/issues/1442">default stateless protocol</a>.</p>  
>   
> <p>When there is more than one backend, agentgateway enables MCP multiplexing. For example, with tools: when listing tools, agentgateway sends tools/list to every backend, then rewrites tool names to the format <code>${backend_name}_${tool_name}</code>. When a tool call comes in, agentgateway routes to the actual backend. For methods that can’t be multiplexed, it returns an invalid method error.</p>  
>   
> <p>Besides forwarding to MCP backends, agentgateway supports converting RESTful APIs to MCP tools using an OpenAPI spec. Impressively, it supports using an entire spec as a backend and includes a fair amount of schema-parsing code. agentgateway positions itself here as an MCP-to-RESTful-API forwarder; it does not itself manage the RESTful APIs described in the OpenAPI spec. Some details are still missing — for example, bodies only support application/json, HTTPS upstreams aren’t supported yet, structured output is not yet supported, etc. There are also finer points (e.g., handling of additionalProperties) I haven’t dug fully into.</p>  
>   
> <p>agentgateway implements OAuth-based MCP authentication. It exposes protected resource metadata at paths like <code>/.well-known/oauth-protected-resource/${resource}</code>. However, if one host contains multiple resources, should each resource’s route-match config explicitly include that resource’s well-known path? Otherwise you can’t guarantee the request will route to the well-known path handler. One nice thing: agentgateway adds CORS headers to metadata responses, so when an MCP client runs in a browser (e.g., the MCP inspector) you don’t need to add a separate CORS middleware.</p>  
>   
> <p>agentgateway fetches public keys from a JWKS path to verify tokens were issued by the corresponding authorization server. There are two JWKS sources:</p>  
>   
> <ol>  
> <li>The user supplies a URL or a file path.  
> </li>  
> <li>The JWKS URL is derived from the issuer URL and issuer type.</li>  
> </ol>  
>   
> <p>The code that gets public keys from JWKS appears to be called <a href="https://github.com/agentgateway/agentgateway/blob/1ca00e32d3f475539a20120f72c45c05aaf80377/crates/agentgateway/src/types/local.rs#L1076">only when parsing configuration</a>. So the JWKS does not seem to be periodically refreshed.</p>  
>   
> <p>Authorization is also implemented via OAuth. It uses a list of CEL expressions as filters, matching on JWT fields and MCP attributes. Example:<br>  
> </p>  
>   
> <div>  
> <pre><code>mcpAuthorization:  
>   rules:  
>   # Allow anyone to call 'echo'  
>   - 'mcp.tool.name == "echo"'  
>   # Only the test-user can call 'add'  
>   - 'jwt.sub == "test-user" &amp;&amp; mcp.tool.name == "add"'  
>   # Any authenticated user with the claim `nested.key == value` can access 'printEnv'  
>   - 'mcp.tool.name == "printEnv" &amp;&amp; jwt.nested.key == "value"'  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Note: in multiplexing scenarios, mcpAuthorization runs before the tool lists are merged, so the tool names here do not include the backend-name prefix.</p>  
>   
> <p>agentgateway provides surprisingly few MCP-related metrics — basically just an mcp_requests counter — so you can’t see details like which tools are taking the most time.</p>  
>   
> <h2>  
>     
>     
>   A2A  
> </h2>  
>   
> <p>For A2A protocol scenarios, agentgateway implements two main features:</p>  
>   
> <ol>  
> <li>Rewrites agent card URLs so they point to the gateway instead of the proxied backend.  
> </li>  
> <li>Parses A2A JSON requests and records the request method for observability.</li>  
> </ol>  
>   
> <h2>  
>     
>     
>   Proxying inference requests to LLM providers  
> </h2>  
>   
> <p>Like other AI gateways, agentgateway can proxy inference requests to LLM providers. This proxying is not just raw forwarding: it adds value such as token-based observability and rate-limiting.</p>  
>   
> <p>When proxying SSE traffic it collects token usage and TTFT metrics. For non-SSE streaming formats (e.g., Bedrock’s AWS event stream) it provides dedicated parsers.</p>  
>   
> <p>I’ll dive into rate limiting, prompt protection, and related features in a follow-up.</p>  
>   
> <p>Another common capability is to lift some LLM client features into the gateway to reduce integration work — for example, smoothing differences between providers and offering an OpenAI-compatible external API.</p>  
>   
> <p>agentgateway supports this to an extent. Its design is not a full generic "X provider to Y provider" converter; instead it implements conversions for specific routing types. Currently it supports two route types:</p>  
>   
> <ol>  
> <li>OpenAI’s /v1/chat/completions  
> </li>  
> <li>Anthropic’s /v1/messages</li>  
> </ol>  
>   
> <p>In practice both /v1/chat/completions and /v1/messages are chat-style routes: OpenAI’s /v1/chat/completions is functionally equivalent to Anthropic’s /v1/messages. They implemented both separately for quick business onboarding: many code agents only implement Anthropic’s /v1/messages, and special-casing that endpoint makes it easy to immediately accept such clients. Implementing a full Anthropic-to-any-provider converter would be a much larger effort.</p>  
>   
> <p>This area is currently roughly sufficient but incomplete. Putting aside support for embeddings, batching, etc., agentgateway does not fully support /v1/chat/completions yet — for example, <a href="https://platform.openai.com/docs/guides/structured-outputs">structured output</a> is not supported at the moment.</p>  
>   
> <h2>  
>     
>     
>   Inference Extension Support  
> </h2>  
>   
> <p>When the gateway API inference extension (<a href="https://gateway-api-inference-extension.sigs.k8s.io/">https://gateway-api-inference-extension.sigs.k8s.io/</a>) first appeared I was skeptical. Distributed inference is a systems engineering problem; it feels presumptuous for a single scheduler implementation to try to become the standard. But with Red Hat driving the LLMD project and treating the inference extension as part of an out-of-the-box experience, the inference extension may gain traction. Red Hat has invested heavily in AI projects (e.g., vLLM) and has the resources to advance this work.</p>  
>   
> <p>Supporting the inference extension is actually not hard. The gateway needs to forward inference requests to a scheduler (called EPP in the inference extension) via Envoy’s gRPC ext_proc protocol. The scheduler’s response includes an x-gateway-destination-endpoint header that contains the target upstream address. The gateway then forwards the inference request to that address. Practically speaking the gateway is only doing forwarding here; the core logic lives in the scheduler. I’ve wondered: if the entire request is sent to the scheduler, why not let the scheduler process the request directly instead of having the gateway forward it? Is the scheduler only capable of handling input tokens and not output tokens?</p>  
>   
> <p>What’s the value of a self-hosted inference system? I think it’s to, under data-security constraints, be reasonably cost-competitive with external LLM providers. Large-model inference benefits from scale economics greatly — a self-hosted system is unlikely to beat cloud providers purely on price. To be more cost-effective you need scheduling innovations (e.g., better load balancing, more flexible disaggregated serving). If inference-extension support just means forwarding requests to the official scheduler, then the gateway isn’t adding meaningful value in that part of the chain.</p>  
>   
> <h2>  
>     
>     
>   Summary  
> </h2>  
>   
> <p>In summary, agentgateway is impressive for a project that’s been developed for only about half a year. Its feature richness stands out. It shows a clear focus on AI scenarios, and its ambition to rebuild the data plane in Rust (to replace the prior Envoy + Go external process approach) demonstrates strong intent and potential to address AI-specific protocols and performance needs.</p>  
>   
> <p>However, the documentation is incomplete: some implemented features (e.g., Anthropic /v1/messages support) <a href="https://github.com/agentgateway/website/blob/02e25020b185ed34c66704d6274708a24ffe098d/content/docs/llm/about.md?plain=1#L30">aren’t documented</a>, while some documented items don’t exist in the code (e.g., the MCP metric list_calls_total referenced in the docs: <a href="https://github.com/agentgateway/website/blob/02e25020b185ed34c66704d6274708a24ffe098d/content/docs/mcp/mcp-observability.md?plain=1#L18">https://github.com/agentgateway/website/blob/02e25020b185ed34c66704d6274708a24ffe098d/content/docs/mcp/mcp-observability.md?plain=1#L18</a>). Overall these are typical, understandable issues for a rapidly iterating early-stage open-source project and do not substantially detract from the project’s promise.</p>

---

## [4/10] YAMLpp is dynamic,self-generating YAML
**Source:** The Practical Developer | **Date:** 2025-12-02T11:15:36.000Z
**URL:** https://dev.to/fralau/yamlpp-is-dynamicself-generating-yaml-2cb8
**Reasoning:** The article discusses dynamic YAML, which is not directly related to our primary or secondary interests.
**Authors:** Laurent Franceschetti

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fhztvaan01dltr0g6d7m6.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p><strong><a href="https://yaml.org/">YAML</a></strong> is a great file format, used for writing configuration files; and more and more, as a foundation for Domain Specific Languages (DSLs), such as script files for Github workflows, Docker or Kubernetes.</p>  
>   
> <h2>  
>     
>     
>   How to make YAML dynamic?  
> </h2>  
>   
> <p>We have all, however, suffered from a frustration: <strong>YAML is fundamentally a static file format</strong>. How can we make it dynamic? </p>  
>   
> <p>It starts with the fact that some string, like <code>http://dev.company.local</code> appears several times across the files.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>server</span><span>:</span>  
>   <span>url</span><span>:</span> <span>http://dev.company.local</span>  
>   <span>...</span>  
>   
> <span>documentation</span><span>:</span>  
>   <span>url</span><span>:</span> <span>http://dev.company.local/docs</span>  
>   <span>...</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>And the day that address changes, what do we do?</p>  
>   
> <ol>  
> <li> Do a <strong>search/replace</strong> in the config files? Nah... <em>this is brittle and things will get worse before they get better</em>. And in any case, it doesn't solve many cases where a "small" change of parameter (from <code>dev</code> to <code>test</code> and from <code>test</code> to <code>live</code>) leads to significant changes in the YAML tree.</li>  
> <li>All right, then solve it with a little bit of <strong>templating</strong>, using Jinja (Python) or the Go native engine. Solved! ...Really? Everything is fine for a while, and then <em>the workflow crashes because some corner case broke the YAML syntax</em>. And we start fiddling-fiddling with spaces and escaping and newlines... and after 5 or 10 abort-debug-rerun cycles, the delivery is running late.</li>  
> <li>OK, what we can do now? Use a <strong>preprocessing language</strong> guaranteed to produce valid syntax, like <a href="https://jsonnet.org/">Jsonnet</a> for json? That wil work, for sure. However, you have now a <em>friction</em> problem: it's new file format that requires implementing its own toolchain (Jsonnet is not json, so editor add-ons, parsers, interpreters, libraries, etc.) and you will need to drive the team through the learning curve of a language with its own syntax and semantic. Will you implement a serious project... or will you have a coffee break at that point, and keep doing things the same old way?</li>  
> </ol>  
>   
> <h2>  
>     
>     
>   Enter YAMLpp  
> </h2>  
>   
> <p>Faced with that <strong>problem</strong>, I decided that it needed a <strong>straightforward solution</strong>, which required basically only <em>one</em> utility (a pre-processor) to convert files into static YAML, and would change nothing else anywhere.</p>  
>   
> <p>Enter <strong><a href="https://yamlpp.readthedocs.io/en/latest/">YAMLpp</a></strong> (<em>YAML Pre-Processor</em>). It is a language that makes YAML dynamic, but <em>it is itself YAML</em>. So you can use your current editor which does your color highlighting, syntax checking, and so on.</p>  
>   
> <p>Let's start with a Hello World example:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>.context</span><span>:</span>  
>   <span>name</span><span>:</span> <span>"</span><span>World"</span>  
>   
> <span>message</span><span>:</span> <span>"</span><span>Hello</span><span> </span><span>{{</span><span> </span><span>name</span><span> </span><span>}}!"</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>I don't think there is a lot to explain here. <code>.context</code> is called a <strong>construct</strong>, because it 'constructs' (builds) a YAML sub-tree or does something useful; in this case it defines a variable <code>name</code> and leaves no trace.</p>  
>   
> <p>All constructs start with a <code>.</code> (a dot). In YAMLpp, <code>.context</code> is known as a <strong>keyword</strong>.</p>  
>   
> <p>Then <code>message</code> contains a string that will be interpolated using the <code>name</code> variable.</p>  
>   
> <p>Here is the result:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>message</span><span>:</span> <span>"</span><span>Hello</span><span> </span><span>World"</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>As you see, the node <code>.context</code> disappeared. That's a rule for all YAMLpp constructs: they disappear and never make it to the target file.</p>  
>   
> <h2>  
>     
>     
>   YAMLpp is a metaprogramming tool  
> </h2>  
>   
> <p>It goes much further than that. </p>  
>   
> <p>It is a <strong>macro language</strong> written in YAML, the same language as the target language YAML.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>.context</span><span>:</span>  
>   <span>.platform</span><span>:</span> <span>dev</span>  
>   
> <span>server</span><span>:</span>  
>   <span>.if</span><span>:</span>  
>     <span>.if</span><span>:</span> <span>"</span><span>(platform,</span><span> </span><span>{{</span><span> </span><span>prod</span><span> </span><span>}})"</span>  
>     <span>.then</span><span>:</span>  
>       <span>url</span><span>:</span> <span>prod.machine.local</span>  
>       <span>...</span>  
>     <span>.else</span><span>:</span>  
>       <span>...</span>  
>       <span>url</span><span>:</span> <span>dev.machine.local</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>The result is thus:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>server</span><span>:</span>  
>    <span>url</span><span>:</span> <span>dev.machine.local</span>  
>    <span>...</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>The <code>.if</code> construct is <strong>expanded</strong>: <code>.if</code>, <code>.then</code> and <code>.else</code> keywords will disappear, and only the intended final YAML nodes will remain.</p>  
>   
> <p><strong>For anyone who had anything to do with LISP or its offspring languages, you know that a language that generates itself through macros, is a huge deal, because it opens the doors to <em>meta-programming</em>.</strong> In essence, are able to create your own <strong>Domain-Specific Language</strong> (DSL), for configuration or scripting.</p>  
>   
> <p>For the others, what you write in <a href="https://yamlpp.readthedocs.io/en/latest/">YAMLpp</a> is still YAML, but allows you to manipulate your target YAML tree in any way you please. You are now able to express <strong>high-level concepts</strong> in your config file or script in a way that is meaningful to <em>you</em> and your team (expressive, shorter, non-repetitive), which will be <strong>expanded</strong> into a form (YAML) that the machine will be able to test/interpret correctly and that everyone else will understand -- because its part of the standard specification. <br>  
> So now, YAML can be either dynamic (when it contains YAMLpp keywords) or static (without them). But regardless, it is still valid YAML.</p>  
> <h2>  
>     
>     
>   Where do I go from there?  
> </h2>  
>   
> <p>A working prototype was developed in Python and it already has its test suite (with pytest). </p>  
>   
> <p>To install it:<br>  
> </p>  
>   
> <div>  
> <pre><code>pip <span>install </span>yamlppx  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>(Note the added 'x').</p>  
>   
> <p>You just need one command:<br>  
> </p>  
>   
> <div>  
> <pre><code>yamlpp input.yaml <span>-o</span> output.yaml  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>For more information see the <a href="https://github.com/fralau/yamlpp">Github repo</a> and the <a href="https://yamlpp.readthedocs.io/">documentation on ReadTheDocs</a>.</p>  
>   
> <p>You can also look at a <a href="https://yamlpp.readthedocs.io/en/latest/guide/">webpage containing two realistic use cases, one for Kubernetes and one for Docker</a>.</p>

---

## [4/10] Code Smell 315 - Cloudflare Feature Explosion
**Source:** The Practical Developer | **Date:** 2025-12-02T11:00:00.000Z
**URL:** https://dev.to/mcsee/code-smell-315-cloudflare-feature-explosion-28p9
**Reasoning:** The article discusses system configuration issues, which is not directly relevant to our interests.
**Authors:** Maxi Contieri

**Content/Abstract:**
> <p><em>When bad configuration kills all internet proxies</em></p> 
>  
> <blockquote> 
> <p>TL;DR: Overly large auto-generated config can crash your system.</p> 
> </blockquote> 
>  
> <h1> 
>    
>    
>   Problems 😔 
> </h1> 
>  
> <ul> 
> <li>Config overload</li> 
> <li> 
> <a href="https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb">Hardcoded</a> limit</li> 
> <li>Lack of validations</li> 
> <li>Crash on overflow</li> 
> <li>Fragile <a href="https://dev.to/mcsee/coupling-the-one-and-only-software-design-problem-2pd7">coupling</a> 
> </li> 
> <li>Cascading Failures</li> 
> <li><a href="https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3">Hidden Assumptions</a></li> 
> <li>Silent duplication</li> 
> <li>Unexpected Crashes</li> 
> <li>Thread panics in critical paths</li> 
> <li>Treating internal data as trusted input</li> 
> <li>Poor observability</li> 
> <li>Single point of failure in internet infrastructure</li> 
> </ul> 
>  
> <h1> 
>    
>    
>   Solutions 😃 
> </h1> 
>  
> <ol> 
> <li>Validate inputs early</li> 
> <li>Enforce soft limits</li> 
> <li>Fail-fast on parse</li> 
> <li>Monitor config diffs</li> 
> <li>Version config safely</li> 
> <li>Use backpressure mechanisms</li> 
> <li>Degrade functionality gracefully</li> 
> <li>Log and continue</li> 
> <li>Improve degradation metrics</li> 
> <li>Implement proper Result/Option handling with fallbacks</li> 
> <li>Treat all configuration as untrusted input</li> 
> </ol> 
>  
> <h1> 
>    
>    
>   Refactorings ⚙️ 
> </h1> 
>  
>  
> <div> 
>   <a href="https://dev.to/mcsee"> 
>     </a><div><a href="https://dev.to/mcsee"> 
>       <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg" alt="mcsee"> 
>     </a></div><a href="https://dev.to/mcsee"> 
>   </a> 
>   <a href="https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc"> 
>     </a><div><a href="https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc"> 
>       </a><h2><a href="https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc">Refactoring 004 - Remove Unhandled Exceptions</a></h2><a href="https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc"> 
>       </a><h3><a href="https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc">Maxi Contieri ・ Feb 10 '22</a></h3><a href="https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc"> 
>       </a><div><a href="https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc"> 
>         <span>#programming</span> 
>         <span>#exceptions</span> 
>         <span>#oop</span> 
>         <span>#cleancod</span> 
>       </a></div><a href="https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc"> 
>     </a></div><a href="https://dev.to/mcsee/refactoring-004-remove-unhandled-exceptions-21kc"> 
>   </a> 
> </div> 
>  
>  
>  
> <div> 
>   <a href="https://dev.to/mcsee"> 
>     </a><div><a href="https://dev.to/mcsee"> 
>       <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg" alt="mcsee"> 
>     </a></div><a href="https://dev.to/mcsee"> 
>   </a> 
>   <a href="https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51"> 
>     </a><div><a href="https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51"> 
>       </a><h2><a href="https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51">Refactoring 024 - Replace Global Variables with Dependency Injection</a></h2><a href="https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51"> 
>       </a><h3><a href="https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51">Maxi Contieri ・ Mar 9</a></h3><a href="https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51"> 
>       </a><div><a href="https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51"> 
>         <span>#webdev</span> 
>         <span>#programming</span> 
>         <span>#javascript</span> 
>         <span>#beginners</span> 
>       </a></div><a href="https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51"> 
>     </a></div><a href="https://dev.to/mcsee/refactoring-024-replace-global-variables-with-dependency-injection-2h51"> 
>   </a> 
> </div> 
>  
>  
>  
> <div> 
>   <a href="https://dev.to/mcsee"> 
>     </a><div><a href="https://dev.to/mcsee"> 
>       <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg" alt="mcsee"> 
>     </a></div><a href="https://dev.to/mcsee"> 
>   </a> 
>   <a href="https://dev.to/mcsee/refactoring-035-separate-exception-types-179j"> 
>     </a><div><a href="https://dev.to/mcsee/refactoring-035-separate-exception-types-179j"> 
>       </a><h2><a href="https://dev.to/mcsee/refactoring-035-separate-exception-types-179j">Refactoring 035 - Separate Exception Types</a></h2><a href="https://dev.to/mcsee/refactoring-035-separate-exception-types-179j"> 
>       </a><h3><a href="https://dev.to/mcsee/refactoring-035-separate-exception-types-179j">Maxi Contieri ・ Oct 14</a></h3><a href="https://dev.to/mcsee/refactoring-035-separate-exception-types-179j"> 
>       </a><div><a href="https://dev.to/mcsee/refactoring-035-separate-exception-types-179j"> 
>         <span>#webdev</span> 
>         <span>#programming</span> 
>         <span>#ai</span> 
>         <span>#beginners</span> 
>       </a></div><a href="https://dev.to/mcsee/refactoring-035-separate-exception-types-179j"> 
>     </a></div><a href="https://dev.to/mcsee/refactoring-035-separate-exception-types-179j"> 
>   </a> 
> </div> 
>  
>  
> <h1> 
>    
>    
>   Context 💬 
> </h1> 
>  
> <p>In the early hours of November 18, 2025, Cloudflare’s global network began failing to deliver core HTTP traffic, generating a <a href="https://mgx.dev/blog/cloudflare1119">flood of 5xx errors</a> to end users.</p> 
>  
> <p>This was not caused by an external attack or security problem.</p> 
>  
> <p>The outage stemmed from an internal "latent defect" triggered by a <a href="https://techcrunch.com/2025/11/18/cloudflare-blames-massive-internet-outage-on-latent-bug/">routine configuration change</a></p> 
>  
> <p>The failure fluctuated over time, until a fix was <a href="https://techcrunch.com/2025/11/18/cloudflare-blames-massive-internet-outage-on-latent-bug/">fully deployed</a>.</p> 
>  
> <p>The root cause lay in a software bug in Cloudflare’s Bot Management module and its downstream proxy logic.</p> 
>  
> <p>The Technical Chain of Events</p> 
>  
> <ol> 
> <li><p><strong>Database Change (11:05 UTC)</strong>: A ClickHouse permissions update made previously implicit table access explicit, allowing users to see metadata from both the <code>default</code> and <code>r0</code> databases.</p></li> 
> <li><p><strong>SQL Query Assumption</strong>: A Bot Management query lacked a database name filter:<br> 
> </p></li> 
> </ol> 
>  
> <div> 
> <pre><code>   <span>SELECT</span> <span>name</span><span>,</span> <span>type</span> <span>FROM</span> <span>system</span><span>.</span><span>columns</span> 
>    <span>WHERE</span> <span>table</span> <span>=</span> <span>'http_requests_features'</span> 
>    <span>ORDER</span> <span>BY</span> <span>name</span><span>;</span> 
> </code></pre> 
>  
> </div> 
>  
>  
> <p>This query began returning duplicate rows—once for <code>default</code> database, once for <code>r0</code> database.</p> 
>  
> <ol> 
> <li><p><strong>Feature File Explosion</strong>: The machine learning feature file doubled from ~60 features to over 200 features with duplicate entries.</p></li> 
> <li><p><strong>Hard Limit Exceeded</strong>: The Bot Management module had a hard-coded limit of 200 features (for memory pre-allocation), which was now exceeded.</p></li> 
> <li><p><strong>The Fatal .unwrap()</strong>: The Rust code called <code>.unwrap()</code> on a Result that was now returning an error, causing the thread to panic with "called Result::unwrap() on an Err value". <em>see code below</em></p></li> 
> <li><p><strong>Global Cascade</strong>: This panic propagated across all 330+ data centers globally, bringing down core CDN services, Workers KV, Cloudflare Access, Turnstile, and the dashboard.</p></li> 
> </ol> 
>  
> <p>The estimated financial impact across affected businesses ranges from $180-360 million.</p> 
> <h1> 
>    
>    
>   Sample Code 📖 
> </h1> 
> <h2> 
>    
>    
>   Wrong ❌ 
> </h2> 
>  
>  
>  
>  
> <div> 
> <pre><code><span>let</span> <span>features</span><span>:</span> <span>Vec</span><span>&lt;</span><span>Feature</span><span>&gt;</span> <span>=</span> <span>load_features_from_db</span><span>();</span> 
> <span>let</span> <span>max</span> <span>=</span> <span>200</span><span>;</span> 
> <span>assert!</span><span>(</span><span>features</span><span>.len</span><span>()</span> <span>&lt;=</span> <span>max</span><span>);</span> 
> <span># This magic number assumption </span> 
> <span># is actually wrong                              </span> 
>  
> <span>for</span> <span>f</span> <span>in</span> <span>features</span> <span>{</span> 
>     <span>proxy</span><span>.add_bot_feature</span><span>(</span><span>f</span><span>.unwrap</span><span>());</span> 
>     <span># You also call unwrap() on every feature. </span> 
>     <span># If the database returns an invalid entry </span> 
>     <span># or a parsing error,</span> 
>     <span># you trigger another panic. </span> 
>     <span># You give your runtime no chance to recover. </span> 
>     <span># You force a crash on a single bad element.</span> 
> <span>}</span> 
>  
> <span># A quiet config expansion turns into</span> 
> <span># a full service outage </span> 
> <span># because you trust input that you should validate </span> 
> <span># and you use failure primitives (assert!, unwrap()) </span> 
> <span># that kills your program </span> 
> <span># instead of guiding it to safety</span> 
> </code></pre> 
>  
> </div> 
>  
> <h2> 
>    
>    
>   Right 👉 
> </h2> 
>  
>  
>  
>  
> <div> 
> <pre><code><span>fn</span> <span>load_and_validate</span><span>(</span><span>max</span><span>:</span> <span>usize</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>Feature</span><span>&gt;</span><span>,</span> <span>String</span><span>&gt;</span> <span>{</span> 
>     <span>let</span> <span>raw</span><span>:</span> <span>Vec</span><span>&lt;</span><span>Result</span><span>&lt;</span><span>Feature</span><span>,</span> <span>Error</span><span>&gt;&gt;</span> <span>=</span> <span>load_features_from_db</span><span>();</span> 
>  
>     <span>if</span> <span>raw</span><span>.len</span><span>()</span> <span>&gt;</span> <span>max</span> <span>{</span> 
>         <span>return</span> <span>Err</span><span>(</span><span>format!</span><span>(</span> 
>             <span>"too many features: {} &gt; {}"</span><span>,</span>  
>             <span>raw</span><span>.len</span><span>(),</span> <span>max</span> 
>         <span>));</span> 
>     <span>}</span> 
>  
>     <span>Ok</span><span>(</span><span>raw</span><span>.into_iter</span><span>()</span> 
>         <span>.filter_map</span><span>(|</span><span>r</span><span>|</span> <span>r</span><span>.ok</span><span>())</span> 
>         <span>.collect</span><span>())</span> 
> <span>}</span> 
> </code></pre> 
>  
> </div> 
>  
> <h1> 
>    
>    
>   Detection 🔍 
> </h1> 
>  
> <p>You can detect this code smell by searching your codebase for specific keywords:</p> 
>  
> <ul> 
> <li> 
> <code>.unwrap()</code> - Any direct call to this method</li> 
> <li> 
> <code>.expect()</code> - Similarly dangerous</li> 
> <li> 
> <code>panic!()</code> - Explicit panics in non-test code</li> 
> <li> 
> <code>thread::panic_any()</code> - Panic without context</li> 
> </ul> 
>  
> <p>When you find these patterns, ask yourself: "What happens to my system when this Result contains an Err?" If your honest answer is "the thread crashes and the request fails," then you've found the smell.</p> 
>  
> <p>You can also use automated linters. Most Rust style guides recommend tools like <code>clippy</code>, which flags <code>unwrap()</code> usage in production code paths. </p> 
>  
> <p>When you configure <code>clippy</code> with the <code>#![deny(unwrap_in_result)]</code> attribute, you prevent new <code>unwrap()</code> calls from entering your codebase.</p> 
> <h1> 
>    
>    
>   Tags 🏷️ 
> </h1> 
>  
> <ul> 
> <li>Fail-Fast</li> 
> </ul> 
> <h1> 
>    
>    
>   Level 🔋 
> </h1> 
>  
> <p>[x] Advanced</p> 
> <h1> 
>    
>    
>   Why the Bijection Is Important 🗺️ 
> </h1> 
>  
> <p>Your internal config generator must map exactly what your code expects. </p> 
>  
> <p>A mismatched config (e.g., duplicated metadata) breaks the <a href="https://dev.to/mcsee/the-one-and-only-software-design-principle-3086">bijection</a> between what your config represents and what your proxy code handles. </p> 
>  
> <p>When you assume "this file will always have ≤200 entries", you break that <a href="https://dev.to/mcsee/what-is-wrong-with-software-5pa">mapping</a>.</p> 
>  
> <p>Reality sends 400 entries → your model explodes → the real world wins, your service loses.</p> 
>  
> <p>That mismatch causes subtle failures that cascade, especially when you ignore validation or size constraints.</p> 
>  
> <p>Ensuring a clean mapping between the config source and code input helps prevent crashes and unpredictable behavior.</p> 
> <h1> 
>    
>    
>   AI Generation 🤖 
> </h1> 
>  
> <p>AI generators often prioritize "correct" logic over "resilient" logic. </p> 
>  
> <p>If you ask an AI to "ensure the list is never larger than 200 items," it might generate an assertion or a panic because that is the most direct way to satisfy the requirement, introducing this smell.</p> 
>  
> <p>The irony: Memory-safe languages like Rust prevent undefined behavior and memory corruption, but they can't prevent logic errors, poor error handling, or architectural assumptions. <strong>Memory safety ≠ System safety.</strong></p> 
> <h1> 
>    
>    
>   AI Detection 🧲 
> </h1> 
>  
> <p>AI can easily detect this if you instruct it to look for availability risks. </p> 
>  
> <p>You can use linters combined with AI to flag panic calls in production code.</p> 
>  
> <p>Human review on critical functions is more important than ever.</p> 
> <h2> 
>    
>    
>   Try Them! 🛠 
> </h2> 
>  
> <p><em>Remember: AI Assistants make lots of mistakes</em></p> 
>  
> <blockquote> 
> <p>Suggested Prompt: remove all .unwrap() and .expect() calls. Return Result instead and validate the vector bounds explicitly</p> 
> </blockquote> 
>  
> <div><table> 
> <thead> 
> <tr> 
> <th>Without Proper Instructions</th> 
> <th>With Specific Instructions</th> 
> </tr> 
> </thead> 
> <tbody> 
> <tr> 
> <td><a href="https://chat.openai.com/?q=Correct+and+explain+this+code%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60">ChatGPT</a></td> 
> <td><a href="https://chat.openai.com/?q=remove+all+.unwrap%28%29+and+.expect%28%29+calls.+Return+Result+instead+and+validate+the+vector+bounds+explicitly%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60">ChatGPT</a></td> 
> </tr> 
> <tr> 
> <td><a href="https://claude.ai/new?q=Correct+and+explain+this+code%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60">Claude</a></td> 
> <td><a href="https://claude.ai/new?q=remove+all+.unwrap%28%29+and+.expect%28%29+calls.+Return+Result+instead+and+validate+the+vector+bounds+explicitly%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60">Claude</a></td> 
> </tr> 
> <tr> 
> <td><a href="https://www.perplexity.ai/?q=Correct+and+explain+this+code%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60">Perplexity</a></td> 
> <td><a href="https://www.perplexity.ai/?q=remove+all+.unwrap%28%29+and+.expect%28%29+calls.+Return+Result+instead+and+validate+the+vector+bounds+explicitly%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60">Perplexity</a></td> 
> </tr> 
> <tr> 
> <td><a href="https://www.bing.com/chat?showconv=1&amp;sendquery=1&amp;q=Correct+and+explain+this+code%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60">Copilot</a></td> 
> <td><a href="https://www.bing.com/chat?showconv=1&amp;sendquery=1&amp;q=remove+all+.unwrap%28%29+and+.expect%28%29+calls.+Return+Result+instead+and+validate+the+vector+bounds+explicitly%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60">Copilot</a></td> 
> </tr> 
> <tr> 
> <td><a href="https://you.com/search?q=Correct+and+explain+this+code%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60">You</a></td> 
> <td><a href="https://you.com/search?q=remove+all+.unwrap%28%29+and+.expect%28%29+calls.+Return+Result+instead+and+validate+the+vector+bounds+explicitly%3A+%60%60%60rust%0D%0Alet+features%3A+Vec%3CFeature%3E+%3D+load_features_from_db%28%29%3B%0D%0Alet+max+%3D+200%3B%0D%0Aassert%21%28features.len%28%29+%3C%3D+max%29%3B%0D%0A%23+This+magic+number+assumption+%0D%0A%23+is+actually+wrong++++++++++++++++++++++++++++++%0D%0A++++++++++++++++++++++++++++++%0D%0Afor+f+in+features+%7B%0D%0A++++proxy.add_bot_feature%28f.unwrap%28%29%29%3B%0D%0A++++%23+You+also+call+unwrap%28%29+on+every+feature.+%0D%0A++++%23+If+the+database+returns+an+invalid+entry+%0D%0A++++%23+or+a+parsing+error%2C%0D%0A++++%23+you+trigger+another+panic.+%0D%0A++++%23+You+give+your+runtime+no+chance+to+recover.+%0D%0A++++%23+You+force+a+crash+on+a+single+bad+element.%0D%0A%7D%0D%0A++++++++++++++++++++++++++++++%0D%0A%23+A+quiet+config+expansion+turns+into%0D%0A%23+a+full+service+outage+%0D%0A%23+because+you+trust+input+that+you+should+validate+%0D%0A%23+and+you+use+failure+primitives+%28assert%21%2C+unwrap%28%29%29+%0D%0A%23+that+kills+your+program+%0D%0A%23+instead+of+guiding+it+to+safety%0D%0A%60%60%60">You</a></td> 
> </tr> 
> <tr> 
> <td><a href="https://gemini.google.com/">Gemini</a></td> 
> <td><a href="https://gemini.google.com/">Gemini</a></td> 
> </tr> 
> <tr> 
> <td><a href="https://chat.deepseek.com/">DeepSeek</a></td> 
> <td><a href="https://chat.deepseek.com/">DeepSeek</a></td> 
> </tr> 
> <tr> 
> <td><a href="https://www.meta.ai/chat">Meta AI</a></td> 
> <td><a href="https://www.meta.ai/">Meta AI</a></td> 
> </tr> 
> <tr> 
> <td><a href="https://grok.com/">Grok</a></td> 
> <td><a href="https://grok.com/">Grok</a></td> 
> </tr> 
> <tr> 
> <td><a href="https://chat.qwen.ai/">Qwen</a></td> 
> <td><a href="https://chat.qwen.ai/">Qwen</a></td> 
> </tr> 
> </tbody> 
> </table></div> 
> <h1> 
>    
>    
>   Conclusion 🏁 
> </h1> 
>  
> <p>Auto-generated config can hide duplication or grow unexpectedly.</p> 
>  
> <p>If your code assumes size limits or blindly trusts its input, you risk a catastrophic crash.</p> 
>  
> <p>Validating inputs is good; crashing because an input is slightly off is a disproportionate response that turns a minor defect into a global outage.</p> 
>  
> <p>Validate config, enforce limits, handle failures, and avoid assumptions.</p> 
>  
> <p>That’s how you keep your system stable and fault-tolerant.</p> 
> <h1> 
>    
>    
>   Relations 👩‍❤️‍💋‍👨 
> </h1> 
>  
>  
> <div> 
>   <a href="https://dev.to/mcsee"> 
>     </a><div><a href="https://dev.to/mcsee"> 
>       <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg" alt="mcsee"> 
>     </a></div><a href="https://dev.to/mcsee"> 
>   </a> 
>   <a href="https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55"> 
>     </a><div><a href="https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55"> 
>       </a><h2><a href="https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55">Code Smell 122 - Primitive Obsession</a></h2><a href="https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55"> 
>       </a><h3><a href="https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55">Maxi Contieri ・ Mar 17 '22</a></h3><a href="https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55"> 
>       </a><div><a href="https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55"> 
>         <span>#oop</span> 
>         <span>#webdev</span> 
>         <span>#tutorial</span> 
>         <span>#beginners</span> 
>       </a></div><a href="https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55"> 
>     </a></div><a href="https://dev.to/mcsee/code-smell-122-primitive-obsession-4a55"> 
>   </a> 
> </div> 
>  
>  
>  
>  
> <div> 
>   <a href="https://dev.to/mcsee"> 
>     </a><div><a href="https://dev.to/mcsee"> 
>       <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg" alt="mcsee"> 
>     </a></div><a href="https://dev.to/mcsee"> 
>   </a> 
>   <a href="https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb"> 
>     </a><div><a href="https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb"> 
>       </a><h2><a href="https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb">Code Smell 02 - Constants and Magic Numbers</a></h2><a href="https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb"> 
>       </a><h3><a href="https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb">Maxi Contieri ・ Oct 21 '20</a></h3><a href="https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb"> 
>       </a><div><a href="https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb"> 
>         <span>#beginners</span> 
>         <span>#codenewbie</span> 
>         <span>#100daysofcode</span> 
>         <span>#codequality</span> 
>       </a></div><a href="https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb"> 
>     </a></div><a href="https://dev.to/mcsee/code-smell-02-constants-and-magic-numbers-obb"> 
>   </a> 
> </div> 
>  
>  
>  
> <div> 
>   <a href="https://dev.to/mcsee"> 
>     </a><div><a href="https://dev.to/mcsee"> 
>       <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg" alt="mcsee"> 
>     </a></div><a href="https://dev.to/mcsee"> 
>   </a> 
>   <a href="https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3"> 
>     </a><div><a href="https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3"> 
>       </a><h2><a href="https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3">Code Smell 198 - Hidden Assumptions</a></h2><a href="https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3"> 
>       </a><h3><a href="https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3">Maxi Contieri ・ Mar 2 '23</a></h3><a href="https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3"> 
>       </a><div><a href="https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3"> 
>         <span>#webdev</span> 
>         <span>#beginners</span> 
>         <span>#programming</span> 
>         <span>#tutorial</span> 
>       </a></div><a href="https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3"> 
>     </a></div><a href="https://dev.to/mcsee/code-smell-198-hidden-assumptions-32i3"> 
>   </a> 
> </div> 
>  
>  
> <h1> 
>    
>    
>   More Information 📕 
> </h1> 
>  
> <p><a href="https://techcrunch.com/2025/11/18/cloudflare-blames-massive-internet-outage-on-latent-bug/">Cloudflare Blog</a></p> 
>  
> <p><a href="https://www.cloudflarestatus.com/incidents/8gmgl950y3h">Cloudflare Status</a></p> 
>  
> <p><a href="https://techcrunch.com/2025/11/18/cloudflare-blames-massive-internet-outage-on-latent-bug/">TechCrunch Coverage</a></p> 
>  
> <p><a href="https://mgx.dev/blog/cloudflare1119">MGX Deep Technical Analysis</a></p> 
>  
> <p><a href="https://hackaday.com/2025/11/20/how-one-uncaught-rust-exception-took-out-cloudflare/">Hackaday: How One Uncaught Rust Exception Took Out Cloudflare</a></p> 
>  
> <p><a href="https://www.cnbc.com/2025/11/18/cloudflare-outage-briefly-takes-chatgpt-claude-services-offline.html">CNBC: Financial Impact Analysis</a></p> 
>  
> <h1> 
>    
>    
>   Disclaimer 📘 
> </h1> 
>  
> <p>Code Smells are my <a href="https://dev.to/mcsee/i-wrote-more-than-90-articles-on-2021-here-is-what-i-learned-1n3a">opinion</a>.</p> 
>  
>  
>  
>  
> <blockquote> 
> <p>A good programmer is someone who always looks both ways before crossing a one-way street<br> 
> Douglas Crockford</p> 
> </blockquote> 
>  
> <p><em>Douglas Crockford</em></p> 
>  
>  
> <div> 
>   <a href="https://dev.to/mcsee"> 
>     </a><div><a href="https://dev.to/mcsee"> 
>       <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg" alt="mcsee"> 
>     </a></div><a href="https://dev.to/mcsee"> 
>   </a> 
>   <a href="https://dev.to/mcsee/software-engineering-great-quotes-26ci"> 
>     </a><div><a href="https://dev.to/mcsee/software-engineering-great-quotes-26ci"> 
>       </a><h2><a href="https://dev.to/mcsee/software-engineering-great-quotes-26ci">Software Engineering Great Quotes</a></h2><a href="https://dev.to/mcsee/software-engineering-great-quotes-26ci"> 
>       </a><h3><a href="https://dev.to/mcsee/software-engineering-great-quotes-26ci">Maxi Contieri ・ Dec 28 '20</a></h3><a href="https://dev.to/mcsee/software-engineering-great-quotes-26ci"> 
>       </a><div><a href="https://dev.to/mcsee/software-engineering-great-quotes-26ci"> 
>         <span>#codenewbie</span> 
>         <span>#programming</span> 
>         <span>#quotes</span> 
>         <span>#software</span> 
>       </a></div><a href="https://dev.to/mcsee/software-engineering-great-quotes-26ci"> 
>     </a></div><a href="https://dev.to/mcsee/software-engineering-great-quotes-26ci"> 
>   </a> 
> </div> 
>  
>  
>  
>  
>  
> <p>This article is part of the CodeSmell Series.</p> 
>  
>  
> <div> 
>   <a href="https://dev.to/mcsee"> 
>     </a><div><a href="https://dev.to/mcsee"> 
>       <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F366059%2F44d4a869-bb26-4b8e-aa73-6e596b4b4b8a.jpg" alt="mcsee"> 
>     </a></div><a href="https://dev.to/mcsee"> 
>   </a> 
>   <a href="https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc"> 
>     </a><div><a href="https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc"> 
>       </a><h2><a href="https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc">How to Find the Stinky parts of your Code</a></h2><a href="https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc"> 
>       </a><h3><a href="https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc">Maxi Contieri ・ May 21 '21</a></h3><a href="https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc"> 
>       </a><div><a href="https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc"> 
>         <span>#codenewbie</span> 
>         <span>#tutorial</span> 
>         <span>#codequality</span> 
>         <span>#beginners</span> 
>       </a></div><a href="https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc"> 
>     </a></div><a href="https://dev.to/mcsee/how-to-find-the-stinky-parts-of-your-code-1dbc"> 
>   </a> 
> </div>

---

## [4/10] “EU AI Act: The Code is the Compliance — Why TAUGuard is Already the Architecture We Needed”
**Source:** The Practical Developer | **Date:** 2025-12-02T10:46:07.000Z
**URL:** https://dev.to/michal_harcej/eu-ai-act-the-code-is-the-compliance-why-tauguard-is-already-the-architecture-we-needed-5e5m
**Reasoning:** The article discusses compliance with the EU AI Act, which is not directly relevant to our interests.
**Authors:** Michal Harcej

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjwkfqu5njw4kgyo3qcqj.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>The Misunderstanding That Reveals the Future</p>  
>   
> <blockquote>  
> <p>“They told us: ‘The EU AI Act is coming — better prepare.’<br><br>  
> But what if some of us didn’t need to prepare?<br><br>  
> What if we <em>built for that world</em> before the ink dried on the legislation?”</p>  
> </blockquote>  
>   
> <p>Some see the upcoming regulation as another compliance burden.<br><br>  
> We built TAUGuard — not as a response, but as the foundation.  </p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   The Regulatory Landscape Isn’t a Barrier — It’s a Signal  
> </h2>  
>   
> <p>The advent of regulation such as the EU AI Act, alongside standards like ISO 42001 and frameworks such as NIST RMF, signals a tectonic shift:  </p>  
>   
> <ul>  
> <li>From <strong>retrospective compliance</strong> (reports, audits) → to <strong>real-time assurance</strong>.  
> </li>  
> <li>From <strong>static documentation</strong> → to <strong>dynamic, executable compliance</strong>.  
> </li>  
> <li>From <strong>paper‑trail governance</strong> → to <strong>code-anchored governance</strong> — fresh, live, unforgeable.  
> </li>  
> </ul>  
>   
> <p>In other words: laws and standards no longer ask whether you did it — they ask whether you can prove it, in real time.</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   TAUGuard — Sovereignty-Native, Not Afterthought  
> </h2>  
>   
> <p>TAUGuard is not “yet to be built.” It exists. It runs. It enforces.  </p>  
>   
> <p>What TAU delivers:</p>  
>   
> <ul>  
> <li>  
> <strong>Sub‑100 ms anomaly detection</strong> — nervous‑system‑level latency for AI infra.  
> </li>  
> <li>  
> <strong>Blockchain-anchored audit trail</strong> — immutable, transparent memory of intent &amp; action.  
> </li>  
> <li>  
> <strong>Live alignment protocols + runtime controls</strong> — ensuring actions stay within allowed boundaries, preventing Loss of Control (LoC).  
> </li>  
> <li>  
> <strong>Permission, provenance, accountability baked in</strong> — co‑authorship boundaries, identity assurance, verified origin.  
> </li>  
> </ul>  
>   
> <p>This isn’t a compliance tool layered on after deployment.<br><br>  
> It’s a sovereign stack built from day one to meet — and exceed — the demands of this new regulatory realm.</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   Why Many Still Don’t Get It (a note to the “show me the demo/pitch deck” crowd)  
> </h2>  
>   
> <p>Because TAU doesn’t stare at you from a UI.<br><br>  
> It pulses in the background of infrastructure.  </p>  
>   
> <p>Much like how the early internet existed — not as flashy websites — but as protocols, routers, invisible trust networks.<br><br>  
> Ask yourself: did you invest in TCP/IP when it was just 1s and 0s running through cables?<br><br>  
> No — yet everything built on top of it changed the world.  </p>  
>   
> <p>TAUGuard isn’t about dashboards or sales decks.<br><br>  
> It’s about embedding trust, memory, and control into the bloodstream of AI workflows.  </p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   From Vision to Reality — A Call to the Guardians of the Next Web  
> </h2>  
>   
> <p>We didn’t wait for the AI Act to codify trust.<br><br>  
> We coded it.  </p>  
>   
> <p>TAUGuard isn’t a “soon-to-launch promise.”<br><br>  
> It is the memory of truth inside an internet that forgot how to prove anything.  </p>  
>   
> <p>If you believe sovereignty over your infrastructure isn’t optional — but inevitable —<br><br>  
> If you believe that real control must be traceable, immutable, and live —  </p>  
>   
> <p>Then TAUGuard isn’t optional.<br><br>  
> It’s essential.  </p>  
>   
> <p>Because the future of AI won’t be a battle of models.<br><br>  
> It will be a battle of <strong>trust stacks</strong>.<br><br>  
> And TAU is already standing — ready for the arms‑race.</p>  
>   
>   
>   
>   
> <p><em>— TAUGuard Core Team</em><br><br>  
> <em>“We don’t adapt to the AI Act. We embody it.”</em></p>

---

## [4/10] Data Analytics in Healthcare: Engineering the Future of Intelligent Medicine
**Source:** The Practical Developer | **Date:** 2025-12-02T10:39:15.000Z
**URL:** https://dev.to/rank_alchemy_5ad282cec75d/data-analytics-in-healthcare-engineering-the-future-of-intelligent-medicine-lmg
**Reasoning:** The article discusses data analytics in healthcare, which is not directly relevant to our interests.
**Authors:** Rank Alchemy

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fnl5hoj9p1m4wx1wxoivs.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>If you work in development, data engineering, AI, or medtech, you’re witnessing one of the biggest transformations happening right now: the shift toward data-driven healthcare.</p>  
>   
> <p>Healthcare has always produced massive datasets, but until recently, most of that data stayed trapped in silos. Today, with modern analytics stacks, AI models, IoMT devices, and scalable infrastructure, engineering teams are helping unlock insights that directly influence patient outcomes.</p>  
>   
> <p>In this DEV-focused breakdown, we’ll explore how data analytics is reshaping healthcare, from predictive modeling to XR-powered training — and why developers play a critical role in this evolution.</p>  
>   
> <h2>  
>     
>     
>   Why Healthcare Is Becoming a Data-First Industry  
> </h2>  
>   
> <p><strong>Healthcare systems generate exabyte-scale data from:</strong></p>  
>   
> <ul>  
> <li>EHRs and hospital databases</li>  
> <li>Medical imaging (MRI, CT, Ultrasound)</li>  
> <li>Wearables and IoMT devices</li>  
> <li>Genomics and biomarkers</li>  
> <li>Pharmacy and insurance records</li>  
> <li>Telehealth and remote monitoring platforms</li>  
> </ul>  
>   
> <p><strong>From an engineering perspective, this data is:</strong></p>  
>   
> <ul>  
> <li>High-volume</li>  
> <li>Highly sensitive (HIPAA/PHI)</li>  
> <li>Multi-modal</li>  
> <li>Often unstructured</li>  
> <li>Time-critical</li>  
> </ul>  
>   
> <p>This makes healthcare one of the most complex — and rewarding — data environments.</p>  
>   
> <h2>  
>     
>     
>   1. Predictive Analytics: Preventing Problems Before They Happen  
> </h2>  
>   
> <p>Predictive models are now integrated directly into clinical workflows.</p>  
>   
> <p><strong>Common use cases:</strong></p>  
>   
> <ul>  
> <li>Early sepsis detection</li>  
> <li>Predicting cardiac risk</li>  
> <li>Hospital readmission forecasting</li>  
> <li>Chronic disease deterioration</li>  
> <li>ER demand prediction</li>  
> </ul>  
>   
> <p><strong>Tech enabling it:</strong></p>  
>   
> <ul>  
> <li>Time-series modeling</li>  
> <li>Gradient boosting</li>  
> <li>Deep learning architectures</li>  
> <li>Real-time event processing</li>  
> <li>FHIR-based APIs</li>  
> </ul>  
>   
> <p>Developers are now building pipelines where raw data becomes actionable intelligence at the bedside.</p>  
>   
> <h2>  
>     
>     
>   2. Data-Driven Personalization of Treatment  
> </h2>  
>   
> <p>Healthcare is moving from one-size-fits-all to individualized plans.</p>  
>   
> <h2>  
>     
>     
>   Inputs powering personalization:  
> </h2>  
>   
> <ul>  
> <li>Genomic datasets</li>  
> <li>Wearable health metrics</li>  
> <li>Lifestyle and behavioral data</li>  
> <li>Longitudinal clinical history</li>  
> </ul>  
>   
> <p><strong>Outputs include:</strong></p>  
>   
> <ul>  
> <li>Personalized drug regimens</li>  
> <li>Tailored cancer therapy pathways</li>  
> <li>Predictive rehabilitation protocols</li>  
> <li>Adaptive treatment recommendations</li>  
> </ul>  
>   
> <p><strong>Behind the scenes, this requires:</strong></p>  
>   
> <ul>  
> <li>Feature engineering across multi-modal data</li>  
> <li>ML orchestration (e.g., Airflow / Prefect)</li>  
> <li>Interoperable data models using HL7/FHIR standards</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   3. Applying AI to Clinical Decision Support  
> </h2>  
>   
> <p>AI isn’t replacing doctors — but it is augmenting their ability to diagnose, triage, and plan treatment.</p>  
>   
> <p>Examples of AI in healthcare analytics:</p>  
>   
> <ul>  
> <li>NLP models extracting insights from physician notes</li>  
> <li>Vision AI detecting anomalies in radiology scans</li>  
> <li>LLM-powered medical assistants</li>  
> <li>Predictive triage for emergency scenarios</li>  
> </ul>  
>   
> <p>Typical technical stack might include:</p>  
>   
> <p>model_type: "Hybrid CNN + Transformer"<br>  
> framework: "TensorFlow / PyTorch"<br>  
> deployment: "Docker + Kubernetes"<br>  
> data_standard: "FHIR R4"<br>  
> security: "HIPAA, SOC 2, PHI Encryption"</p>  
>   
> <p>If you want to see the broader use-case landscape, here’s the full overview:<br>  
> 👉 [<a href="https://citrusbits.com/data-analytics-in-healthcare/">https://citrusbits.com/data-analytics-in-healthcare/</a>]</p>  
>   
> <h2>  
>     
>     
>   4. XR + Data Analytics: The Future of Hands-On Medicine  
> </h2>  
>   
> <p>Extended Reality (XR) and Virtual Reality (VR) are gaining momentum in medtech, especially when combined with analytics.</p>  
>   
> <p>Developer-focused XR use cases:</p>  
>   
> <ul>  
> <li>Surgical rehearsal using patient-specific data</li>  
> <li>Digital twins for complex procedure planning</li>  
> <li>Data-driven VR rehabilitation platforms</li>  
> <li>Interactive anatomy learning powered by real datasets</li>  
> </ul>  
>   
> <p><strong>XR tech stack often involves:</strong></p>  
>   
> <ul>  
> <li>Unity or Unreal Engine</li>  
> <li>OpenXR framework</li>  
> <li>Custom API endpoints for clinical datasets</li>  
> <li>ML-integrated simulations</li>  
> </ul>  
>   
> <p>Healthcare training is becoming immersive, and coders are the ones building it.</p>  
>   
> <h2>  
>     
>     
>   The Medtech Engineering Convergence  
> </h2>  
>   
> <p>Whether you're a backend engineer, data scientist, ML researcher, or XR developer, the future of medtech relies on your work.</p>  
>   
> <p>Teams like the <a href="https://citrusbits.com/">Medtech and AI Healthcare Innovation Company</a><br>  
>  At CitrusBits are engineering:</p>  
>   
> <ul>  
> <li>Predictive analytics systems</li>  
> <li>Medical imaging AI pipelines</li>  
> <li>IoMT device ecosystems</li>  
> <li>XR surgical planning tools</li>  
> <li>Patient-centric mobile and web platforms</li>  
> <li>Intelligent healthcare dashboards</li>  
> </ul>  
>   
> <p>This intersection of data, AI, and immersive tech is where the next decade of healthcare innovation lives.</p>  
>   
> <h2>  
>     
>     
>   Final Thoughts  
> </h2>  
>   
> <p>Healthcare is no longer just a clinical field — it’s a data engineering challenge. The developers who can work with sensitive data, scalable architectures, AI models, interoperability standards, and immersive technologies will shape the next generation of patient care.</p>  
>   
> <p>If you’re building in this space, you’re not just coding apps,<br>  
> You’re coding the future of medicine.</p>

---

## [4/10] How to Achieve Significant Build Performance Gains With TeamCity
**Source:** JetBrains Company Blog | **Date:** 2025-12-02T10:34:32.000Z
**URL:** https://blog.jetbrains.com/teamcity/2025/12/build-performance-gains/
**Reasoning:** The article discusses build performance in TeamCity, which is not directly relevant to our interests.
**Authors:** Olga Bedrina

**Content/Abstract:**
> <p><em>This article is brought to you by <a href="https://www.technewstoday.com/author/mduduzi/">Mdu Sibisi</a>, draft.dev.</em></p> 
>  
>  
>  
> <p>Long <a href="https://software.land/build-time/">build times</a> interfere with developer momentum, disrupt flow states, and negatively impact productivity. While seemingly minor gaps in work rate may appear negligible in the moment, they quickly accumulate into significant increases in release cycle times and infrastructure costs. (There’s a reason high-performance development teams <a href="https://cloud.google.com/blog/products/devops-sre/announcing-the-2024-dora-report">run their CI/CD pipelines several times a day</a>).</p> 
>  
>  
>  
> <p>While it’s tempting to reach for the latest software or methodology that promises to maximize your engineering team’s output, carelessly swapping tools with a trial-and-error-like approach isn’t a good idea: <a href="https://www.researchgate.net/profile/Omowunmi-Adebayo-4/publication/388122113_Software_Adoption_in_Project_Management_and_Their_Impact_on_Project_Efficiency_and_Collaboration/links/678b3b1f82501639f5f64d6e/Software-Adoption-in-Project-Management-and-Their-Impact-on-Project-Efficiency-and-Collaboration.pdf">Premature adoption</a> can often do more harm than good. It’s far more effective to adopt a strategy informed by data, where you first identify leaks and performance bottlenecks and remedy them accordingly.</p> 
>  
>  
>  
> <p>After all, you must ensure your team is using the most optimal tools <em>and</em> that they’re using those tools optimally.</p> 
>  
>  
>  
> <p>This guide offers a three-phase blueprint for achieving significant CI/CD pipeline performance boosts that will stick around even as you scale:</p> 
>  
>  
>  
> <ul> 
> <li>Identify: Pinpoint pipeline flaws.</li> 
>  
>  
>  
> <li>Optimize: Apply effective solutions.</li> 
>  
>  
>  
> <li>Continuously improve: Apply long-term strategies to avert future faults.</li> 
> </ul> 
>  
>  
>  
> <h2><strong>Phase 1: Assess performance and identify bottlenecks</strong></h2> 
>  
>  
>  
> <p>Before attempting any optimization, first assess the current performance of your pipeline and identify the causes of poor performance. This allows you to make informed improvements and prove their value.</p> 
>  
>  
>  
> <p><em>Time</em> is one of the most reliable and straightforward metrics to start with, particularly in the early stages of your pipeline audit:</p> 
>  
>  
>  
> <ul> 
> <li>How long does full pipeline completion take (from commit to production)?</li> 
>  
>  
>  
> <li>What is the average duration of each pipeline stage (build, test, and deploy)?</li> 
>  
>  
>  
> <li>Which steps consistently take the longest?</li> 
> </ul> 
>  
>  
>  
> <p>Also, it’s important to survey or speak to developers about their time perception of the pipeline, and observe their behavior. They may be batching work or avoiding commits due to long feedback cycles.</p> 
>  
>  
>  
> <p>They might even be avoiding experimentation based on unhealthy cultures centered around “waiting for the build”.</p> 
>  
>  
>  
> <p>Use the data gathered from these sources to set and adjust your benchmarks and <a href="https://www.ibm.com/think/topics/service-level-agreement">service-level agreements</a>, such as build duration (time) and CPU cores or memory usage (resource utilization).</p> 
>  
>  
>  
> <p>You don’t necessarily have to start with granular benchmarks and monitoring criteria per stage or process. You can keep it as broad as measuring overall pipeline completion time for the sake of simplicity.</p> 
>  
>  
>  
> <p>While legacy platforms might hide away data in text logs, modern CI/CD platforms provide you with the visibility you need to assess performance out of the box. </p> 
>  
>  
>  
> <p>In <a href="https://www.jetbrains.com/teamcity/">TeamCity</a>, the <em>Statistics</em> dashboard provides deeper insights into the performance of each build:</p> 
>  
>  
>  
> <img style="height:auto;" src="https://blog.jetbrains.com/wp-content/uploads/2025/12/image-1_statistics-dashboard-in-teamcity.png" alt=""> 
>  
>  
>  
> <p>This view visualizes recent and historical build data using key metrics like build duration, success rate, test count, artifact size, and queue time. You can also create custom charts to track the metrics that matter the most to your business.</p> 
>  
>  
>  
> <p>For a more granular view, the <a href="https://www.jetbrains.com/help/teamcity/build-time-report.html"><em>Build Time</em> report</a> shows the time taken by each stage of your pipeline so you can pinpoint the most significant bottlenecks:</p> 
>  
>  
>  
> <img style="height:auto;" src="https://blog.jetbrains.com/wp-content/uploads/2025/12/image-2_build-time.png" alt=""> 
>  
>  
>  
> <p>By default, TeamCity attaches its <a href="https://www.jetbrains.com/help/teamcity/performance-monitor.html">Performance Monitor</a> to each of your build configurations created from a URL. This provides you with a handy way of tracking resource utilization:</p> 
>  
>  
>  
> <img style="height:auto;" src="https://blog.jetbrains.com/wp-content/uploads/2025/12/image-3_perfmon.png" alt=""> 
>  
>  
>  
> <p>In addition to the Performance Monitor (and a host of other <a href="https://www.jetbrains.com/help/teamcity/adding-build-features.html">build features</a>), TeamCity’s notifier can trigger real-time alerts for any important build events (such as fails, successful completions, or hangs) via email or Slack.</p> 
>  
>  
>  
> <h2><strong>Phase 2: Optimize performance</strong></h2> 
>  
>  
>  
> <p>Once you know what’s not working, you want to identify the highest-impact changes your team can make to boost performance.</p> 
>  
>  
>  
> <p>Avoid initiatives that may break the system or result in widespread interruptions, like upgrading build tools without validating downstream dependencies or switching shell or OS environments. Small incremental changes that can be isolated are the least likely to disrupt your team’s productivity and the easiest to trace and roll back if something goes wrong.</p> 
>  
>  
>  
> <p>Some sure-fire techniques that are usually relatively simple to apply include the following:</p> 
>  
>  
>  
> <ul> 
> <li><strong>Build caches:</strong> Cache external (package manager) dependencies and build artifacts.</li> 
>  
>  
>  
> <li><strong>Parallelization:</strong> Run tests in parallel across agents or containers, splitting test suites by file, type, or historical timing data. You can also build microservices or modules concurrently.</li> 
>  
>  
>  
> <li><strong>Modularization:</strong> Break monolithic pipelines into modular build configurations.</li> 
>  
>  
>  
> <li><strong>Selective and incremental building:</strong> Use VCS triggers and path filters to build only affected modules. Use build fingerprint or checksums to avoid rebuilding unchanged components.</li> 
> </ul> 
>  
>  
>  
> <p>TeamCity’s built-in features can help your team achieve several quick wins.For example, the <a href="https://www.jetbrains.com/help/teamcity/build-cache.html">build caches</a> can be enabled to intelligently reuse compiled artifacts and dependencies:</p> 
>  
>  
>  
> <img style="height:auto;" src="https://blog.jetbrains.com/wp-content/uploads/2025/12/image-4_build-with-cache-without-cache.png" alt=""> 
>  
>  
>  
> <p>In the example above, using a build cache reduced the build time by <strong>73 percent</strong>. (Keep in mind that the time you’ll save depends on the complexity of your build and which files you choose to cache.)</p> 
>  
>  
>  
> <p><a href="https://www.jetbrains.com/help/teamcity/artifact-dependencies.html">Artifact dependencies</a> let you reuse files and curb any redundant compilation or build steps, allowing for pipeline modularization and faster executions:</p> 
>  
>  
>  
> <img style="height:auto;" src="https://blog.jetbrains.com/wp-content/uploads/2025/12/image-5_after-artifact-dependency-added.png" alt=""> 
>  
>  
>  
> <p>TeamCity’s architecture allows tests to run in parallel using separate <a href="https://www.jetbrains.com/help/teamcity/install-and-start-teamcity-agents.html">build agents</a>. So instead of two (or more) builds running consecutively, they’ll run simultaneously, providing faster feedback to developers and thus shortening the release cycle:</p> 
>  
>  
>  
> <img style="height:auto;" src="https://blog.jetbrains.com/wp-content/uploads/2025/12/image-6_no-changes.png" alt=""> 
>  
>  
>  
> <h2><strong>Phase 3: Continuously improve</strong></h2> 
>  
>  
>  
> <p>Performance is not a one-off fix; it must be continually monitored and sustained as your team and codebase scale.</p> 
>  
>  
>  
> <p>Keep the following three considerations in mind to ensure continuous improvement:</p> 
>  
>  
>  
> <ul> 
> <li>Use elastic infrastructure that can scale up to meet peak demand and scale down to save costs.</li> 
>  
>  
>  
> <li>Design pipelines that only build what has changed to speed up builds.</li> 
>  
>  
>  
> <li>Create a culture of performance: Empower your team with the necessary visibility and tools to track trends, catch regressions, and implement improvements.</li> 
> </ul> 
>  
>  
>  
> <p>TeamCity supports continuous improvement, regardless of your scale. Its <a href="https://www.jetbrains.com/help/teamcity/cloud/teamcity-integration-with-cloud-solutions.html#Cloud+Agents+and+Executors">cloud profiles</a> let you manage cloud agents on AWS, Azure, or GCP so you can scale infrastructure up and down to meet demand.</p> 
>  
>  
>  
> <p>When it comes to orchestration, you can use features like <a href="https://www.jetbrains.com/help/teamcity/snapshot-dependencies.html">snapshot dependencies</a> to create build chains that only build the components impacted by change, which is crucial for managing microservices architecture and speeding up builds. <a href="https://www.jetbrains.com/help/teamcity/build-configuration-template.html#Creating+build+configuration+template">Build templates</a>, on the other hand, allow your team to define a configuration once and apply it across multiple projects, which simplifies updates and maintenance.</p> 
>  
>  
>  
> <p>Finally, TeamCity’s <a href="https://www.jetbrains.com/help/teamcity/tracking-user-actions.html">audit</a> log and historical performance reports offer the transparency and accountability needed for a performance-oriented culture by letting you track changes and share successes with stakeholders.</p> 
>  
>  
>  
> <img style="height:auto;" src="https://blog.jetbrains.com/wp-content/uploads/2025/12/image-7_teamcity-audit.png" alt=""> 
>  
>  
>  
> <p>You can even use TeamCity’s REST API to export data into external tools such as Grafana and Excel to present to stakeholders.</p> 
>  
>  
>  
> <h2><strong>Conclusion</strong></h2> 
>  
>  
>  
> <p>Implementing meaningful change is not always easy. However, by identifying and addressing performance bottlenecks and measuring progress, you and your team will be rewarded with visible improvements in product output and quality. Use this positive feedback to maintain your momentum.</p> 
>  
>  
>  
> <p>While addressing process bottlenecks is crucial to improving performance, the tools your team uses also shape the way they work. Legacy CI/CD tools that require extensive customization, external plugins, and specialized expertise often hinder performance because they keep teams stuck in ineffective workloads.</p> 
>  
>  
>  
> <p>If you’re secretly wondering whether your CI/CD setup is holding back your team, be sure to check out the following resources:</p> 
>  
>  
>  
> <ul> 
> <li>Is your CI/CD tool helping or hindering performance?</li> 
>  
>  
>  
> <li>The migration decision framework: Quantify the risk of your legacy system to determine whether migration is worth consideration or not.</li> 
> </ul>

---

## [4/10] From 1.2GB to 54MB: My Docker Image Went on a Diet
**Source:** The Practical Developer | **Date:** 2025-12-02T10:36:20.000Z
**URL:** https://dev.to/tusharsharma099/from-12gb-to-54mb-my-docker-image-went-on-a-diet-17nd
**Reasoning:** The article discusses Docker image optimization, which is not directly relevant to our interests.
**Authors:** tusharsharma099

**Content/Abstract:**
> <p>When I first containerized a Node.js app, I felt pretty good about myself. I had a Dockerfile, I built it, and it worked.</p> 
>  
> <p>Then I checked the size.</p> 
>  
> <p><strong>1.2GB. For a single Node.js service.</strong></p> 
>  
> <p>That’s when reality hit me. My image wasn’t lean—it was obese. It slowed down builds, bloated my CI/CD pipeline, took forever to push to the registry, and ate storage like there was no tomorrow.</p> 
>  
> <p>So, I put my Docker image on a strict diet. After a few rounds of optimizations, it went from <strong>1.2GB → 250MB → 54MB</strong>.</p> 
>  
> <p>Here’s the story of how I cut the fat—and how you can too.</p> 
>  
> <p><strong>Step 1: The Heavyweight Start</strong><br> 
> Here’s what my original Dockerfile looked like:</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgw7tx40tka895ys6afko.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgw7tx40tka895ys6afko.png" alt="" width="800" height="297"></a></p> 
>  
> <ul> 
> <li><p>node:16 is Debian-based and heavy (~350MB).</p></li> 
> <li><p>npm install installed <strong>everything</strong>-dev and production dependencies.</p></li> 
> <li><p>No .dockerignore, so logs, git history, and node_modules sneaked into the image</p></li> 
> </ul> 
>  
> <p>The result? A <strong>1.2GB monster</strong> that slowed everything down.</p> 
>  
> <p><strong>Step 2: Choosing a Leaner Base</strong><br> 
> The first fix was swapping node:16 for node:16-alpine.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fob03v461nlhpqbdpwhl7.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fob03v461nlhpqbdpwhl7.png" alt="" width="800" height="72"></a></p> 
>  
> <p>That <strong>one-line change cut my image down to ~250MB.</strong></p> 
>  
> <p>Lesson: <strong>Your base image choice can make or break your build.</strong></p> 
>  
> <p>⚠️ Caveat: Alpine uses <strong>musl</strong> instead of glibc. If your app has native modules (sharp, bcrypt, canvas), you may need extra packages.</p> 
>  
> <p><strong>Step 3: Multi-Stage Builds</strong><br> 
> My app uses TypeScript, so I had build tools sitting inside the final image. Big mistake. They added hundreds of MBs I didn’t need in production.</p> 
>  
> <p>Enter <strong>multi-stage builds:</strong></p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9sjeqt981a5xvtlg4xng.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9sjeqt981a5xvtlg4xng.png" alt="" width="800" height="500"></a></p> 
>  
> <p>Now, the final image contains only:</p> 
>  
> <ul> 
> <li>Compiled JavaScript (dist/)</li> 
> <li>Production dependencies 
> No dev dependencies. No build cache. No clutter.</li> 
> </ul> 
>  
> <p>This dropped my image to ~120MB.</p> 
>  
> <p><strong>Step 4: Prune and Ignore Junk</strong><br> 
> Another culprit: files that had no business being in production.</p> 
>  
> <p>I added a .dockerignore:</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvn0laqnk0h3geixd9409.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvn0laqnk0h3geixd9409.png" alt="" width="800" height="238"></a></p> 
>  
> <p>And I cleaned up caches in the Dockerfile:</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F4603x6yjw50v0bwydcwj.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F4603x6yjw50v0bwydcwj.png" alt="" width="800" height="125"></a><br> 
> End result: no accidental junk, no wasted MBs.</p> 
>  
> <p><strong>Step 5: Minimize Layers</strong><br> 
> At first, I had a Dockerfile with multiple RUN statements:</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fevu9umlmp6ljnkkf0yyr.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fevu9umlmp6ljnkkf0yyr.png" alt="" width="800" height="113"></a><br> 
> Each RUN adds a layer. I combined them into one:</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjgryccmdmgt0obybbzru.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjgryccmdmgt0obybbzru.png" alt="" width="800" height="115"></a><br> 
> This small tweak shaved off ~15MB. Not huge, but every MB counts when you’re pulling images in production.</p> 
>  
> <p><strong>Step 6: Measuring and Iterating</strong><br> 
> The key to trimming images is measuring:</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftr1i6zove38uezyfalx6.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftr1i6zove38uezyfalx6.png" alt="" width="800" height="92"></a><br> 
> With docker history, I saw exactly which layer was eating space and optimized from there.</p> 
>  
> <p><strong>Final Weight Check</strong><br> 
> Original: 1.2GB<br> 
> <strong>After switching to Alpine :</strong> ~250MB<br> 
> <strong>After multi-stage + pruning:</strong> 120MB<br> 
> <strong>After .dockerignore + cleanup:</strong> 54MB 🎉<br> 
> That’s a <strong>~95% reduction</strong>. Pulls went from minutes to seconds, and CI/CD pipelines stopped crawling.</p> 
>  
> <p><strong>Lessons Learned</strong><br> 
> <strong>Pick the right base image</strong> – Defaults are rarely optimal.<br> 
> <strong>Multi-stage builds are gold</strong> – Keep dev tools out of production.<br> 
> <strong>Use .dockerignore religiously</strong>– Don’t ship junk.<br> 
> <strong>Prune aggressively</strong> – Caches, logs, temp files… delete them.<br> 
> <strong>Measure constantly</strong>– Know what’s eating space before fixing it.</p> 
>  
> <p><strong>Conclusion</strong><br> 
> Cutting Docker image size isn’t just about bragging rights—it’s about faster deploys, lower registry costs, and fewer headaches.</p> 
>  
> <p>My Node.js image went on a diet and lost <strong>1.1GB</strong>, and I’ll never go back to lazy Dockerfiles again.</p> 
>  
> <p>If your containers are bloated, trust me: a few tweaks can make them featherweight.</p> 
>  
> <p>So… is your Docker image on a healthy diet?</p> 
>  
> <p><strong>📬 Contact</strong><br> 
> If you’d like to connect, collaborate, or discuss DevOps, feel free to reach out:</p> 
>  
> <p>GitHub: <a href="https://github.com/tusharsharma099">https://github.com/tusharsharma099</a><br> 
> LinkedIn: <a href="http://www.linkedin.com/in/tushar-sharma-77063225b">www.linkedin.com/in/tushar-sharma-77063225b</a></p>

---

## [4/10] Accenture and OpenAI accelerate enterprise AI success
**Source:** OpenAI News | **Date:** 2025-12-01T05:00:00.000Z
**URL:** https://openai.com/index/accenture-partnership
**Reasoning:** While it involves AI in enterprise, it lacks specific relevance to code intelligence or context engineering.

**Content/Abstract:**
> Accenture and OpenAI are collaborating to help enterprises bring agentic AI capabilities into the core of their business and unlock new levels of growth.

---

## [4/10] OpenAI takes an ownership stake in Thrive Holdings to accelerate enterprise AI adoption
**Source:** OpenAI News | **Date:** 2025-12-01T05:00:00.000Z
**URL:** https://openai.com/index/thrive-holdings
**Reasoning:** This is about enterprise AI adoption, not specifically about code intelligence or context engineering tools.

**Content/Abstract:**
> OpenAI takes an ownership stake in Thrive Holdings to accelerate enterprise AI adoption, embedding frontier research and engineering directly into accounting and IT services to boost speed, accuracy, and efficiency while creating a scalable model for industry-wide transformation.

---

## [3/10] A Plug-and-Play Spatially-Constrained Representation Enhancement Framework for Local-Life Recommendation
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251112947J/abstract
**Reasoning:** The paper discusses local-life recommendation systems, which are not relevant to our interests in code intelligence or AI in software engineering.
**Authors:** Jiang, Hao, Wang, Guoquan, Yu, Sheng, Zeng, Yang, Zeng, Wencong, Zhou, Guorui

**Content/Abstract:**
> Local-life recommendation have witnessed rapid growth, providing users with convenient access to daily essentials. However, this domain faces two key challenges: (1) spatial constraints, driven by the requirements of the local-life scenario, where items are usually shown only to users within a limited geographic area, indirectly reducing their exposure probability; and (2) long-tail sparsity, where few popular items dominate user interactions, while many high-quality long-tail items are largely overlooked due to imbalanced interaction opportunities. Existing methods typically adopt a user-centric perspective, such as modeling spatial user preferences or enhancing long-tail representations with collaborative filtering signals. However, we argue that an item-centric perspective is more suitable for this domain, focusing on enhancing long-tail items representation that align with the spatially-constrained characteristics of local lifestyle services. To tackle this issue, we propose ReST, a Plug-And-Play Spatially-Constrained Representation Enhancement Framework for Long-Tail Local-Life Recommendation. Specifically, we first introduce a Meta ID Warm-up Network, which initializes fundamental ID representations by injecting their basic attribute-level semantic information. Subsequently, we propose a novel Spatially-Constrained ID Representation Enhancement Network (SIDENet) based on contrastive learning, which incorporates two efficient strategies: a spatially-constrained hard sampling strategy and a dynamic representation alignment strategy. This design adaptively identifies weak ID representations based on their attribute-level information during training. It additionally enhances them by capturing latent item relationships within the spatially-constrained characteristics of local lifestyle services, while preserving compatibility with popular items.

---

## [3/10] MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251112449N/abstract
**Reasoning:** The focus is on multimodal representation learning for e-commerce, which is not directly related to our primary or secondary interests.
**Authors:** Nie, Zhanheng, Fu, Chenghan, Zhang, Daoze, Wu, Junxian, Guan, Wanxian, Wang, Pengjie, Xu, Jian, Zheng, Bo

**Content/Abstract:**
> The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.

---

## [3/10] Quantifying consistency and accuracy of Latent Dirichlet Allocation
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251112850M/abstract
**Reasoning:** The paper focuses on topic modeling in NLP, which is not directly related to code intelligence or context engineering.
**Authors:** Magsarjav, Saranzaya, Humphries, Melissa, Tuke, Jonathan, Mitchell, Lewis

**Content/Abstract:**
> Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.

---

## [3/10] HART: A Hybrid Addressing Scheme for Self-Balancing Binary Search Trees in Phase Change Memory (PCM)
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251103994D/abstract
**Reasoning:** The focus is on memory technologies for binary search trees, which is not directly related to our primary interests.
**Authors:** Desai, Mahek, Rumale, Apoorva, Asadinia, Marjan

**Content/Abstract:**
> As DRAM and other transistor-based memory technologies approach their scalability limits, alternative storage solutions like Phase-Change Memory (PCM) are gaining attention for their scalability, fast access times, and zero leakage power. However, current memory-intensive algorithms, especially those used in big data systems, often overlook PCM's endurance limitations (10^6 to 10^8 writes before degradation) and write asymmetry. Self-balancing binary search trees (BSTs), which are widely used for large-scale data management, were developed without considering PCM's unique properties, leading to potential performance degradation. This paper introduces HART, a novel hybrid addressing scheme for self-balancing BSTs, designed to optimize PCM's characteristics. By combining DFATGray code addressing for deeper nodes with linear addressing for shallower nodes, HART balances reduced bit flips during frequent rotations at deeper levels with computational simplicity at shallow levels. Experimental results on PCM-aware AVL trees demonstrate significant improvements in performance, with a reduction in bit flips leading to enhanced endurance, increased lifetime, and lower write energy and latency. Notably, these benefits are achieved without imposing substantial computational overhead, making HART an efficient solution for big data applications.

---

## [3/10] Commonality in Few: Few-Shot Multimodal Anomaly Detection via Hypergraph-Enhanced Memory
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251105966L/abstract
**Reasoning:** The focus is on anomaly detection in industrial settings, which is not directly related to our primary interests.
**Authors:** Lin, Yuxuan, Yan, Hanjing, Tong, Xuan, Chang, Yang, Wang, Huanzhen, Zhou, Ziheng, Gao, Shuyong, Wang, Yan, Zhang, Wenqiang

**Content/Abstract:**
> Few-shot multimodal industrial anomaly detection is a critical yet underexplored task, offering the ability to quickly adapt to complex industrial scenarios. In few-shot settings, insufficient training samples often fail to cover the diverse patterns present in test samples. This challenge can be mitigated by extracting structural commonality from a small number of training samples. In this paper, we propose a novel few-shot unsupervised multimodal industrial anomaly detection method based on structural commonality, CIF (Commonality In Few). To extract intra-class structural information, we employ hypergraphs, which are capable of modeling higher-order correlations, to capture the structural commonality within training samples, and use a memory bank to store this intra-class structural prior. Firstly, we design a semantic-aware hypergraph construction module tailored for single-semantic industrial images, from which we extract common structures to guide the construction of the memory bank. Secondly, we use a training-free hypergraph message passing module to update the visual features of test samples, reducing the distribution gap between test features and features in the memory bank. We further propose a hyperedge-guided memory search module, which utilizes structural information to assist the memory search process and reduce the false positive rate. Experimental results on the MVTec 3D-AD dataset and the Eyecandies dataset show that our method outperforms the state-of-the-art (SOTA) methods in few-shot settings. Code is available at https://github.com/Sunny5250/CIF.

---

## [3/10] EMO100DB: An Open Dataset of Improvised Songs with Emotion Data
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251104755H/abstract
**Reasoning:** The paper introduces a dataset related to music and emotion, which is not relevant to our interests in code intelligence or AI in software engineering.
**Authors:** Hwang, Daeun, Park, Saebyul

**Content/Abstract:**
> In this study, we introduce Emo100DB: a dataset consisting of improvised songs that were recorded and transcribed with emotion data based on Russell's circumplex model of emotion. The dataset was developed by collecting improvised songs that consist of melody, lyrics, and an instrumental accompaniment played, sung, and recorded by 20 young adults. Before recording each song, the participants were asked to report their emotional state, with the axes representing arousal and valence based on Russell's circumplex model of emotions. The dataset is organized into four emotion quadrants, and it includes the lyrics text and MIDI file of the melody extracted from the participant recordings, along with the original audio in WAV format. By providing an integrated composition of data and analysis, this study aims to offer a comprehensive dataset that allows for a diverse exploration of the relationship between music and emotion.

---

## [3/10] SilverTorch: A Unified Model-based System to Democratize Large-Scale Recommendation on GPUs
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251114881X/abstract
**Reasoning:** The focus is on recommendation systems on GPUs, which is not directly related to our primary interests.
**Authors:** Xue, Bi, Wu, Hong, Chen, Lei, Yang, Chao, Ma, Yiming, Ding, Fei, Wang, Zhen, Wang, Liang, Mao, Xiaoheng, Huang, Ke, Li, Xialu, Xia, Peng, Jian, Rui, Zhao, Yanli, Huang, Yanzun, Deng, Yijie, Tran, Harry, Chang, Ryan, Yu, Min, Dong, Eric, Wang, Jiazhou, Zhang, Qianqian, Zhai, Keke, Yin, Hongzhang, Garbacki, Pawel, Fang, Zheng, Pan, Yiyi, Ni, Min, Liu, Yang

**Content/Abstract:**
> Serving deep learning based recommendation models (DLRM) at scale is challenging. Existing systems rely on CPU-based ANN indexing and filtering services, suffering from non-negligible costs and forgoing joint optimization opportunities. Such inefficiency makes them difficult to support more complex model architectures, such as learned similarities and multi-task retrieval. In this paper, we propose SilverTorch, a model-based system for serving recommendation models on GPUs. SilverTorch unifies model serving by replacing standalone indexing and filtering services with layers of served models. We propose a Bloom index algorithm on GPUs for feature filtering and a tensor-native fused Int8 ANN kernel on GPUs for nearest neighbor search. We further co-design the ANN search index and filtering index to reduce GPU memory utilization and eliminate unnecessary computation. Benefit from SilverTorch's serving paradigm, we introduce a OverArch scoring layer and a Value Model to aggregate results across multi-tasks. These advancements improve the accuracy for retrieval and enable future studies for serving more complex models. For ranking, SilverTorch's design accelerates item embedding calculation by caching the pre-calculated embeddings inside the serving model. Our evaluation on the industry-scale datasets show that SilverTorch achieves up to 5.6x lower latency and 23.7x higher throughput compared to the state-of-the-art approaches. We also demonstrate that SilverTorch's solution is 13.35x more cost-efficient than CPU-based solution while improving accuracy via serving more complex models. SilverTorch serves over hundreds of models online across major products and recommends contents for billions of daily active users.

---

## [3/10] Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251100635L/abstract
**Reasoning:** The paper discusses SLAM for autonomous vehicles, which is not relevant to our interests in code intelligence or AI in software engineering.
**Authors:** Lim, Hyungtae, Kim, Daebeom, Myung, Hyun

**Content/Abstract:**
> As various 3D light detection and ranging (LiDAR) sensors have been introduced to the market, research on multi-session simultaneous localization and mapping (MSS) using heterogeneous LiDAR sensors has been actively conducted. Existing MSS methods mostly rely on loop closure detection for inter-session alignment; however, the performance of loop closure detection can be potentially degraded owing to the differences in the density and field of view (FoV) of the sensors used in different sessions. In this study, we challenge the existing paradigm that relies heavily on loop detection modules and propose a novel MSS framework, called Multi-Mapcher, that employs large-scale map-to-map registration to perform inter-session initial alignment, which is commonly assumed to be infeasible, by leveraging outlier-robust 3D point cloud registration. Next, after finding inter-session loops by radius search based on the assumption that the inter-session initial alignment is sufficiently precise, anchor node-based robust pose graph optimization is employed to build a consistent global map. As demonstrated in our experiments, our approach shows substantially better MSS performance for various LiDAR sensors used to capture the sessions and is faster than state-of-the-art approaches. Our code is available at https://github.com/url-kaist/multi-mapcher.

---

## [3/10] Diversifying Counterattacks: Orthogonal Exploration for Robust CLIP Inference
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251109064J/abstract
**Reasoning:** The paper discusses adversarial robustness in vision-language models, which is not directly relevant to our primary interests.
**Authors:** Jiang, Chengze, Dong, Minjing, Shi, Xinli, Gui, Jie

**Content/Abstract:**
> Vision-language pre-training models (VLPs) demonstrate strong multimodal understanding and zero-shot generalization, yet remain vulnerable to adversarial examples, raising concerns about their reliability. Recent work, Test-Time Counterattack (TTC), improves robustness by generating perturbations that maximize the embedding deviation of adversarial inputs using PGD, pushing them away from their adversarial representations. However, due to the fundamental difference in optimization objectives between adversarial attacks and counterattacks, generating counterattacks solely based on gradients with respect to the adversarial input confines the search to a narrow space. As a result, the counterattacks could overfit limited adversarial patterns and lack the diversity to fully neutralize a broad range of perturbations. In this work, we argue that enhancing the diversity and coverage of counterattacks is crucial to improving adversarial robustness in test-time defense. Accordingly, we propose Directional Orthogonal Counterattack (DOC), which augments counterattack optimization by incorporating orthogonal gradient directions and momentum-based updates. This design expands the exploration of the counterattack space and increases the diversity of perturbations, which facilitates the discovery of more generalizable counterattacks and ultimately improves the ability to neutralize adversarial perturbations. Meanwhile, we present a directional sensitivity score based on averaged cosine similarity to boost DOC by improving example discrimination and adaptively modulating the counterattack strength. Extensive experiments on 16 datasets demonstrate that DOC improves adversarial robustness under various attacks while maintaining competitive clean accuracy. Code is available at https://github.com/bookman233/DOC.

---

## [3/10] Enhancing Group Recommendation using Soft Impute Singular Value Decomposition
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251111172S/abstract
**Reasoning:** The paper on group recommendation using SVD is not relevant to our focus on code and context engineering.
**Authors:** Sani Ibrahim, Mubaraka, Saidu, Isah Charles, Csato, Lehel

**Content/Abstract:**
> The growing popularity of group activities increased the need to develop methods for providing recommendations to a group of users based on the collective preferences of the group members. Several group recommender systems have been proposed, but these methods often struggle due to sparsity and high-dimensionality of the available data, common in many real-world applications. In this paper, we propose a group recommender system called Group Soft-Impute SVD, which leverages soft-impute singular value decomposition to enhance group recommendations. This approach addresses the challenge of sparse high-dimensional data using low-rank matrix completion. We compared the performance of Group Soft-Impute SVD with Group MF based approaches and found that our method outperforms the baselines in recall for small user groups while achieving comparable results across all group sizes when tasked on Goodbooks, Movielens, and Synthetic datasets. Furthermore, our method recovers lower matrix ranks than the baselines, demonstrating its effectiveness in handling high-dimensional data.

---

## [3/10] Infer As You Train: A Symmetric Paradigm of Masked Generative for Click-Through Rate Prediction
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251114403Z/abstract
**Reasoning:** The paper on generative models for CTR prediction is not aligned with our primary interests in code or context engineering.
**Authors:** Zhang, Moyu, Jin, Yujun, Chen, Yun, Hu, Jinxin, Zhang, Yu, Zeng, Xiaoyi

**Content/Abstract:**
> Generative models are increasingly being explored in click-through rate (CTR) prediction field to overcome the limitations of the conventional discriminative paradigm, which rely on a simple binary classification objective. However, existing generative models typically confine the generative paradigm to the training phase, primarily for representation learning. During online inference, they revert to a standard discriminative paradigm, failing to leverage their powerful generative capabilities to further improve prediction accuracy. This fundamental asymmetry between the training and inference phases prevents the generative paradigm from realizing its full potential. To address this limitation, we propose the Symmetric Masked Generative Paradigm for CTR prediction (SGCTR), a novel framework that establishes symmetry between the training and inference phases. Specifically, after acquiring generative capabilities by learning feature dependencies during training, SGCTR applies the generative capabilities during online inference to iteratively redefine the features of input samples, which mitigates the impact of noisy features and enhances prediction accuracy. Extensive experiments validate the superiority of SGCTR, demonstrating that applying the generative paradigm symmetrically across both training and inference significantly unlocks its power in CTR prediction.

---

## [3/10] Esim: EVM Bytecode Similarity Detection Based on Stable-Semantic Graph
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251112971C/abstract
**Reasoning:** The focus on EVM bytecode similarity detection is more relevant to blockchain and crypto, which is not directly related to our primary interests.
**Authors:** Chen, Zhuo, Ji, Gaoqiang, He, Yiling, Wu, Lei, Zhou, Yajin

**Content/Abstract:**
> Decentralized finance (DeFi) is experiencing rapid expansion. However, prevalent code reuse and limited open-source contributions have introduced significant challenges to the blockchain ecosystem, including plagiarism and the propagation of vulnerable code. Consequently, an effective and accurate similarity detection method for EVM bytecode is urgently needed to identify similar contracts. Traditional binary similarity detection methods are typically based on instruction stream or control flow graph (CFG), which have limitations on EVM bytecode due to specific features like low-level EVM bytecode and heavily-reused basic blocks. Moreover, the highly-diverse Solidity Compiler (Solc) versions further complicate accurate similarity detection. Motivated by these challenges, we propose a novel EVM bytecode representation called Stable-Semantic Graph (SSG), which captures relationships between 'stable instructions' (special instructions identified by our study). Moreover, we implement a prototype, Esim, which embeds SSG into matrices for similarity detection using a heterogeneous graph neural network. Esim demonstrates high accuracy in SSG construction, achieving F1-scores of 100% for control flow and 95.16% for data flow, and its similarity detection performance reaches 96.3% AUC, surpassing traditional approaches. Our large-scale study, analyzing 2,675,573 smart contracts on six EVM-compatible chains over a one-year period, also demonstrates that Esim outperforms the SOTA tool Etherscan in vulnerability search.

---

## [3/10] Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251113166S/abstract
**Reasoning:** The paper focuses on collaborative filtering for recommender systems, which is not directly related to our primary interests in code intelligence.
**Authors:** Shen, Zhaoxin, Wu, Dan

**Content/Abstract:**
> To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.

---

## [3/10] Expanding data residency access to business customers worldwide
**Source:** OpenAI News | **Date:** 2025-11-25T22:00:00.000Z
**URL:** https://openai.com/index/expanding-data-residency-access-to-business-customers-worldwide
**Reasoning:** The update on data residency for OpenAI services is general AI news and not directly related to our primary interests.

**Content/Abstract:**
> OpenAI expands data residency for ChatGPT Enterprise, ChatGPT Edu, and the API Platform, enabling eligible customers to store data at rest in-region.

---

## [3/10] Comment on Toxic positivity at work: how to spot it and squash it by WorkWell &ndash; How to Avoid 'Toxic Positivity' in Teams &ndash; WorkWell
**Source:** Comments for Atlassian Blog Work Life | **Date:** 2025-12-02T14:02:34.000Z
**URL:** https://www.atlassian.com/blog/communication/toxic-positivity#comment-24919
**Reasoning:** The comment on workplace culture and toxic positivity is not relevant to our interests in code intelligence or AI in software engineering.
**Authors:** WorkWell &#8211; How to Avoid &#039;Toxic Positivity&#039; in Teams &#8211; WorkWell

**Content/Abstract:**
> <p><img src="https://www.atlassian.com/blog/wp-content/uploads/2023/09/toxic-positivity_1120x545@2x-scaled.jpg" alt="toxic-positivity_1120x545@2x-scaled.jpg"></p><p>[…] Effective leadership is necessary in combating toxic positivity within teams. Leaders set the tone for the workplace culture, so it’s vital for you to acknowledge and validate your team’s emotions. By fostering an environment where employees feel safe to express themselves, you can help mitigate the adverse effects of unrealistic optimism. Learn more about Toxic positivity at work – Work Life by Atlassian. […]</p>

---

## [3/10] Comment on Migrating the Jira and Confluence applications to AWS Graviton by Atlassian now available on AWS Marketplace - Work Life by Atlassian
**Source:** Comments for Atlassian Blog Work Life | **Date:** 2025-12-02T14:01:09.000Z
**URL:** https://www.atlassian.com/blog/atlassian-engineering/migrating-the-jira-and-confluence-applications-to-aws-graviton#comment-24918
**Reasoning:** The comment on Atlassian's migration to AWS Graviton is more about cloud performance and not directly relevant to our primary interests.
**Authors:** Atlassian now available on AWS Marketplace - Work Life by Atlassian

**Content/Abstract:**
> <p><img src="https://www.atlassian.com/blog/wp-content/uploads/2025/11/fy26-sil-engblog-40-1120x545_@2x.png" alt="fy26-sil-engblog-40-1120x545_@2x.png"></p><p>[…] cloud performance: Atlassian’s migration of Jira and Confluence to Graviton delivers faster speeds and reduced latency for enterprise […]</p>

---

## [3/10] Forecasting in Offline Reinforcement Learning for Non-stationary Environments
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T18:45:05.000Z
**URL:** https://arxiv.org/abs/2512.01987v1
**Reasoning:** The focus on offline reinforcement learning for non-stationary environments is not directly related to our primary interests in code intelligence or context engineering.
**Authors:** Suzan Ece Ada

**Content/Abstract:**
> Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific pattern of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent's experience, we aim to bridge the gap between offline RL and the complexities of real-world, non-stationary environments.

---

## [3/10] A Dual Approach for Hierarchical Information-Theoretic Tree Abstractions
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T18:43:11.000Z
**URL:** https://arxiv.org/abs/2512.01985v1
**Reasoning:** The paper on hierarchical information-theoretic tree abstractions is not directly related to our primary interests in code intelligence or context engineering.
**Authors:** Daniel T. Larsson

**Content/Abstract:**
> In this paper, we consider establishing a formal connection between two distinct tree-abstraction problems inspired by the information-bottleneck (IB) method. Specifically, we consider the hard- and soft-constrained formulations that have recently appeared in the literature to determine the conditions for which the two approaches are equivalent. Our analysis leverages concepts from Lagrangian relaxation and duality theory to relate the dual function of the hard-constrained problem to the Q-function employed in Q-tree search and shows the connection between tree phase transitions and solutions to the dual problem obtained by exploiting the problem structure. An algorithm is proposed that employs knowledge of the tree phase transitions to find a setting of the dual variable that solves the dual problem. Furthermore, we present an alternative approach to select the dual variable that leverages the integer programming formulation of the hard-constrained problem and the strong duality of linear programming. To obtain a linear program, we establish that a relaxation of the integer programming formulation of the hard-constrained tree-search problem has the integrality property by showing that the program constraint matrix is totally unimodular. Empirical results that corroborate the theoretical developments are presented and discussed throughout.

---

## [3/10] Feature-Based Semantics-Aware Scheduling for Energy-Harvesting Federated Learning
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T18:40:26.000Z
**URL:** https://arxiv.org/abs/2512.01983v1
**Reasoning:** The focus on federated learning and energy harvesting is not directly related to our primary interests in code intelligence or AI in software engineering.
**Authors:** Eunjeong Jeong

**Content/Abstract:**
> Federated Learning (FL) on resource-constrained edge devices faces a critical challenge: The computational energy required for training Deep Neural Networks (DNNs) often dominates communication costs. However, most existing Energy-Harvesting FL (EHFL) strategies fail to account for this reality, resulting in wasted energy due to redundant local computations. For efficient and proactive resource management, algorithms that predict local update contributions must be devised. We propose a lightweight client scheduling framework using the Version Age of Information (VAoI), a semantics-aware metric that quantifies update timeliness and significance. Crucially, we overcome VAoI's typical prohibitive computational cost, which requires statistical distance over the entire parameter space, by introducing a feature-based proxy. This proxy estimates model redundancy using intermediate-layer extraction from a single forward pass, dramatically reducing computational complexity. Experiments conducted under extreme non-IID data distributions and scarce energy availability demonstrate superior learning performance while achieving energy reduction compared to existing baseline selection policies. Our framework establishes semantics-aware scheduling as a practical and vital solution for EHFL in realistic scenarios where training costs dominate transmission costs.

---

## [3/10] Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T18:37:19.000Z
**URL:** https://arxiv.org/abs/2512.01979v1
**Reasoning:** The focus on GUI grounding and multimodal models is not directly related to our primary interests in code intelligence or context engineering.
**Authors:** Aiden Yiliu Li

**Content/Abstract:**
> GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.

---

## [3/10] AI-Driven Optimization under Uncertainty for Mineral Processing Operations
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T18:35:54.000Z
**URL:** https://arxiv.org/abs/2512.01977v1
**Reasoning:** The focus on AI-driven optimization for mineral processing is not directly related to our primary interests in code intelligence or AI in software engineering.
**Authors:** William Xu

**Content/Abstract:**
> The global capacity for mineral processing must expand rapidly to meet the demand for critical minerals, which are essential for building the clean energy technologies necessary to mitigate climate change. However, the efficiency of mineral processing is severely limited by uncertainty, which arises from both the variability of feedstock and the complexity of process dynamics. To optimize mineral processing circuits under uncertainty, we introduce an AI-driven approach that formulates mineral processing as a Partially Observable Markov Decision Process (POMDP). We demonstrate the capabilities of this approach in handling both feedstock uncertainty and process model uncertainty to optimize the operation of a simulated, simplified flotation cell as an example. We show that by integrating the process of information gathering (i.e., uncertainty reduction) and process optimization, this approach has the potential to consistently perform better than traditional approaches at maximizing an overall objective, such as net present value (NPV). Our methodological demonstration of this optimization-under-uncertainty approach for a synthetic case provides a mathematical and computational framework for later real-world application, with the potential to improve both the laboratory-scale design of experiments and industrial-scale operation of mineral processing circuits without any additional hardware.

---

## [3/10] A List of Complexity Bounds for Property Testing by Quantum Sample-to-Query Lifting
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T18:27:37.000Z
**URL:** https://arxiv.org/abs/2512.01971v1
**Reasoning:** The paper on quantum sample-to-query lifting is not directly related to our primary interests in code intelligence or context engineering.
**Authors:** Kean Chen

**Content/Abstract:**
> Quantum sample-to-query lifting, a relation between quantum sample complexity and quantum query complexity presented in Wang and Zhang (SIAM J. Comput. 2025), was significantly strengthened by Tang, Wright, and Zhandry (2025) to the case of state-preparation oracles. In this paper, we compile a list of quantum lower and upper bounds for property testing that are obtained by quantum sample-to-query lifting. The problems of interest include testing properties of probability distributions and quantum states, such as entropy and closeness. This collection contains new results, as well as new proofs of known bounds. In total, we present 49 complexity bounds, where 41 are new and 18 are (near-)optimal.

---

## [3/10] From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T18:27:25.000Z
**URL:** https://arxiv.org/abs/2512.01970v1
**Reasoning:** The focus on reinforcement learning and complementary reasoning is not directly related to our primary interests in code intelligence or context engineering.
**Authors:** Sitao Cheng

**Content/Abstract:**
> The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.

---

## [3/10] GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T18:03:29.000Z
**URL:** https://arxiv.org/abs/2512.01952v1
**Reasoning:** The topic of video world modeling and reinforcement learning is not relevant to code intelligence or context engineering.
**Authors:** Haoyang He

**Content/Abstract:**
> Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.

---

## [3/10] SVRG and Beyond via Posterior Correction
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T17:45:30.000Z
**URL:** https://arxiv.org/abs/2512.01930v1
**Reasoning:** The focus on SVRG and Bayesian methods is not directly relevant to code intelligence or context engineering.
**Authors:** Nico Daheim

**Content/Abstract:**
> Stochastic Variance Reduced Gradient (SVRG) and its variants aim to speed-up training by using gradient corrections, but have seen limited success in deep learning. Here, we show surprising new foundational connections of SVRG to a recently proposed Bayesian method called posterior correction. Specifically, we show that SVRG is recovered as a special case of posterior correction over the isotropic-Gaussian family, while novel extensions are automatically obtained by using more flexible exponential families. We derive two new SVRG variants by using Gaussian families: First, a Newton-like variant that employs novel Hessian corrections, and second, an Adam-like extension that improves pretraining and finetuning of Transformer language models. This is the first work to connect SVRG to Bayes and use it to boost variational training for deep networks.

---

## [3/10] Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T17:41:01.000Z
**URL:** https://arxiv.org/abs/2512.01924v1
**Reasoning:** The topic of robot control using deep active inference is not relevant to code intelligence or context engineering.
**Authors:** Kentaro Fujii

**Content/Abstract:**
> Robots in uncertain real-world environments must perform both goal-directed and exploratory actions. However, most deep learning-based control methods neglect exploration and struggle under uncertainty. To address this, we adopt deep active inference, a framework that accounts for human goal-directed and exploratory actions. Yet, conventional deep active inference approaches face challenges due to limited environmental representation capacity and high computational cost in action selection. We propose a novel deep active inference framework that consists of a world model, an action model, and an abstract world model. The world model encodes environmental dynamics into hidden state representations at slow and fast timescales. The action model compresses action sequences into abstract actions using vector quantization, and the abstract world model predicts future slow states conditioned on the abstract action, enabling low-cost action selection. We evaluate the framework on object-manipulation tasks with a real-world robot. Results show that it achieves high success rates across diverse manipulation tasks and switches between goal-directed and exploratory actions in uncertain settings, while making action selection computationally tractable. These findings highlight the importance of modeling multiple timescale dynamics and abstracting actions and state transitions.

---

## [3/10] Digital Twin Aided Millimeter Wave MIMO: Site-Specific Beam Codebook Learning
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T17:24:07.000Z
**URL:** https://arxiv.org/abs/2512.01902v1
**Reasoning:** The digital twin framework for MIMO is not related to code intelligence or context engineering.
**Authors:** Hao Luo

**Content/Abstract:**
> Learning site-specific beams that adapt to the deployment environment, interference sources, and hardware imperfections can lead to noticeable performance gains in coverage, data rate, and power saving, among other interesting advantages. This learning process, however, typically requires a large number of active interactions/iterations, which limits its practical feasibility and leads to excessive overhead. To address these challenges, we propose a digital twin aided codebook learning framework, where a site-specific digital twin is leveraged to generate synthetic channel data for codebook learning. We also propose to learn separate codebooks for line-of-sight and non-line-of-sight users, leveraging the geometric information provided by the digital twin. Simulation results demonstrate that the codebook learned from the digital twin can adapt to the environment geometry and user distribution, leading to high received signal-to-noise ratio performance. Moreover, we identify the ray-tracing accuracy as the most critical factor in digital twin fidelity that impacts the learned codebook performance.

---

## [3/10] Exploring Human Perceptions of AI Responses: Insights from a Mixed-Methods Study on Risk Mitigation in Generative Models
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T17:12:28.000Z
**URL:** https://arxiv.org/abs/2512.01892v1
**Reasoning:** The study on human perceptions of AI responses is not relevant to code intelligence or context engineering.
**Authors:** Heloisa Candello

**Content/Abstract:**
> With the rapid uptake of generative AI, investigating human perceptions of generated responses has become crucial. A major challenge is their `aptitude' for hallucinating and generating harmful contents. Despite major efforts for implementing guardrails, human perceptions of these mitigation strategies are largely unknown. We conducted a mixed-method experiment for evaluating the responses of a mitigation strategy across multiple-dimensions: faithfulness, fairness, harm-removal capacity, and relevance. In a within-subject study design, 57 participants assessed the responses under two conditions: harmful response plus its mitigation and solely mitigated response. Results revealed that participants' native language, AI work experience, and annotation familiarity significantly influenced evaluations. Participants showed high sensitivity to linguistic and contextual attributes, penalizing minor grammar errors while rewarding preserved semantic contexts. This contrasts with how language is often treated in the quantitative evaluation of LLMs. We also introduced new metrics for training and evaluating mitigation strategies and insights for human-AI evaluation studies.

---

## [3/10] Predicting Human Chess Moves: An AI Assisted Analysis of Chess Games Using Skill-group Specific n-gram Language Models
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T17:02:07.000Z
**URL:** https://arxiv.org/abs/2512.01880v1
**Reasoning:** The article focuses on AI in chess, which is not directly relevant to our interests in code intelligence and context engineering.
**Authors:** Daren Zhong

**Content/Abstract:**
> Chess, a deterministic game with perfect information, has long served as a benchmark for studying strategic decision-making and artificial intelligence. Traditional chess engines or tools for analysis primarily focus on calculating optimal moves, often neglecting the variability inherent in human chess playing, particularly across different skill levels. 
>   To overcome this limitation, we propose a novel and computationally efficient move prediction framework that approaches chess move prediction as a behavioral analysis task. The framework employs n-gram language models to capture move patterns characteristic of specific player skill levels. By dividing players into seven distinct skill groups, from novice to expert, we trained separate models using data from the open-source chess platform Lichess. The framework dynamically selects the most suitable model for prediction tasks and generates player moves based on preceding sequences. 
>   Evaluation on real-world game data demonstrates that the model selector module within the framework can classify skill levels with an accuracy of up to 31.7\% when utilizing early game information (16 half-moves). The move prediction framework also shows substantial accuracy improvements, with our Selector Assisted Accuracy being up to 39.1\% more accurate than our benchmark accuracy. The computational efficiency of the framework further enhances its suitability for real-time chess analysis.

---

## [3/10] Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T16:51:38.000Z
**URL:** https://arxiv.org/abs/2512.01870v1
**Reasoning:** The study of transformer learnability on arithmetic sequences is not directly relevant to our primary interests.
**Authors:** Alessandro Breccia

**Content/Abstract:**
> We study whether a Large Language Model can learn the deterministic sequence of trees generated by the iterated prime factorization of the natural numbers. Each integer is mapped into a rooted planar tree and the resulting sequence $ \mathbb{N}\mathcal{T}$ defines an arithmetic text with measurable statistical structure. A transformer network (the GPT-2 architecture) is trained from scratch on the first $10^{11}$ elements to subsequently test its predictive ability under next-word and masked-word prediction tasks. Our results show that the model partially learns the internal grammar of $\mathbb{N}\mathcal{T}$, capturing non-trivial regularities and correlations. This suggests that learnability may extend beyond empirical data to the very structure of arithmetic.

---

## [3/10] BHRAM-IL: A Benchmark for Hallucination Recognition and Assessment in Multiple Indian Languages
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T16:37:34.000Z
**URL:** https://arxiv.org/abs/2512.01852v1
**Reasoning:** The benchmark for hallucination recognition in Indian languages is not directly relevant to our core interests.
**Authors:** Hrishikesh Terdalkar

**Content/Abstract:**
> Large language models (LLMs) are increasingly deployed in multilingual applications but often generate plausible yet incorrect or misleading outputs, known as hallucinations. While hallucination detection has been studied extensively in English, under-resourced Indian languages remain largely unexplored. We present BHRAM-IL, a benchmark for hallucination recognition and assessment in multiple Indian languages, covering Hindi, Gujarati, Marathi, Odia, along with English. The benchmark comprises 36,047 curated questions across nine categories spanning factual, numerical, reasoning, and linguistic tasks. We evaluate 14 state-of-the-art multilingual LLMs on a benchmark subset of 10,265 questions, analyzing cross-lingual and factual hallucinations across languages, models, scales, categories, and domains using category-specific metrics normalized to (0,1) range. Aggregation over all categories and models yields a primary score of 0.23 and a language-corrected fuzzy score of 0.385, demonstrating the usefulness of BHRAM-IL for hallucination-focused evaluation. The dataset, and the code for generation and evaluation are available on GitHub (https://github.com/sambhashana/BHRAM-IL/) and HuggingFace (https://huggingface.co/datasets/sambhashana/BHRAM-IL/) to support future research in multilingual hallucination detection and mitigation.

---

## [3/10] Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T16:35:34.000Z
**URL:** https://arxiv.org/abs/2512.01848v1
**Reasoning:** The article on reinforcement learning for safer reasoning models is not directly related to our focus on code intelligence.
**Authors:** Jinghan Jia

**Content/Abstract:**
> Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. While intuitive, we find that SFT produces inconsistent safety improvements, degrades reasoning ability, and generalizes poorly across model families. These limitations suggest that purely supervised approaches are insufficient for robust safety alignment in LRMs. To address this, we investigate reinforcement learning (RL) as a complementary optimization framework for LRM safety training. Unlike SFT, RL directly optimizes model policies with reward feedback, enabling more adaptive and stable alignment. Extensive experiments across multiple model families and benchmarks show that RL achieves stronger and more consistent safety gains while maintaining reasoning competence. Further analysis of reflection dynamics and token-level entropy reveals that RL suppresses unsafe exploratory reasoning while preserving reflective depth, leading to safer and more reliable reasoning processes.

---

## [3/10] Deconstructing Generative Diversity: An Information Bottleneck Analysis of Discrete Latent Generative Models
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T16:13:23.000Z
**URL:** https://arxiv.org/abs/2512.01831v1
**Reasoning:** The article on generative diversity in latent models is not directly related to our primary interests.
**Authors:** Yudi Wu

**Content/Abstract:**
> Generative diversity varies significantly across discrete latent generative models such as AR, MIM, and Diffusion. We propose a diagnostic framework, grounded in Information Bottleneck (IB) theory, to analyze the underlying strategies resolving this behavior. The framework models generation as a conflict between a 'Compression Pressure' - a drive to minimize overall codebook entropy - and a 'Diversity Pressure' - a drive to maximize conditional entropy given an input. We further decompose this diversity into two primary sources: 'Path Diversity', representing the choice of high-level generative strategies, and 'Execution Diversity', the randomness in executing a chosen strategy. To make this decomposition operational, we introduce three zero-shot, inference-time interventions that directly perturb the latent generative process and reveal how models allocate and express diversity. Application of this probe-based framework to representative AR, MIM, and Diffusion systems reveals three distinct strategies: "Diversity-Prioritized" (MIM), "Compression-Prioritized" (AR), and "Decoupled" (Diffusion). Our analysis provides a principled explanation for their behavioral differences and informs a novel inference-time diversity enhancement technique.

---

## [3/10] InnoGym: Benchmarking the Innovation Potential of AI Agents
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T16:03:04.000Z
**URL:** https://arxiv.org/abs/2512.01822v1
**Reasoning:** The focus on benchmarking innovation potential of AI agents is not directly related to our core interests in code intelligence.
**Authors:** Jintian Zhang

**Content/Abstract:**
> LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.

---

## [3/10] Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T15:52:31.000Z
**URL:** https://arxiv.org/abs/2512.01816v1
**Reasoning:** The article on multimodal models for causal insights is not directly relevant to our primary interests.
**Authors:** Juanxi Tian

**Content/Abstract:**
> Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.

---

## [3/10] H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T15:32:14.000Z
**URL:** https://arxiv.org/abs/2512.01797v1
**Reasoning:** The focus on hallucination-associated neurons in LLMs is not directly related to our core interests.
**Authors:** Cheng Gao

**Content/Abstract:**
> Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than $0.1\%$ of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.

---

## [3/10] Dual Randomized Smoothing: Beyond Global Noise Variance
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T15:23:00.000Z
**URL:** https://arxiv.org/abs/2512.01782v1
**Reasoning:** The topic of randomized smoothing for neural networks is not relevant to code intelligence or context engineering.
**Authors:** Chenhao Sun

**Content/Abstract:**
> Randomized Smoothing (RS) is a prominent technique for certifying the robustness of neural networks against adversarial perturbations. With RS, achieving high accuracy at small radii requires a small noise variance, while achieving high accuracy at large radii requires a large noise variance. However, the global noise variance used in the standard RS formulation leads to a fundamental limitation: there exists no global noise variance that simultaneously achieves strong performance at both small and large radii. To break through the global variance limitation, we propose a dual RS framework which enables input-dependent noise variances. To achieve that, we first prove that RS remains valid with input-dependent noise variances, provided the variance is locally constant around each input. Building on this result, we introduce two components which form our dual RS framework: (i) a variance estimator first predicts an optimal noise variance for each input, (ii) this estimated variance is then used by a standard RS classifier. The variance estimator is independently smoothed via RS to ensure local constancy, enabling flexible design. We also introduce training strategies to iteratively optimize the two components. Extensive experiments on CIFAR-10 show that our dual RS method provides strong performance for both small and large radii-unattainable with global noise variance-while incurring only a 60% computational overhead at inference. Moreover, it consistently outperforms prior input-dependent noise approaches across most radii, with particularly large gains at radii 0.5, 0.75, and 1.0, achieving relative improvements of 19%, 24%, and 21%, respectively. On ImageNet, dual RS remains effective across all radii. Additionally, the dual RS framework naturally provides a routing perspective for certified robustness, improving the accuracy-robustness trade-off with off-the-shelf expert RS models.

---

## [3/10] Secure Over-the-Air Computation Against Multiple Eavesdroppers using Correlated Artificial Noise
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T15:19:47.000Z
**URL:** https://arxiv.org/abs/2512.01778v1
**Reasoning:** The paper on secure over-the-air computation is not related to code intelligence or context engineering.
**Authors:** David Nordlund

**Content/Abstract:**
> In the era of the Internet of Things and massive connectivity, many engineering applications, such as sensor fusion and federated edge learning, rely on efficient data aggregation from geographically distributed users over wireless networks. Over-the-air computation shows promising potential for enhancing resource efficiency and scalability in such scenarios by leveraging the superposition property of wireless channels. However, due to the use of uncoded transmission with linear mapping, it also suffers from security vulnerabilities that must be dealt with to allow widespread adoption. In this work, we consider a scenario where multiple cooperating eavesdroppers attempt to infer information about the aggregation result. We derive the optimal joint estimator for the eavesdroppers and provide bounds on the achievable estimation accuracy for both the eavesdroppers and the intended receiver. We show that significant inherent security exists against individual eavesdroppers due to channel misalignment. However, the security level is greatly compromised when the eavesdroppers can cooperate, motivating the need for deliberate security measures. A common measure is to add carefully calibrated perturbation signals (artificial noise) prior to data transmission to improve the security level. To this end, we propose a zero-forced artificial noise design that achieves a high level of security against cooperative eavesdroppers without compromising the aggregation accuracy.

---

## [3/10] VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T15:09:46.000Z
**URL:** https://arxiv.org/abs/2512.01769v1
**Reasoning:** Video analysis frameworks are not relevant to code intelligence or context engineering.
**Authors:** Hafsa Billah

**Content/Abstract:**
> Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is done manually with a human in the loop, which is error-prone and labor-intensive, or through custom algorithms designed for specific video types or situations. These algorithms are not general-purpose and require a new algorithm/software for each new situation or video from a new domain. 
>   This report proposes a general-purpose VSA framework that overcomes the above limitations. Video contents are extracted once using state-of-the-art Video Content Extraction technologies. They are represented using two alternative models -- the extended relational model (R++) and graph models. When represented using R++, the extracted contents can be used as data streams, enabling Continuous Query Processing via the proposed Continuous Query Language for Video Analysis. The graph models complement this by enabling the detection of situations that are difficult or impossible to detect using the relational model alone. Existing graph algorithms and newly developed algorithms support a wide variety of situation detection. To support domain independence, primitive situation variants across domains are identified and expressed as parameterized templates. Extensive experiments were conducted across several interesting situations from three domains -- AL, CM, and SL-- to evaluate the accuracy, efficiency, and robustness of the proposed approach using a dataset of videos of varying lengths from these domains.

---

## [3/10] Weight Space Representation Learning with Neural Fields
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T15:05:01.000Z
**URL:** https://arxiv.org/abs/2512.01759v1
**Reasoning:** Weight space representation learning is not directly related to code intelligence or context engineering.
**Authors:** Zhuoqian Yang

**Content/Abstract:**
> In this work, we investigate the potential of weights to serve as effective representations, focusing on neural fields. Our key insight is that constraining the optimization space through a pre-trained base model and low-rank adaptation (LoRA) can induce structure in weight space. Across reconstruction, generation, and analysis tasks on 2D and 3D data, we find that multiplicative LoRA weights achieve high representation quality while exhibiting distinctiveness and semantic structure. When used with latent diffusion models, multiplicative LoRA weights enable higher-quality generation than existing weight-space methods.

---

## [3/10] Reasoning About the Unsaid: Misinformation Detection with Omission-Aware Graph Inference
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T14:37:00.000Z
**URL:** https://arxiv.org/abs/2512.01728v1
**Reasoning:** Misinformation detection with graph inference is not directly related to code intelligence or context engineering.
**Authors:** Zhengjia Wang

**Content/Abstract:**
> This paper investigates the detection of misinformation, which deceives readers by explicitly fabricating misleading content or implicitly omitting important information necessary for informed judgment. While the former has been extensively studied, omission-based deception remains largely overlooked, even though it can subtly guide readers toward false conclusions under the illusion of completeness. To pioneer in this direction, this paper presents OmiGraph, the first omission-aware framework for misinformation detection. Specifically, OmiGraph constructs an omission-aware graph for the target news by utilizing a contextual environment that captures complementary perspectives of the same event, thereby surfacing potentially omitted contents. Based on this graph, omission-oriented relation modeling is then proposed to identify the internal contextual dependencies, as well as the dynamic omission intents, formulating a comprehensive omission relation representation. Finally, to extract omission patterns for detection, OmiGraph introduces omission-aware message-passing and aggregation that establishes holistic deception perception by integrating the omission contents and relations. Experiments show that, by considering the omission perspective, our approach attains remarkable performance, achieving average improvements of +5.4% F1 and +5.3% ACC on two large-scale benchmarks.

---

## [3/10] Beware of Reasoning Overconfidence: Pitfalls in the Reasoning Process for Multi-solution Tasks
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T14:35:06.000Z
**URL:** https://arxiv.org/abs/2512.01725v1
**Reasoning:** Reasoning overconfidence in LLMs is not directly related to code intelligence or context engineering.
**Authors:** Jiannan Guan

**Content/Abstract:**
> Large Language Models (LLMs) excel in reasoning tasks requiring a single correct answer, but they perform poorly in multi-solution tasks that require generating comprehensive and diverse answers. We attribute this limitation to \textbf{reasoning overconfidence}: a tendency to express undue certainty in an incomplete solution set. To examine the effect, we introduce \textit{MuSoBench}, a benchmark of multi-solution problems. Experiments show that the conventional short chain-of-thought (Short-CoT) prompting paradigm exhibits pronounced overconfidence, whereas the emerging long chain-of-thought (Long-CoT) approach mitigates it through iterative exploration and self-reflection. We further characterise observable behaviours and influential factors. To probe the underlying cause, we propose the \textbf{cognitive-rigidity hypothesis}, which posits that overconfidence arises when the reasoning process prematurely converges on a narrow set of thought paths. An attention-entropy analysis offers preliminary support for this view. These findings provide tools for assessing the completeness of LLM reasoning and highlight the need to move evaluation beyond single-answer accuracy toward comprehensive exploration.

---

## [3/10] Probabilistic Neuro-Symbolic Reasoning for Sparse Historical Data: A Framework Integrating Bayesian Inference, Causal Models, and Game-Theoretic Allocation
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T14:35:04.000Z
**URL:** https://arxiv.org/abs/2512.01723v1
**Reasoning:** The paper on neuro-symbolic reasoning for historical data is not relevant to code intelligence or context engineering.
**Authors:** Saba Kublashvili

**Content/Abstract:**
> Modeling historical events poses fundamental challenges for machine learning: extreme data scarcity (N &lt;&lt; 100), heterogeneous and noisy measurements, missing counterfactuals, and the requirement for human interpretable explanations. We present HistoricalML, a probabilistic neuro-symbolic framework that addresses these challenges through principled integration of (1) Bayesian uncertainty quantification to separate epistemic from aleatoric uncertainty, (2) structural causal models for counterfactual reasoning under confounding, (3) cooperative game theory (Shapley values) for fair allocation modeling, and (4) attention based neural architectures for context dependent factor weighting. We provide theoretical analysis showing that our approach achieves consistent estimation in the sparse data regime when strong priors from domain knowledge are available, and that Shapley based allocation satisfies axiomatic fairness guarantees that pure regression approaches cannot provide. We instantiate the framework on two historical case studies: the 19th century partition of Africa (N = 7 colonial powers) and the Second Punic War (N = 2 factions). Our model identifies Germany's +107.9 percent discrepancy as a quantifiable structural tension preceding World War I, with tension factor 36.43 and 0.79 naval arms race correlation. For the Punic Wars, Monte Carlo battle simulations achieve a 57.3 percent win probability for Carthage at Cannae and 57.8 percent for Rome at Zama, aligning with historical outcomes. Counterfactual analysis reveals that Carthaginian political support (support score 6.4 vs Napoleon's 7.1), rather than military capability, was the decisive factor.

---

## [3/10] Self-Supervised Borrowing Detection on Multilingual Wordlists
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T14:20:03.000Z
**URL:** https://arxiv.org/abs/2512.01713v1
**Reasoning:** Multilingual wordlist borrowing detection is not relevant to code intelligence or context engineering.
**Authors:** Tim Wientzek

**Content/Abstract:**
> This paper presents a fully self-supervised approach to borrowing detection in multilingual wordlists. The method combines two sources of information: PMI similarities based on a global correspondence model and a lightweight contrastive component trained on phonetic feature vectors. It further includes an automatic procedure for selecting decision thresholds without requiring labeled data. Experiments on benchmark datasets show that PMI alone already improves over existing string similarity measures such as NED and SCA, and that the combined similarity performs on par with or better than supervised baselines. An ablation study highlights the importance of character encoding, temperature settings and augmentation strategies. The approach scales to datasets of different sizes, works without manual supervision and is provided with a command-line tool that allows researchers to conduct their own studies.

---

## [3/10] Train a Custom Z‑Image Turbo LoRA with the Ostris AI Toolkit (RunPod Edition)
**Source:** The Practical Developer | **Date:** 2025-12-02T14:34:26.000Z
**URL:** https://dev.to/promptingpixels/train-a-custom-z-image-turbo-lora-with-the-ostris-ai-toolkit-runpod-edition-1n4h
**Reasoning:** The article on training a custom Z-Image Turbo LoRA is not relevant to code intelligence or context engineering.
**Authors:** Prompting Pixels

**Content/Abstract:**
> <h2> 
>    
>    
>   What we’re building 
> </h2> 
>  
> <p>A complete, reproducible workflow to train a Z‑Image Turbo LoRA with the <a href="https://github.com/ostris/ai-toolkit">Ostris AI Toolkit</a>, running on a rented GPU (RunPod). We’ll go from blank slate to a downloadable .safetensors LoRA, then load it into a downstream workflow (e.g., ComfyUI) to test the results with a trigger token.</p> 
>  
> <p>You’ll learn:</p> 
>  
> <ul> 
> <li>How to spin up the right environment on RunPod</li> 
> <li>How to assemble and configure a dataset for concept training</li> 
> <li>How to pick the right model, adapter, and sample prompts for monitoring</li> 
> <li>How to kick off and observe training progress</li> 
> <li>How to export and use your LoRA in your own pipeline</li> 
> </ul> 
>  
> <blockquote> 
> <p>💡 Pro tip: Z‑Image Turbo is fast and surprisingly VRAM‑friendly. Even before the base model drops, the distilled weights already make for practical LoRA experimentation.</p> 
> </blockquote> 
>  
> <p><a href="https://youtu.be/ePybOjM2sbE">Check out the accompanying video on YouTube</a>.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   TL;DR (Quick Reference) 
> </h2> 
>  
> <ol> 
> <li>Start a RunPod instance using the “Ostris AI Toolkit” template.</li> 
> <li>Create a dataset (8–20 images is a good starting point). Optionally add captions.</li> 
> <li>New job → select Z‑Image Turbo + LoRA target.</li> 
> <li>Set a unique trigger token (e.g., myuniqueconcept) and configure sample prompts.</li> 
> <li>Run ~3,000 steps to start; expect ~1 hour on a high-end GPU (e.g., RTX 5090).</li> 
> <li>Download the resulting LoRA (.safetensors) from the job’s Checkpoints.</li> 
> <li>Load the LoRA into your favorite workflow (ComfyUI, etc.) and prompt with the trigger.</li> 
> </ol> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Step 1 — Spin up the GPU workspace 
> </h2> 
>  
> <p>On RunPod, search for and launch the Ostris AI Toolkit template. Keep disk size generous (datasets and samples eat space as you iterate).</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fn0syab4j0ayngdqyttha.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fn0syab4j0ayngdqyttha.png" alt="RunPod 'Deploy a Pod' UI screenshot with red arrows: select 'AI Toolkit - ostris - ui - official' template, edit/change template, adjust disk size, and press purple 'Deploy On-Demand' button; shows On-Demand $0.89/hr and RTX 5090 pod summary (200 GB disk)." width="800" height="520"></a></p> 
>  
> <blockquote> 
> <p>🧪 Debug tip: If you see 0% GPU utilization during training, your job likely didn’t start or is stuck on CPU. Check the Training Queue and logs.</p> 
> </blockquote> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Step 2 — Assemble a tiny but consistent dataset 
> </h2> 
>  
> <p>Hop into Datasets → New Dataset. Name it something meaningful; I like a short handle that matches my future trigger token.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft7lk4tgf89ydqo0f5ldg.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft7lk4tgf89ydqo0f5ldg.png" alt="Dark-mode web UI 'OSTRIS AI-TOOLKIT' showing Datasets page with left sidebar (Dashboard, New Job, Training Queue, Datasets highlighted, Settings), main area saying 'Empty' and 'Refresh', and red annotated arrows labeled '1. Navigate to" width="800" height="520"></a></p> 
>  
> <p>Upload 8–20 representative images. Keep variety in poses and contexts, but a consistent subject identity.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fcv4vs05yeiidcoo955kb.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fcv4vs05yeiidcoo955kb.png" alt="OSTRIS AI-Toolkit dataset 'teach3r' screenshot: 3x3 grid of teacher thumbnails with overlays and trash icons, left nav and Add Images button." width="800" height="520"></a></p> 
>  
> <p>Captions are optional. If you add them, keep the phrasing consistent (e.g., always include your trigger token).</p> 
>  
> <blockquote> 
> <p>🧭 Guideline: Resolution 1024×1024 is a solid baseline with Z‑Image Turbo. If your source images vary wildly, consider pre-cropping/centering the subject.</p> 
> </blockquote> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Step 3 — Configure the training job like a pro 
> </h2> 
>  
> <p>Head to New Job:</p> 
>  
> <ul> 
> <li>Training name: something short you’ll recognize later</li> 
> <li>Trigger token: a unique string (avoid real words; e.g., xqteachu, zimg_concept01)</li> 
> <li>Architecture: Z‑Image Turbo (LoRA target)</li> 
> </ul> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fu57lrzo6tjsx5ihrmw9f.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fu57lrzo6tjsx5ihrmw9f.png" alt="OSTRIS AI-TOOLKIT 'New Training Job' UI screenshot; red arrows highlight Training Name/Trigger 'teach3r' and Model Architecture dropdown set to 'Z-Image Turbo'. Fields show GPU #0, Steps 3000, Target LoRA." width="800" height="520"></a></p> 
>  
> <p>You’ll see a training adapter path. There’s also a newer “v2” adapter rolling out. If it’s available in your build, you can switch the file name from v1 to v2 to try it out.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbu1t7mroo0yy47hjlwsu.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbu1t7mroo0yy47hjlwsu.png" alt="Screenshot of a tweet about a v2 z-image-turbo training adapter above a split image: left shows model settings selecting Z-Image Turbo and training_adapter_v2.safetensors with Low VRAM on; right shows config lines highlighting training_adapter_v1.safetensors and training_adapter_v2.safetensors" width="800" height="864"></a></p> 
>  
> <p>Attach your dataset and set preview sampling. Samples during training are clutch—they confirm your LoRA is “taking.”</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F05efx2pxj4rtgj6de7n5.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F05efx2pxj4rtgj6de7n5.png" alt="OSTRIS New Training Job UI: Dataset 1 panel, red arrow 'Select your Dataset...', target teach3r, 1024x1024 selected, sample settings shown, prompt contains 'bomb'." width="800" height="520"></a></p> 
>  
> <p>For samples, create two contrasting prompts so you can inspect generalization:</p> 
>  
> <ul> 
> <li>“{trigger}, cinematic portrait, soft light, 85mm, bokeh”</li> 
> <li>“{trigger}, full body action scene, dynamic pose, outdoor, golden hour”</li> 
> </ul> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgdptz72qnwhc1t3tf73s.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgdptz72qnwhc1t3tf73s.png" alt="Ostris AI-TOOLKIT New Training Job UI showing SAMPLE settings (Sample Every 250, Width/Height 1024, Seed 42), two sample prompts with seeds and LoRA scale, and a red arrow and large red note: 'Recommended to change the prompts to test LoRA outputs during training'." width="800" height="520"></a></p> 
>  
> <blockquote> 
> <p>💡 Pro tip: Keep the LoRA strength modest when previewing (e.g., 0.7–0.9). Too high can overcook and hide issues until it’s too late.</p> 
> </blockquote> 
>  
> <p>If your GPU is tight on VRAM, turn on the Low VRAM option in the model panel.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Step 4 — Start the job and watch it like a hawk 
> </h2> 
>  
> <p>Create Job → Training Queue → Play → Start.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fy0veymeeiiwp70sqqvfn.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fy0veymeeiiwp70sqqvfn.png" alt="Dark OSTRIS AI-Toolkit view for 'teach3r' showing progress and GPU/CPU stats; red arrow and text 'Click the play button to start training' point to the play icon top-right." width="800" height="520"></a></p> 
>  
> <p>On a 5090, ~3k steps typically finishes around the 1‑hour mark (defaults). If samples are configured every 250 steps, you’ll see the subject “phase in” across iterations.</p> 
>  
> <blockquote> 
> <p>🧪 Debug tip: If loss flatlines suspiciously early or samples look unrelated to your subject after ~1k steps, your trigger might not be present in the sample prompts, or your dataset is too small/too noisy.</p> 
> </blockquote> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Step 5 — Evaluate progress and export the LoRA 
> </h2> 
>  
> <p>Open the Samples tab to review the training trajectory. You’ll usually notice early frames not obeying the trigger, then progressively adapting to your subject.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Flx78lhhbyb0gvrfjrgj2.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Flx78lhhbyb0gvrfjrgj2.png" alt="Screenshot of OSTRIS AI-TOOLKIT 'Job: ma1a' Samples tab showing four illustrated teacher-classroom panels, a hand cursor over the teacher, and left navigation menu" width="800" height="507"></a></p> 
>  
> <p>When it’s done, jump to the job Overview → Checkpoints. Download the newest .safetensors file—this is your LoRA.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F74dsa4xju1grbxg0zfc8.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F74dsa4xju1grbxg0zfc8.png" alt="OSTRIS AI-TOOLKIT job 'ma1a' UI showing 'Training completed' banner, terminal logs and progress bar, right sidebar with CPU/GPU stats and a checkpoints list; red annotation arrow points to the ma1a.safetensors download icon and cursor." width="800" height="426"></a></p> 
>  
> <blockquote> 
> <p>📦 Housekeeping: Save the training config alongside the .safetensors so you can reproduce tweaks later (steps, adapter version, dataset size, etc.).</p> 
> </blockquote> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   Step 6 — Try the LoRA in your workflow 
> </h2> 
>  
> <p>I like to validate in ComfyUI with a simple graph: base Z‑Image Turbo → CLIP encode prompt (including trigger) → sampler → VAE → preview.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvbro34pgw6e2iu5opreu.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvbro34pgw6e2iu5opreu.png" alt="ComfyUI node graph showing Load models and CLIP Text Encode nodes with prompt 'mala, school teacher shooting a basketball, smiling', connected sampler and VAE nodes, and a right-side cartoon image preview of a woman shooting a basketball on an outdoor court" width="800" height="520"></a></p> 
>  
> <p>Example prompt:</p> 
>  
> <ul> 
> <li>“myuniqueconcept, cheerful portrait, natural light, editorial style”</li> 
> </ul> 
>  
> <p>If the result skews too strongly to the subject or artifacts creep in, lower the LoRA strength a bit and re‑sample.</p> 
>  
> <p>Final output from one of my runs:</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fctxkzfuwmc78ex2k7717.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fctxkzfuwmc78ex2k7717.png" alt="Smiling girl in a yellow cardigan and blue jeans tossing a basketball toward a hoop on an outdoor court with trees and a building in the background" width="768" height="1024"></a></p>

---

## [3/10] SAML vs OIDC: Choosing the Right Authentication Protocol for Your App
**Source:** The Practical Developer | **Date:** 2025-12-02T14:22:16.000Z
**URL:** https://dev.to/iamdevbox/saml-vs-oidc-choosing-the-right-authentication-protocol-for-your-app-1h6o
**Reasoning:** The article on authentication protocols is not relevant to code intelligence or context engineering.
**Authors:** IAMDevBox

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8eimuzeyyd6rfaomlw89.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>When it comes to authentication, two protocols reign supreme: SAML and OIDC. Both have their strengths and weaknesses, but which one is right for your app? In this article, we'll dive into the details of each protocol and help you make an informed decision.<br><br>  
> SAML, or Security Assertion Markup Language, is an XML-based protocol that allows users to access multiple applications with a single set of login credentials. It's commonly used for enterprise-level authentication, where security is paramount. On the other hand, OIDC, or OpenID Connect, is an OAuth-based protocol that provides a more standardized and flexible approach to authentication. It's perfect for web applications and services that require a high level of customization.<br><br>  
> Both protocols have their advantages and disadvantages. SAML is more secure, but can be complex and inflexible. OIDC is more flexible, but may compromise on security. So, when should you use each? If you're building an enterprise-level application that requires top-notch security, SAML might be the way to go. But if you're building a web application that requires customization and flexibility, OIDC is the better choice.</p>  
>   
> <p>Read more: <a href="https://iamdevbox.com/posts/saml-vs-oidc-when-to-use-which-protocol-in-2025/?utm_source=devto&amp;utm_medium=social&amp;utm_campaign=blog_post">SAML vs OIDC: Choosing the Right Authentication Protocol for Your App</a></p>

---

## [3/10] What is Low Code Tools?What are the tools?
**Source:** The Practical Developer | **Date:** 2025-12-02T14:20:43.000Z
**URL:** https://dev.to/aj_arul/what-is-low-code-toolswhat-are-the-tools-2k7p
**Reasoning:** The article on low code tools is not relevant to code intelligence or context engineering.
**Authors:** Arul .A

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fx8udvpbi316fjpiqcpy6.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><div>  
> <pre><code>The Low Code Tools are Software platforms that let you build applications using drag and drop interfaces, pre built components and minimal coding, instead of writing the full code manually.  
> </code></pre>  
>   
> </div>  
>   
> <p>The Tools Are :</p>  
>   
> <ul>  
> <li><p>Microsoft power apps (Microsoft 365)</p></li>  
> <li><p>Mendix (Enterprice Scale Apps)</p></li>  
> <li><p>OutSystems (Enterprise mobile + web apps)</p></li>  
> <li><p>App sheets(Google) (simple mobile/web apps from spreadsheets)</p></li>  
> <li><p>Bubble(SaaS-like web applications without coding)</p></li>  
> <li><p>Zoho Creator(business workflows, forms, process automation)</p></li>  
> <li><p>Salesforce Lightning(apps inside Salesforce ecosystem)</p></li>  
> </ul>  
>   
> <h2>  
>     
>     
>   -   
> </h2>  
>   
> <ul>  
> <li>  
> </li>  
> </ul>

---

## [3/10] Can an AI Predict the Taste of Your Salad? Here’s What I Built. 🥗🤖
**Source:** The Practical Developer | **Date:** 2025-12-02T14:16:45.000Z
**URL:** https://dev.to/hari_narayanan_c557f6488b/can-an-ai-predict-the-taste-of-your-salad-heres-what-i-built-1ejd
**Reasoning:** The article on AI predicting salad taste is not relevant to code intelligence or context engineering.
**Authors:** hari narayanan

**Content/Abstract:**
> <p>In recent months, I’ve been developing a Salad Taste Prediction AI — a system designed to analyze selected ingredients and forecast the overall flavor profile of a salad. Using structured flavor datasets, ingredient attributes, and machine learning–based taste mapping, the model predicts whether a combination will result in something fresh, tangy, sweet, savory, or otherwise distinctive.</p> 
>  
> <p>The goal behind this project is simple: to bring more clarity and creativity into food experimentation. Whether you’re a nutrition enthusiast, a culinary innovator, or simply someone who enjoys exploring flavors, this technology aims to offer a smarter and more insightful way to understand ingredient interactions.</p> 
>  
> <p>At this stage, I’m interested in understanding how many people would genuinely be excited to use a tool like this. Your interest will help guide the next steps, including making it publicly accessible.</p> 
>  
> <p>If you find this concept valuable or would like to explore such a system in the future, I’d appreciate hearing your thoughts in the comments. Your feedback will play a key role in shaping its development. 🥗✨<br> 
> <a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsdit7cpby1plt3kkegt8.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsdit7cpby1plt3kkegt8.png" alt="" width="800" height="467"></a></p>

---

## [3/10] The Game Math Behind a Candy Craze Game: Probability, Cascades, and Level Pacing in HTML5 Puzzlers
**Source:** The Practical Developer | **Date:** 2025-12-02T14:01:48.000Z
**URL:** https://dev.to/gamh5games/the-game-math-behind-a-candy-craze-game-probability-cascades-and-level-pacing-in-html5-puzzlers-32k6
**Reasoning:** The article is about game mathematics, which is irrelevant to our interests.
**Authors:** gamh5games

**Content/Abstract:**
> <p>Candy puzzle games—often referred to as “candy craze games”—look colorful and playful on the surface, but their underlying mechanics rely heavily on math, probability weighting, grid logic, and pacing algorithms that control difficulty, randomness, and player satisfaction.</p> 
>  
> <p>In this article, we’ll explore the mathematical and algorithmic foundations behind a <a href="https://gamh5.com/game/candy-craze/">candy craze game</a>:</p> 
>  
> <p>How grids are generated</p> 
>  
> <p>How randomness is controlled (not actually random!)</p> 
>  
> <p>How cascades are determined</p> 
>  
> <p>How difficulty ramps up</p> 
>  
> <p>And how to design a fair but exciting experience</p> 
>  
> <p>Whether you're building your own HTML5 puzzle game or analyzing browser games for UI/logic inspiration, understanding these principles is crucial.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbzy7mvesp0wivy738tbt.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbzy7mvesp0wivy738tbt.png" alt="" width="402" height="689"></a><br> 
> 🍬 1. Why Candy Games Need Math (More Than You Think)</p> 
>  
> <p>A candy craze game appears simple:</p> 
>  
> <p>match 3 candies</p> 
>  
> <p>clear them</p> 
>  
> <p>trigger cascades</p> 
>  
> <p>refill the board</p> 
>  
> <p>But fairness and difficulty depend on precisely controlled randomness.</p> 
>  
> <p>If the game is too random → players feel cheated.<br> 
> If it is too predictable → boredom.</p> 
>  
> <p>The solution: guided randomness backed by probability design.</p> 
>  
> <p>🎲 2. Generating the Board: Not Fully Random</p> 
>  
> <p>Most beginners try this:</p> 
>  
> <p>grid[r][c] = randomCandy();</p> 
>  
> <p>But this often produces:</p> 
>  
> <p>instant combos at start (unfair advantage)</p> 
>  
> <p>unwinnable states</p> 
>  
> <p>repetitive patterns</p> 
>  
> <p>too many cascades</p> 
>  
> <p>A professional candy craze game uses “no-initial-matches” logic.<br> 
> function generateCell(r, c) {<br> 
>   let tile;<br> 
>   do {<br> 
>     tile = randomCandy();<br> 
>   } while (<br> 
>     matchesLeft(r, c, tile) ||   // avoid horizontal match<br> 
>     matchesUp(r, c, tile)        // avoid vertical match<br> 
>   );<br> 
>   return tile;<br> 
> }</p> 
>  
> <p>This ensures fair starts while still feeling random.</p> 
>  
> <p>📉 3. Probability Balancing (Weighted Randomness)</p> 
>  
> <p>If all candies have equal probability:</p> 
>  
> <p>Math.random() &lt; 1 / candyTypes</p> 
>  
> <p>the game may generate:</p> 
>  
> <p>too many large combo chains</p> 
>  
> <p>too few matches</p> 
>  
> <p>impossible boards</p> 
>  
> <p>Developers solve this using weighted probability tables:</p> 
>  
> <p>const weights = {<br> 
>   red: 0.20,<br> 
>   blue: 0.20,<br> 
>   yellow: 0.20,<br> 
>   green: 0.20,<br> 
>   purple: 0.20,<br> 
>   // rare types (e.g., special candies)<br> 
>   rainbow: 0.02<br> 
> };</p> 
>  
> <p>This allows designers to tune game feel without changing code.</p> 
>  
> <p>🌊 4. Cascade Logic: “Chain Reactions” That Feel Good</p> 
>  
> <p>The emotional peak of a candy craze game is the cascade:</p> 
>  
> <p>Match 3</p> 
>  
> <p>They disappear</p> 
>  
> <p>Above tiles fall</p> 
>  
> <p>New tiles fill</p> 
>  
> <p>New matches occur</p> 
>  
> <p>Repeat</p> 
>  
> <p>The math behind cascades:</p> 
>  
> <p>A cascade continues as long as there is a match:</p> 
>  
> <p>function cascade() {<br> 
>   let totalCleared = 0;<br> 
>   let matches;</p> 
>  
> <p>do {<br> 
>     matches = findMatches(grid);<br> 
>     clearMatches(matches);<br> 
>     applyGravity(grid);<br> 
>     refillGrid(grid);<br> 
>     totalCleared += matches.length;<br> 
>   } while (matches.length &gt; 0);</p> 
>  
> <p>return totalCleared;<br> 
> }</p> 
>  
> <p>This often results in “lucky streaks,” but even those streaks are mathematically predictable based on candy distribution.</p> 
>  
> <p>📈 5. Level Pacing: The Secret Ingredient</p> 
>  
> <p>Great candy games feel like this:</p> 
>  
> <p>Easy start</p> 
>  
> <p>Moderate difficulty</p> 
>  
> <p>Tension rise</p> 
>  
> <p>Mini breather</p> 
>  
> <p>Challenge spike</p> 
>  
> <p>Reward moment</p> 
>  
> <p>Continue loop</p> 
>  
> <p>This pacing is algorithm driven.</p> 
>  
> <p>Controlled difficulty curve:</p> 
>  
> <p>early levels → high match probability</p> 
>  
> <p>mid levels → introduce obstacles (jelly, blockers, frozen tiles)</p> 
>  
> <p>late levels → reduce match probability</p> 
>  
> <p>Example tuning:</p> 
>  
> <p>function getSpawnWeights(level) {<br> 
>   if (level &lt; 5) return easyWeights;<br> 
>   if (level &lt; 15) return mediumWeights;<br> 
>   return hardWeights;<br> 
> }</p> 
>  
> <p>Difficulty becomes predictable, testable, and adjustable without touching core code.</p> 
>  
> <p>🔥 6. Special Candies: Probability + Trigger Rules</p> 
>  
> <p>Special candies (e.g., bombs, striped candies, color blasters) are generated by:</p> 
>  
> <p>clearing 4</p> 
>  
> <p>clearing 5</p> 
>  
> <p>forming T or L shapes</p> 
>  
> <p>chaining certain combos</p> 
>  
> <p>Special candy detection logic:</p> 
>  
> <p>function detectSpecial(matches) {<br> 
>   if (matches.length === 4) return "line";<br> 
>   if (matches.length === 5) return "rainbow";<br> 
>   if (isTShape(matches) || isLShape(matches)) return "bomb";<br> 
> }</p> 
>  
> <p>These rules drastically impact:</p> 
>  
> <p>board predictability</p> 
>  
> <p>chain potential</p> 
>  
> <p>player satisfaction</p> 
>  
> <p>difficulty scaling</p> 
>  
> <p>Again—mathematics at work.</p> 
>  
> <p>🎮 7. Why Candy Craze Games Are Ideal for HTML5</p> 
>  
> <p>HTML5 (Canvas or WebGL) perfectly matches this genre:</p> 
>  
> <p>grid logic is lightweight</p> 
>  
> <p>animations are small and repeated</p> 
>  
> <p>user input is simple</p> 
>  
> <p>scene transitions are minimal</p> 
>  
> <p>game loop works at 60 FPS</p> 
>  
> <p>assets are easy to compress</p> 
>  
> <p>mobile browsers handle it well</p> 
>  
> <p>Even large animations can be optimized with:</p> 
>  
> <p>sprite atlases</p> 
>  
> <p>batched draw calls</p> 
>  
> <p>GPU-accelerated WebGL pipelines</p> 
>  
> <p>This is why many modern candy craze games run entirely in browsers.</p> 
>  
> <p>🌐 8. Studying Real HTML5 Puzzle Game Implementations</p> 
>  
> <p>If you're researching candy-style games, match-3 mechanics, or grid-based animation behaviors, you can explore collections of HTML5 browser games here:</p> 
>  
> <p>GamH5 — HTML5 Browser Game UI &amp; Logic Examples<br> 
> (Insert your link here)</p> 
>  
> <p>Useful for studying:</p> 
>  
> <p>grid states</p> 
>  
> <p>animation timing</p> 
>  
> <p>cascade sequences</p> 
>  
> <p>difficulty pacing</p> 
>  
> <p>candy-type balancing</p> 
>  
> <p>Great reference material for developers.</p> 
>  
> <p>🧠 Final Thoughts</p> 
>  
> <p>A <a href="https://gamh5.com/game/candy-craze/">candy craze game</a> is a brilliant blend of:</p> 
>  
> <p>probability</p> 
>  
> <p>grid logic</p> 
>  
> <p>pacing systems</p> 
>  
> <p>animation engineering</p> 
>  
> <p>UI feedback design</p> 
>  
> <p>It’s far more mathematical than it appears, and understanding these algorithms helps developers:</p> 
>  
> <p>balance difficulty</p> 
>  
> <p>avoid unfair states</p> 
>  
> <p>control cascades</p> 
>  
> <p>design better user experiences</p> 
>  
> <p>write cleaner, more scalable game logic</p> 
>  
> <p>Beneath the sugar coating lies a finely tuned machine.</p> 
>  
> <p>If you’re building or analyzing HTML5 puzzle games, mastering this math-driven architecture will dramatically improve your ability to create fair, fun, and addictive gameplay loops.</p>

---

## [3/10] Engineering a “Candy Craze Game”: Interaction Model
**Source:** The Practical Developer | **Date:** 2025-12-02T13:59:11.000Z
**URL:** https://dev.to/gamh5games/engineering-a-candy-craze-game-interaction-model-6ka
**Reasoning:** The article is about game engineering, which is irrelevant to our interests.
**Authors:** gamh5games

**Content/Abstract:**
> <p>Candy craze” games—fast, colorful, tap-driven puzzle or matching games—are among the most influential patterns in modern HTML5 casual gaming. Behind the bright animations and candy explosions lies a surprisingly elegant technical architecture built on event systems, rendering pipelines, interaction logic, and resource-efficient desi</p> 
>  
> <p>In this article, we’ll examine the engineering principles behind building a c, f</p> 
>  
> <p>If you're designing or studying HTML5 games, this genre is a near-perfect cas</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ffwy15g2tpdn794ohfpsz.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ffwy15g2tpdn794ohfpsz.png" alt="" width="402" height="689"></a><br> 
> 🍬 1. What Defines</p> 
>  
> <p>A</p> 
>  
> <p>Grid-ba</p> 
>  
> <p>T</p> 
>  
> <p>Chain reacti</p> 
>  
> <p>Particle effec</p> 
>  
> <p>Color-rich sprites and animations</p> 
>  
> <p>Mobile-first design</p> 
>  
> <p>This genre forces the developer to solve:</p> 
>  
> <p>how to detect matches efficiently</p> 
>  
> <p>how to animate falling pieces</p> 
>  
> <p>how to avoid layout jank</p> 
>  
> <p>how to maintain 60 FPS</p> 
>  
> <p>how to sequence chain reactions without blocking</p> 
>  
> <p>A perfect playground for frontend engineering.</p> 
>  
> <p>🧩 2. Grid Architecture: The Heart of the Game</p> 
>  
> <p>Most <a href="https://gamh5.com/game/candy-craze/">candy craze game</a>s are built on a 2D matrix where each cell stores:</p> 
>  
> <p>candy type</p> 
>  
> <p>animation state</p> 
>  
> <p>position</p> 
>  
> <p>matched flag</p> 
>  
> <p>fall velocity</p> 
>  
> <p>A clean grid representation:</p> 
>  
> <p>const grid = Array.from({ length: ROWS }, () =&gt;<br> 
>   Array.from({ length: COLS }, () =&gt; ({<br> 
>     type: randomCandy(),<br> 
>     matched: false<br> 
>   }))<br> 
> );</p> 
>  
> <p>This structure keeps the core logic clean:</p> 
>  
> <p>match detection</p> 
>  
> <p>falling candies</p> 
>  
> <p>refilling the board</p> 
>  
> <p>cascading combos</p> 
>  
> <p>🔍 3. Match Detection (Core Algorithm)</p> 
>  
> <p>The most common detection approach scans horizontally and vertically:</p> 
>  
> <p>function findMatches(grid) {<br> 
>   const matches = [];</p> 
>  
> <p>// Horizontal matches<br> 
>   for (let r = 0; r &lt; ROWS; r++) {<br> 
>     for (let c = 0; c &lt; COLS - 2; c++) {<br> 
>       if (<br> 
>         grid[r][c].type === grid[r][c + 1].type &amp;&amp;<br> 
>         grid[r][c].type === grid[r][c + 2].type<br> 
>       ) {<br> 
>         matches.push([r, c], [r, c + 1], [r, c + 2]);<br> 
>       }<br> 
>     }<br> 
>   }</p> 
>  
> <p>// Vertical matches…<br> 
>   // (Same pattern)</p> 
>  
> <p>return matches;<br> 
> }</p> 
>  
> <p>Simple, predictable, efficient—perfect for mobile.</p> 
>  
> <p>🌀 4. Animation Pipeline: Making It Feel “Crazy”</p> 
>  
> <p>Candy craze games rely on smooth, continuous animations:</p> 
>  
> <p>candies popping</p> 
>  
> <p>falling into empty spaces</p> 
>  
> <p>horizontal swipes</p> 
>  
> <p>combo explosions</p> 
>  
> <p>particle bursts</p> 
>  
> <p>Why real-time animations matter</p> 
>  
> <p>Animations communicate:</p> 
>  
> <p>success</p> 
>  
> <p>progress</p> 
>  
> <p>urgency</p> 
>  
> <p>reward</p> 
>  
> <p>Even micro animations increase retention significantly.</p> 
>  
> <p>Techniques commonly used:</p> 
>  
> <p>WebGL with PixiJS for particles</p> 
>  
> <p>Canvas 2D for simplicity</p> 
>  
> <p>Tween libraries (GSAP, Phaser)</p> 
>  
> <p>Sprite atlas animations</p> 
>  
> <p>Example “falling candy” tween:</p> 
>  
> <p>tween.to(candy, {<br> 
>   y: targetY,<br> 
>   duration: 180,<br> 
>   easing: "easeOutQuad"<br> 
> });</p> 
>  
> <p>Short, snappy animations → addictive gameplay.</p> 
>  
> <p>🎮 5. Input Handling: Tap, Swap, and Interaction Models</p> 
>  
> <p>Candy craze games use simple gestures:</p> 
>  
> <p>tap-to-select</p> 
>  
> <p>swap two adjacent candies</p> 
>  
> <p>drag interaction (optional)</p> 
>  
> <p>A unified input handler for canvas-based games:</p> 
>  
> <p>canvas.addEventListener("pointerdown", (e) =&gt; {<br> 
>   const pos = getPointerPosition(e);<br> 
>   const cell = gridCoords(pos.x, pos.y);<br> 
>   handleSelect(cell);<br> 
> });</p> 
>  
> <p>Mobile browsers require:</p> 
>  
> <p>no 300 ms delay</p> 
>  
> <p>passive event listeners</p> 
>  
> <p>fast hit detection</p> 
>  
> <p>pre-calculated grid cell boundaries</p> 
>  
> <p>This ensures smooth-feeling interaction, even under heavy animation load.</p> 
>  
> <p>⚡ 6. Performance Optimization for Mobile Browsers</p> 
>  
> <p>Candy craze games contain many sprites, animations, and sound events.<br> 
> To maintain 60 FPS:</p> 
>  
> <p>✔ Batch draw calls</p> 
>  
> <p>Reduces overhead for Canvas and WebGL.</p> 
>  
> <p>✔ Use sprite sheets</p> 
>  
> <p>Minimize texture switching.</p> 
>  
> <p>✔ Avoid large PNGs</p> 
>  
> <p>Use WebP or compressed atlases.</p> 
>  
> <p>✔ Cache repeated calculations</p> 
>  
> <p>Grid positions, hitboxes, and candy types.</p> 
>  
> <p>✔ Avoid reflow-heavy DOM</p> 
>  
> <p>Keep everything on  or a WebGL surface.</p> 
>  
> <p>✔ Limit particle count</p> 
>  
> <p>Too many particles = frame drops.</p> 
>  
> <p>Candy craze games feel “crazy” only when performance stays stable.<br> 
> Smoothness is the key to the experience.</p> 
>  
> <p>🎨 7. Game Feel: Why Candy Games Are So Addictive</p> 
>  
> <p>A candy craze game is defined not by logic, but by feedback quality:</p> 
>  
> <p>juicy sounds</p> 
>  
> <p>combo text</p> 
>  
> <p>particle bursts</p> 
>  
> <p>bounce effects</p> 
>  
> <p>glowing candy highlights</p> 
>  
> <p>rhythmic pace</p> 
>  
> <p>Example “pop” effect:</p> 
>  
> <p>function pop(candy) {<br> 
>   tween.to(candy, {<br> 
>     scale: 1.3,<br> 
>     duration: 120,<br> 
>     yoyo: true<br> 
>   });<br> 
> }</p> 
>  
> <p>These tiny moments make the game memorable.</p> 
>  
> <p>🔧 8. Avoiding Spaghetti Code: Recommended Architecture</p> 
>  
> <p>To keep the code scalable:</p> 
>  
> <p>✔ Use a scene-based structure</p> 
>  
> <p>Menu → Game → Results</p> 
>  
> <p>✔ Separate rendering, logic, and input</p> 
>  
> <p>Never mix UI drawing with match detection.</p> 
>  
> <p>✔ Event-based game flow</p> 
>  
> <p>Use an event bus for transitions.</p> 
>  
> <p>✔ Keep animations asynchronous</p> 
>  
> <p>Don’t block the game loop.</p> 
>  
> <p>✔ Use dedicated managers</p> 
>  
> <p>GridManager</p> 
>  
> <p>AnimationManager</p> 
>  
> <p>InputManager</p> 
>  
> <p>SoundManager</p> 
>  
> <p>ComboManager</p> 
>  
> <p>This prevents the logic from becoming unmaintainable as features grow.</p> 
>  
> <p>🌐 9. Studying Real HTML5 Game Implementations</p> 
>  
> <p>If you're researching how real-world HTML5 games structure their UI, grids, animations, or performance pipelines, you can browse collections of lightweight browser games here:</p> 
>  
> <p>GamH5 — HTML5 Browser Game Examples &amp; UI Patterns<br> 
> (Insert your link here)</p> 
>  
> <p>These examples help developers understand how candy-style games structure their feedback loops, cascading logic, and rendering strategy.</p> 
>  
> <p>🧠 Final Thoughts</p> 
>  
> <p>A <a href="https://gamh5.com/game/candy-craze/">candy craze game</a> is more than colorful sprites and candy explosions.<br> 
> From a technical perspective, it's a carefully tuned system built on:</p> 
>  
> <p>optimized grid algorithms</p> 
>  
> <p>event-driven gameplay</p> 
>  
> <p>animation pipelines</p> 
>  
> <p>touch interaction models</p> 
>  
> <p>resource-efficient rendering</p> 
>  
> <p>performance-first thinking</p> 
>  
> <p>This genre may appear simple, but engineering it well requires the same principles used in larger 2D game systems — just in a small, elegant package.</p> 
>  
> <p>For developers building or studying HTML5 games, the candy craze format is one of the best starting points for learning real-time interactive design.</p>

---

## [3/10] StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T14:15:44.000Z
**URL:** https://arxiv.org/abs/2512.01707v1
**Reasoning:** The article is about gaze-guided reasoning in streaming videos, which is irrelevant to our interests.
**Authors:** Daeun Lee

**Content/Abstract:**
> Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.

---

## [3/10] LLM-Driven Multi-Agent Curation and Expansion of Metal-Organic Frameworks Database
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T13:59:55.000Z
**URL:** https://arxiv.org/abs/2512.01693v1
**Reasoning:** The article is about MOF databases, which is irrelevant to our interests.
**Authors:** Honghui Kim

**Content/Abstract:**
> Metal-organic framework (MOF) databases have grown rapidly through experimental deposition and large-scale literature extraction, but recent analyses show that nearly half of their entries contain substantial structural errors. These inaccuracies propagate through high-throughput screening and machine-learning workflows, limiting the reliability of data-driven MOF discovery. Correcting such errors is exceptionally difficult because true repairs require integrating crystallographic files, synthesis descriptions, and contextual evidence scattered across the literature. Here we introduce LitMOF, a large language model-driven multi-agent framework that validates crystallographic information directly from the original literature and cross-validates it with database entries to repair structural errors. Applying LitMOF to the experimental MOF database (the CSD MOF Subset), we constructed LitMOF-DB, a curated set 118,464 computation-ready structures, including corrections of 69% (6,161 MOFs) of the invalid MOFs in the latest CoRE MOF database. Additionally, the system uncovered 12,646 experimentally reported MOFs absent from existing resources, substantially expanding the known experimental design space. This work establishes a scalable pathway toward self-correcting scientific databases and a generalizable paradigm for LLM-driven curation in materials science.

---

## [3/10] Morphling: Fast, Fused, and Flexible GNN Training at Scale
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T13:45:03.000Z
**URL:** https://arxiv.org/abs/2512.01678v1
**Reasoning:** The article is about GNN training, which is irrelevant to our interests.
**Authors:** Anubhab

**Content/Abstract:**
> Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. The results show that Morphling improves per-epoch training throughput by an average of 20X on CPUs and 19X on GPUs over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.

---

## [3/10] Mapping the Landscape of Open Access Dashboards - A Dataset for Research and Infrastructure Development
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T13:39:22.000Z
**URL:** https://arxiv.org/abs/2512.01669v1
**Reasoning:** The article is about open access dashboards, which is irrelevant to our interests.
**Authors:** Johannes Schneider

**Content/Abstract:**
> As Open Access continues to gain importance in science policy, understanding the proportion of Open Access publications relative to the total research output of research-performing organizations, individual countries, or even globally has become increasingly relevant. In response, dashboards are being developed to capture and communicate progress in this area. To provide an overview of these dashboards and their characteristics, an extensive survey was conducted, resulting in the identification of nearly 60 dashboards. To support a detailed and structured description, a dedicated metadata schema was developed, and the identified dashboards were systematically indexed accordingly. To foster community engagement and ensure ongoing development, a participatory process was launched, allowing interested stakeholders to contribute to the dataset. The dataset is particularly relevant for researchers in Library and Information Science (LIS) and Science and Technology Studies (STS), supporting both empirical analyses of Open Access and the methodological refinement of indicators and policy instruments in the context of Open Science.

---

## [3/10] Computing Treedepth Obstructions
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T13:30:53.000Z
**URL:** https://arxiv.org/abs/2512.01658v1
**Reasoning:** The topic is about graph theory and treedepth, which is not directly relevant to our interests in code intelligence or context engineering.
**Authors:** Kolja Kühn

**Content/Abstract:**
> The graph parameter treedepth is minor-monotone; hence, the class of graphs with treedepth at most $k$ is minor-closed. By the Graph Minor Theorem, such a class is characterized by a finite set of forbidden minors. A conjecture of Dvořák, Giannopoulou, and Thilikos states that every such forbidden minor has at most $2^k$ vertices. We present an algorithm that, given $n, k \in \mathbb{N}$, computes the set of forbidden minors, forbidden subgraphs, and forbidden induced subgraphs on at most $n$ vertices, for the class of graphs of treedepth at most $k$. Applying this algorithm to $k = 4$ and $n = 16$, we enumerate 1546 forbidden minors, 1718 forbidden subgraphs, and 12204 forbidden induced subgraphs. Assuming the above conjecture holds, these sets constitute the complete obstruction sets for graphs of treedepth at most 4.

---

## [3/10] In-context Inverse Optimality for Fair Digital Twins: A Preference-based approach
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T13:23:27.000Z
**URL:** https://arxiv.org/abs/2512.01650v1
**Reasoning:** The focus is on digital twins and fairness, which is not directly related to our primary interests.
**Authors:** Daniele Masti

**Content/Abstract:**
> Digital Twins (DTs) are increasingly used as autonomous decision-makers in complex socio-technical systems. Their mathematically optimal decisions often diverge from human expectations, exposing a persistent gap between algorithmic and bounded human rationality. This work addresses this gap by proposing a framework that operationalizes fairness as a learnable objective within optimization-based Digital Twins. We introduce a preference-driven learning pipeline that infers latent fairness objectives directly from human pairwise preferences over feasible decisions. A novel Siamese neural network is developed to generate convex quadratic cost functions conditioned on contextual information. The resulting surrogate objectives align optimization outcomes with human-perceived fairness while maintaining computational efficiency. The approach is demonstrated on a COVID-19 hospital resource allocation scenario. This study provides an actionable path toward embedding human-centered fairness in the design of autonomous decision-making systems.

---

## [3/10] CLIP-RL: Aligning Language and Policy Representations for Task Transfer in Reinforcement Learning
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T12:37:01.000Z
**URL:** https://arxiv.org/abs/2512.01616v1
**Reasoning:** The focus is on reinforcement learning and task transfer, which is not directly related to our primary interests.
**Authors:** Chainesh Gautam

**Content/Abstract:**
> Recently, there has been an increasing need to develop agents capable of solving multiple tasks within the same environment, especially when these tasks are naturally associated with language. In this work, we propose a novel approach that leverages combinations of pre-trained (language, policy) pairs to establish an efficient transfer pipeline. Our algorithm is inspired by the principles of Contrastive Language-Image Pretraining (CLIP) in Computer Vision, which aligns representations across different modalities under the philosophy that ''two modalities representing the same concept should have similar representations.'' The central idea here is that the instruction and corresponding policy of a task represent the same concept, the task itself, in two different modalities. Therefore, by extending the idea of CLIP to RL, our method creates a unified representation space for natural language and policy embeddings. Experimental results demonstrate the utility of our algorithm in achieving faster transfer across tasks.

---

## [3/10] MAC-SLU: Multi-Intent Automotive Cabin Spoken Language Understanding Benchmark
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T12:23:19.000Z
**URL:** https://arxiv.org/abs/2512.01603v1
**Reasoning:** The focus is on spoken language understanding in automotive settings, which is not directly related to our primary interests.
**Authors:** Yuezhang Peng

**Content/Abstract:**
> Spoken Language Understanding (SLU), which aims to extract user semantics to execute downstream tasks, is a crucial component of task-oriented dialog systems. Existing SLU datasets generally lack sufficient diversity and complexity, and there is an absence of a unified benchmark for the latest Large Language Models (LLMs) and Large Audio Language Models (LALMs). This work introduces MAC-SLU, a novel Multi-Intent Automotive Cabin Spoken Language Understanding Dataset, which increases the difficulty of the SLU task by incorporating authentic and complex multi-intent data. Based on MAC-SLU, we conducted a comprehensive benchmark of leading open-source LLMs and LALMs, covering methods like in-context learning, supervised fine-tuning (SFT), and end-to-end (E2E) and pipeline paradigms. Our experiments show that while LLMs and LALMs have the potential to complete SLU tasks through in-context learning, their performance still lags significantly behind SFT. Meanwhile, E2E LALMs demonstrate performance comparable to pipeline approaches and effectively avoid error propagation from speech recognition. Code\footnote{https://github.com/Gatsby-web/MAC\_SLU} and datasets\footnote{huggingface.co/datasets/Gatsby1984/MAC\_SLU} are released publicly.

---

## [3/10] Separator Theorem for Minor-Free Graphs in Linear Time
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T12:01:04.000Z
**URL:** https://arxiv.org/abs/2512.01587v1
**Reasoning:** The topic is about graph theory and separator theorems, which is not directly related to our primary interests.
**Authors:** Édouard Bonnet

**Content/Abstract:**
> The planar separator theorem by Lipton and Tarjan [FOCS '77, SIAM Journal on Applied Mathematics '79] states that any planar graph with $n$ vertices has a balanced separator of size $O(\sqrt{n})$ that can be found in linear time. This landmark result kicked off decades of research on designing linear or nearly linear-time algorithms on planar graphs. In an attempt to generalize Lipton-Tarjan's theorem to nonplanar graphs, Alon, Seymour, and Thomas [STOC '90, Journal of the AMS '90] showed that any minor-free graph admits a balanced separator of size $O(\sqrt{n})$ that can be found in $O(n^{3/2})$ time. The superlinear running time in their separator theorem is a key bottleneck for generalizing algorithmic results from planar to minor-free graphs. Despite extensive research for more than two decades, finding a balanced separator of size $O(\sqrt{n})$ in (linear) $O(n)$ time for minor-free graphs remains a major open problem. Known algorithms either give a separator of size much larger than $O(\sqrt{n})$ or have superlinear running time, or both. 
>   In this paper, we answer the open problem affirmatively. Our algorithm is very simple: it runs a vertex-weighted variant of breadth-first search (BFS) a constant number of times on the input graph. Our key technical contribution is a weighting scheme on the vertices to guide the search for a balanced separator, offering a new connection between the size of a balanced separator and the existence of a clique-minor model. We believe that our weighting scheme may be of independent interest.

---

## [3/10] RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T11:59:03.000Z
**URL:** https://arxiv.org/abs/2512.01582v1
**Reasoning:** The focus is on human motion datasets, which is not directly related to our primary interests.
**Authors:** Junran Peng

**Content/Abstract:**
> In this paper, we introduce RoleMotion, a large-scale human motion dataset that encompasses a wealth of role-playing and functional motion data tailored to fit various specific scenes. Existing text datasets are mainly constructed decentrally as amalgamation of assorted subsets that their data are nonfunctional and isolated to work together to cover social activities in various scenes. Also, the quality of motion data is inconsistent, and textual annotation lacks fine-grained details in these datasets. In contrast, RoleMotion is meticulously designed and collected with a particular focus on scenes and roles. The dataset features 25 classic scenes, 110 functional roles, over 500 behaviors, and 10296 high-quality human motion sequences of body and hands, annotated with 27831 fine-grained text descriptions. We build an evaluator stronger than existing counterparts, prove its reliability, and evaluate various text-to-motion methods on our dataset. Finally, we explore the interplay of motion generation of body and hands. Experimental results demonstrate the high-quality and functionality of our dataset on text-driven whole-body generation.

---

## [3/10] From Black Hole to Galaxy: Neural Operator: Framework for Accretion and Feedback Dynamics
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T11:47:49.000Z
**URL:** https://arxiv.org/abs/2512.01576v1
**Reasoning:** The topic is about astrophysics and neural operators, which is not directly related to our primary interests.
**Authors:** Nihaal Bhojwani

**Content/Abstract:**
> Modeling how supermassive black holes co-evolve with their host galaxies is notoriously hard because the relevant physics spans nine orders of magnitude in scale-from milliparsecs to megaparsecs--making end-to-end first-principles simulation infeasible. To characterize the feedback from the small scales, existing methods employ a static subgrid scheme or one based on theoretical guesses, which usually struggle to capture the time variability and derive physically faithful results. Neural operators are a class of machine learning models that achieve significant speed-up in simulating complex dynamics. We introduce a neural-operator-based ''subgrid black hole'' that learns the small-scale local dynamics and embeds it within the direct multi-level simulations. Trained on small-domain (general relativistic) magnetohydrodynamic data, the model predicts the unresolved dynamics needed to supply boundary conditions and fluxes at coarser levels across timesteps, enabling stable long-horizon rollouts without hand-crafted closures. Thanks to the great speedup in fine-scale evolution, our approach for the first time captures intrinsic variability in accretion-driven feedback, allowing dynamic coupling between the central black hole and galaxy-scale gas. This work reframes subgrid modeling in computational astrophysics with scale separation and provides a scalable path toward data-driven closures for a broad class of systems with central accretors.

---

## [3/10] Reconstructing Multi-Scale Physical Fields from Extremely Sparse Measurements with an Autoencoder-Diffusion Cascade
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T11:46:14.000Z
**URL:** https://arxiv.org/abs/2512.01572v1
**Reasoning:** The focus is on physical field reconstruction, which is not directly related to our primary interests.
**Authors:** Letian Yi

**Content/Abstract:**
> Reconstructing full fields from extremely sparse and random measurements is a longstanding ill-posed inverse problem. A powerful framework for addressing such challenges is hierarchical probabilistic modeling, where uncertainty is represented by intermediate variables and resolved through marginalization during inference. Inspired by this principle, we propose Cascaded Sensing (Cas-Sensing), a hierarchical reconstruction framework that integrates an autoencoder-diffusion cascade. First, a neural operator-based functional autoencoder reconstructs the dominant structures of the original field - including large-scale components and geometric boundaries - from arbitrary sparse inputs, serving as an intermediate variable. Then, a conditional diffusion model, trained with a mask-cascade strategy, generates fine-scale details conditioned on these large-scale structures. To further enhance fidelity, measurement consistency is enforced via the manifold constrained gradient based on Bayesian posterior sampling during the generation process. This cascaded pipeline substantially alleviates ill-posedness, delivering accurate and robust reconstructions. Experiments on both simulation and real-world datasets demonstrate that Cas-Sensing generalizes well across varying sensor configurations and geometric boundaries, making it a promising tool for practical deployment in scientific and engineering applications.

---

## [3/10] Deep FlexQP: Accelerated Nonlinear Programming via Deep Unfolding
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T11:38:45.000Z
**URL:** https://arxiv.org/abs/2512.01565v1
**Reasoning:** The article is about nonlinear programming optimization, which is not directly relevant to our interests.
**Authors:** Alex Oshin

**Content/Abstract:**
> We propose an always-feasible quadratic programming (QP) optimizer, FlexQP, which is based on an exact relaxation of the QP constraints. If the original constraints are feasible, then the optimizer finds the optimal solution to the original QP. On the other hand, if the constraints are infeasible, the optimizer identifies a solution that minimizes the constraint violation in a sparse manner. FlexQP scales favorably with respect to the problem dimension, is robust to both feasible and infeasible QPs with minimal assumptions on the problem data, and can be effectively warm-started. We subsequently apply deep unfolding to improve our optimizer through data-driven techniques, leading to an accelerated Deep FlexQP. By learning dimension-agnostic feedback policies for the parameters from a small number of training examples, Deep FlexQP generalizes to problems with larger dimensions and can optimize for many more iterations than it was initially trained for. Our approach outperforms two recently proposed state-of-the-art accelerated QP approaches on a suite of benchmark systems including portfolio optimization, classification, and regression problems. We provide guarantees on the expected performance of our deep QP optimizer through probably approximately correct (PAC) Bayes generalization bounds. These certificates are used to design an accelerated sequential quadratic programming solver that solves nonlinear optimal control and predictive safety filter problems faster than traditional approaches. Overall, our approach is very robust and greatly outperforms existing non-learning and learning-based optimizers in terms of both runtime and convergence to the optimal solution across multiple classes of NLPs.

---

## [3/10] Language Diversity: Evaluating Language Usage and AI Performance on African Languages in Digital Spaces
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T11:27:13.000Z
**URL:** https://arxiv.org/abs/2512.01557v1
**Reasoning:** The article is about language diversity and AI performance, which is not directly related to our core interests.
**Authors:** Edward Ajayi

**Content/Abstract:**
> This study examines the digital representation of African languages and the challenges this presents for current language detection tools. We evaluate their performance on Yoruba, Kinyarwanda, and Amharic. While these languages are spoken by millions, their online usage on conversational platforms is often sparse, heavily influenced by English, and not representative of the authentic, monolingual conversations prevalent among native speakers. This lack of readily available authentic data online creates a challenge of scarcity of conversational data for training language models. To investigate this, data was collected from subreddits and local news sources for each language. The analysis showed a stark contrast between the two sources. Reddit data was minimal and characterized by heavy code-switching. Conversely, local news media offered a robust source of clean, monolingual language data, which also prompted more user engagement in the local language on the news publishers social media pages. Language detection models, including the specialized AfroLID and a general LLM, performed with near-perfect accuracy on the clean news data but struggled with the code-switched Reddit posts. The study concludes that professionally curated news content is a more reliable and effective source for training context-rich AI models for African languages than data from conversational platforms. It also highlights the need for future models that can process clean and code-switched text to improve the detection accuracy for African languages.

---

## [3/10] Delta Sum Learning: an approach for fast and global convergence in Gossip Learning
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T11:23:51.000Z
**URL:** https://arxiv.org/abs/2512.01549v1
**Reasoning:** The article is about gossip learning, which is not directly relevant to our core interests.
**Authors:** Tom Goethals

**Content/Abstract:**
> Federated Learning is a popular approach for distributed learning due to its security and computational benefits. With the advent of powerful devices in the network edge, Gossip Learning further decentralizes Federated Learning by removing centralized integration and relying fully on peer to peer updates. However, the averaging methods generally used in both Federated and Gossip Learning are not ideal for model accuracy and global convergence. Additionally, there are few options to deploy Learning workloads in the edge as part of a larger application using a declarative approach such as Kubernetes manifests. This paper proposes Delta Sum Learning as a method to improve the basic aggregation operation in Gossip Learning, and implements it in a decentralized orchestration framework based on Open Application Model, which allows for dynamic node discovery and intent-driven deployment of multi-workload applications. Evaluation results show that Delta Sum performance is on par with alternative integration methods for 10 node topologies, but results in a 58% lower global accuracy drop when scaling to 50 nodes. Overall, it shows strong global convergence and a logarithmic loss of accuracy with increasing topology size compared to a linear loss for alternatives under limited connectivity.

---

## [3/10] LPCD: Unified Framework from Layer-Wise to Submodule Quantization
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T11:21:18.000Z
**URL:** https://arxiv.org/abs/2512.01546v1
**Reasoning:** The article discusses quantization in neural networks, which is not directly related to our focus areas.
**Authors:** Yuma Ichikawa

**Content/Abstract:**
> Post-training quantization (PTQ) aims to preserve model-level behavior; however, most methods focus on individual linear layers. Even recent extensions, such as QEP and LoaQ, which mitigate error propagation or target specific submodules, still rely on layer-wise formulations and fail to capture the behavior of larger submodules. We introduce Layer-Projected Coordinate Descent (LPCD), a unified framework that extends PTQ beyond layers by optimizing relaxed objectives across arbitrary submodules and projecting the solutions with layer-wise quantizers. LPCD generalizes existing methods and provides a principled approach to quantizing complex submodules while maintaining the efficiency and compatibility of layer-wise PTQ pipelines. Across diverse LLM architectures and bit-widths, LPCD-based submodule quantization consistently enhances both layer-wise PTQ methods and existing submodule approaches.

---

## [3/10] MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T10:39:12.000Z
**URL:** https://arxiv.org/abs/2512.01512v1
**Reasoning:** The article is about speech-to-text translation, which is not directly relevant to our core interests.
**Authors:** Yexing Du

**Content/Abstract:**
> Multimodal Large Language Models (MLLMs) have achieved great success in Speech-to-Text Translation (S2TT) tasks. However, current research is constrained by two key challenges: language coverage and efficiency. Most of the popular S2TT datasets are substantially English-centric, which restricts the scaling-up of MLLMs' many-to-many translation capabilities. Moreover, the inference speed of MLLMs degrades dramatically when the speech is converted into long sequences (e.g., 750 tokens). To address these limitations, we propose a Multilingual Cost-effective Accelerated Speech-to-Text Translator (MCAT) framework, which includes two innovations. First, a language scaling method that leverages curriculum learning and a data balancing strategy is introduced to extend the language coverage supported by MLLMs to 70 languages and achieve mutual translation among these languages. Second, an optimized speech adapter module is designed to reduce the length of the speech sequence to only 30 tokens. Extensive experiments were conducted on MLLMs of different scales (9B and 27B). The experimental results demonstrate that MCAT not only surpasses state-of-the-art end-to-end models on the FLEURS dataset across 70x69 directions but also enhances batch inference efficiency. This is achieved with only ~100M trainable parameters and by using only 10 hours of S2TT data per language. Furthermore, we have released MCAT as open-source to promote the development of MLLMs for robust S2TT capabilities. The code and models are released at https://github.com/yxduir/m2m-70.

---

## [3/10] SynthStrategy: Extracting and Formalizing Latent Strategic Insights from LLMs in Organic Chemistry
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T10:33:00.000Z
**URL:** https://arxiv.org/abs/2512.01507v1
**Reasoning:** The article is about using LLMs in organic chemistry, which is not relevant to our focus areas.
**Authors:** Daniel Armstrong

**Content/Abstract:**
> Modern computer-assisted synthesis planning (CASP) systems show promises at generating chemically valid reaction steps but struggle to incorporate strategic considerations such as convergent assembly, protecting group minimization, and optimal ring-forming sequences. We introduce a methodology that leverages Large Language Models to distill synthetic knowledge into code. Our system analyzes synthesis routes and translates strategic principles into Python functions representing diverse strategic and tactical rules, such as strategic functional group interconversions and ring construction strategies. By formalizing this knowledge as verifiable code rather than simple heuristics, we create testable, interpretable representations of synthetic strategy. We release the complete codebase and the USPTO-ST dataset -- synthesis routes annotated with strategic tags. This framework unlocks a novel capability for CASP: natural language-based route retrieval, achieving 75\% Top-3 accuracy on our benchmark. We further validate our library through temporal analysis of historical trends and chemically intuitive route clustering that offers more granular partitioning than common previous methods. This work bridges the tactical-strategic divide in CASP, enabling specification, search, and evaluation of routes by strategic criteria rather than structure alone.

---

## [3/10] Formal Verification of Noisy Quantum Reinforcement Learning Policies
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T10:26:33.000Z
**URL:** https://arxiv.org/abs/2512.01502v1
**Reasoning:** The article on quantum reinforcement learning is not directly related to our primary or secondary interests.
**Authors:** Dennis Gross

**Content/Abstract:**
> Quantum reinforcement learning (QRL) aims to use quantum effects to create sequential decision-making policies that achieve tasks more effectively than their classical counterparts. However, QRL policies face uncertainty from quantum measurements and hardware noise, such as bit-flip, phase-flip, and depolarizing errors, which can lead to unsafe behavior. Existing work offers no systematic way to verify whether trained QRL policies meet safety requirements under specific noise conditions. 
>   We introduce QVerifier, a formal verification method that applies probabilistic model checking to analyze trained QRL policies with and without modeled quantum noise. QVerifier builds a complete model of the policy-environment interaction, incorporates quantum uncertainty directly into the transition probabilities, and then checks safety properties using the Storm model checker. 
>   Experiments across multiple QRL environments show that QVerifier precisely measures how different noise models influence safety, revealing both performance degradation and cases where noise can help. By enabling rigorous safety verification before deployment, QVerifier addresses a critical need: because access to quantum hardware is expensive, pre-deployment verification is essential for any safety-critical use of QRL. QVerifier targets a potential classical-quantum sweet spot: trained QRL policies that execute efficiently on quantum hardware, yet remain tractable for classical probabilistic model checking despite being too slow for real-time classical deployment.

---

## [3/10] Multi-view diffusion geometry using intertwined diffusion trajectories
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T10:05:19.000Z
**URL:** https://arxiv.org/abs/2512.01484v1
**Reasoning:** The paper on multi-view diffusion geometry is not relevant to our interests in code intelligence or context engineering.
**Authors:** Gwendal Debaussart-Joniec

**Content/Abstract:**
> This paper introduces a comprehensive unified framework for constructing multi-view diffusion geometries through intertwined multi-view diffusion trajectories (MDTs), a class of inhomogeneous diffusion processes that iteratively combine the random walk operators of multiple data views. Each MDT defines a trajectory-dependent diffusion operator with a clear probabilistic and geometric interpretation, capturing over time the interplay between data views. Our formulation encompasses existing multi-view diffusion models, while providing new degrees of freedom for view interaction and fusion. We establish theoretical properties under mild assumptions, including ergodicity of both the point-wise operator and the process in itself. We also derive MDT-based diffusion distances, and associated embeddings via singular value decompositions. Finally, we propose various strategies for learning MDT operators within the defined operator space, guided by internal quality measures. Beyond enabling flexible model design, MDTs also offer a neutral baseline for evaluating diffusion-based approaches through comparison with randomly selected MDTs. Experiments show the practical impact of the MDT operators in a manifold learning and data clustering context.

---

## [3/10] CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T09:58:24.000Z
**URL:** https://arxiv.org/abs/2512.01478v1
**Reasoning:** The article on basketball motion representations is not related to our focus areas of code intelligence or context engineering.
**Authors:** Omer Sela

**Content/Abstract:**
> This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach first processes skeletal tracking data through Graph Neural Networks to capture nuanced motion patterns, then employs a Transformer architecture with specialized attention mechanisms to model player interactions. We introduce event projection heads that explicitly connect player movements to basketball events like passes, shots, and steals, training the model to associate physical motion patterns with their tactical purposes. Experiments on NBA tracking data demonstrate significant improvements over position-only baselines: 35% reduction in trajectory prediction error compared to state-of-the-art position-based models and consistent performance gains across key basketball analytics tasks. The resulting pretrained model serves as a powerful foundation for multiple downstream tasks, with pick detection, shot taker identification, assist prediction, shot location classification, and shot type recognition demonstrating substantial improvements over existing methods.

---

## [3/10] Does Flatness imply Generalization for Logistic Loss in Univariate Two-Layer ReLU Network?
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T09:57:11.000Z
**URL:** https://arxiv.org/abs/2512.01473v1
**Reasoning:** The discussion on generalization in neural networks is not directly relevant to our primary or secondary interests.
**Authors:** Dan Qiao

**Content/Abstract:**
> We consider the problem of generalization of arbitrarily overparameterized two-layer ReLU Neural Networks with univariate input. Recent work showed that under square loss, flat solutions (motivated by flat / stable minima and Edge of Stability phenomenon) provably cannot overfit, but it remains unclear whether the same phenomenon holds for logistic loss. This is a puzzling open problem because existing work on logistic loss shows that gradient descent with increasing step size converges to interpolating solutions (at infinity, for the margin-separable cases). In this paper, we prove that the \emph{flatness implied generalization} is more delicate under logistic loss. On the positive side, we show that flat solutions enjoy near-optimal generalization bounds within a region between the left-most and right-most \emph{uncertain} sets determined by each candidate solution. On the negative side, we show that there exist arbitrarily flat yet overfitting solutions at infinity that are (falsely) certain everywhere, thus certifying that flatness alone is insufficient for generalization in general. We demonstrate the effects predicted by our theory in a well-controlled simulation study.

---

## [3/10] Seeking Career guidance
**Source:** The Practical Developer | **Date:** 2025-12-02T13:54:29.000Z
**URL:** https://dev.to/abby21/seeking-career-guidance-45hb
**Reasoning:** The request for career guidance does not align with our interests in AI, DevTools, or Software Engineering Trends.
**Authors:** Abby

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F4d3m9h8yqs88yua3e2hq.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>I'm looking for career guidance. I work as senior software engineer. I have 10+yrs of experience. <br>  
> I have worked in java, gwt, angular,react, aws lambda, accessibility,  little bit of mongodb atlas ui. I work well but now when I look back I dont remember these skills in interview poit of view. I also feel lost in career. <br>  
> I want to refresh and upskill to align with current expectations for my experience. Also would like to know what skills and roles will be good choice,so that I can work on it? I have no guidance so I'm seeking for guidance so that I can have a good roadmap.</p>

---

## [3/10] Behind the Scenes of a “Crazy Pizza Game”: How HTML5 Casual Games Are Built
**Source:** The Practical Developer | **Date:** 2025-12-02T13:49:12.000Z
**URL:** https://dev.to/gamh5games/behind-the-scenes-of-a-crazy-pizza-game-how-html5-casual-games-are-built-1hjn
**Reasoning:** The article on building HTML5 casual games is not relevant to our focus on code intelligence or context engineering.
**Authors:** gamh5games

**Content/Abstract:**
> <p>Casual browser games have exploded in popularity over the last few years, and one of the most recognizable formats is the fast-paced “<a href="https://gamh5.com/game/crazy-pizza/">crazy pizza game</a>” — a game where players quickly assemble pizzas, match ingredients, or manage orders under time pressure.</p> 
>  
> <p>But what does it take to build a game like this?<br> 
> Why do HTML5 engines handle these mechanics so well?<br> 
> And how do developers make such games run smoothly even on low-end mobile devices?</p> 
>  
> <p>In this article, we’ll break down the design and technical foundations behind a “crazy pizza game,” from core gameplay loops to rendering, performance optimization, asset pipelines, and browser considerations.</p> 
>  
> <p>🍕 1. What Defines a “Crazy Pizza Game”?</p> 
>  
> <p>A typical crazy pizza game includes these characteristics:</p> 
>  
> <p>Fast decision-making</p> 
>  
> <p>Time-based challenges (countdowns, increasing speed, etc.)</p> 
>  
> <p>Ingredient combinations (drag-and-drop, tap-to-select, matching patterns)</p> 
>  
> <p>Continuous feedback (animations, sound effects, combo popups)</p> 
>  
> <p>Short gameplay loops that encourage replayability</p> 
>  
> <p>Supports both desktop and mobile browsers</p> 
>  
> <p>These features make the genre ideal for HTML5/JavaScript, because the UI interactions are lightweight and the gameplay loop is simple but addictive.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fryu8horx7qw9bgqfvubk.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fryu8horx7qw9bgqfvubk.png" alt="" width="800" height="564"></a></p> 
>  
> <p>🚀 2. Choosing the Right HTML5 Engine</p> 
>  
> <p>For a crazy pizza game, any of the major HTML5 engines can work:</p> 
>  
> <p>✔ Phaser</p> 
>  
> <p>Most common choice</p> 
>  
> <p>Great for 2D animations, sprites, state machines</p> 
>  
> <p>Built-in physics and input systems</p> 
>  
> <p>✔ PixiJS</p> 
>  
> <p>Perfect for fast rendering</p> 
>  
> <p>Good for animation-heavy gameplay</p> 
>  
> <p>✔ Pure JavaScript + Canvas</p> 
>  
> <p>Best for lightweight casual games</p> 
>  
> <p>Minimal engine overhead</p> 
>  
> <p>Great performance on mobile</p> 
>  
> <p>Here’s a small example of a simple pizza ingredient click-handler using vanilla JS:</p> 
>  
> <p>canvas.addEventListener("click", (e) =&gt; {<br> 
>   const x = e.offsetX;<br> 
>   const y = e.offsetY;</p> 
>  
> <p>ingredients.forEach(item =&gt; {<br> 
>     if (item.isClicked(x, y)) {<br> 
>       item.select();<br> 
>       score++;<br> 
>     }<br> 
>   });<br> 
> });</p> 
>  
> <p>The core interaction is simple — which is why many successful HTML5 pizza games are under 200 KB in script size.</p> 
>  
> <p>🎮 3. Designing the Core Gameplay Loop</p> 
>  
> <p>Every successful crazy pizza game relies on a tight loop:</p> 
>  
> <p>Display random ingredients</p> 
>  
> <p>Player selects or assembles them</p> 
>  
> <p>Timer decreases or speed increases</p> 
>  
> <p>Feedback (points, sounds, animations)</p> 
>  
> <p>New round begins instantly</p> 
>  
> <p>A clean gameplay loop example:</p> 
>  
> <p>function gameLoop() {<br> 
>   spawnIngredients();</p> 
>  
> <p>timer.start(30);</p> 
>  
> <p>timer.onTick(() =&gt; updateUI());</p> 
>  
> <p>timer.onEnd(() =&gt; {<br> 
>     endGame(score);<br> 
>   });<br> 
> }</p> 
>  
> <p>The trick is to keep everything fast and readable, especially since mobile players have less patience for lag.</p> 
>  
> <p>🎨 4. Graphics &amp; Assets Optimization</p> 
>  
> <p>Crazy pizza games often include dozens of PNG icons:</p> 
>  
> <p>Sauce</p> 
>  
> <p>Cheese</p> 
>  
> <p>Pepperoni</p> 
>  
> <p>Mushroom</p> 
>  
> <p>Onion</p> 
>  
> <p>Oven effects</p> 
>  
> <p>Combo icons<br> 
> …and more.</p> 
>  
> <p>To keep performance high:</p> 
>  
> <p>✔ Use sprite sheets instead of individual images<br> 
> ✔ Compress PNGs with TinyPNG or Squoosh<br> 
> ✔ Preload assets before starting the game<br> 
> ✔ Keep your texture count low to reduce GPU swaps</p> 
>  
> <p>A typical loading script:</p> 
>  
> <p>const assets = [<br> 
>   "pizza-base.png",<br> 
>   "pepperoni.png",<br> 
>   "cheese.png",<br> 
>   "combo.png",<br> 
>   "timer.png"<br> 
> ];</p> 
>  
> <p>Promise.all(assets.map(loadImage)).then(startGame);</p> 
>  
> <p>📱 5. Mobile Performance Best Practices</p> 
>  
> <p>Since most crazy pizza game players are on mobile browsers, optimization is essential:</p> 
>  
> <p>Avoid unnecessary DOM updates</p> 
>  
> <p>Use a single  when possible</p> 
>  
> <p>Keep animation frame rates stable (use requestAnimationFrame)</p> 
>  
> <p>Minify and bundle your scripts</p> 
>  
> <p>Limit physics calculations</p> 
>  
> <p>Pre-calculate ingredient positions</p> 
>  
> <p>Even a simple change like caching your ingredient hit-boxes can improve performance dramatically.</p> 
>  
> <p>🔊 6. Creating Player Feedback &amp; Game Feel</p> 
>  
> <p>The “crazy” feeling of pizza games comes from feedback:</p> 
>  
> <p>Fast pop animations</p> 
>  
> <p>Combo counters</p> 
>  
> <p>Sound effects</p> 
>  
> <p>Ingredient “snap” motions</p> 
>  
> <p>Quick color flashes</p> 
>  
> <p>Most designers use:</p> 
>  
> <p>Tween libraries (GSAP, Phaser tweens)</p> 
>  
> <p>Lightweight sound libraries like Howler.js</p> 
>  
> <p>Example: adding a satisfying “ingredient placed” animation:</p> 
>  
> <p>tween.to(ingredient, {<br> 
>   scale: 1.2,<br> 
>   duration: 80,<br> 
>   yoyo: true<br> 
> });</p> 
>  
> <p>Fast. Simple. Effective.</p> 
>  
> <p>🌐 7. Where to Explore Examples of Crazy Pizza Games</p> 
>  
> <p>If you're studying the design or UI patterns of crazy pizza games, you can browse collections of lightweight HTML5 browser games here:</p> 
>  
> <p>GamH5 — HTML5 Browser Game Examples<br> 
> (Insert your link here)</p> 
>  
> <p>It’s a useful reference if you're researching casual game mechanics or UI flows typically used in the genre.</p> 
>  
> <p>🧠 Final Thoughts</p> 
>  
> <p>A “crazy pizza game” may look simple on the surface, but behind the scenes it combines:</p> 
>  
> <p>event-driven UI</p> 
>  
> <p>sprite rendering</p> 
>  
> <p>performance optimization</p> 
>  
> <p>user psychology</p> 
>  
> <p>touch interaction design</p> 
>  
> <p>asset management</p> 
>  
> <p>These micro-games are a great way for developers to practice HTML5 game development skills while delivering fun, fast-paced experiences to players.</p> 
>  
> <p>If you’re building your own <a href="https://gamh5.com/game/crazy-pizza/">crazy pizza game</a> or optimizing one, remember:<br> 
> smooth performance + instant feedback = addictive gameplay.</p>

---

## [3/10] AuroraCanvas — A Cross-Platform Generative Art Experience
**Source:** The Practical Developer | **Date:** 2025-12-02T13:48:45.000Z
**URL:** https://dev.to/s10olamide/auroracanvas-a-cross-platform-generative-art-experience-5ac4
**Reasoning:** The description of a generative art experience is not relevant to our focus on code intelligence or context engineering.
**Authors:** s10olamide

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9ngehicxcc9ztmrhlzia.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>What I Built</p>  
>   
> <p>AuroraCanvas is a visually immersive generative art playground built with Uno Platform and powered by AI-assisted color and particle effects.</p>  
>   
> <p>Theme: Cosmic / Ambient / Fluid Visuals<br>  
> Special features:</p>  
>   
> <p>✨ Dynamic AI-generated color palettes</p>  
>   
> <p>🌌 Touch/mouse-responsive particle flows</p>  
>   
> <p>🎵 Audio-reactive animations</p>  
>   
> <p>💫 “Surprise Me” mode: continuously evolving scenes</p>  
>   
> <p>🎥 Demo</p>  
>   
> <p>Live Demo (WebAssembly): [Insert link]<br>  
> GitHub Repository: [Insert link]</p>  
>   
> <p>Screenshots / GIFs:</p>  
>   
> <p>🖥️ Windows: Full-screen particle animations</p>  
>   
> <p>📱 iOS / Android: Touch painting mode</p>  
>   
> <p>🌐 WASM: Browser-based experience</p>  
>   
> <p>Test Account (if login required):</p>  
>   
> <p>Email: <a href="mailto:test@demo.com">test@demo.com</a></p>  
>   
> <p>Password: Demo123!</p>  
>   
> <p>🧩 Cross-Platform Magic</p>  
>   
> <p>Platforms supported:</p>  
>   
> <p>iOS, Android</p>  
>   
> <p>Windows, macOS, Linux (Skia backend)</p>  
>   
> <p>WebAssembly</p>  
>   
> <p>Single codebase benefits:</p>  
>   
> <p>95% shared code</p>  
>   
> <p>XAML-based UI</p>  
>   
> <p>Shared logic in .NET</p>  
>   
> <p>Only small platform-specific hooks (GPU, gestures)</p>  
>   
> <p>Seeing the same shimmering art run identically on mobile, desktop, and web—that’s the true magic of Uno Platform.</p>  
>   
> <p>🕹️ Interactive Features</p>  
>   
> <p>Touch + Mouse Painting: Draw with shimmering, ripple, or exploding particles</p>  
>   
> <p>AI Palette Generator: Instantly generates new color schemes</p>  
>   
> <p>Scene Presets: Nebula, Aurora, Watercolor, Starfall</p>  
>   
> <p>Animations: SmoothSpring transitions, parallax layers, GPU shader effects</p>  
>   
> <p>Customizable Controls: Brush size, particle behavior, speed, gravity fields</p>  
>   
> <p>Every interaction feels satisfying and alive.</p>  
>   
> <p>🌠 The Wow Factor</p>  
>   
> <p>Real-time generative visuals that feel alive</p>  
>   
> <p>AI-assisted scene creation makes every session unique</p>  
>   
> <p>Identical behavior across all platforms</p>  
>   
> <p>Zero-lag animations, even in WebAssembly</p>  
>   
> <p>“Living wallpaper” mode for ambient art displays</p>  
>   
> <p>2️⃣ Cover Image Suggestion</p>  
>   
> <p>Theme: Aurora / Nebula / Particle Flow</p>  
>   
> <p>Text overlay: “AuroraCanvas — Generative Art Everywhere”</p>  
>   
> <p>Optional animation: subtle moving particles in GIF format for social posts</p>  
>   
> <p>3️⃣ GIF Demo Suggestions</p>  
>   
> <p>Screen recording of:</p>  
>   
> <p>Touch painting with particle bursts</p>  
>   
> <p>“Surprise Me” mode generating a new scene</p>  
>   
> <p>Switching between desktop, mobile, and browser</p>  
>   
> <p>Tip: Keep each GIF 5–10 seconds long to highlight interactivity.</p>  
>   
> <p>4️⃣ Code Snippets for DEV Submission</p>  
>   
> <p>Shared UI example (XAML):</p>  
>   
> <p><br>  
>       
>             PointerMoved="OnPointerMoved"/&gt;<br>  
>     <br>  
> </p>  
>   
> <p>Shared Logic (C#):</p>  
>   
> <p>public void OnPointerMoved(object sender, PointerRoutedEventArgs e)<br>  
> {<br>  
>     var point = e.GetCurrentPoint(ParticleCanvas).Position;<br>  
>     ParticleEngine.SpawnParticle(point, currentPalette);<br>  
> }</p>  
>   
> <p>public void GenerateNewScene(object sender, RoutedEventArgs e)<br>  
> {<br>  
>     currentPalette = AIPaletteGenerator.CreateRandomPalette();<br>  
>     ParticleEngine.ResetScene();<br>  
> }</p>  
>   
> <p>Cross-platform note:</p>  
>   
> <p>GPU effects use SkiaSharp on Windows/macOS/Linux and Canvas2D / WebGL for WebAssembly.</p>  
>   
> <p>Particle engine runs identically across mobile and desktop thanks to shared .NET logic.</p>  
>   
> <p>5️⃣ Complete README for GitHub</p>  
>   
> <h1>  
>     
>     
>   AuroraCanvas 🌌  
> </h1>  
>   
> <p>AuroraCanvas is a <strong>cross-platform generative art playground</strong> built with <strong>Uno Platform</strong>.</p>  
>   
> <h2>  
>     
>     
>   Features  
> </h2>  
>   
> <ul>  
> <li>Touch/mouse responsive particle painting</li>  
> <li>AI-generated color palettes</li>  
> <li>Real-time animations</li>  
> <li>“Surprise Me” mode: continuously evolving scenes</li>  
> <li>Cross-platform: iOS, Android, Windows, macOS, Linux, WebAssembly</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   Demo  
> </h2>  
>   
> <ul>  
> <li>Web: [Insert link]</li>  
> <li>GitHub: [Insert repo link]</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   Installation  
> </h2>  
>   
>   
>   
> <div>  
> <pre><code>  
> bash  
> git clone https://github.com/yourusername/AuroraCanvas.git  
> cd AuroraCanvas  
>   
>   
> Follow Uno Platform instructions for your target platform.  
>   
> Usage  
>   
> Launch app on any supported platform  
>   
> Touch/click the canvas to draw particles  
>   
> Press "Surprise Me" to generate a new scene  
>   
> Adjust brush, particle speed, and colors in settings  
>   
> Contributing  
>   
> PRs and issues are welcome!  
>   
> License  
>   
> MIT License  
> </code></pre>  
>   
> </div>

---

## [3/10] 9 Essential Developer Tools You Should be Exploring Right Now ⚡️🔎
**Source:** The Practical Developer | **Date:** 2025-12-02T13:48:06.000Z
**URL:** https://dev.to/madza/9-essential-developer-tools-you-should-be-exploring-right-now-2kdj
**Reasoning:** The article on developer tools is a generic list and does not focus on our specific interests in code intelligence or context engineering.
**Authors:** Madza

**Content/Abstract:**
> <p>Modern developer tools have become fundamental components which enhance coding operations while shortening the duration needed to finish projects.</p> 
>  
> <p>However the curation and selection process for developer tools that might be useful in the practice can feel as a daunting task because numerous platforms and utilities exist.</p> 
>  
> <p>In this article I’ve curated 9 of my recent favorite developer tools which help developers create applications through advanced authentication systems, headless CMS backends, visual website builders, API SDK generators and much more.</p> 
>  
> <p>These tools help developers save time while reducing obstacles to work on developing robust software.</p> 
>  
> <p>All the tools provide brief descriptions along with essential features, workflow image snaphots and direct access links which enable you to evaluate their suitability for your technology infrastructure.</p> 
>  
> <p>Let’s dive in and discover how these essential tools can improve your coding workflow.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   1. <a href="https://fandf.co/4ozoC1R">Depot</a> – Build Docker Images up to 40X Faster 
> </h2> 
>  
> <p>Depot is a powerful remote container build platform which speeds up Docker builds through its cloud-based infrastructure with built-in caching and native multi-architecture support and automated CI/CD system integration.</p> 
>  
> <p>The platform uses remote builders to replace slow local builds and CI runners which enables the resources of your machine to remain available while cutting down build duration for faster deployment and development cycles.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzhoptwdt2rpbwm8gv3s3.jpeg"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzhoptwdt2rpbwm8gv3s3.jpeg" alt="Depot preview" width="800" height="444"></a></p> 
>  
> <p><strong>Some of the most useful features include:</strong></p> 
>  
> <p>🤖 <strong>Agent sandbox environments</strong>: Run AI coding agents such as Claude Code in safe, separated Depot sandboxes that are fully Docker supported. These AI tools can thus develop and verify code in a secure environment without any risk for your local machine.</p> 
>  
> <p>📦 <strong>Docker Bake support</strong>: Speed up highly complex multi-target builds by using native Docker Bake support. You can continue to use your current build definitions while taking advantage of Depot's speed for complex build matrices and parallel ​‍​‌‍​‍‌workflows.</p> 
>  
> <p>🔐 <strong>Private resource access via Tailscale</strong>: Connect Depot builders to your private networks, databases, and internal registries through Tailscale integration, enabling secure builds that need access to protected resources.</p> 
>  
> <p>🔽 <strong>Self-hosted option with Depot Managed</strong>: Deploy the entire Depot infrastructure in your own AWS account for complete control over security, compliance, and data sovereignty while keeping all the speed benefits.</p> 
>  
> <p>📊 <strong>Receive detailed analytics and insights</strong>: Run multiple builds concurrently with intelligent queueing and get detailed analytics on build performance, cache hit rates, and cost breakdowns to optimize.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsqt03avxuvd599naaw1n.jpeg"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsqt03avxuvd599naaw1n.jpeg" alt="Depot graph" width="800" height="360"></a></p> 
>  
> <p>Want to boost your Docker build workflow and save time on every container build? <a href="https://fandf.co/43YsW2b">Try Depot today</a> and experience faster, and more resource efficient container builds without complexity!</p> 
>  
> <p>🌎 Website Link: <a href="https://fandf.co/485QPHv">https://depot.dev/</a></p> 
>  
> <p>Thanks to the Depot team for sponsoring this article!</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   2. <a href="https://www.builder.io/">Builder.io</a> - Make modern web experiences with AI 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fq36yu54zsqr73oxxs6xz.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fq36yu54zsqr73oxxs6xz.png" alt="BuilderIO" width="800" height="466"></a></p> 
>  
> <p>Builder is an AI-powered visual development platform that lets you provide direct prompts to create both the editable design and the code. Integrates well with Figma, so you can use your existing workflow.</p> 
>  
> <p><strong>Key features &amp; why to use it:</strong></p> 
>  
> <ul> 
> <li><p>AI generates production code from Figma design or from the prompt you provide.</p></li> 
> <li><p>Visual editor lets you easily preview the output and make any changes you want.</p></li> 
> <li><p>Integrates with other platforms such as GitHub for seamless deployment.</p></li> 
> </ul> 
>  
> <p>🌎 Website Link: <a href="https://www.builder.io/">https://www.builder.io/</a></p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   3. <a href="https://webflow.com/">Webflow</a> - Design and launch websites visually 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbzn091cwup0gmu7wugp8.jpeg"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbzn091cwup0gmu7wugp8.jpeg" alt="Webflow" width="800" height="472"></a></p> 
>  
> <p>Webflow is a powerful website design tool that allows designers and developers to create responsive, real-world-ready sites in no time without the need for coding.</p> 
>  
> <p><strong>Key features &amp; why to use it:</strong></p> 
>  
> <ul> 
> <li><p>Visual designer combines design and CMS management into a single platform.</p></li> 
> <li><p>Automatically generates clean, semantic HTML, CSS, and JavaScript.</p></li> 
> <li><p>Launch and optimize site easily with the help of integrated hosting and SEO tools.</p></li> 
> </ul> 
>  
> <p>🌎 Website Link: <a href="https://webflow.com/">https://webflow.com/</a></p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   4. <a href="https://medusajs.com/">Medusa</a> - Develop headless e-commerce apps 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fy2qkreb0zxaf1pvlioiv.jpeg"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fy2qkreb0zxaf1pvlioiv.jpeg" alt="Medusa" width="800" height="480"></a></p> 
>  
> <p>Medusa is a headless e-commerce platform that is open source and developer-friendly. It lets developers create the backend of scalable and customizable online stores quickly, while integrating with modern frontends.</p> 
>  
> <p><strong>Key features &amp; why to use it:</strong></p> 
>  
> <ul> 
> <li><p>API-first architecture that is fully customizable for various commerce needs.</p></li> 
> <li><p>Supports multiple commonly used payment providers out of the box.</p></li> 
> <li><p>The open-source nature of the platform ensures extensibility and control over your backend.</p></li> 
> </ul> 
>  
> <p>🌎 Website Link: <a href="https://medusajs.com/">https://medusajs.com/</a></p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   5. <a href="https://clerk.com/">Clerk</a> - Manage user authentication 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjy1eik93w829cmloj0j2.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjy1eik93w829cmloj0j2.png" alt="Clerk" width="800" height="432"></a></p> 
>  
> <p>Clerk is an all-around solution for the authentication needs of modern web apps. It offers a set of customizable, ready-to-use UI components and security features that are advanced yet easy to implement.</p> 
>  
> <p><strong>Key features &amp; why to use it:</strong></p> 
>  
> <ul> 
> <li><p>Passwordless authentication, magic links, and social login support.</p></li> 
> <li><p>Built-in bot detection and brute-force attack prevention.</p></li> 
> <li><p>Comes with multi-tenancy, role management, and session lifecycle control.</p></li> 
> </ul> 
>  
> <p>🌎 Website Link: <a href="https://clerk.com/">https://clerk.com/</a></p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   6. <a href="https://payloadcms.com/">Payload</a> - Create CMS and backend systems 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwmg79ahrdlr6dnwk4fzf.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwmg79ahrdlr6dnwk4fzf.png" alt="Payload" width="800" height="404"></a></p> 
>  
> <p>Payload is an open-source backend CMS for NextJS that provides developers with full control over content and application data, while at the same time, gives marketers the power of a user-friendly editing interface.</p> 
>  
> <p><strong>Key features &amp; why to use it:</strong></p> 
>  
> <ul> 
> <li><p>Your entire Payload config can be installed with a single line into any existing NextJS app.</p></li> 
> <li><p>Fully customizable schemas and a powerful API for content management.</p></li> 
> <li><p>Developer-friendly open-source architecture with full control.</p></li> 
> </ul> 
>  
> <p>🌎 Website Link: <a href="https://payloadcms.com/">https://payloadcms.com/</a></p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   7. <a href="https://www.buildwithfern.com/">Fern</a> - Generate SDKs and API docs 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft1yx0u6uugrdyipso61u.jpeg"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft1yx0u6uugrdyipso61u.jpeg" alt="Fern" width="800" height="444"></a></p> 
>  
> <p>Fern simplifies the process of SDK generation and the creation of accompanying documentation, thus providing clean, and complete API docs which in turn, leads to enhanced developer experience and decreased engineering work.</p> 
>  
> <p><strong>Key features &amp; why to use it:</strong></p> 
>  
> <ul> 
> <li><p>Supports multiple languages like TypeScript, Python, Go, Java, C#, etc.</p></li> 
> <li><p>Continuous SDK updates are integrated into CI/CD pipelines.</p></li> 
> <li><p>Comes with useful features OAuth 2.0, server-sent events, and auto-pagination.</p></li> 
> </ul> 
>  
> <p>🌎 Website Link: <a href="https://www.buildwithfern.com/">https://www.buildwithfern.com/</a></p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   8. <a href="https://openpanel.com/">OpenPanel</a> - Host a server control panel 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzt2e1qizrcyl796bgu5l.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzt2e1qizrcyl796bgu5l.png" alt="OpenPanel" width="800" height="438"></a></p> 
>  
> <p>OpenPanel​‍​‌‍​‍‌ is a lightweight control panel that enables humans to control a web server and services in a very simple and straightforward manner with no need for complicated configurations.</p> 
>  
> <p><strong>Key features &amp; why to use it:</strong></p> 
>  
> <ul> 
> <li><p>An easy-to-use UI for managing websites, databases, and mail servers.</p></li> 
> <li><p>Facilitates SSL certificate automation and backup scheduling.</p></li> 
> <li><p>Its design focuses on ease of installation and minimal resource utilization.</p></li> 
> </ul> 
>  
> <p>🌎 Website Link: <a href="https://openpanel.com/">https://openpanel.com/</a></p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   9. <a href="https://grafana.com/">Grafana</a> - Visualize app metrics and logs 
> </h2> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Falritrmbpdjq6wwpjfly.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Falritrmbpdjq6wwpjfly.png" alt="Grafana" width="800" height="443"></a></p> 
>  
> <p>Grafana is an open-source project for monitoring and observability. It lets you to build dashboards that allow easy understanding of system metrics, logs, and traces, and even in real-time.</p> 
>  
> <p><strong>Key features &amp; why to use it:</strong></p> 
>  
> <ul> 
> <li><p>Integrates with diverse data sources for unified monitoring.</p></li> 
> <li><p>Comes with options for representation of the data, e.g. graphs, heatmaps, or alerts.</p></li> 
> <li><p>It can be used independently of scale, like in a small environment or a big enterprise ​‍​‌‍​‍‌environment.</p></li> 
> </ul> 
>  
> <p>🌎 Website Link: <a href="https://grafana.com/">https://grafana.com/</a></p> 
>  
>  
>  
>  
> <h3> 
>    
>    
>   Did you like the resources? Here is more 👇<br><br> 
> </h3> 
>  
> <p>Join 6,000+ others to receive the best DEV resources, tools, productivity tips, and career growth advice I discover by subscribing to <a href="https://madzadev.substack.com/">my newsletter</a>!</p> 
>  
> <p><a href="https://madzadev.substack.com/"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F5nb15k9rlvy91bc7yd4c.png" alt="The Developer Toolbox" width="800" height="181"></a></p> 
>  
> <p>Also, connect with me on <a href="https://twitter.com/madzadev">Twitter</a>, <a href="https://www.linkedin.com/in/madzadev/">LinkedIn</a>, and <a href="https://github.com/madzadev">GitHub</a>!</p> 
>  
> <p>Writing has always been my passion, and it gives me pleasure to help and inspire people. If you want to get featured or partner up, feel free to <a href="https://www.madza.dev/contact">get in touch</a>!</p>

---

## [3/10] Describe promotions on AI services available now
**Source:** The Practical Developer | **Date:** 2025-12-02T13:47:52.000Z
**URL:** https://dev.to/wadie_realme_733c52996966/describe-promotions-on-ai-services-available-now-4ml7
**Reasoning:** The article on AI service promotions is generic marketing and not relevant to our specific interests.
**Authors:** Wadie Realme

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fsibdb9oyildlll7w4qms.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><h1>  
>     
>     
>   Unleash AI Potential for Less: Top Promotions on AI Services You Can Grab Now!  
> </h1>  
>   
> <p>The world of Artificial Intelligence is evolving at lightning speed, offering unprecedented capabilities from advanced large language models to sophisticated image generation and robust data analytics. As businesses and developers race to integrate these powerful tools, a key consideration often arises: cost. The good news? AI service providers understand this need for accessible innovation, and exciting promotions on AI models, subscriptions, and paid plan benefits are blooming!</p>  
>   
> <p>This post dives into the current landscape of AI service promotions, helping you identify opportunities to maximize your AI budget, experiment with new technologies, and scale your projects without breaking the bank. Whether you're a startup, a seasoned enterprise, or an individual developer, there's likely a deal waiting to amplify your AI journey.</p>  
>   
> <h2>  
>     
>     
>   Why Promotions Matter in the AI Space  
> </h2>  
>   
> <p>The competitive nature of the AI market means providers are constantly vying for your attention. This creates a fantastic environment for consumers, leading to a variety of promotional offers designed to:</p>  
>   
> <ul>  
> <li>  <strong>Attract New Users:</strong> Lower the barrier to entry for those curious about AI.</li>  
> <li>  <strong>Encourage Adoption:</strong> Help users explore full feature sets without immediate heavy investment.</li>  
> <li>  <strong>Reward Loyalty:</strong> Provide incentives for long-term commitment.</li>  
> <li>  <strong>Boost Usage:</strong> Offer tiered pricing or credit bonuses that make scaling more appealing.</li>  
> <li>  <strong>Showcase New Features:</strong> Entice users to try cutting-edge models or services.</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   Navigating the AI Promotion Landscape: Key Categories and Types of Deals  
> </h2>  
>   
> <p>Promotions on AI services typically fall into several categories, each catering to different needs and scales of operation.</p>  
>   
> <h3>  
>     
>     
>   1. Cloud AI Platform Credits &amp; Free Tiers (AWS, Azure, GCP, IBM Cloud)  
> </h3>  
>   
> <p>Major cloud providers are often the first stop for AI infrastructure, offering a vast array of machine learning services, compute power, and specialized AI APIs.</p>  
>   
> <ul>  
> <li>  <strong>Extended Free Tiers:</strong> While perpetual free tiers exist for basic services, many providers offer <em>extended</em> free periods (e.g., 6-12 months) for specific AI services or higher usage limits for new accounts. This is perfect for prototyping and initial development.</li>  
> <li>  <strong>Startup Credits:</strong> Cloud providers frequently partner with startup accelerators or offer direct programs providing substantial credits (often thousands of dollars) for qualifying startups. These can be a game-changer for early-stage companies building AI-first products.</li>  
> <li>  <strong>Compute Instance Discounts:</strong> Look for promotional pricing on GPU-enabled virtual machines or specialized AI accelerators, particularly for new regions or during specific seasonal campaigns.</li>  
> <li>  <strong>Managed Service Discounts:</strong> Occasionally, specific managed AI services (like cognitive services, MLOps platforms, or data labeling tools) will feature discounted usage rates for a limited time.</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   2. Generative AI Model API Subscriptions &amp; Credit Bundles  
> </h3>  
>   
> <p>The explosion of Large Language Models (LLMs), image generation, and other generative AI has led to a new wave of API-first providers. These are hotbeds for promotions.</p>  
>   
> <ul>  
> <li>  <strong>New User Credit Bonuses:</strong> Many platforms offer a bonus amount of credits when you sign up or make your first purchase. For example, "Buy 100,000 tokens, get 50,000 free."</li>  
> <li>  <strong>Discounted Monthly/Annual Subscriptions:</strong> Opting for an annual plan often provides a significant discount compared to month-to-month billing. Some providers also offer introductory rates for the first few months of a new subscription tier.</li>  
> <li>  <strong>Tiered Pricing Incentives:</strong> While not strictly a promotion, many providers structure their pricing such that higher volume usage (e.g., more API calls, more tokens) comes with a lower per-unit cost. Promotions might temporarily lower the threshold for these higher tiers.</li>  
> <li>  <strong>Early Access Programs:</strong> Be on the lookout for beta programs or early access to new, advanced models. These sometimes come with reduced pricing or free usage periods in exchange for feedback.</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   3. AI Development Tools &amp; MLOps Platform Premium Features  
> </h3>  
>   
> <p>Beyond just the models, the tools you use to build, deploy, and manage AI models also offer promotional opportunities.</p>  
>   
> <ul>  
> <li>  <strong>IDE &amp; Workbench Trials/Discounts:</strong> Premium features in AI-focused IDEs or collaborative MLOps workbenches might offer extended free trials or significant discounts for annual licenses.</li>  
> <li>  <strong>Feature Unlocks:</strong> Some platforms might temporarily unlock advanced features (e.g., parallel experimentation, advanced monitoring, more robust security) for existing users to encourage upgrades.</li>  
> <li>  <strong>Bundle Deals:</strong> Occasionally, MLOps platforms might bundle their services with cloud compute credits or integrate third-party tools at a discounted rate.</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   4. Specialized AI Services (Data Annotation, Custom Model Training)  
> </h3>  
>   
> <p>Even highly specialized services can have promotional windows.</p>  
>   
> <ul>  
> <li>  <strong>First Project Discounts:</strong> Companies offering data annotation, custom model fine-tuning, or specific AI consulting services might offer a discount on your first project to demonstrate their capabilities.</li>  
> <li>  <strong>Volume-Based Tiers:</strong> Similar to API usage, larger projects for data labeling or custom training often receive better per-unit pricing.</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   Maximizing Your Savings: Tips for Leveraging AI Promotions  
> </h2>  
>   
> <ol>  
> <li> <strong>Read the Fine Print:</strong> Always understand the terms – duration, usage limits, eligibility, and what happens after the promotional period ends.</li>  
> <li> <strong>Plan Your Projects:</strong> Align your development roadmap with promotional offers. If you know you'll need a specific AI service in a few months, check for upcoming or current deals.</li>  
> <li> <strong>Explore New Providers:</strong> Don't limit yourself to one vendor. New entrants often have aggressive promotions to gain market share.</li>  
> <li> <strong>Join Beta Programs:</strong> Access to cutting-edge AI features often comes with reduced costs or free usage during the testing phase.</li>  
> <li> <strong>Leverage Startup Programs:</strong> If you're a startup, actively seek out and apply for startup credit programs from major cloud and AI service providers.</li>  
> <li> <strong>Stay Subscribed:</strong> Sign up for newsletters and follow social media channels of your preferred AI service providers. Promotions are often announced there first.</li>  
> </ol>  
>   
> <h2>  
>     
>     
>   The Undeniable Value of Paid AI Plans  
> </h2>  
>   
> <p>While promotions are fantastic for cost savings, remember the inherent value of paid AI plans:</p>  
>   
> <ul>  
> <li>  <strong>Reliability &amp; Performance:</strong> Guaranteed uptime, faster response times, and dedicated resources.</li>  
> <li>  <strong>Higher Usage Limits:</strong> Crucial for scaling production-grade applications.</li>  
> <li>  <strong>Dedicated Support:</strong> Access to technical assistance when you need it most.</li>  
> <li>  <strong>Advanced Features:</strong> Access to cutting-edge models, fine-tuning capabilities, security features, and MLOps tools unavailable in free tiers.</li>  
> <li>  <strong>Compliance &amp; Security:</strong> Robust features to meet enterprise-grade security and compliance requirements.</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   Conclusion: Seize the AI Opportunity  
> </h2>  
>   
> <p>The current wave of promotions on AI services presents an incredible opportunity. Whether you're looking to integrate an advanced LLM, power sophisticated image generation, build robust MLOps pipelines, or simply experiment with the latest AI models, there's never been a better time to invest. By strategically leveraging these offers, you can significantly reduce your initial outlay, accelerate your development, and unlock the transformative power of Artificial Intelligence for your projects and business.</p>  
>   
> <p>Don't let budget constraints hold you back from innovation. Start exploring these promotions today and take your AI initiatives to the next level!</p>

---

## [3/10] &#128293; 10+ End-to-End DevOps Projects &mdash; Docker, AWS, Jenkins, Terraform, Ansible & More
**Source:** The Practical Developer | **Date:** 2025-12-02T13:25:27.000Z
**URL:** https://dev.to/ritesh355/10-end-to-end-devops-projects-docker-aws-jenkins-terraform-ansible-more-9ka
**Reasoning:** The collection of DevOps projects is not directly related to our focus on code intelligence or context engineering.
**Authors:** Ritesh Singh

**Content/Abstract:**
> <h1> 
>    
>    
>   🚀 My Complete DevOps Projects Collection – From Beginner to Advanced (All in One Place) 
> </h1> 
>  
> <p>Over the last few months, I’ve been learning DevOps deeply and building real-world projects using tools like <strong>Docker, Jenkins, AWS, GitHub Actions, Ansible, Terraform, Lambda, MongoDB, CloudFront, Prometheus, Grafana, and more</strong>.</p> 
>  
> <p>To organize everything, I created a single repository called <strong>DevOps_Projects</strong>, where I have added <strong>all my DevOps, Cloud, and CI/CD projects</strong>, each inside its own folder.</p> 
>  
> <p>This blog is a complete guide to my journey — what I built, what tools I used, and what I learned from each project.</p> 
>  
> <p>Whether you're a beginner or someone preparing for DevOps interviews, this collection will help you understand real DevOps concepts with practical implementation.</p> 
>  
>  
>  
>  
> <h1> 
>    
>    
>   🎯 Why I Created This Repository 
> </h1> 
>  
> <p>As a fresher, one thing became clear:</p> 
>  
> <blockquote> 
> <p><strong>Companies don’t want just theoretical knowledge — they want proof of hands-on experience.</strong></p> 
> </blockquote> 
>  
> <p>So instead of building random mini projects, I decided to learn DevOps properly and build <strong>real-world, practical, production-like projects</strong>.</p> 
>  
> <p>This repository is the outcome of that journey.<br><br> 
> And I will keep adding more advanced projects as I grow.</p> 
>  
>  
>  
>  
> <h1> 
>    
>    
>   📂 What’s Inside This Repository? 
> </h1> 
>  
> <p>Each project has its <strong>own folder</strong>, containing:</p> 
>  
> <ul> 
> <li>Complete project code 
> </li> 
> <li>Dockerfiles 
> </li> 
> <li>CI/CD workflows 
> </li> 
> <li>Configuration files 
> </li> 
> <li>Project-specific README 
> </li> 
> <li>Architecture diagrams (for some projects)</li> 
> </ul> 
>  
> <p>Below is the complete list of projects included so far:</p> 
>  
>  
>  
>  
> <h1> 
>    
>    
>   🧩 Project Overview 
> </h1> 
>  
> <h2> 
>    
>    
>   <strong>📁 PROJECT-1 — Dockerized Flask App</strong> <a href="https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-1">PROJECT-1</a> 
> </h2> 
>  
> <p>A beginner-friendly app containerized using Docker.</p> 
>  
> <p><strong>What I learned:</strong></p> 
>  
> <ul> 
> <li>Creating Dockerfiles 
> </li> 
> <li>Building/running containers 
> </li> 
> <li>Exposing ports 
> </li> 
> </ul> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   <strong>📁 PROJECT-2 — Node.js + MongoDB App with Docker Compose</strong> <a href="https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-2">PROJECT-2</a> 
> </h2> 
>  
> <p>A multi-container app using Docker Compose.</p> 
>  
> <p><strong>Key Learnings:</strong></p> 
>  
> <ul> 
> <li>Container networking 
> </li> 
> <li>Volumes 
> </li> 
> <li>Service orchestration 
> </li> 
> </ul> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   <strong>📁 PROJECT-3 — Jenkins CI/CD Pipeline with Docker &amp; Node.js</strong> <a href="https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-3">PROJECT-3</a> 
> </h2> 
>  
> <p>Automated CI/CD workflow using Jenkins.</p> 
>  
> <p><strong>Key Learnings:</strong></p> 
>  
> <ul> 
> <li>Jenkinsfile stages 
> </li> 
> <li>Automated build → test → deploy 
> </li> 
> <li>Docker integration 
> </li> 
> </ul> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fn12z0wdkxsuc543350ql.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fn12z0wdkxsuc543350ql.png" alt="" width="800" height="483"></a></p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   <strong>📁 PROJECT-4 — AWS CI/CD Pipeline (Node.js → EC2)</strong> <a href="https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-4">PROJECT-4</a> 
> </h2> 
>  
> <p>A production-ready AWS CI/CD pipeline.</p> 
>  
> <p><strong>AWS Services Used:</strong></p> 
>  
> <ul> 
> <li>CodePipeline 
> </li> 
> <li>CodeBuild 
> </li> 
> <li>CodeDeploy 
> </li> 
> <li>EC2 
> </li> 
> </ul> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fx71aw4ege5io5k1f0k5y.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fx71aw4ege5io5k1f0k5y.png" alt="" width="800" height="273"></a></p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   <strong>📁 PROJECT-5 — CI/CD Pipeline for Web App</strong> <a href="https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-5">PROJECT-5</a> 
> </h2> 
>  
> <p>A simple but complete CI/CD pipeline demonstrating core DevOps concepts.</p> 
>  
> <h2> 
>    
>    
>   <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzarauiw2nn2a9jgahequ.png" alt="" width="800" height="366"> 
> </h2> 
>  
> <h2> 
>    
>    
>   <strong>📁 PROJECT-6 — Static Website Hosting (S3 + CloudFront + CI/CD)</strong><a href="https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-6">PROJECT-6</a> 
> </h2> 
>  
> <p>A global static website hosted on AWS.</p> 
>  
> <p><strong>What I learned:</strong></p> 
>  
> <ul> 
> <li>Hosting using S3 
> </li> 
> <li>CDN with CloudFront 
> </li> 
> <li>Automated deployment using GitHub Actions 
> </li> 
> </ul> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fm7jjr7155w6ac7o2xim9.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fm7jjr7155w6ac7o2xim9.png" alt="" width="800" height="415"></a></p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   <strong>📁 PROJECT-7 — Flask + AWS DynamoDB CRUD App</strong> <a href="https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-7">PROJECT-7</a> 
> </h2> 
>  
> <p>A cloud backend performing CRUD operations on DynamoDB.<br> 
> <strong>Key Learnings:</strong></p> 
>  
> <ul> 
> <li>Boto3 integration 
> </li> 
> <li>NoSQL CRUD 
> </li> 
> <li>Deploying Flask + AWS SDK inside Docker </li> 
> </ul> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F14jeurn6jmrsnyfbawfx.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F14jeurn6jmrsnyfbawfx.png" alt="" width="800" height="348"></a> </p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   <strong>📁 PROJECT-8 — Serverless CI/CD Pipeline</strong> <a href="https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-8">PROJECT-8</a> 
> </h2> 
>  
> <p>A complete serverless pipeline using AWS.<br> 
> <strong>AWS Services Used:</strong></p> 
>  
> <ul> 
> <li>Lambda 
> </li> 
> <li>API Gateway 
> </li> 
> <li>S3 
> </li> 
> <li>CodePipeline 
> </li> 
> <li>CodeBuild 
> </li> 
> </ul> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F3sjkwdkmvus1vlnt87md.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F3sjkwdkmvus1vlnt87md.png" alt="" width="800" height="388"></a></p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   <strong>📁 PROJECT-9 — Full-Stack DevOps Automation</strong><a href="https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-9">PROJECT-9</a> 
> </h2> 
>  
> <p>A large-scale app combining frontend, backend, cloud, monitoring, and CI/CD.</p> 
>  
> <p><strong>Tech Used:</strong></p> 
>  
> <ul> 
> <li>Next.js 
> </li> 
> <li>Docker 
> </li> 
> <li>GitHub Actions 
> </li> 
> <li>AWS 
> </li> 
> <li>Prometheus &amp; Grafana 
> </li> 
> <li>Route 53 
> </li> 
> </ul> 
>  
> <p><strong>Key Learnings:</strong></p> 
>  
> <ul> 
> <li>Observability 
> </li> 
> <li>Automation workflows 
> </li> 
> <li>Production-like infrastructure </li> 
> </ul> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjbhg2bsphkk2mxdrooho.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjbhg2bsphkk2mxdrooho.png" alt="" width="800" height="446"></a></p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   <strong>📁 PROJECT-10 — DevSecOps Pipeline</strong><a href="https://github.com/ritesh355/DevOps_Projects/tree/main/PROJECT-10">PROJECT-10</a> 
> </h2> 
>  
> <p>A complete DevSecOps pipeline integrating CI/CD with security checks.</p> 
>  
> <p><strong>Tools Used:</strong></p> 
>  
> <ul> 
> <li>Jenkins 
> </li> 
> <li>Terraform 
> </li> 
> <li>Ansible (with Ansible Vault) 
> </li> 
> <li>Docker 
> </li> 
> <li>Trivy 
> </li> 
> <li>AWS 
> </li> 
> </ul> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fc33372mif8u69ml9wpwa.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fc33372mif8u69ml9wpwa.png" alt="" width="800" height="514"></a></p> 
>  
> <p><strong>Key Learnings:</strong></p> 
>  
> <ul> 
> <li>Infrastructure provisioning with Terraform 
> </li> 
> <li>Secure configuration management 
> </li> 
> <li>Vulnerability scanning 
> </li> 
> <li>Secure CI/CD pipelines 
> </li> 
> </ul> 
>  
>  
>  
>  
> <h1> 
>    
>    
>   🛠 Tech Stack Covered (So Far) 
> </h1> 
>  
> <h3> 
>    
>    
>   <strong>CI/CD</strong> 
> </h3> 
>  
> <ul> 
> <li>Jenkins 
> </li> 
> <li>GitHub Actions 
> </li> 
> <li>AWS Developer Tools 
> </li> 
> </ul> 
>  
> <h3> 
>    
>    
>   <strong>Containerization</strong> 
> </h3> 
>  
> <ul> 
> <li>Docker 
> </li> 
> <li>Docker Compose 
> </li> 
> </ul> 
>  
> <h3> 
>    
>    
>   <strong>Cloud</strong> 
> </h3> 
>  
> <ul> 
> <li>AWS (EC2, Lambda, S3, CloudFront, Route53, DynamoDB)</li> 
> </ul> 
>  
> <h3> 
>    
>    
>   <strong>IaC &amp; Automation</strong> 
> </h3> 
>  
> <ul> 
> <li>Terraform 
> </li> 
> <li>Ansible 
> </li> 
> </ul> 
>  
> <h3> 
>    
>    
>   <strong>Security</strong> 
> </h3> 
>  
> <ul> 
> <li>Trivy 
> </li> 
> <li>IAM 
> </li> 
> </ul> 
>  
> <h3> 
>    
>    
>   <strong>Monitoring</strong> 
> </h3> 
>  
> <ul> 
> <li>Prometheus 
> </li> 
> <li>Grafana 
> </li> 
> </ul> 
>  
>  
>  
>  
> <h1> 
>    
>    
>   🔥 What’s Coming Next? 
> </h1> 
>  
> <p>I will continue expanding this repository with:</p> 
>  
> <ul> 
> <li>Kubernetes Projects 
> </li> 
> <li>EKS Deployment 
> </li> 
> <li>Helm Charts 
> </li> 
> <li>Multi-Region Terraform Architecture 
> </li> 
> <li>Microservices with Lambda + API Gateway 
> </li> 
> <li>ELK / Loki Log Stack 
> </li> 
> <li>GitOps with ArgoCD 
> </li> 
> </ul> 
>  
>  
>  
>  
> <h1> 
>    
>    
>   🌟 Final Words 
> </h1> 
>  
> <p>This repository reflects my journey from beginner to someone who can design <strong>real-world DevOps systems</strong>.</p> 
>  
> <p>If you're also learning DevOps, this repo can help you understand:</p> 
>  
> <ul> 
> <li>Real CI/CD pipelines 
> </li> 
> <li>Docker in production 
> </li> 
> <li>AWS workflow integration 
> </li> 
> <li>How DevOps engineers think and solve problems 
> </li> 
> </ul> 
>  
> <p>If you find it helpful, feel free to ⭐ star the repository — it motivates me to build more.</p> 
>  
>  
>  
>  
> <h2> 
>    
>    
>   👨‍💻 Author 
> </h2> 
>  
> <p><strong>Ritesh Singh</strong></p> 
>  
> <p>🌐 <a href="https://www.linkedin.com/in/ritesh-singh-092b84340/">LinkedIn</a> </p> 
>  
> <p>📝 <a href="https://ritesh-devops.hashnode.dev/">Hashnode</a> </p> 
>  
> <p>💻<a href="https://github.com/ritesh355/">GitHub</a></p> 
>  
> <p>🌐 <a href="https://dev.to/ritesh355">LinkedIn</a></p>

---

## [3/10] Python Data Science Handbook
**Source:** Hacker News: Front Page | **Date:** 2025-12-02T12:38:28.000Z
**URL:** https://jakevdp.github.io/PythonDataScienceHandbook/
**Reasoning:** The Python Data Science Handbook is a general resource and not directly related to our focus areas.
**Authors:** cl3misch

**Content/Abstract:**
> <p>Article URL: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/">https://jakevdp.github.io/PythonDataScienceHandbook/</a></p> 
> <p>Comments URL: <a href="https://news.ycombinator.com/item?id=46120611">https://news.ycombinator.com/item?id=46120611</a></p> 
> <p>Points: 9</p> 
> <p># Comments: 1</p>

---

## [3/10] At Anthropic, we believe that AI can increase nonprofit capacity. And we've worked with over 100 organizations so far on getting it right - Fortune
**Source:** "Anthropic" - Google News | **Date:** 2025-12-02T14:00:00.000Z
**URL:** https://news.google.com/rss/articles/CBMigwFBVV95cUxNSU92MkJQS2NHT1NoOTM3S1ExeF84M013cndhdHRuNm1FT3diaWFVQUhEd3JYcEk5VlVlbzhhN1QtNFRDRkVLUXJuQkktaTJlQ1hhR25Xb2IyWVY5bUpRVFRSWjJybk82bEdXM3ctRVA1cWk4NEpjd2J1UllWSUZqODBFVQ?oc=5
**Reasoning:** The article is about AI increasing nonprofit capacity, which is not directly related to our primary or secondary interests.

**Content/Abstract:**
> <a href="https://news.google.com/rss/articles/CBMigwFBVV95cUxNSU92MkJQS2NHT1NoOTM3S1ExeF84M013cndhdHRuNm1FT3diaWFVQUhEd3JYcEk5VlVlbzhhN1QtNFRDRkVLUXJuQkktaTJlQ1hhR25Xb2IyWVY5bUpRVFRSWjJybk82bEdXM3ctRVA1cWk4NEpjd2J1UllWSUZqODBFVQ?oc=5">At Anthropic, we believe that AI can increase nonprofit capacity. And we've worked with over 100 organizations so far on getting it right</a>  Fortune

---

## [3/10] Blackbaud Partners with Anthropic on Groundbreaking New Way to Experience the Power of Purpose-Built AI - PR Newswire
**Source:** "Anthropic" - Google News | **Date:** 2025-12-02T14:00:00.000Z
**URL:** https://news.google.com/rss/articles/CBMi8AFBVV95cUxOTWZ6NUF0YTBnRDBUZW1iLUdHWlhCcmhYdVRRY0xRZVZWZ0JJUnl3d01LM0lwRlowbXlKc2dEbWxCcTJ0cDFYbndteTIyYmZaSWc5T1ZQUlFZWlpTTmo1Ump6RUJCZ1g5cmRNSk56eFlsd01VMmp1R05FYkY0ZHdBLUJYOERSd1NmYjZwVVZJR3dRdVVLSWlmc1VQZVZyQjFfaFRaRm04dE5nQlhmRXk4QVhTMzFBRF9BMHBKQzMyYjI1NUU0eUNoSzVkT1pBLTdJdm5OSGFXdUUwRUlNcmZ5RnBlakc0b1lVYTNvbUF1a20?oc=5
**Reasoning:** This is a generic marketing piece about AI partnership, not relevant to our focus areas.

**Content/Abstract:**
> <a href="https://news.google.com/rss/articles/CBMi8AFBVV95cUxOTWZ6NUF0YTBnRDBUZW1iLUdHWlhCcmhYdVRRY0xRZVZWZ0JJUnl3d01LM0lwRlowbXlKc2dEbWxCcTJ0cDFYbndteTIyYmZaSWc5T1ZQUlFZWlpTTmo1Ump6RUJCZ1g5cmRNSk56eFlsd01VMmp1R05FYkY0ZHdBLUJYOERSd1NmYjZwVVZJR3dRdVVLSWlmc1VQZVZyQjFfaFRaRm04dE5nQlhmRXk4QVhTMzFBRF9BMHBKQzMyYjI1NUU0eUNoSzVkT1pBLTdJdm5OSGFXdUUwRUlNcmZ5RnBlakc0b1lVYTNvbUF1a20?oc=5">Blackbaud Partners with Anthropic on Groundbreaking New Way to Experience the Power of Purpose-Built AI</a>  PR Newswire

---

## [3/10] Comprehensive Guide to Load and Stress Testing Types with Locust Implementation
**Source:** The Practical Developer | **Date:** 2025-12-02T13:34:03.000Z
**URL:** https://dev.to/mohsen_akbari_ebe53d7cbc2/comprehensive-guide-to-load-and-stress-testing-types-with-locust-implementation-40o6
**Reasoning:** The article is a tutorial on load and stress testing, not directly related to our primary interests.
**Authors:** Mohsen Akbari

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fs439a1jpvk272q1mqb9j.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><blockquote>  
> <p>Part 1: Understanding Load and Stress Testing Types</p>  
>   
> <p>1.1 Introduction to Load Testing Fundamentals</p>  
> </blockquote>  
>   
> <p>Load Testing is the process of simulating real-world usage on software applications to understand behavior under expected load conditions. It helps identify performance bottlenecks, establish baselines, and ensure applications can handle anticipated traffic.</p>  
>   
> <p>Stress Testing pushes systems beyond normal operational capacity to determine breaking points and understand failure modes. Unlike load testing, which validates performance under expected conditions, stress testing explores system behavior at and beyond limits.<br>  
> 1.2 Conventional Load Testing Types</p>  
>   
> <blockquote>  
> <p>1.2.1 Baseline Testing<br>  
> Purpose: Establish performance benchmarks under normal conditions</p>  
> </blockquote>  
>   
> <p>Metrics: Response times, throughput, resource utilization</p>  
>   
> <p>Use Case: Initial performance assessment, regression testing</p>  
>   
> <p>Typical Scenario: Simulating average daily users with normal behavior patterns</p>  
>   
> <blockquote>  
> <p>1.2.2 Load Testing<br>  
> Purpose: Verify system behavior under expected peak load</p>  
> </blockquote>  
>   
> <p>Metrics: Error rates, latency at peak, throughput capacity</p>  
>   
> <p>Use Case: Pre-deployment validation, capacity planning</p>  
>   
> <p>Typical Scenario: Simulating Black Friday traffic for e-commerce</p>  
>   
> <blockquote>  
> <p>1.2.3 Stress Testing<br>  
> Purpose: Identify maximum capacity and breaking points</p>  
> </blockquote>  
>   
> <p>Metrics: System failure points, recovery behavior, error handling</p>  
>   
> <p>Use Case: Determining scalability limits, disaster recovery planning</p>  
>   
> <p>Typical Scenario: Gradual increase until system failure</p>  
>   
> <blockquote>  
> <p>1.2.4 Soak Testing (Endurance Testing)<br>  
> Purpose: Identify performance degradation over extended periods</p>  
> </blockquote>  
>   
> <p>Metrics: Memory leaks, resource exhaustion, response time drift</p>  
>   
> <p>Use Case: Long-running process validation, memory management testing</p>  
>   
> <p>Typical Scenario: 24-72 hour continuous load simulation</p>  
>   
> <blockquote>  
> <p>1.2.5 Spike Testing<br>  
> Purpose: Evaluate system response to sudden traffic surges</p>  
> </blockquote>  
>   
> <p>Metrics: Recovery time, error spikes, system stability</p>  
>   
> <p>Use Case: Handling viral content, emergency notifications</p>  
>   
> <p>Typical Scenario: Instant 10x traffic increase for 5 minutes</p>  
>   
> <blockquote>  
> <p>1.2.6 Volume Testing<br>  
> Purpose: Test system with large amounts of data</p>  
> </blockquote>  
>   
> <p>Metrics: Database performance, storage utilization, data processing time</p>  
>   
> <p>Use Case: Big data applications, reporting systems</p>  
>   
> <p>Typical Scenario: Processing millions of records simultaneously</p>  
>   
> <blockquote>  
> <p>1.2.7 Scalability Testing<br>  
> Purpose: Verify system performance as resources increase</p>  
> </blockquote>  
>   
> <p>Metrics: Linear scaling capability, resource efficiency</p>  
>   
> <p>Use Case: Horizontal scaling validation, cloud resource planning</p>  
>   
> <p>Typical Scenario: Adding nodes/containers while increasing load</p>  
>   
> <blockquote>  
> <p>1.3 Advanced Stress Analogies from Material Science<br>  
> Modern distributed systems exhibit behaviors remarkably similar to physical materials under stress. Understanding these analogies helps identify subtle performance issues that conventional testing might miss.</p>  
>   
> <p>1.3.1 Residual Stresses<br>  
> Definition: Internal stresses that remain in a system after the original cause of stress has been removed.</p>  
> </blockquote>  
>   
> <p>System Analog: Performance degradation lingering after high-load events</p>  
>   
> <p>Examples:</p>  
>   
> <p>Memory fragmentation after garbage collection</p>  
>   
> <p>Database connection pool saturation</p>  
>   
> <p>Cache invalidation patterns are causing subsequent slowdowns</p>  
>   
> <p>Session state corruption after recovery</p>  
>   
> <blockquote>  
> <p>1.3.2 Structural Stresses<br>  
> Definition: Stresses resulting from architectural design limitations or component interactions.</p>  
> </blockquote>  
>   
> <p>System Analog: Bottlenecks caused by system architecture</p>  
>   
> <p>Examples:</p>  
>   
> <p>Microservice communication overhead</p>  
>   
> <p>Database schema design limitations</p>  
>   
> <p>API gateway throughput limits</p>  
>   
> <p>Message queue backpressure</p>  
>   
> <p>Service mesh latency</p>  
>   
> <blockquote>  
> <p>1.3.3 Pressure Stresses<br>  
> Definition: Uniform stress applied across a system's surface area.</p>  
> </blockquote>  
>   
> <p>System Analog: Evenly distributed load causing systemic issues</p>  
>   
> <p>Examples:</p>  
>   
> <p>Rate limiting across all endpoints</p>  
>   
> <p>Database connection limits</p>  
>   
> <p>Bandwidth saturation</p>  
>   
> <p>CPU throttling across all nodes</p>  
>   
> <blockquote>  
> <p>1.3.4 Flow Stresses<br>  
> Definition: Stresses caused by fluid movement or streaming through a system.</p>  
> </blockquote>  
>   
> <p>System Analog: Data streaming and processing bottlenecks</p>  
>   
> <p>Examples:</p>  
>   
> <p>Real-time data processing pipelines</p>  
>   
> <p>WebSocket connection handling</p>  
>   
> <p>Streaming API throughput</p>  
>   
> <p>Event-driven architecture backpressure</p>  
>   
> <p>Data ingestion rate limitations</p>  
>   
> <p>Memory pressure from multiple services</p>  
>   
> <blockquote>  
> <p>1.3.5 Thermal Stresses<br>  
> Definition: Stresses caused by temperature changes leading to expansion/contraction.</p>  
> </blockquote>  
>   
> <p>System Analog: Resource utilisation causing performance throttling</p>  
>   
> <p>Examples:</p>  
>   
> <p>CPU thermal throttling under sustained load</p>  
>   
> <p>Memory heat-induced errors</p>  
>   
> <p>Disk I/O thermal limitations</p>  
>   
> <p>Network equipment overheating</p>  
>   
> <p>Container orchestration auto-scaling delays</p>  
>   
> <blockquote>  
> <p>1.3.6 Fatigue Stresses<br>  
> Definition: Progressive structural damage under cyclic loading.</p>  
> </blockquote>  
>   
> <p>System Analog: Performance degradation under repeated load cycles</p>  
>   
> <p>Examples:</p>  
>   
> <p>Memory leaks over multiple test cycles</p>  
>   
> <p>Database connection pool degradation</p>  
>   
> <p>File descriptor exhaustion</p>  
>   
> <p>Thread pool starvation patterns</p>  
>   
> <p>Garbage collection efficiency degradation</p>  
>   
> <blockquote>  
> <p>1.4 Load Testing Strategy Matrix<br>  
> </p>  
>   
>   
> </blockquote>  
>   
> <div>  
> <pre><code>**Test Type     Primary Goal         Key Metrics **                          Duration    User Pattern  
> Baseline      Establish norms      Response time, throughput             Short       Normal distribution  
> Load          Validate capacity    Error rate, latency                   Medium      Expected peak  
> Stress        Find limits          Breaking points, recovery             Medium-High Gradual increase  
> Soak          Detect leaks         Memory usage, degradation             Long        Steady state  
> Spike         Test resilience      Recovery time, errors                 Short       Instant surge  
> Volume        Data handling        Processing time, storage              Medium      Large datasets  
> Scalability   Scaling efficiency   Linear scaling, cost                  Medium      Incremental load  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <blockquote>  
> <p>1.5 Performance Metrics Framework<br>  
> 1.5.1 Response Metrics<br>  
> Response Time: 50th, 95th, 99th percentiles</p>  
> </blockquote>  
>   
> <p>Throughput: Requests/second, transactions/minute</p>  
>   
> <p>Error Rate: Percentage of failed requests</p>  
>   
> <p>Success Rate: Percentage of successful operations</p>  
>   
> <blockquote>  
> <p>1.5.2 Resource Metrics<br>  
> CPU Utilization: Percentage across all nodes</p>  
> </blockquote>  
>   
> <p>Memory Usage: Heap, stack, native memory</p>  
>   
> <p>I/O Operations: Disk read/write, network throughput</p>  
>   
> <p>Connection Count: Active connections, pool utilization</p>  
>   
> <blockquote>  
> <p>1.5.3 Business Metrics<br>  
> Conversion Rate: Under load conditions</p>  
> </blockquote>  
>   
> <p>User Satisfaction: Synthetic user experience scoring</p>  
>   
> <p>Revenue Impact: Performance effect on transactions</p>  
>   
> <p>Abandonment Rate: User drop-off under stress</p>  
>   
> <blockquote>  
> <p>1.6 Risk-Based Testing Prioritization<br>  
> High-Risk Areas (Test First):<br>  
> Core Transaction Paths: Checkout, login, payment</p>  
> </blockquote>  
>   
> <p>Data Integrity Operations: Orders, financial transactions</p>  
>   
> <p>Third-Party Integrations: Payment gateways, external APIs</p>  
>   
> <p>Stateful Operations: User sessions, shopping carts</p>  
>   
> <blockquote>  
> <p>Medium-Risk Areas:<br>  
> Search and Browse: Product discovery</p>  
> </blockquote>  
>   
> <p>Content Delivery: Images, videos, static assets</p>  
>   
> <p>Reporting and Analytics: Data aggregation</p>  
>   
> <blockquote>  
> <p>Low-Risk Areas:<br>  
> Static Pages: About us, contact information</p>  
> </blockquote>  
>   
> <p>Administrative Functions: Back-office operations</p>  
>   
> <p>Non-critical Features: User preferences, wishlists</p>

---

## [3/10] HTTP/1.1 vs HTTP/2 vs HTTP/3 – Which One Are You Still Using in 2025?
**Source:** The Practical Developer | **Date:** 2025-12-02T13:33:40.000Z
**URL:** https://dev.to/sreekanth_kuruba_91721e5d/http11-vs-http2-vs-http3-which-one-are-you-still-using-in-2025-4aah
**Reasoning:** The article discusses HTTP protocols, which is not directly related to our focus on AI and code intelligence.
**Authors:** Sreekanth Kuruba

**Content/Abstract:**
> <p>🚀 <strong>Networking for DevOps &amp; SRE – 2025 Edition • Part 2/10</strong></p> 
>  
> <p>Most teams think they’ve already moved to HTTP/2 or HTTP/3.</p> 
>  
> <p>But when we checked real production traffic in 2025, the truth was surprising:</p> 
>  
> <p>We pulled protocol stats from our CDN + load balancer logs...</p> 
>  
> <p>63% of all requests were still hitting us over HTTP/1.1 — mostly from corporate proxies, middleboxes, and legacy devices.</p> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Famg2d7tt8848uuzzxw49.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Famg2d7tt8848uuzzxw49.png" alt="Pie chart showing HTTP protocol distribution: 63% HTTP/1.1, 25% HTTP/2, and 12% HTTP/3, based on real production traffic analysis" width="800" height="628"></a></p> 
>  
> <p>That means 2 out of every 3 requests were paying an unnecessary 150–300ms latency tax just because outdated protocols were still in the path.</p> 
>  
> <h2> 
>    
>    
>   The Web’s 2025 Protocol Reality Check 
> </h2> 
>  
> <p>All three versions move data between browser and backend.<br> 
> But how they do it — TCP vs multiplexing vs QUIC — creates massive differences in:</p> 
>  
> <ul> 
> <li>Page load speed</li> 
> <li>API latency</li> 
> <li>Core Web Vitals</li> 
> <li>CDN routing efficiency</li> 
> <li>Mobile reliability</li> 
> </ul> 
>  
> <p>Here’s the 2025 snapshot:</p> 
>  
> <p>📊 <strong>HTTP/1.1 vs HTTP/2 vs HTTP/3 (2025 Edition)</strong></p> 
>  
> <div><table> 
> <thead> 
> <tr> 
> <th>Feature</th> 
> <th>HTTP/1.1 (1997)</th> 
> <th>HTTP/2 (2015)</th> 
> <th>HTTP/3 (2022+, QUIC)</th> 
> </tr> 
> </thead> 
> <tbody> 
> <tr> 
> <td>Transport</td> 
> <td>TCP</td> 
> <td>TCP</td> 
> <td>UDP (QUIC)</td> 
> </tr> 
> <tr> 
> <td>Multiplexing</td> 
> <td>No</td> 
> <td>Yes</td> 
> <td>Yes (independent streams)</td> 
> </tr> 
> <tr> 
> <td>HOL Blocking</td> 
> <td>Yes</td> 
> <td>Yes (TCP)</td> 
> <td>None</td> 
> </tr> 
> <tr> 
> <td>Header Compression</td> 
> <td>None</td> 
> <td>HPACK</td> 
> <td>QPACK</td> 
> </tr> 
> <tr> 
> <td>Connection Setup</td> 
> <td>1–3 RTT</td> 
> <td>1–3 RTT</td> 
> <td>0–1 RTT</td> 
> </tr> 
> <tr> 
> <td>Mobile Performance</td> 
> <td>Poor</td> 
> <td>Decent</td> 
> <td>Best</td> 
> </tr> 
> <tr> 
> <td>Real Adoption (2025)</td> 
> <td>15–20%</td> 
> <td>60–65%</td> 
> <td>25–30% and rising</td> 
> </tr> 
> <tr> 
> <td>Browser Support</td> 
> <td>100%</td> 
> <td>~98%</td> 
> <td>~95–97%</td> 
> </tr> 
> </tbody> 
> </table></div> 
>  
> <p>🦕 <strong>HTTP/1.1 – The Dinosaur That Refuses to Die</strong><br><br> 
> Why it still dominates:</p> 
>  
> <ul> 
> <li>Corporate proxies downgrade connections</li> 
> <li>Old load balancers downgrade traffic back to HTTP/1.1</li> 
> <li>Cheap hosting providers</li> 
> <li>Legacy browsers &amp; IoT devices</li> 
> <li>Internal APIs nobody migrated</li> 
> </ul> 
>  
> <p>Problems:</p> 
>  
> <ul> 
> <li>No multiplexing</li> 
> <li>HOL blocking</li> 
> <li>Browser opens 6 parallel connections</li> 
> <li>Massive header repetition</li> 
> </ul> 
>  
> <p>If you still rely on HTTP/1.1 in 2025, you are paying a latency tax every single day.</p> 
>  
> <p>⚡ <strong>HTTP/2 – The Multiplexing Hero (With One Big Problem)</strong><br><br> 
> HTTP/2 solved a lot:</p> 
>  
> <ul> 
> <li>Binary framing</li> 
> <li>Multiplexing</li> 
> <li>Header compression</li> 
> <li>Single connection</li> 
> </ul> 
>  
> <p>But it still suffers from <strong>TCP Head-of-Line Blocking</strong>:<br><br> 
> One lost packet → all streams wait.<br><br> 
> On flaky networks (mobile, 3–5% packet loss), H2 often performs worse than people expect.<br><br> 
> Still excellent for: CDNs, production APIs, stable networks.</p> 
>  
> <p><strong>HTTP/3 – QUIC Is the Real Upgrade</strong><br><br> 
> HTTP/3 ditches TCP entirely and uses QUIC over UDP.<br><br> 
> Big wins:</p> 
>  
> <ul> 
> <li>0-RTT resume</li> 
> <li>No HOL blocking</li> 
> <li>Faster handshakes</li> 
> <li>Better encryption (TLS 1.3 built-in)</li> 
> <li>Superior mobile performance</li> 
> <li>Stable under packet loss</li> 
> </ul> 
>  
> <p>This is the first protocol designed for modern, mobile, global internet traffic.</p> 
>  
> <p>📈 <strong>Real 2025 Performance Results</strong></p> 
>  
> <div><table> 
> <thead> 
> <tr> 
> <th>Scenario</th> 
> <th>HTTP/1.1</th> 
> <th>HTTP/2</th> 
> <th>HTTP/3</th> 
> </tr> 
> </thead> 
> <tbody> 
> <tr> 
> <td>100 small assets</td> 
> <td>4–6s</td> 
> <td>~1.2s</td> 
> <td>~0.9s</td> 
> </tr> 
> <tr> 
> <td>3% packet loss</td> 
> <td>Terrible</td> 
> <td>Bad</td> 
> <td>Good</td> 
> </tr> 
> <tr> 
> <td>Flaky mobile</td> 
> <td>Painful</td> 
> <td>Okay</td> 
> <td>Best</td> 
> </tr> 
> <tr> 
> <td>First load</td> 
> <td>Slow</td> 
> <td>Slow</td> 
> <td>Fastest</td> 
> </tr> 
> <tr> 
> <td>Repeat visits</td> 
> <td>~Same</td> 
> <td>~Same</td> 
> <td>Instant (0-RTT)</td> 
> </tr> 
> </tbody> 
> </table></div> 
>  
> <p><strong>The Problem Nobody Mentions</strong><br><br> 
> Even if your CDN + app support HTTP/3:<br><br> 
> Many users still fall back to 1.1 or 2.0 due to network intermediaries.<br><br> 
> Common blockers:</p> 
>  
> <ul> 
> <li>Corporate firewalls</li> 
> <li>Middleboxes that strip UDP</li> 
> <li>Legacy devices</li> 
> <li>Some enterprise proxies</li> 
> <li>Outdated routers</li> 
> <li>Misconfigured hosting</li> 
> </ul> 
>  
> <p>This is why simply enabling HTTP/3 is not enough – everything in the path must support it.</p> 
>  
> <p><strong>So What Should You Use in 2025?</strong><br><br> 
> HTTP/1.1 → Only for legacy systems<br><br> 
> Or internal APIs that never changed.<br> 
> HTTP/2 → Still excellent and widely reliable<br><br> 
> Stable, cheap, widely supported.<br> 
> HTTP/3 → Enable it everywhere you can<br><br> 
> (Cloudflare, CloudFront, Fastly, Akamai, Bunny — all support it now)<br> 
> <strong>Quick Checklist to Move to HTTP/3 (2025)</strong></p> 
>  
> <p><strong>CDN</strong>  </p> 
>  
> <ul> 
> <li>Cloudflare → Enable QUIC + HTTP/3 
> </li> 
> <li>CloudFront → Supported on new distributions 
> </li> 
> <li>Fastly/Akamai/Bunny → Native support</li> 
> </ul> 
>  
> <p><strong>Self-Hosted</strong>  </p> 
>  
> <ul> 
> <li>Nginx 1.25+ QUIC 
> </li> 
> <li>Caddy 2.6+ (auto HTTP/3) 
> </li> 
> <li>Traefik v3 
> </li> 
> <li>LiteSpeed / OpenLiteSpeed</li> 
> </ul> 
>  
> <p><strong>Backend</strong>  </p> 
>  
> <ul> 
> <li>Node.js 21+ with QUIC 
> </li> 
> <li>Go, Rust, Java (Netty) → great QUIC libraries 
> </li> 
> <li>Python → aioquic or reverse proxy</li> 
> </ul> 
>  
> <p><strong>Final Verdict (2025)</strong><br><br> 
> HTTP/1.1 → Legacy tech<br><br> 
> HTTP/2 → Today’s safe default<br><br> 
> HTTP/3 → Today’s <strong>performance baseline</strong></p> 
>  
> <p>HTTP/3 isn’t “future tech” anymore - it’s the baseline for fast global apps in 2025.</p> 
>  
> <p><strong>The question isn’t if you should upgrade…</strong><br><br> 
> <strong>It’s how much faster your users will be when you do.</strong></p> 
>  
>  
>  
>  
> <p><strong>Part of the “Networking for DevOps &amp; SRE – 2025 Edition” series</strong><br><br> 
> Part 1 → HTTP/HTTPS/TCP/UDP Foundations<br><br> 
> Part 3 → TLS 1.2 vs TLS 1.3 in Production: (drops Next Tuesday 7:30 PM IST)<br> 
> Subscribe or follow so you don’t miss it.</p>

---

## [3/10] JAI Router - lightweight Spring Boot routing starter (open for contributors)
**Source:** The Practical Developer | **Date:** 2025-12-02T13:25:07.000Z
**URL:** https://dev.to/rrezart_prebreza_60ef7bfb/jai-router-lightweight-spring-boot-routing-starter-open-for-contributors-379b
**Reasoning:** The article is about a Spring Boot project, which is not directly related to our focus areas.
**Authors:** Rrezart Prebreza

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fud543o4omm203fzsd7rd.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>JAI Router is a minimal Spring Boot starter that centralizes dynamic route registration and exposes a small admin API for runtime routing. The project is ready for contributors — see CONTRIBUTING.md and issues labeled good first issue and help wanted.</p>  
>   
> <p>Problem</p>  
>   
> <ul>  
> <li>Dynamic route registration is often scattered across services and hard to test.</li>  
> <li>Existing solutions introduce heavy dependencies or tightly couple to framework internals.</li>  
> </ul>  
>   
> <p>What JAI Router does</p>  
>   
> <ul>  
> <li>Provides a small, testable API to register and manage routes at runtime.</li>  
> <li>Integrates as a Spring Boot starter with minimal dependencies.</li>  
> <li>Keeps route storage and dispatching modular and easy to mock in tests.</li>  
> </ul>  
>   
> <p>Key features</p>  
>   
> <ul>  
> <li>Runtime route registration and removal.</li>  
> <li>Pluggable route matching and handler resolution.</li>  
> <li>Small API surface suitable for microservices and integration tests  
> .</li>  
> </ul>  
>   
> <p>Quickstart</p>  
>   
> <ul>  
> <li>Clone the repo: git clone hhttps://github.com/JAI-create-spec/JAI-Router/tree/develop</li>  
> <li>Run locally: ./gradlew bootRun (Java 17)</li>  
> <li>See README.md for quick-start snippets and example endpoints.</li>  
> </ul>  
>   
> <p>Example usage<br>  
> Register a route via the admin API and validate it with an integration test. See README.md for concrete request and response examples.</p>  
>   
> <p>How to contribute<br>  
> Read CONTRIBUTING.md for setup, tests, and branch conventions.</p>  
>   
> <p>Pick a starter issue labeled good first issue or help wanted.</p>  
>   
> <p>Open small PRs with tests; maintainers aim to review quickly.</p>  
>   
> <p>Repo and license<br>  
> Repository: <a href="https://github.com/JAI-create-spec/JAI-Router/tree/develop">https://github.com/JAI-create-spec/JAI-Router/tree/develop</a></p>  
>   
> <p>License: see LICENSE</p>  
>   
> <p>Call to action<br>  
> If you work on Spring Boot routing or infrastructure, try the quickstart and pick a starter issue — contributions are welcome.</p>

---

## [3/10] 🚀 ProblemPad — The Game-Changing Platform Built for Real Problems, Real Voices, Real Solutions 🎤⚡
**Source:** The Practical Developer | **Date:** 2025-12-02T13:23:41.000Z
**URL:** https://dev.to/puneetkumar2010/problempad-the-game-changing-platform-built-for-real-problems-real-voices-real-solutions-1ei5
**Reasoning:** The article is about a community platform, not relevant to our primary or secondary interests.
**Authors:** Puneet-Kumar2010

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fj2je6vtxynw1u1o25xpn.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>Hey dev fam 👋<br><br>  
> Today I’m dropping something wild — a project built straight from real-world needs, tested in the field, and wrapped in clean Node.js engineering.</p>  
>   
> <p>Introducing <strong>ProblemPad</strong>: a community-powered problem-solving platform designed for neighborhoods, societies, and professional workers.<br><br>  
> Think of it like a digital notice board + service marketplace + community hub… all merged into one clean experience.  </p>  
>   
> <p>And yes — it’s built with <strong>zero bloat</strong>, <strong>no GridFS</strong>, and an audio system running on pure MongoDB binary power. ⚡</p>  
>   
> <p>GitHub Repo 👉 <strong><a href="https://github.com/DeveloperPuneet/ProblemPad">https://github.com/DeveloperPuneet/ProblemPad</a></strong></p>  
>   
>   
> <h2>  
>     
>     
>   🌟 What ProblemPad Solves  
> </h2>  
>   
> <p>Let’s be honest — everyone has daily issues:</p>  
>   
> <ul>  
> <li>Electrical faults ⚡  
> </li>  
> <li>Plumbing breakdowns 💧  
> </li>  
> <li>Device malfunctions 🔌  
> </li>  
> <li>Community chaos 🏘️  
> </li>  
> </ul>  
>   
> <p>But the real chaos?<br><br>  
> <em>No unified place to report these issues and get them solved quickly.</em></p>  
>   
> <p>ProblemPad fixes that with one simple idea:</p>  
>   
> <blockquote>  
> <p><strong>Let communities report problems, and let workers respond instantly.</strong></p>  
> </blockquote>  
>   
> <p>A platform where:</p>  
>   
> <ul>  
> <li>Users post issues  
> </li>  
> <li>Workers provide solutions  
> </li>  
> <li>Communities stay organized  
> </li>  
> <li>Everyone saves time  
> </li>  
> </ul>  
>   
>   
> <h2>  
>     
>     
>   🧩 Core Features (The Fun Stuff)  
> </h2>  
> <h3>  
>     
>     
>   👥 User Accounts  
> </h3>  
>   
> <ul>  
> <li>Mobile-based registration  
> </li>  
> <li>Two roles: <strong>Users</strong> &amp; <strong>Service Workers</strong>  
> </li>  
> <li>Location-based community discovery  
> </li>  
> <li>Skill-tagged worker profiles  
> </li>  
> </ul>  
> <h3>  
>     
>     
>   🏘️ Community System  
> </h3>  
>   
> <ul>  
> <li>Create communities for buildings/neighborhoods  
> </li>  
> <li>Invite users by phone  
> </li>  
> <li>Real-time notifications for new problems  
> </li>  
> <li>Member and role management  
> </li>  
> </ul>  
> <h3>  
>     
>     
>   🆘 Problem Reporting  
> </h3>  
>   
> <ul>  
> <li>Text-based issue reporting  
> </li>  
> <li>  
> <p><strong>Audio Reporting</strong> 🎤  </p>  
>   
> <ul>  
> <li>Uses MediaRecorder API  
> </li>  
> <li>Encoded as Base64  
> </li>  
> <li>Stored directly in MongoDB as Binary/BSON  
> </li>  
> <li>No GridFS. No bulky pipelines. Clean and fast.</li>  
> </ul>  
> </li>  
> <li><p>Categorized issues (electrical, technical, plumbing, etc.)</p></li>  
> </ul>  
> <h3>  
>     
>     
>   🔧 Problem Resolution  
> </h3>  
>   
> <ul>  
> <li>Workers get instant alerts  
> </li>  
> <li>Can provide solution remarks  
> </li>  
> <li>User confirms the fix  
> </li>  
> <li>Rating system to validate worker quality  
> </li>  
> </ul>  
> <h3>  
>     
>     
>   🔔 Notifications  
> </h3>  
>   
> <ul>  
> <li>New problem notifications  
> </li>  
> <li>Community invites  
> </li>  
> <li>Solution updates  
> </li>  
> <li>Auto-cleanup alerts  
> </li>  
> </ul>  
>   
>   
> <h2>  
>     
>     
>   🧹 Auto-Cleanup System (Your DB Will Thank You)  
> </h2>  
>   
> <p>A literal lifesaver:</p>  
>   
> <ul>  
> <li>Solved + confirmed problems auto-delete after <strong>30 days</strong>  
> </li>  
> <li>Runs via daily cron job  
> </li>  
> <li>Audio + problem data gets cleaned  
> </li>  
> <li>Keeps MongoDB lean and happy  
> </li>  
> </ul>  
> <div>  
> <pre><code><span>cron</span><span>.</span><span>schedule</span><span>(</span><span>"</span><span>0 2 * * *</span><span>"</span><span>,</span> <span>cleanupFunction</span><span>);</span>  
> </code></pre>  
>   
> </div>  
>   
>   
> <p>Set it and forget it. 🛏️</p>  
>   
>   
> <h2>  
>     
>     
>   🔊 Audio Storage — No GridFS Needed  
> </h2>  
>   
> <p>This is where ProblemPad goes brrrrr ⚡</p>  
>   
> <p><strong>How audio is handled:</strong></p>  
>   
> <ol>  
> <li>Record using browser’s <strong>MediaRecorder</strong>  
> </li>  
> <li>Convert to Base64  
> </li>  
> <li>Store as Buffer/Binary inside the Problem document  
> </li>  
> <li>Serve via a <code>/problem-audio/:id</code> route  
> </li>  
> </ol>  
>   
> <p>It’s lightweight, effective, and perfect for short community audio clips.</p>  
>   
>   
> <h2>  
>     
>     
>   🛠️ Tech Stack (Simple but Powerful)  
> </h2>  
> <h3>  
>     
>     
>   Backend  
> </h3>  
>   
> <ul>  
> <li>Node.js + Express  
> </li>  
> <li>MongoDB + Mongoose  
> </li>  
> <li>express-session auth  
> </li>  
> <li>node-cron  
> </li>  
> <li>randomstring for unique IDs  
> </li>  
> </ul>  
> <h3>  
>     
>     
>   Frontend  
> </h3>  
>   
> <ul>  
> <li>Pug Templates  
> </li>  
> <li>Vanilla JS  
> </li>  
> <li>Responsive UI  
> </li>  
> <li>Dark-mode theme  
> </li>  
> </ul>  
> <h3>  
>     
>     
>   Real-Time  
> </h3>  
>   
> <ul>  
> <li>Socket.io for instant sync  
> </li>  
> </ul>  
>   
>   
> <h2>  
>     
>     
>   📦 Installation  
> </h2>  
>   
>   
> <div>  
> <pre><code>git clone https://github.com/DeveloperPuneet/ProblemPad.git  
> <span>cd </span>ProblemPad  
> npm <span>install</span>  
> </code></pre>  
>   
> </div>  
>   
>   
> <p>Create <code>.env</code>:<br>  
> </p>  
>   
> <div>  
> <pre><code>SESSION_SECRET=yourkey  
> MONGODB_URI=mongodb+srv://...  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Run:<br>  
> </p>  
>   
> <div>  
> <pre><code>npm start  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Open:<br><br>  
> <code>http://localhost:3000</code></p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   🗂️ Project Structure  
> </h2>  
>   
>   
>   
> <div>  
> <pre><code>ProblemPad/  
> │── controllers/  
> │── models/  
> │── routes/  
> │── views/  
> │── utils/  
> ├── public/  
> └── app.js  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Clean, modular, and scalable.</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   🧪 Why This Project Matters  
> </h2>  
>   
> <p>ProblemPad isn’t just “another MERN project.”</p>  
>   
> <p>It’s built for <strong>real people</strong>, with <strong>real needs</strong>, and solves <strong>real community problems</strong>.</p>  
>   
> <p>This platform can be used by:</p>  
>   
> <ul>  
> <li>RWAs  
> </li>  
> <li>Colonies  
> </li>  
> <li>Apartment complexes  
> </li>  
> <li>Small towns  
> </li>  
> <li>Service workers  
> </li>  
> <li>Maintenance teams  
> </li>  
> </ul>  
>   
> <p>Even defense housing communities 👀<br><br>  
> (Yes — it was originally built for one.)</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   🔮 Future Upgrades  
> </h2>  
>   
> <ul>  
> <li>Mobile app (React Native / Flutter)  
> </li>  
> <li>Payment gateway for premium services  
> </li>  
> <li>Worker badges &amp; ranking  
> </li>  
> <li>Real-time chat  
> </li>  
> <li>Advanced analytics  
> </li>  
> </ul>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   💬 Final Thoughts  
> </h2>  
>   
> <p>ProblemPad isn’t just software — it’s community empowerment.<br><br>  
> A tool for people to help each other, grow together, and solve problems with speed and clarity.</p>  
>   
> <p>If you vibe with the mission:</p>  
>   
> <p>⭐ <strong>Star the repo</strong><br><br>  
> 🍴 <strong>Fork it</strong><br><br>  
> 🛠️ <strong>Contribute</strong><br><br>  
> 💬 <strong>Drop feedback</strong>  </p>  
>   
> <p>Made with ❤️ in India<br><br>  
> <em>By DeveloperPuneet</em></p>

---

## [3/10] New Opportunity - Growth Partner For Software Business
**Source:** The Practical Developer | **Date:** 2025-12-02T13:21:05.000Z
**URL:** https://dev.to/passionmuse16/new-opportunity-growth-partner-for-software-business-aa1
**Reasoning:** The article is about AWS CDK, which is not directly related to our focus on AI and code intelligence.
**Authors:** Takeshi Kato

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbwhynffrqfnd1wgm3fkk.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>Warm greetings,</p>  
>   
> <p>My name is Takeshi, a senior software engineer based in Japan leading a small development team focused on Web and AI applications. We’re now preparing to expand into the US software market.</p>  
>   
> <p>We’re looking for a partner who can help us secure software jobs or contracts by engaging with clients and managing the interview, while my team handles all job applications, the entire development process, and delivery.</p>  
>   
> <p><strong>What You Gain</strong><br>  
> Revenue: A defined percentage income from each successful project<br>  
> Flexibility: No schedule restrictions - continue your lifestyle<br>  
> Support: Managing All technical operations by my team<br>  
> Optional: Supporting your company or projects under a subcontracting arrangement at competitive rates</p>  
>   
> <p><strong>Requirements</strong><br>  
> Native or bilingual English proficiency<br>  
> US work authorization<br>  
> 3 years of software development or related experience<br>  
> Strong ability to communicate</p>  
>   
> <p>If this sound like a good fit for you, please schedule a short call using the following link: Calendly Link<br>  
> Thank you for your reading, and hope to talk soon.</p>  
>   
> <p>Best regards,<br>  
> Takeshi</p>  
>   
> <p>Senior Software Engineer | Team Manager<br>  
> Email: <a href="mailto:passionmuse16@gmail.com">passionmuse16@gmail.com</a><br>  
> GitHub: <a href="https://github.com/sven3270350">https://github.com/sven3270350</a></p>

---

## [3/10] DebateMaster-AI
**Source:** The Practical Developer | **Date:** 2025-12-02T13:11:40.000Z
**URL:** https://dev.to/abbasmir12/debatemaster-ai-2n8b
**Reasoning:** Focuses on a debate coaching AI, not relevant to code intelligence or context engineering.
**Authors:** Mir Abbas

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F899i6wcyq1lwcban3sxc.jpg" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>Debating is an art – it’s not just what you say, but <em>how</em> you say it. Imagine having a personal AI coach that not only challenges your arguments in real-time but also tracks your performance, gives constructive feedback, and helps you grow as a debater. That’s exactly what I built for this Hackathon: <strong>DebateMaster AI</strong>.</p>  
>   
> <h2>  
>     
>     
>   What is DebateMaster-AI?  
> </h2>  
>   
> <p>DebateMaster-I is a <strong>real-time debate coaching platform</strong> that leverages <strong>Google's Gemini Live API</strong> for bidirectional voice streaming. Unlike traditional debate apps, this platform is designed to <strong>analyze your performance on the fly</strong>, track your improvement, and help you refine your skills through AI-powered insights.</p>  
>   
> <p>Here’s how it works:</p>  
>   
> <ol>  
> <li>Enter a debate topic, for example: <em>"Impact of Pollution on Living Organisms"</em>  
> </li>  
> <li>Choose your debate style: <strong>Coach mode</strong> for supportive feedback or <strong>Fierce mode</strong> for challenging opposition.</li>  
> <li>Configure your Gemini API key, hit <strong>Start Debate</strong>, and the AI engages you in natural conversation.</li>  
> </ol>  
>   
> <p>During the debate, the interface shows live transcription, audio visualization, and a running timer. When the debate ends, you receive a <strong>comprehensive performance report</strong> including:</p>  
>   
> <ul>  
> <li>  
> <strong>Vocabulary Richness</strong> – tracks your lexical diversity.</li>  
> <li>  
> <strong>Confidence Level</strong> – analyzes speech patterns and delivery.</li>  
> <li>  
> <strong>Argument Strength</strong> – evaluates logical structure and evidence.</li>  
> <li>  
> <strong>Response Time</strong> – measures how quickly you counter arguments.</li>  
> <li>  
> <strong>Engagement Score</strong> – tracks active participation.</li>  
> </ul>  
>   
> <p>The AI also generates <strong>personalized suggestions</strong> to improve your debating skills.</p>  
>   
> <h2>  
>     
>     
>   Tracking Progress and Persona Analysis  
> </h2>  
>   
> <p>DebateMaster AI isn’t just a one-time experience. The <strong>Activity tab</strong> visualizes your progress through score trends, heatmaps, and session history. Additionally, the <strong>Persona system</strong> assigns a debate archetype – Strategist, Orator, Analyst, or Diplomat – showing your strengths and evolution over time. Achievements like <em>Vocabulary Master</em> or <em>Debate Marathon</em> gamify the learning experience.</p>  
>   
> <h2>  
>     
>     
>   How Kiro IDE Made This Possible  
> </h2>  
>   
> <p>The real star behind the scenes is <strong>Kiro IDE</strong>. I used:</p>  
>   
> <ul>  
> <li>  
> <strong>Multi-Spec Development</strong> – I split the project into four independent specs: core debate functionality, activity view, persona system, and UI redesign. This allowed parallel development without conflicts.</li>  
> <li>  
> <strong>Steering Documents</strong> – ensured consistent styling, architecture, and API usage across features.</li>  
> <li>  
> <strong>Agent Hooks</strong> – automated repetitive tasks like linting, grammar checking, and README updates, saving hours of manual work.</li>  
> <li>  
> <strong>Model Context Protocol (MCP)</strong> – integrated live documentation for Gemini API, TypeScript, React, and Tailwind, so Kiro always had the latest references.</li>  
> </ul>  
>   
> <p>This workflow accelerated development, improved code quality, and let me focus on building the best user experience rather than boilerplate tasks.</p>  
>   
> <h2>  
>     
>     
>   Why DebateMaster AI Matters  
> </h2>  
>   
> <p>Whether you’re a student preparing for competitions, a professional improving public speaking, or someone who wants to sharpen critical thinking, DebateMaster AI offers a <strong>complete coaching platform</strong>. In just six weeks, the combination of AI-assisted development and Kiro IDE helped me turn a complex idea into a polished, fully functional app.</p>  
>   
> <h2>  
>     
>     
>   Try It Out  
> </h2>  
>   
> <p>DebateMaster AI is more than a hackathon project – it’s a learning platform. Check out the <a href="https://github.com/abbasmir12/debatemasterai">repository</a> and give it a try. Your next debate could be your best one yet!</p>

---

## [3/10] Cyber Startup Frenetik Launches with Patented Deception Technology That Bets Against the AI Arms Race
**Source:** DevOps.com | **Date:** 2025-12-02T13:00:56.000Z
**URL:** https://devops.com/cyber-startup-frenetik-launches-with-patented-deception-technology-that-bets-against-the-ai-arms-race/
**Reasoning:** Generic marketing for a startup, not directly relevant to our interests.
**Authors:** cybernewswire

**Content/Abstract:**
> <div><img width="1200" height="720" src="https://devops.com/wp-content/uploads/2025/12/frenetik_17640073059qwMUZNORw.jpg" alt="" style="margin-bottom:0px;"></div><img width="150" height="150" src="https://devops.com/wp-content/uploads/2025/12/frenetik_17640073059qwMUZNORw-150x150.jpg" alt="">Bethesda, USA / Maryland, 2nd December 2025, CyberNewsWire

---

## [3/10] How Quesby Handles SEO (Without Plugins, Frameworks, or Runtime Code)
**Source:** The Practical Developer | **Date:** 2025-12-02T13:00:00.000Z
**URL:** https://dev.to/quesby/how-quesby-handles-seo-without-plugins-frameworks-or-runtime-code-251
**Reasoning:** Focuses on SEO techniques, not relevant to our primary or secondary interests.
**Authors:** Quesby

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fngssuuo64wqleft7z6xf.webp" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><h2>  
>     
>     
>   Automatic Metadata, Open Graph, and JSON-LD — Eleventy Style  
> </h2>  
>   
> <p>Most Eleventy setups start the same way: every project re-implements SEO from scratch.<br><br>  
> Another <code>base.njk</code>, another batch of <code>&lt;meta&gt;</code> tags, another copy-pasted JSON-LD snippet from schema.org.</p>  
>   
> <p>It’s boring, error-prone, and inconsistent across projects.</p>  
>   
> <p><strong>Quesby solves this the only sane way: one SEO model, one source of truth, generated at build time.</strong><br><br>  
> No plugins.<br><br>  
> No runtime.<br><br>  
> No React components disguised as SEO helpers.</p>  
>   
> <p>This is how it works.</p>  
>   
>   
> <h2>  
>     
>     
>   The Idea in 10 Seconds  
> </h2>  
>   
> <p>Quesby takes:<br>  
> </p>  
>   
> <div>  
> <pre><code>site.json + frontmatter → SEO model → complete &lt;head&gt; + JSON-LD  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Everything happens during the Eleventy build.<br><br>  
> Templates stay clean.<br><br>  
> SEO stays consistent.</p>  
>   
> <p>In <code>base.njk</code>, it looks like this:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>{%</span><span>-</span> <span>set</span> <span>seoModel</span> <span>=</span> <span>page</span> <span>| </span><span>seoModel</span><span>(</span><span>site</span><span>,</span> <span>pageData</span><span>)</span> <span>-</span><span>%}</span>  
> <span>{{</span> <span>seoModel</span> <span>| </span><span>seoHeadHtml</span><span>(</span><span>site</span><span>)</span> <span>| </span><span>safe</span> <span>}}</span>  
> <span>{{</span> <span>seoModel</span> <span>| </span><span>seoJsonLd</span><span>(</span><span>site</span><span>)</span> <span>| </span><span>safe</span> <span>}}</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>That’s the entire integration surface.</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   Site-Level Defaults (One File)  
> </h2>  
>   
> <p>Global SEO config lives in:<br>  
> </p>  
>   
> <div>  
> <pre><code>src/_data/site.json  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Example:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>{</span><span>  
>   </span><span>"name"</span><span>:</span><span> </span><span>"Quesby"</span><span>,</span><span>  
>   </span><span>"url"</span><span>:</span><span> </span><span>"https://quesby.dev"</span><span>,</span><span>  
>   </span><span>"description"</span><span>:</span><span> </span><span>"A modern Eleventy boilerplate"</span><span>,</span><span>  
>   </span><span>"socialImage"</span><span>:</span><span> </span><span>"/assets/images/og-default.jpg"</span><span>,</span><span>  
>   </span><span>"twitter"</span><span>:</span><span> </span><span>"@quesby"</span><span>,</span><span>  
>   </span><span>"language"</span><span>:</span><span> </span><span>"en-US"</span><span>  
> </span><span>}</span><span>  
> </span></code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Quesby uses this for:</p>  
>   
> <ul>  
> <li>default description</li>  
> <li>default Open Graph image</li>  
> <li>canonical URL generation</li>  
> <li>Twitter Card data</li>  
> <li>JSON-LD publisher info</li>  
> </ul>  
>   
> <p>You define it once.<br><br>  
> Every page inherits it automatically.</p>  
>   
>   
> <h2>  
>     
>     
>   Frontmatter: Minimal When You Want, Detailed When You Need  
> </h2>  
>   
> <p>Simple page:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>---</span>  
> <span>title</span><span>:</span> <span>"</span><span>My</span><span> </span><span>Page"</span>  
> <span>description</span><span>:</span> <span>"</span><span>Short</span><span> </span><span>summary."</span>  
> <span>---</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Full blog post:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>---</span>  
> <span>title</span><span>:</span> <span>"</span><span>How</span><span> </span><span>to</span><span> </span><span>Build</span><span> </span><span>Better</span><span> </span><span>Websites"</span>  
> <span>seoTitle</span><span>:</span> <span>"</span><span>Complete</span><span> </span><span>Guide</span><span> </span><span>to</span><span> </span><span>Modern</span><span> </span><span>Web</span><span> </span><span>Development"</span>  
> <span>description</span><span>:</span> <span>"</span><span>Learn</span><span> </span><span>the</span><span> </span><span>principles</span><span> </span><span>behind</span><span> </span><span>fast,</span><span> </span><span>accessible</span><span> </span><span>websites."</span>  
> <span>image</span><span>:</span> <span>"</span><span>/assets/images/cover.jpg"</span>  
> <span>postType</span><span>:</span> <span>"</span><span>article"</span>  
> <span>author</span><span>:</span> <span>"</span><span>John</span><span> </span><span>Doe"</span>  
> <span>date</span><span>:</span> <span>"</span><span>2024-01-15"</span>  
> <span>---</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Quesby applies fallback rules:</p>  
>   
> <ul>  
> <li>Title: <code>`seoTitle` → `title` → `site.name`</code>  
> </li>  
> <li>Description: <code>`description` → `site.description`</code>  
> </li>  
> <li>Image: <code>`image` → `site.socialImage`</code>  
> </li>  
> </ul>  
>   
> <p>You don’t repeat yourself.<br><br>  
> Pages don’t need boilerplate.</p>  
>   
>   
> <h2>  
>     
>     
>   Automatic Head Metadata  
> </h2>  
>   
> <p>From the SEO model, Quesby generates:</p>  
> <h3>  
>     
>     
>   Basic SEO  
> </h3>  
>   
> <ul>  
> <li><code>&lt;title&gt;</code></li>  
> <li><code>&lt;meta name="description"&gt;</code></li>  
> <li><code>&lt;link rel="canonical"&gt;</code></li>  
> <li>  
> <code>&lt;meta name="robots"&gt;</code> (with <code>noindex</code> support)</li>  
> </ul>  
> <h3>  
>     
>     
>   Open Graph  
> </h3>  
>   
> <ul>  
> <li><code>og:type</code></li>  
> <li><code>og:title</code></li>  
> <li><code>og:description</code></li>  
> <li><code>og:url</code></li>  
> <li><code>og:image</code></li>  
> </ul>  
> <h3>  
>     
>     
>   Twitter Cards  
> </h3>  
>   
> <ul>  
> <li>  
> <code>`twitter:card`</code> (<code>`summary_large_image`</code>)</li>  
> <li><code>`twitter:site`</code></li>  
> <li><code>`twitter:title`</code></li>  
> <li><code>`twitter:description`</code></li>  
> <li><code>`twitter:image`</code></li>  
> </ul>  
>   
> <p>No manual tags.<br><br>  
> No plugin configuration.<br><br>  
> No React component SEO helper nonsense.</p>  
>   
>   
> <h2>  
>     
>     
>   JSON-LD Structured Data  
> </h2>  
>   
> <p>Quesby also outputs JSON-LD for:</p>  
> <h3>  
>     
>     
>   Blog posts (<code>BlogPosting</code>)  
> </h3>  
>   
> <ul>  
> <li>headline  
> </li>  
> <li>description  
> </li>  
> <li>image  
> </li>  
> <li>author  
> </li>  
> <li>publisher  
> </li>  
> <li>datePublished  
> </li>  
> <li>dateModified  
> </li>  
> </ul>  
> <h3>  
>     
>     
>   Site (<code>WebSite</code>)  
> </h3>  
>   
> <ul>  
> <li>name  
> </li>  
> <li>url  
> </li>  
> <li>description  
> </li>  
> <li>publisher  
> </li>  
> </ul>  
>   
> <p>Rendered via:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>{{</span> <span>seoModel</span> <span>| </span><span>seoJsonLd</span><span>(</span><span>site</span><span>)</span> <span>| </span><span>safe</span> <span>}}</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Again: static, not runtime.</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   Sitemap, Robots, and Noindex Integration  
> </h2>  
>   
> <p>Quesby integrates SEO beyond the <code>&lt;head&gt;</code>:</p>  
>   
> <ul>  
> <li>Sitemap generated from the <code>`sitemap`</code> collection  
> </li>  
> <li>Pages with <code>`noindex`</code> or <code>`eleventyExcludeFromCollections`</code> are auto-excluded  
> </li>  
> <li>  
> <code>`robots.txt`</code> generated automatically and linked to the sitemap  
> </li>  
> </ul>  
>   
> <p>Everything stays consistent with the same SEO model.</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   Opt-Out When Needed  
> </h2>  
>   
> <p>For special pages:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>---</span>  
> <span>seoDisableCoreHead</span><span>:</span> <span>true</span>  
> <span>seoDisableCoreJsonLd</span><span>:</span> <span>true</span>  
> <span>---</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>You can disable core SEO on a per-page basis and write your own tags.<br><br>  
> No lock-in.</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   Why This Matters for Eleventy Developers  
> </h2>  
>   
> <p>If you build Eleventy sites regularly, you already know the pain:</p>  
>   
> <ul>  
> <li>copy-pasting head tags  
> </li>  
> <li>rewriting JSON-LD  
> </li>  
> <li>fixing Open Graph mismatches  
> </li>  
> <li>forgetting canonical URLs  
> </li>  
> <li>duplicating logic across pages  
> </li>  
> </ul>  
>   
> <p>Quesby removes all of that.</p>  
>   
> <ul>  
> <li>One config file  
> </li>  
> <li>One model  
> </li>  
> <li>One consistent output  
> </li>  
> <li>Zero runtime JavaScript  
> </li>  
> <li>Zero framework dependencies  
> </li>  
> </ul>  
>   
> <p>HTML-first, static-first, and boring in all the right ways.</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   Final Thoughts  
> </h2>  
>   
> <p>If you like Eleventy but don’t want to rebuild SEO infrastructure for every project, Quesby gives you:</p>  
>   
> <ul>  
> <li>strong defaults  
> </li>  
> <li>static, predictable output  
> </li>  
> <li>extensibility when needed  
> </li>  
> <li>zero client-side bloat  
> </li>  
> </ul>  
>   
> <p>It’s a lightweight alternative to plugin-heavy or framework-heavy approaches, built specifically for people who prefer clean HTML and simple pipelines.</p>  
>   
> <p>More info:<br><br>  
> <a href="https://quesby.dev">https://quesby.dev</a></p>

---

## [3/10] Authenticating Users in Astro & React Apps with Better Auth
**Source:** The Practical Developer | **Date:** 2025-12-02T12:40:13.000Z
**URL:** https://dev.to/isnan__h/authenticating-users-in-astro-react-apps-with-better-auth-3loe
**Reasoning:** Tutorial on authentication, not relevant to our primary or secondary interests.
**Authors:** ishan

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fvea9213biytatuykfyr6.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>This post was originally published in <a href="https://www.ishan-manandhar.com.np/">my website here</a>.</p>  
>   
> <p>Authentication is a critical foundation for every contemporary web application, but establishing it is often one of the most significant, time-intensive hurdles developers encounter. The solution? Introducing Better Auth API: a framework-independent authentication service designed to revolutionize user management implementation. This detailed tutorial will guide you through creating a fully functional, full-stack application, showcasing the efficiency and ease of Better Auth, powered by Astro for optimal performance.</p>  
>   
> <h3>  
>     
>     
>   Why Better Auth?  
> </h3>  
>   
> <p>Traditional auth solutions often require extensive boilerplate code and complex configuration. Better Auth offers:</p>  
>   
> <ul>  
> <li>Zero boilerplate setup</li>  
> <li>Type-safe APIs with full TypeScript support</li>  
> <li>Multiple provider support (email/password, OAuth, passkeys)</li>  
> <li>Framework-agnostic design that works with Astro, React, and beyond</li>  
> </ul>  
>   
> <p>Better Auth automatically handles Astro's unique server-client boundary, providing authentication that works across both environments. Better Auth provides a unified API that adapts to your stack—whether you're using React, Vue, Svelte, or vanilla JavaScript on the frontend, and Node.js, Bun, Deno or any framekworks on javascript ecosystem on the backend. </p>  
>   
> <h3>  
>     
>     
>   Setting up Astro  
> </h3>  
>   
> <p>Astro's hybrid architecture (server and client components) works seamlessly with Better Auth. Better Auth requires a database to persist user and session data and leverages Server-Side Rendering (SSR) to manage sessions via cookies.</p>  
>   
> <p>🛠️ Prerequisites</p>  
>   
> <ul>  
> <li>A new Astro project (e.g., initialized with pnpm create astro@latest).</li>  
> <li>A database (e.g., PostgreSQL or MongoDB) configured and ready.</li>  
> <li>An SSR adapter for Astro (e.g., Cloudflare, Vercel, Netlify, or Node) added to your project.  
> </li>  
> </ul>  
>   
> <div>  
> <pre><code>pnpm create astro@latest  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <h3>  
>     
>     
>   Create a Neon (Serverless Postgres) app  
> </h3>  
>   
> <p>If you do not have one already, create a Neon (Serverless Postgres) project. </p>  
>   
> <p>Save your connection details including your password. Here is the steps involved:</p>  
>   
> <ul>  
> <li>Navigate to the Projects page in the (Neon Console)[<a href="https://neon.com/">https://neon.com/</a>].</li>  
> <li>Click New Project.</li>  
> <li>Specify your project settings and click Create Project.</li>  
> <li>Create a .env file in the root directory of your Astro project, and define a DATABASE_URL key with the connection string obtained as the value.</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   Adding dependencies  
> </h3>  
>   
>   
>   
> <div>  
> <pre><code>pnpm i better-auth pg @types/pg @astrojs/cloudflare @astrojs/react @tailwindcss/vite react tailwindcss wrangler  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <ul>  
> <li>better-auth — Authentication toolkit providing server/client auth APIs for modern web apps.</li>  
> <li>pg — PostgreSQL client for Node.js used to connect and query Postgres databases.</li>  
> <li>@types/pg — TypeScript type definitions for the pg (PostgreSQL) library.</li>  
> <li>@astrojs/cloudflare — Astro adapter for deploying your Astro app to Cloudflare Workers.</li>  
> <li>@astrojs/react — Enables using React components inside an Astro project.</li>  
> <li>  
> <a href="https://dev.to/tailwindcss">@tailwindcss</a>/vite — Integrates Tailwind CSS directly into Astro/Vite for fast builds and hot reload.</li>  
> <li>react — UI library for building interactive components inside Astro.</li>  
> <li>tailwindcss — Utility-first CSS framework used for styling your Astro + React project.</li>  
> <li>wrangler — Cloudflare’s CLI tool for building and deploying Workers and Worker-based apps.</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   Adding environment variables  
> </h3>  
>   
>   
>   
> <div>  
> <pre><code><span>BETTER_AUTH_SECRET</span><span>=</span><span>"a-32-character-secret"</span>  
> <span>DATABASE_URL</span><span>=</span><span>"your database connection string"</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>You can generate a secret with the code snippet below<br>  
> </p>  
>   
> <div>  
> <pre><code><span>node</span> <span>-</span><span>e</span> <span>"</span><span>console.log(require('crypto').randomBytes(32).toString('base64'))</span><span>"</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Astro gives you access to Vite’s built-in environment variables support and includes some default environment variables for your project that allow you to access configuration values for your current project.</p>  
>   
> <h3>  
>     
>     
>   Adding better-auth  
> </h3>  
>   
> <p>Better Auth comes with first class support for Astro.</p>  
>   
> <p>We will be creating two utility files, src/auth.ts and src/auth-client.ts to access Better Auth on the server side and client side, respectively.</p>  
>   
> <p>In the src/auth-client.ts (code below), we are instantiating a new better-auth instance to be used in the client side interactions.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>//src/auth-client.ts</span>  
> <span>import</span> <span>{</span> <span>createAuthClient</span> <span>}</span> <span>from</span> <span>"</span><span>better-auth/react</span><span>"</span>  
> <span>export</span> <span>const</span> <span>{</span> <span>useSession</span><span>,</span> <span>authClient</span> <span>}</span> <span>=</span> <span>createAuthClient</span><span>()</span>   
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>In src/auth.ts (code below), you are going to use a Pool to connect to your Postgres instance to persist the sessions and user object in the database, and enable email and password authentication. Astro uses Vite’s built-in support for environment variables, which are statically replaced at build time, and lets you use any of its methods to work with them.</p>  
>   
> <p>We use import.meta.env.DATABASE_URL to access environment variables inside Astro.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>//src/auth.ts</span>  
> <span>import</span> <span>pkg</span> <span>from</span> <span>'</span><span>pg</span><span>'</span>  
> <span>import</span> <span>{</span> <span>betterAuth</span> <span>}</span> <span>from</span> <span>'</span><span>better-auth</span><span>'</span>  
>   
> <span>const</span> <span>{</span> <span>Pool</span> <span>}</span> <span>=</span> <span>pkg</span>  
>   
> <span>export</span> <span>const</span> <span>auth</span> <span>=</span> <span>betterAuth</span><span>({</span>  
>   <span>emailAndPassword</span><span>:</span> <span>{</span> <span>enabled</span><span>:</span> <span>true</span> <span>},</span>  
>   <span>database</span><span>:</span> <span>new</span> <span>Pool</span><span>({</span> <span>connectionString</span><span>:</span> <span>import</span><span>.</span><span>meta</span><span>.</span><span>env</span><span>.</span><span>DATABASE_URL</span> <span>})</span>  
> <span>})</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <h3>  
>     
>     
>   Generating schema and API routes  
> </h3>  
>   
> <p>To create the schema per our configuration defined in src/auth.ts file in your database automatically, execute the command below:<br>  
> </p>  
>   
> <div>  
> <pre><code>npx @better-auth/cli migrate  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Let's define an API route to allow you to authenticate users.</p>  
>   
> <p>Better Auth does the heavy lifting of creating and managing the logic of validating credentials, creating (or updating) the user and relevant session objects in the database. You just need to create a catch-all api route in your Astro project as follows in the src/pages/api/auth/[...all].ts file:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>import</span> <span>{</span> <span>auth</span> <span>}</span> <span>from</span> <span>'</span><span>@/auth</span><span>'</span>  
> <span>import</span> <span>type</span> <span>{</span> <span>APIRoute</span> <span>}</span> <span>from</span> <span>'</span><span>astro</span><span>'</span>  
>   
> <span>export</span> <span>const</span> <span>ALL</span><span>:</span> <span>APIRoute</span> <span>=</span> <span>async </span><span>(</span><span>ctx</span><span>)</span> <span>=&gt;</span> <span>{</span>  
>   <span>return</span> <span>auth</span><span>.</span><span>handler</span><span>(</span><span>ctx</span><span>.</span><span>request</span><span>)</span>  
> <span>}</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <h3>  
>     
>     
>   Intercept all incoming requests using Astro middleware  
> </h3>  
>   
> <p>To make sure that each request maintains a user and session information is accessible over the server-side endpoints, and in .astro pages during server-side rendering, you are going create a middleware that uses Better Auth to decode/encode a user session from the cookie.</p>  
>   
> <p>Create a file middleware.ts in the src directory with the following code:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>import</span> <span>{</span> <span>auth</span> <span>}</span> <span>from</span> <span>'</span><span>@/auth</span><span>'</span>  
> <span>import</span> <span>{</span> <span>defineMiddleware</span> <span>}</span> <span>from</span> <span>'</span><span>astro:middleware</span><span>'</span>  
>   
> <span>export</span> <span>const</span> <span>onRequest</span> <span>=</span> <span>defineMiddleware</span><span>(</span><span>async </span><span>(</span><span>context</span><span>,</span> <span>next</span><span>)</span> <span>=&gt;</span> <span>{</span>  
>   <span>const</span> <span>isAuthed</span> <span>=</span> <span>await</span> <span>auth</span><span>.</span><span>api</span><span>.</span><span>getSession</span><span>({</span>  
>     <span>headers</span><span>:</span> <span>context</span><span>.</span><span>request</span><span>.</span><span>headers</span><span>,</span>  
>   <span>})</span>  
>   <span>if </span><span>(</span><span>isAuthed</span><span>)</span> <span>{</span>  
>     <span>context</span><span>.</span><span>locals</span><span>.</span><span>user</span> <span>=</span> <span>isAuthed</span><span>.</span><span>user</span>  
>     <span>context</span><span>.</span><span>locals</span><span>.</span><span>session</span> <span>=</span> <span>isAuthed</span><span>.</span><span>session</span>  
>   <span>}</span> <span>else</span> <span>{</span>  
>     <span>context</span><span>.</span><span>locals</span><span>.</span><span>user</span> <span>=</span> <span>null</span>  
>     <span>context</span><span>.</span><span>locals</span><span>.</span><span>session</span> <span>=</span> <span>null</span>  
>   <span>}</span>  
>   <span>return</span> <span>next</span><span>()</span>  
> <span>})</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>This will make sure to mark both user and session as null by default, and assign the respective values obtained from the database using the relevant cookies from the request. This allows you to always know the correct state of user authentication.</p>  
>   
> <h3>  
>     
>     
>   Creating Index Page  
> </h3>  
>   
>   
>   
> <div>  
> <pre><code><span>---</span>  
> <span>import</span> <span>"</span><span>@/styles/global.css</span><span>"</span><span>;</span>  
> <span>import</span> <span>DashboardPage</span> <span>from</span> <span>"</span><span>@/components/Dashboard</span><span>"</span><span>;</span>  
> <span>import</span> <span>Layout</span> <span>from</span> <span>"</span><span>@/layouts/Layout.astro</span><span>"</span><span>;</span>  
>   
> <span>if </span><span>(</span><span>!</span><span>Astro</span><span>.</span><span>locals</span><span>.</span><span>user</span><span>?.</span><span>id</span><span>)</span> <span>return</span> <span>Astro</span><span>.</span><span>redirect</span><span>(</span><span>"</span><span>/signin</span><span>"</span><span>);</span>  
> <span>---</span>  
>   
> <span>&lt;</span><span>html</span> <span>lang</span><span>=</span><span>"</span><span>en</span><span>"</span><span>&gt;</span>  
>   <span>&lt;</span><span>head</span><span>&gt;</span>  
>     <span>&lt;</span><span>meta</span> <span>charset</span><span>=</span><span>"</span><span>utf-8</span><span>"</span> <span>/&gt;</span>  
>     <span>&lt;</span><span>meta</span> <span>name</span><span>=</span><span>"</span><span>viewport</span><span>"</span> <span>content</span><span>=</span><span>"</span><span>width=device-width</span><span>"</span> <span>/&gt;</span>  
>   <span>&lt;</span><span>/head</span><span>&gt;  
> </span>  <span>&lt;</span><span>body</span> <span>class</span><span>=</span><span>"</span><span>container mx-auto p-4</span><span>"</span><span>&gt;</span>  
>     <span>&lt;</span><span>Layout</span><span>&gt;</span>  
>       <span>&lt;</span><span>DashboardPage</span> <span>client</span><span>:</span><span>load</span> <span>/&gt;</span>  
>       <span>&lt;</span><span>Layout</span> <span>/&gt;</span>  
>     <span>&lt;</span><span>/Layout</span><span>&gt;  
> </span>  <span>&lt;</span><span>/body</span><span>&gt;  
> </span><span>&lt;</span><span>/html</span><span>&gt;  
> </span></code></pre>  
>   
> </div>  
>   
>   
>   
> <p>This Redirects user to the sign in page if the user is not authenticated.<br>  
> Shows the information stored in the user object pertaining to the authenticated user.</p>  
>   
> <p>Let's create a react component for our dashboard page.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>import</span> <span>{</span> <span>useEffect</span> <span>}</span> <span>from</span> <span>"</span><span>react</span><span>"</span><span>;</span>  
> <span>import</span> <span>{</span> <span>authClient</span><span>,</span> <span>useSession</span> <span>}</span> <span>from</span> <span>"</span><span>@/auth-client</span><span>"</span><span>;</span>  
> <span>import</span> <span>"</span><span>@/styles/global.css</span><span>"</span><span>;</span>  
>   
> <span>export</span> <span>default</span> <span>function</span> <span>DashboardPage</span><span>()</span> <span>{</span>  
>     <span>const</span> <span>{</span>  
>         <span>data</span><span>:</span> <span>session</span><span>,</span>  
>         <span>isPending</span><span>,</span>   
>         <span>error</span><span>,</span>   
>         <span>refetch</span>   
>     <span>}</span> <span>=</span> <span>useSession</span><span>()</span>  
>   
>   <span>useEffect</span><span>(()</span> <span>=&gt;</span> <span>{</span>  
>     <span>// If not logged in → redirect to signin</span>  
>     <span>if </span><span>(</span><span>!</span><span>isPending</span> <span>&amp;&amp;</span> <span>!</span><span>session</span><span>?.</span><span>user</span><span>?.</span><span>id</span><span>)</span> <span>{</span>  
>       <span>window</span><span>.</span><span>location</span><span>.</span><span>href</span><span>=</span><span>'</span><span>/</span><span>'</span>  
>     <span>}</span>  
>   <span>},</span> <span>[</span><span>session</span><span>,</span> <span>isPending</span><span>]);</span>  
>   
>   <span>const</span> <span>handleSignOut</span> <span>=</span> <span>async </span><span>():</span> <span>Promise</span><span>&lt;</span><span>void</span><span>&gt;</span> <span>=&gt;</span> <span>{</span>  
>     <span>await</span> <span>authClient</span><span>.</span><span>signOut</span><span>();</span>  
>       <span>window</span><span>.</span><span>location</span><span>.</span><span>href</span> <span>=</span> <span>'</span><span>/signin</span><span>'</span>  
>   <span>};</span>  
>   
>   <span>if </span><span>(</span><span>isPending</span><span>)</span> <span>{</span>  
>     <span>return</span> <span>&lt;</span><span>p</span><span>&gt;</span><span>Loading</span><span>...</span><span>&lt;</span><span>/p&gt;</span><span>;  
> </span>  <span>}</span>  
>   
>   <span>if </span><span>(</span><span>!</span><span>session</span><span>?.</span><span>user</span><span>)</span> <span>{</span>  
>     <span>return</span> <span>null</span><span>;</span>   
>   <span>}</span>  
>   
>   <span>return </span><span>(</span>  
>     <span>&lt;</span><span>div</span> <span>className</span><span>=</span><span>"</span><span>container mx-auto py-4</span><span>"</span><span>&gt;</span>  
>       <span>&lt;</span><span>h1</span> <span>className</span><span>=</span><span>"</span><span>font-bold text-xl</span><span>"</span><span>&gt;</span>  
>         <span>Hello</span> <span>{</span><span>session</span><span>.</span><span>user</span><span>.</span><span>name</span> <span>??</span> <span>"</span><span>User</span><span>"</span><span>}</span>  
>       <span>&lt;</span><span>/h1</span><span>&gt;  
> </span>  
>       <span>&lt;</span><span>h2</span> <span>className</span><span>=</span><span>"</span><span>text-green-500 font-bold text-3xl</span><span>"</span><span>&gt;</span>  
>         <span>You</span><span>&amp;</span><span>apos</span><span>;</span><span>ve</span> <span>authenticated</span> <span>successfully</span><span>!</span>  
>       <span>&lt;</span><span>/h2</span><span>&gt;  
> </span>  
>       <span>&lt;</span><span>pre</span> <span>className</span><span>=</span><span>"</span><span>mt-4 bg-gray-100 p-4 rounded text-sm</span><span>"</span><span>&gt;</span>  
>         <span>{</span><span>JSON</span><span>.</span><span>stringify</span><span>(</span><span>session</span><span>.</span><span>user</span><span>,</span> <span>null</span><span>,</span> <span>2</span><span>)}</span>  
>       <span>&lt;</span><span>/pre</span><span>&gt;  
> </span>  
>       <span>&lt;</span><span>button</span>  
>         <span>onClick</span><span>=</span><span>{</span><span>handleSignOut</span><span>}</span>  
>         <span>className</span><span>=</span><span>"</span><span>mt-4 px-4 py-2 bg-red-600 text-white rounded</span><span>"</span>  
>       <span>&gt;</span>  
>         <span>Sign</span> <span>Out</span>  
>       <span>&lt;</span><span>/button</span><span>&gt;  
> </span>    <span>&lt;</span><span>/div</span><span>&gt;  
> </span>  <span>);</span>  
> <span>}</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <h3>  
>     
>     
>   Creating Signup  
> </h3>  
>   
> <p>We created a signup page inside the src/pages and imported a React component and used a client:load directive so its hydrated in client. This helps to load and hydrate the component JavaScript immediately on page load.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>---</span>  
> <span>import</span> <span>SignupForm</span> <span>from</span> <span>'</span><span>@/components/SignupForm</span><span>'</span>   
> <span>import</span> <span>Layout</span> <span>from</span> <span>'</span><span>@/layouts/Layout.astro</span><span>'</span>  
> <span>---</span>  
>   
> <span>&lt;</span><span>html</span> <span>lang</span><span>=</span><span>"</span><span>en</span><span>"</span><span>&gt;</span>  
>   <span>&lt;</span><span>head</span><span>&gt;</span>  
>     <span>&lt;</span><span>meta</span> <span>charset</span><span>=</span><span>"</span><span>utf-8</span><span>"</span> <span>/&gt;</span>  
>     <span>&lt;</span><span>meta</span> <span>name</span><span>=</span><span>"</span><span>viewport</span><span>"</span> <span>content</span><span>=</span><span>"</span><span>width=device-width</span><span>"</span> <span>/&gt;</span>  
>   <span>&lt;</span><span>/head</span><span>&gt;  
> </span>  <span>&lt;</span><span>body</span><span>&gt;</span>  
>     <span>&lt;</span><span>Layout</span><span>&gt;</span>  
>       <span>&lt;</span><span>SignupForm</span> <span>client</span><span>:</span><span>load</span><span>/&gt;</span>  
>     <span>&lt;</span><span>Layout</span> <span>/&gt;</span>  
>   <span>&lt;</span><span>/body</span><span>&gt;  
> </span><span>&lt;</span><span>/html</span><span>&gt;  
> </span></code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Also we will make a signup form component which will be a simple form to create a user inside of our Neon database.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>import</span> <span>React</span><span>,</span> <span>{</span> <span>useState</span> <span>}</span> <span>from</span> <span>"</span><span>react</span><span>"</span><span>;</span>  
> <span>import</span> <span>type</span> <span>{</span> <span>FormEvent</span> <span>}</span> <span>from</span> <span>"</span><span>react</span><span>"</span><span>;</span>  
> <span>import</span> <span>{</span> <span>authClient</span> <span>}</span> <span>from</span> <span>"</span><span>@/auth-client</span><span>"</span><span>;</span>  
>   
> <span>export</span> <span>default</span> <span>function</span> <span>SignupPage</span><span>():</span> <span>React</span><span>.</span><span>JSX</span><span>.</span><span>Element</span> <span>{</span>  
>   <span>const</span> <span>[</span><span>isLoading</span><span>,</span> <span>setIsLoading</span><span>]</span> <span>=</span> <span>useState</span><span>&lt;</span><span>boolean</span><span>&gt;</span><span>(</span><span>false</span><span>);</span>  
>   <span>const</span> <span>[</span><span>error</span><span>,</span> <span>setError</span><span>]</span> <span>=</span> <span>useState</span><span>&lt;</span><span>string</span><span>&gt;</span><span>(</span><span>""</span><span>);</span>  
>   
>   <span>const</span> <span>handleSubmit</span> <span>=</span> <span>async </span><span>(</span><span>e</span><span>:</span> <span>FormEvent</span><span>&lt;</span><span>HTMLFormElement</span><span>&gt;</span><span>):</span> <span>Promise</span><span>&lt;</span><span>void</span><span>&gt;</span> <span>=&gt;</span> <span>{</span>  
>     <span>e</span><span>.</span><span>preventDefault</span><span>();</span>  
>     <span>setIsLoading</span><span>(</span><span>true</span><span>);</span>  
>     <span>setError</span><span>(</span><span>""</span><span>);</span>  
>   
>     <span>const</span> <span>formData</span> <span>=</span> <span>new</span> <span>FormData</span><span>(</span><span>e</span><span>.</span><span>currentTarget</span><span>);</span>  
>   
>     <span>const</span> <span>name</span> <span>=</span> <span>(</span><span>formData</span><span>.</span><span>get</span><span>(</span><span>"</span><span>name</span><span>"</span><span>)</span> <span>as</span> <span>string</span><span>)</span> <span>??</span> <span>""</span><span>;</span>  
>     <span>const</span> <span>email</span> <span>=</span> <span>(</span><span>formData</span><span>.</span><span>get</span><span>(</span><span>"</span><span>email</span><span>"</span><span>)</span> <span>as</span> <span>string</span><span>)</span> <span>??</span> <span>""</span><span>;</span>  
>     <span>const</span> <span>password</span> <span>=</span> <span>(</span><span>formData</span><span>.</span><span>get</span><span>(</span><span>"</span><span>password</span><span>"</span><span>)</span> <span>as</span> <span>string</span><span>)</span> <span>??</span> <span>""</span><span>;</span>  
>   
>     <span>const</span> <span>response</span> <span>=</span> <span>await</span> <span>authClient</span><span>.</span><span>signUp</span><span>.</span><span>email</span><span>({</span>  
>       <span>name</span><span>,</span>  
>       <span>email</span><span>,</span>  
>       <span>password</span><span>,</span>  
>     <span>});</span>  
>   
>     <span>if </span><span>(</span><span>!</span><span>response</span><span>.</span><span>error</span><span>)</span> <span>{</span>  
>       <span>window</span><span>.</span><span>location</span><span>.</span><span>href</span> <span>=</span> <span>'</span><span>/</span><span>'</span>  
>     <span>}</span> <span>else</span> <span>{</span>  
>       <span>setError</span><span>(</span><span>response</span><span>.</span><span>error</span><span>.</span><span>message</span> <span>??</span> <span>"</span><span>Signup failed</span><span>"</span><span>);</span>  
>     <span>}</span>  
>   
>     <span>setIsLoading</span><span>(</span><span>false</span><span>);</span>  
>   <span>};</span>  
>   
>   <span>return </span><span>(</span>  
>     <span>&lt;</span><span>div</span> <span>className</span><span>=</span><span>"</span><span>min-h-screen flex flex-col items-center justify-center p-4</span><span>"</span><span>&gt;</span>  
>       <span>&lt;</span><span>h1</span> <span>className</span><span>=</span><span>"</span><span>font-bold text-3xl py-8</span><span>"</span><span>&gt;</span>  
>         <span>Sign</span> <span>up</span>  
>       <span>&lt;</span><span>/h1</span><span>&gt;  
> </span>      <span>&lt;</span><span>form</span>  
>         <span>onSubmit</span><span>=</span><span>{</span><span>handleSubmit</span><span>}</span>  
>         <span>className</span><span>=</span><span>"</span><span>flex flex-col gap-3 w-full max-w-sm</span><span>"</span>  
>       <span>&gt;</span>  
>         <span>&lt;</span><span>input</span>  
>           <span>required</span>  
>           <span>type</span><span>=</span><span>"</span><span>text</span><span>"</span>  
>           <span>name</span><span>=</span><span>"</span><span>name</span><span>"</span>  
>           <span>placeholder</span><span>=</span><span>"</span><span>Name</span><span>"</span>  
>           <span>className</span><span>=</span><span>"</span><span>border p-2 rounded</span><span>"</span>  
>         <span>/&gt;</span>  
>   
>         <span>&lt;</span><span>input</span>  
>           <span>required</span>  
>           <span>type</span><span>=</span><span>"</span><span>email</span><span>"</span>  
>           <span>name</span><span>=</span><span>"</span><span>email</span><span>"</span>  
>           <span>placeholder</span><span>=</span><span>"</span><span>Email</span><span>"</span>  
>           <span>className</span><span>=</span><span>"</span><span>border p-2 rounded</span><span>"</span>  
>         <span>/&gt;</span>  
>   
>         <span>&lt;</span><span>input</span>  
>           <span>required</span>  
>           <span>type</span><span>=</span><span>"</span><span>password</span><span>"</span>  
>           <span>name</span><span>=</span><span>"</span><span>password</span><span>"</span>  
>           <span>placeholder</span><span>=</span><span>"</span><span>Password</span><span>"</span>  
>           <span>className</span><span>=</span><span>"</span><span>border p-2 rounded</span><span>"</span>  
>         <span>/&gt;</span>  
>   
>         <span>&lt;</span><span>button</span>  
>           <span>type</span><span>=</span><span>"</span><span>submit</span><span>"</span>  
>           <span>disabled</span><span>=</span><span>{</span><span>isLoading</span><span>}</span>  
>           <span>className</span><span>=</span><span>"</span><span>p-2 bg-blue-600 text-white rounded</span><span>"</span>  
>         <span>&gt;</span>  
>           <span>{</span><span>isLoading</span> <span>?</span> <span>"</span><span>Signing up...</span><span>"</span> <span>:</span> <span>"</span><span>Sign up</span><span>"</span><span>}</span>  
>         <span>&lt;</span><span>/button</span><span>&gt;  
> </span>  
>         <span>{</span><span>error</span> <span>&amp;&amp;</span> <span>&lt;</span><span>p</span> <span>className</span><span>=</span><span>"</span><span>text-red-500 text-sm</span><span>"</span><span>&gt;</span><span>{</span><span>error</span><span>}</span><span>&lt;</span><span>/p&gt;</span><span>}  
> </span>      <span>&lt;</span><span>/form</span><span>&gt;  
> </span>  
>       <span>&lt;</span><span>p</span> <span>className</span><span>=</span><span>"</span><span>mt-4</span><span>"</span><span>&gt;</span>  
>         <span>Already</span> <span>have</span> <span>an</span> <span>account</span><span>?{</span><span>"</span><span> </span><span>"</span><span>}</span>  
>         <span>&lt;</span><span>a</span> <span>href</span><span>=</span><span>"</span><span>/signin</span><span>"</span> <span>className</span><span>=</span><span>"</span><span>underline</span><span>"</span><span>&gt;</span>  
>           <span>Sign</span> <span>in</span>  
>         <span>&lt;</span><span>/a</span><span>&gt;  
> </span>      <span>&lt;</span><span>/p</span><span>&gt;  
> </span>    <span>&lt;</span><span>/div</span><span>&gt;  
> </span>  <span>);</span>  
> <span>}</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <h3>  
>     
>     
>   Creating Signin  
> </h3>  
>   
> <p>Let's also create a signin page so that we can verify user exists in our DB.</p>  
>   
> <p>In the frontmatter we check if user is available we redirect to main route.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>---</span>  
> <span>import</span> <span>Layout</span> <span>from</span> <span>"</span><span>@/layouts/Layout.astro</span><span>"</span>  
> <span>import</span> <span>SigninForm</span> <span>from</span> <span>"</span><span>@/components/SigninForm</span><span>"</span>  
>   
> <span>if </span><span>(</span><span>Astro</span><span>.</span><span>locals</span><span>.</span><span>user</span><span>?.</span><span>id</span><span>)</span> <span>return</span> <span>Astro</span><span>.</span><span>redirect</span><span>(</span><span>'</span><span>/</span><span>'</span><span>)</span>  
> <span>---</span>  
>   
> <span>&lt;</span><span>html</span> <span>lang</span><span>=</span><span>"</span><span>en</span><span>"</span><span>&gt;</span>  
>   <span>&lt;</span><span>head</span><span>&gt;</span>  
>     <span>&lt;</span><span>meta</span> <span>charset</span><span>=</span><span>"</span><span>utf-8</span><span>"</span> <span>/&gt;</span>  
>     <span>&lt;</span><span>meta</span> <span>name</span><span>=</span><span>"</span><span>viewport</span><span>"</span> <span>content</span><span>=</span><span>"</span><span>width=device-width</span><span>"</span> <span>/&gt;</span>  
>   <span>&lt;</span><span>/head</span><span>&gt;  
> </span>  <span>&lt;</span><span>body</span><span>&gt;</span>  
>     <span>&lt;</span><span>Layout</span><span>&gt;</span>  
>       <span>&lt;</span><span>SigninForm</span> <span>client</span><span>:</span><span>load</span><span>/&gt;</span>  
>     <span>&lt;</span><span>Layout</span> <span>/&gt;</span>  
>   <span>&lt;</span><span>/body</span><span>&gt;  
> </span><span>&lt;</span><span>/html</span><span>&gt;  
> </span></code></pre>  
>   
> </div>  
>   
>   
>   
> <p>Same like before we will create a form submission so that we can login the user and redirect to our desired route.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>import</span> <span>type</span> <span>{</span> <span>FormEvent</span> <span>}</span> <span>from</span> <span>"</span><span>react</span><span>"</span><span>;</span>  
> <span>import</span> <span>{</span> <span>useState</span> <span>}</span> <span>from</span> <span>"</span><span>react</span><span>"</span><span>;</span>  
> <span>import</span> <span>{</span> <span>authClient</span> <span>}</span> <span>from</span> <span>"</span><span>@/auth-client</span><span>"</span><span>;</span>  
>   
> <span>export</span> <span>default</span> <span>function</span> <span>SigninPage</span><span>()</span> <span>{</span>  
>   <span>const</span> <span>[</span><span>isLoading</span><span>,</span> <span>setIsLoading</span><span>]</span> <span>=</span> <span>useState</span><span>&lt;</span><span>boolean</span><span>&gt;</span><span>(</span><span>false</span><span>);</span>  
>   <span>const</span> <span>[</span><span>error</span><span>,</span> <span>setError</span><span>]</span> <span>=</span> <span>useState</span><span>&lt;</span><span>string</span><span>&gt;</span><span>(</span><span>""</span><span>);</span>  
>   
>   <span>const</span> <span>handleSubmit</span> <span>=</span> <span>async </span><span>(</span><span>e</span><span>:</span> <span>FormEvent</span><span>&lt;</span><span>HTMLFormElement</span><span>&gt;</span><span>):</span> <span>Promise</span><span>&lt;</span><span>void</span><span>&gt;</span> <span>=&gt;</span> <span>{</span>  
>     <span>e</span><span>.</span><span>preventDefault</span><span>();</span>  
>     <span>setIsLoading</span><span>(</span><span>true</span><span>);</span>  
>     <span>setError</span><span>(</span><span>""</span><span>);</span>  
>   
>     <span>const</span> <span>formData</span> <span>=</span> <span>new</span> <span>FormData</span><span>(</span><span>e</span><span>.</span><span>currentTarget</span><span>);</span>  
>   
>     <span>const</span> <span>email</span> <span>=</span> <span>(</span><span>formData</span><span>.</span><span>get</span><span>(</span><span>"</span><span>email</span><span>"</span><span>)</span> <span>as</span> <span>string</span><span>)</span> <span>??</span> <span>""</span><span>;</span>  
>     <span>const</span> <span>password</span> <span>=</span> <span>(</span><span>formData</span><span>.</span><span>get</span><span>(</span><span>"</span><span>password</span><span>"</span><span>)</span> <span>as</span> <span>string</span><span>)</span> <span>??</span> <span>""</span><span>;</span>  
>   
>     <span>const</span> <span>response</span> <span>=</span> <span>await</span> <span>authClient</span><span>.</span><span>signIn</span><span>.</span><span>email</span><span>({</span>  
>       <span>email</span><span>,</span>  
>       <span>password</span><span>,</span>  
>     <span>});</span>  
>   
>     <span>if </span><span>(</span><span>!</span><span>response</span><span>.</span><span>error</span><span>)</span> <span>{</span>  
>       <span>window</span><span>.</span><span>location</span><span>.</span><span>href</span> <span>=</span> <span>'</span><span>/</span><span>'</span><span>;</span>  
>     <span>}</span> <span>else</span> <span>{</span>  
>       <span>setError</span><span>(</span><span>response</span><span>.</span><span>error</span><span>.</span><span>message</span> <span>??</span> <span>"</span><span>Sign-in failed</span><span>"</span><span>);</span>  
>     <span>}</span>  
>   
>     <span>setIsLoading</span><span>(</span><span>false</span><span>);</span>  
>   <span>};</span>  
>   
>   <span>return </span><span>(</span>  
>     <span>&lt;</span><span>div</span> <span>className</span><span>=</span><span>"</span><span>min-h-screen flex flex-col items-center justify-center p-4</span><span>"</span><span>&gt;</span>  
>       <span>&lt;</span><span>h1</span> <span>className</span><span>=</span><span>"</span><span>font-bold text-3xl py-8</span><span>"</span><span>&gt;</span>  
>         <span>Sign</span> <span>in</span>  
>       <span>&lt;</span><span>/h1</span><span>&gt;  
> </span>      <span>&lt;</span><span>form</span>  
>         <span>onSubmit</span><span>=</span><span>{</span><span>handleSubmit</span><span>}</span>  
>         <span>className</span><span>=</span><span>"</span><span>flex flex-col gap-3 w-full max-w-sm</span><span>"</span>  
>       <span>&gt;</span>  
>         <span>&lt;</span><span>input</span>  
>           <span>required</span>  
>           <span>type</span><span>=</span><span>"</span><span>email</span><span>"</span>  
>           <span>name</span><span>=</span><span>"</span><span>email</span><span>"</span>  
>           <span>placeholder</span><span>=</span><span>"</span><span>Email</span><span>"</span>  
>           <span>className</span><span>=</span><span>"</span><span>border p-2 rounded</span><span>"</span>  
>         <span>/&gt;</span>  
>   
>         <span>&lt;</span><span>input</span>  
>           <span>required</span>  
>           <span>type</span><span>=</span><span>"</span><span>password</span><span>"</span>  
>           <span>name</span><span>=</span><span>"</span><span>password</span><span>"</span>  
>           <span>placeholder</span><span>=</span><span>"</span><span>Password</span><span>"</span>  
>           <span>className</span><span>=</span><span>"</span><span>border p-2 rounded</span><span>"</span>  
>         <span>/&gt;</span>  
>   
>         <span>&lt;</span><span>button</span>  
>           <span>type</span><span>=</span><span>"</span><span>submit</span><span>"</span>  
>           <span>disabled</span><span>=</span><span>{</span><span>isLoading</span><span>}</span>  
>           <span>className</span><span>=</span><span>"</span><span>p-2 bg-blue-600 text-white rounded</span><span>"</span>  
>         <span>&gt;</span>  
>           <span>{</span><span>isLoading</span> <span>?</span> <span>"</span><span>Signing in...</span><span>"</span> <span>:</span> <span>"</span><span>Sign In</span><span>"</span><span>}</span>  
>         <span>&lt;</span><span>/button</span><span>&gt;  
> </span>  
>         <span>{</span><span>error</span> <span>&amp;&amp;</span> <span>&lt;</span><span>p</span> <span>className</span><span>=</span><span>"</span><span>text-red-500 text-sm</span><span>"</span><span>&gt;</span><span>{</span><span>error</span><span>}</span><span>&lt;</span><span>/p&gt;</span><span>}  
> </span>      <span>&lt;</span><span>/form</span><span>&gt;  
> </span>  
>       <span>&lt;</span><span>p</span> <span>className</span><span>=</span><span>"</span><span>mt-4</span><span>"</span><span>&gt;</span>  
>         <span>Don</span><span>&amp;</span><span>apos</span><span>;</span><span>t</span> <span>have</span> <span>an</span> <span>account</span><span>?{</span><span>"</span><span> </span><span>"</span><span>}</span>  
>         <span>&lt;</span><span>a</span> <span>href</span><span>=</span><span>"</span><span>/signup</span><span>"</span> <span>className</span><span>=</span><span>"</span><span>underline</span><span>"</span><span>&gt;</span>  
>           <span>Sign</span> <span>up</span>  
>         <span>&lt;</span><span>/a</span><span>&gt;  
> </span>      <span>&lt;</span><span>/p</span><span>&gt;  
> </span>    <span>&lt;</span><span>/div</span><span>&gt;  
> </span>  <span>);</span>  
> <span>}</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>We enabled user authentication via credentials method with the help of Better Auth in an Astro application. You have also gained some experience with using middleware in Astro, and understanding how it can help you build dynamic user interfaces.</p>  
>   
> <h3>  
>     
>     
>   Conclusion  
> </h3>  
>   
> <p>Whether we're building a content-focused Astro site or a dynamic React application, Better Auth provides the secure, modern authentication solution you need without the typical complexity.</p>  
>   
> <p>Better Auth's greatest strength might be its developer experience. With automatic API route generation in Astro and React hooks that provide immediate user state, you can implement complete auth flows in minutes rather than days.</p>  
>   
> <p>Have a question, feedback or simply wish to contact me privately? Shoot me a DM and I'll do my best to get back to you.</p>  
>   
> <p>You can find the complete source code in <a href="https://github.com/isNan909/astro-betterauth">Github Link here</a>.</p>  
>   
> <p>Thank you!</p>

---

## [3/10] How Crypto Businesses Can Prepare for MiCA Authorization in the European Union
**Source:** The Practical Developer | **Date:** 2025-12-02T12:15:53.000Z
**URL:** https://dev.to/aleksandr_rozental_1b7caa/how-crypto-businesses-can-prepare-for-mica-authorization-in-the-european-union-3i2h
**Reasoning:** The article is about crypto regulation, which is not relevant to our interests unless it involves infrastructure.
**Authors:** Aleksandr Rozental

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F46cuh99892gycvsewyf1.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>The European regulatory environment for crypto is evolving rapidly, and MiCA is the most significant development shaping how businesses operate across the European Union. Companies launching exchanges, custody solutions, OTC desks, tokenization platforms or payment systems must now comply with a unified regulatory structure that is far more demanding than the previous patchwork of national rules. Preparing for authorization requires a strategic, methodical and well-organized approach that goes far beyond producing a few basic compliance documents.</p>  
>   
> <p>MiCA introduces a new regulatory architecture designed to ensure consumer protection, market integrity and financial stability. It defines a complete framework for authorization, governance, internal controls, AML procedures, ICT security, operational resilience and oversight. Companies entering the EU market must understand that authorization is not a one-time formality but a long-term commitment to maintaining a high degree of organizational discipline.</p>  
>   
> <p>One of the most challenging aspects of preparing for authorization is realizing how interconnected the requirements are. A company cannot simply create a standalone AML policy or a single risk statement. Every procedure must align with operational reality, governance structure, staffing, technology and internal oversight. Regulators expect consistency across all documentation, and even small discrepancies between internal policies can lead to questions or delays. Many applications fail because companies assemble documents from different sources without ensuring they reflect a coherent internal system.</p>  
>   
> <p>Another common barrier arises from governance. MiCA requires a clear organizational structure, well-defined responsibilities, transparent management roles and reliable control functions. Regulators want to see real oversight, not symbolic titles. Senior management must be fit and proper, competent and actively involved in the company’s operations. Decision-making processes need to be documented and supported by actual workflows. Applicants who treat governance as a checkbox exercise often face long regulator feedback cycles.</p>  
>   
> <p>Operational resilience is another crucial area. Companies must demonstrate that they have considered incident response, business continuity, third-party dependencies, cybersecurity, data protection, operational risks and ICT vulnerabilities. Regulatory expectations include the ability to detect, manage and report incidents effectively. If a company relies heavily on outsourced technology, it must show that it remains fully responsible for oversight and control. Regulators are particularly cautious about technology providers located outside the EU or in jurisdictions with weak regulatory safeguards.</p>  
>   
> <p>AML and CTF compliance remains one of the toughest components. Companies must design robust onboarding procedures, transaction monitoring tools, risk scoring models, suspicious activity processes, record-keeping methods and continuous review mechanisms. The Travel Rule must be fully integrated into operational systems. Regulators want proof that a company can detect high-risk behavior and respond appropriately. Many applicants underestimate the technical complexity of AML implementation and rely too heavily on third-party KYC tools without building internal procedures that reflect their business model.</p>  
>   
> <p>ICT and cybersecurity requirements are heavily influenced by the Digital Operational Resilience Act. Although MiCA and DORA are separate frameworks, they interact closely. Applicants must show that their systems are secure, resilient and regularly tested. Documentation must cover access controls, encryption, monitoring, incident detection, penetration testing, data protection and disaster recovery. Companies need to provide evidence that their systems are not only designed securely but are maintained securely through ongoing operational processes.</p>  
>   
> <p>Another area that often leads to rejections is the lack of consistency between business plans and internal controls. If a company claims it will offer a large range of activities but provides minimal staffing or insufficient capital, regulators will immediately question the feasibility of the business model. Financial projections must be realistic, risk-aware and aligned with the resources described in the application. Underestimating capital or staffing needs can significantly delay the authorization process.</p>  
>   
> <p>Despite the complexity of authorization, companies that prepare early and treat the process seriously can navigate it successfully. Clear documentation, consistent internal controls, realistic planning and structured governance significantly increase the chances of approval. It is also essential to maintain open communication with the regulator, respond quickly to feedback and update policies when required. Regulators appreciate transparency and professionalism, and companies that demonstrate commitment tend to complete the process more efficiently.</p>  
>   
> <p>Regulatory advisory firms like <a href="https://licensium.io/">Licensium</a>  help companies manage the entire authorization journey. They assist with governance design, AML documentation, ICT frameworks, risk assessment, internal policies, application preparation and communication with regulators. This level of professional support is often decisive, especially for companies without dedicated compliance teams. By organizing the authorization process from the beginning, firms significantly reduce the risk of delays and accelerate the approval timeline.</p>  
>   
> <p>As the regulatory landscape continues to evolve, businesses that invest in compliance and operational maturity gain long-term advantages. MiCA introduces clear rules that reward stability, transparency and responsible management. Companies that adapt proactively will be better positioned to enter the European market, expand their operations and build trust with users and partners. A strong regulatory foundation is not only a legal requirement but also a strategic asset in a competitive global industry.</p>  
>   
> <p>Preparing for MiCA authorization is a complex yet achievable process. With the right structure, documentation and strategic planning, crypto businesses can meet the new standards and establish themselves as credible, compliant and resilient participants in the European market. The companies that take regulation seriously today will become the leaders of tomorrow’s digital asset ecosystem.</p>

---

## [3/10] Snapshots, Brooms, and Arch Linux Chaos Control
**Source:** The Practical Developer | **Date:** 2025-12-02T12:07:10.000Z
**URL:** https://dev.to/akshay_gupta/snapshots-brooms-and-arch-linux-chaos-control-hie
**Reasoning:** The article is about Arch Linux setup, which is not relevant to our interests.
**Authors:** Akshay Gupta

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frpr1si1ux5whibfov0q7.jpeg" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>Arch gives you the steering wheel <em>and</em> the broom. On my Arch setup, Limine boots a BTRFS root while Snapper snaps a before/after every time I poke <code>pacman</code>. It's gorgeous, rollbacks for days, but the snapshots stack up faster than RGB stickers unless I sweep them out. So I wrote this simple <code>cleanup.sh</code>, my tiny digital janitor.</p>  
>   
> <blockquote>  
> <p>"I use Arch btw."</p>  
> </blockquote>  
>   
> <h2>  
>     
>     
>   Why I even bother  
> </h2>  
>   
> <ul>  
> <li>Pacman + Snapper hooks = instant safety net. If a wile build breaks stuff, I just reboot, pick the previous snapshot in Limine, and carry on.</li>  
> <li>I settled on <a href="https://cachyos.org/">CachyOS</a> and <a href="https://hydeproject.pages.dev/">HyDE</a> on top of it, after trying and experimenting with multiple setups. Now CachyOS loves performance tweaks, which means I love experimenting. Snapshots let me go full mad scientist without sweating the fallout.</li>  
> <li>Doing the cleanup myself keeps that classic Arch vibe: I decide what stays, what goes, and when my SSD gets to breathe.</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   How my cleanup script keeps me sane  
> </h2>  
>   
> <p>You can keep the below script anywhere on your system, and yes, it wants root (<code>sudo ./cleanup.sh [KEEP]</code>). Default keep count is 2, but bump it to whatever number keeps you cozy. The flow:</p>  
>   
> <ul>  
> <li>  
> <strong>Are we legit?</strong>: Bails if you're not root or if <code>btrfs</code> tools are missing.</li>  
> <li>  
> <strong>Roll call</strong>: Lists every <code>/.snapshots/&lt;id&gt;/snapshot</code>, nicely sorted so we know what's oldest.</li>  
> <li>  
> <strong>Pick favourites</strong>: Shows what stays, what goes, and then asks "you sure?" so you don't rage-delete your lifeline.</li>  
> <li>  
> <strong>Yeet + sync</strong>: Deletes the dusty stuff, cleans empty directories, and syncs the filesystem so everything feels fresh.</li>  
> </ul>  
>   
> <p>Slide the whole script into your toolbox:<br>  
> </p>  
>   
> <div>  
> <pre><code><span>#!/usr/bin/env bash</span>  
> <span>set</span> <span>-euo</span> pipefail  
>   
> <span># How many latest snapshots to keep (default: 2)</span>  
> <span>KEEP</span><span>=</span><span>"</span><span>${</span><span>1</span><span>:-</span><span>2</span><span>}</span><span>"</span>  
>   
> <span>if</span> <span>[[</span> <span>"</span><span>$EUID</span><span>"</span> <span>-ne</span> 0 <span>]]</span><span>;</span> <span>then  
>   </span><span>echo</span> <span>"Please run this script as root:"</span>  
>   <span>echo</span> <span>"  sudo </span><span>$0</span><span> [KEEP]"</span>  
>   <span>exit </span>1  
> <span>fi  
>   
> if</span> <span>!</span> <span>command</span> <span>-v</span> btrfs &amp;&gt;/dev/null<span>;</span> <span>then  
>   </span><span>echo</span> <span>"Error: 'btrfs' command not found. Is btrfs-progs installed?"</span>  
>   <span>exit </span>1  
> <span>fi  
>   
> </span><span>echo</span> <span>"Scanning for snapshots under /.snapshots ..."</span>  
> <span>mapfile</span> <span>-t</span> SNAPSHOTS &lt; &lt;<span>(</span>  
>   btrfs subvolume list / <span>\</span>  
>     | <span>awk</span> <span>'$9 ~ /^\.snapshots\/[0-9]+\/snapshot$/ {print $9}'</span> <span>\</span>  
>     | <span>sort</span> <span>-t</span><span>'/'</span> <span>-k2</span>,2n  
> <span>)</span>  
>   
> <span>TOTAL</span><span>=</span><span>${#</span><span>SNAPSHOTS</span><span>[@]</span><span>}</span>  
>   
> <span>if</span> <span>((</span> TOTAL <span>==</span> 0 <span>))</span><span>;</span> <span>then  
>   </span><span>echo</span> <span>"No snapshots found under /.snapshots."</span>  
>   <span>exit </span>0  
> <span>fi  
>   
> </span><span>echo</span> <span>"Found </span><span>$TOTAL</span><span> snapshot subvolumes:"</span>  
> <span>printf</span> <span>'  %s\n'</span> <span>"</span><span>${</span><span>SNAPSHOTS</span><span>[@]</span><span>}</span><span>"</span>  
> <span>echo  
>   
> </span><span>if</span> <span>((</span> TOTAL &lt;<span>=</span> KEEP <span>))</span><span>;</span> <span>then  
>   </span><span>echo</span> <span>"Total snapshots (</span><span>$TOTAL</span><span>) &lt;= KEEP (</span><span>$KEEP</span><span>). Nothing to delete."</span>  
>   <span>exit </span>0  
> <span>fi</span>  
>   
> <span># Old ones to delete = everything except the last KEEP entries</span>  
> <span>DELETE_COUNT</span><span>=</span><span>$((</span> TOTAL <span>-</span> KEEP <span>))</span>  
> <span>TO_DELETE</span><span>=(</span> <span>"</span><span>${</span><span>SNAPSHOTS</span><span>[@]</span>:0:DELETE_COUNT<span>}</span><span>"</span> <span>)</span>  
> <span>TO_KEEP</span><span>=(</span> <span>"</span><span>${</span><span>SNAPSHOTS</span><span>[@]</span>:DELETE_COUNT<span>}</span><span>"</span> <span>)</span>  
>   
> <span>echo</span> <span>"Will KEEP the latest </span><span>$KEEP</span><span> snapshot(s):"</span>  
> <span>printf</span> <span>'  %s\n'</span> <span>"</span><span>${</span><span>TO_KEEP</span><span>[@]</span><span>}</span><span>"</span>  
> <span>echo  
> echo</span> <span>"Will DELETE </span><span>$DELETE_COUNT</span><span> older snapshot(s):"</span>  
> <span>printf</span> <span>'  %s\n'</span> <span>"</span><span>${</span><span>TO_DELETE</span><span>[@]</span><span>}</span><span>"</span>  
> <span>echo  
>   
> read</span> <span>-rp</span> <span>"Proceed with deletion? [y/N] "</span> ans  
> <span>case</span> <span>"</span><span>$ans</span><span>"</span> <span>in  
>   </span>y|Y<span>)</span> <span>;;</span>  
>   <span>*</span><span>)</span> <span>echo</span> <span>"Aborted."</span><span>;</span> <span>exit </span>0 <span>;;</span>  
> <span>esac</span>  
>   
> <span>for </span>relpath <span>in</span> <span>"</span><span>${</span><span>TO_DELETE</span><span>[@]</span><span>}</span><span>"</span><span>;</span> <span>do  
>   </span><span>fullpath</span><span>=</span><span>"/</span><span>$relpath</span><span>"</span>  
>   <span>echo</span> <span>"Deleting subvolume: </span><span>$fullpath</span><span>"</span>  
>   btrfs subvolume delete <span>"</span><span>$fullpath</span><span>"</span>  
>   
>   <span># Try to remove the parent directory (/.snapshots/&lt;id&gt;) if it is empty</span>  
>   <span>parent</span><span>=</span><span>"/</span><span>$(</span><span>dirname</span> <span>"</span><span>$relpath</span><span>"</span><span>)</span><span>"</span>  
>   <span>if </span><span>rmdir</span> <span>"</span><span>$parent</span><span>"</span> 2&gt;/dev/null<span>;</span> <span>then  
>     </span><span>echo</span> <span>"Removed empty directory: </span><span>$parent</span><span>"</span>  
>   <span>fi  
> done  
>   
> </span><span>echo</span> <span>"Syncing filesystem..."</span>  
> btrfs filesystem <span>sync</span> /  
>   
> <span>echo</span> <span>"Cleanup complete."</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <h2>  
>     
>     
>   Why this feels 100% Arch  
> </h2>  
>   
> <ul>  
> <li>I'm the one dialling in how many snapshots survive, not some mystery cron job.</li>  
> <li>Limine keeps every snapshot bootable but leaves the housekeeping to me, which is exactly how I like it.</li>  
> <li>No silent "optimization" services, just a bash script, a terminal prompt, and the knowledge my restore points are ones I actually care about.</li>  
> </ul>  
>   
> <p>Next time you finish spicy upgrade on your Arch system, kick back for a second and run <code>cleanup.sh</code>. Five seconds of sweeping, and boom: your Arch universe stays tidy, intentional, and totally yours.</p>

---

## [3/10] Beyond `console.log('Great Product!')`: Architecting B2B Testimonials That Actually Convert
**Source:** The Practical Developer | **Date:** 2025-12-02T12:00:36.000Z
**URL:** https://dev.to/michaelaiglobal/beyond-consoleloggreat-product-architecting-b2b-testimonials-that-actually-convert-2fnh
**Reasoning:** The article is about marketing testimonials, which is not relevant to our interests.
**Authors:** Michael

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fog715azb60vxmqxk1op6.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>As developers, we’re masters of systems, logic, and architecture. We build complex applications, orchestrate data pipelines, and design elegant APIs. So why, when it comes to marketing our own products, do we often settle for testimonials that are the equivalent of a <code>console.log('They have great customer service!')</code>?</p>  
>   
> <p>Flat, generic quotes are the null pointers of B2B marketing. They point to nothing, carry no emotional weight, and fail to build the trust needed for another technical team to adopt your tool. </p>  
>   
> <p>It’s time to stop collecting quotes and start architecting stories. Let’s refactor our approach to testimonials by treating them not as static strings, but as compelling narratives with a clear structure.</p>  
>   
> <h2>  
>     
>     
>   The Testimonial Monomyth: Deconstructing the Hero's Journey  
> </h2>  
>   
> <p>Every great story, from Star Wars to The Matrix, follows a predictable pattern: The Hero's Journey. Your customer is that hero. Your product isn't the hero—it's the lightsaber, the red pill, the powerful tool that enables their transformation.</p>  
>   
> <p>By mapping your customer's experience to this narrative arc, you move from a bland statement to an authentic, emotional journey that other developers can see themselves in. </p>  
>   
> <p>Instead of a flat data object, you create a structured narrative.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>// The OLD way: Low-information testimonial</span>  
> <span>const</span> <span>flatTestimonial</span> <span>=</span> <span>{</span>  
>   <span>customer</span><span>:</span> <span>"</span><span>Jane Doe, Lead Engineer @ Acme Corp</span><span>"</span><span>,</span>  
>   <span>quote</span><span>:</span> <span>"</span><span>This product saved us a lot of time. We highly recommend it.</span><span>"</span><span>,</span>  
>   <span>rating</span><span>:</span> <span>5</span>  
> <span>};</span>  
>   
> <span>// The NEW way: A structured story arc</span>  
> <span>const</span> <span>heroJourneyTestimonial</span> <span>=</span> <span>{</span>  
>   <span>hero</span><span>:</span> <span>"</span><span>Jane Doe, Lead Engineer @ Acme Corp</span><span>"</span><span>,</span>  
>   <span>act_1_the_problem</span><span>:</span> <span>{</span>  
>     <span>ordinary_world</span><span>:</span> <span>"</span><span>We were manually deploying microservices, spending 10 hours a week in YAML hell.</span><span>"</span><span>,</span>  
>     <span>call_to_adventure</span><span>:</span> <span>"</span><span>Our monolithic CI/CD pipeline finally broke during a critical release.</span><span>"</span>  
>   <span>},</span>  
>   <span>act_2_the_solution</span><span>:</span> <span>{</span>  
>     <span>meeting_the_mentor</span><span>:</span> <span>"</span><span>A colleague mentioned your platform in a Slack channel.</span><span>"</span><span>,</span>  
>     <span>crossing_the_threshold</span><span>:</span> <span>"</span><span>We signed up for a trial and integrated our first service in 30 minutes.</span><span>"</span><span>,</span>  
>     <span>tests_and_allies</span><span>:</span> <span>"</span><span>The biggest challenge was migrating legacy configs, but the support docs and our dedicated support engineer (our 'ally') were amazing.</span><span>"</span>  
>   <span>},</span>  
>   <span>act_3_the_transformation</span><span>:</span> <span>{</span>  
>     <span>the_reward</span><span>:</span> <span>"</span><span>We've automated 95% of our deployment process, cutting release times from hours to minutes.</span><span>"</span><span>,</span>  
>     <span>the_elixir</span><span>:</span> <span>"</span><span>Now, our dev team focuses on building features, not fighting the pipeline. We're the heroes who brought real velocity back to the org.</span><span>"</span>  
>   <span>}</span>  
> <span>};</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>See the difference? The second example isn't just a review; it's a relatable struggle with a triumphant resolution. It provides context, detail, and an emotional payoff.</p>  
>   
> <h2>  
>     
>     
>   The <code>getStory()</code> Function: An API for Authenticity  
> </h2>  
>   
> <p>You can't get a story if you don't ask for one. Stop sending out surveys with a single text field asking for "feedback." Instead, engage your power users in a conversation designed to extract the narrative. Think of it as an API call where the payload is their journey.</p>  
>   
> <h3>  
>     
>     
>   Endpoint: <code>GET /customer-story</code>  
> </h3>  
>   
> <p>The key is to ask open-ended questions that prompt storytelling. Ditch the old script and try these prompts instead.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>// Deprecated questions</span>  
> <span>const</span> <span>oldQuestions</span> <span>=</span> <span>[</span>  
>   <span>"</span><span>What do you like about our product?</span><span>"</span><span>,</span>  
>   <span>"</span><span>How has our product helped you?</span><span>"</span><span>,</span>  
>   <span>"</span><span>Would you recommend us to a friend?</span><span>"</span>  
> <span>];</span>  
>   
> <span>// The new, story-driven prompts</span>  
> <span>const</span> <span>newQuestions</span> <span>=</span> <span>[</span>  
>   <span>"</span><span>Can you describe your workflow *before* you found us? Paint a picture of the frustration.</span><span>"</span><span>,</span> <span>// Establishes the 'Ordinary World'</span>  
>   <span>"</span><span>What was the specific event that made you realize you needed a new solution?</span><span>"</span><span>,</span> <span>// The 'Call to Adventure'</span>  
>   <span>"</span><span>Walk me through the 'aha!' moment you had while using the tool for the first time.</span><span>"</span><span>,</span> <span>// 'Crossing the Threshold'</span>  
>   <span>"</span><span>What was the single biggest hurdle you overcame using our platform?</span><span>"</span><span>,</span> <span>// 'The Ordeal'</span>  
>   <span>"</span><span>Tell me about a specific metric or outcome that changed. How did that make your team look?</span><span>"</span> <span>// 'The Reward / The Elixir'</span>  
> <span>];</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>These questions guide the customer to tell their own hero story, naturally and authentically.</p>  
>   
> <h2>  
>     
>     
>   The Medium: Best Practices for Video Testimonials  
> </h2>  
>   
> <p>While a written story is powerful, seeing and hearing your customer hero tell it themselves is unbeatable. But this doesn't mean you need a Hollywood budget. For a technical audience, authenticity trumps production value every time.</p>  
>   
> <h3>  
>     
>     
>   Authenticity &gt; Polish  
> </h3>  
>   
> <p>A slightly grainy Zoom recording where an engineer is genuinely excited about solving a problem is infinitely more credible than a C-level executive reading a script from a teleprompter in a sterile office. Don't be afraid of remote recordings; they feel real and immediate.</p>  
>   
> <h3>  
>     
>     
>   B-Roll is Your Context Layer  
> </h3>  
>   
> <p>This is where you connect the story to the product. While the customer talks about their YAML hell, show a screen recording of their confusing old config files. When they describe their 'aha!' moment, show their cursor clicking the exact button in your UI. This visual evidence grounds the narrative in reality and makes it more compelling for other builders.</p>  
>   
> <h3>  
>     
>     
>   Use Jump Cuts to Your Advantage  
> </h3>  
>   
> <p>No one wants to watch someone ramble for five minutes. Be ruthless in your editing. Use jump cuts to clip out pauses, filler words ("um," "ah"), and rambling tangents. The result is a high-energy, information-dense story that respects the viewer's time—something every developer appreciates.</p>  
>   
> <h2>  
>     
>     
>   From User to Hero  
> </h2>  
>   
> <p>Your customers aren't just entries in a database. They are the heroes who fought back against technical debt, inefficient workflows, and broken pipelines. Your product was their secret weapon.</p>  
>   
> <p>When you stop asking for quotes and start seeking out these stories, you do more than just collect marketing assets. You humanize your brand, build deep emotional connections, and create authentic marketing that actually resonates with the most skeptical audience out there: other developers.</p>  
>   
> <p>Originally published at <a href="https://getmichaelai.com/blog/from-happy-customer-to-hero-the-art-of-storytelling-in-b2b-t">https://getmichaelai.com/blog/from-happy-customer-to-hero-the-art-of-storytelling-in-b2b-t</a></p>

---

## [3/10] Google Introduces Nano Banana Pro with Grounded, Multimodal Image Synthesis
**Source:** InfoQ | **Date:** 2025-12-02T11:52:00.000Z
**URL:** https://www.infoq.com/news/2025/12/nano-banana-pro/?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global
**Reasoning:** The article is about multimodal image synthesis, which is not relevant to our interests.
**Authors:** Robert Krzaczyński

**Content/Abstract:**
> <img src="https://res.infoq.com/news/2025/12/nano-banana-pro/en/headerimage/generatedHeaderImage-1764673167516.jpg" alt="generatedHeaderImage-1764673167516.jpg"><p>Google has released Nano Banana Pro. The system moves beyond conventional diffusion workflows by tightly coupling image generation with Gemini’s multimodal reasoning stack. The result: visuals that are not only aesthetically pleasing, but structurally, contextually, and informationally accurate.</p> <i>By Robert Krzaczyński</i>

---

## [3/10] Building a Blockchain in 2026: From-Scratch Engineering vs. Modern SDKs
**Source:** The Practical Developer | **Date:** 2025-12-02T11:35:02.000Z
**URL:** https://dev.to/thevenice/building-a-blockchain-in-2026-from-scratch-engineering-vs-modern-sdks-34jn
**Reasoning:** The article is about blockchain engineering, which is not relevant to our interests unless it involves infrastructure.
**Authors:** Prakash Pawar

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fpzadp8c025t6dbj464f2.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>The blockchain ecosystem has evolved dramatically since Bitcoin’s launch in 2009. In the early years, designing a blockchain meant implementing every component yourself: networking, consensus, block validation, mining, difficulty adjustment, peer discovery, and wallet logic. Today, developers have access to mature frameworks such as Cosmos SDK, Substrate, Polygon CDK, and the OP Stack. These frameworks abstract many of the lower-level components, allowing teams to focus on business logic instead of reinventing infrastructure.</p>  
>   
> <p>This raises an important question for engineers: <strong>Is it still worth building a blockchain entirely from scratch, like Bitcoin, or is relying on an SDK the practical approach in 2026?</strong><br>  
> This blog breaks down the engineering reality.</p>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>1. What “from scratch” really means in blockchain engineering</strong>  
> </h1>  
>   
> <p>Many people think building a blockchain from scratch means creating new block types and hashing them. In reality, a “true from-scratch blockchain” requires implementing several subsystems:</p>  
>   
> <h3>  
>     
>     
>   <strong>1.1 Network Layer</strong>  
> </h3>  
>   
> <p>Responsible for:</p>  
>   
> <ul>  
> <li>Peer discovery</li>  
> <li>Message propagation</li>  
> <li>Gossip protocol</li>  
> <li>Maintaining network topology</li>  
> <li>Flood protection</li>  
> <li>Peer scoring</li>  
> <li>Bandwidth control</li>  
> </ul>  
>   
> <p>This layer alone can require months of engineering effort.</p>  
>   
> <h3>  
>     
>     
>   <strong>1.2 Consensus Algorithm</strong>  
> </h3>  
>   
> <p>From scratch means you implement:</p>  
>   
> <ul>  
> <li>Proof-of-Work or Proof-of-Stake logic</li>  
> <li>Block proposal, voting, and finality</li>  
> <li>Fork choice rules</li>  
> <li>Difficulty or validator rotation</li>  
> <li>Security assumptions</li>  
> <li>Recovery logic</li>  
> </ul>  
>   
> <p>Bitcoin uses PoW with a simple longest-chain rule, while modern chains often use BFT-style consensus, which is significantly more complex.</p>  
>   
> <h3>  
>     
>     
>   <strong>1.3 Mempool &amp; Transaction Lifecycle</strong>  
> </h3>  
>   
> <p>You need to define:</p>  
>   
> <ul>  
> <li>Transaction format</li>  
> <li>Validation logic</li>  
> <li>Prioritization rules</li>  
> <li>Fee market design</li>  
> <li>Relay policies</li>  
> </ul>  
>   
> <p>This becomes even more challenging if you support smart contracts.</p>  
>   
> <h3>  
>     
>     
>   <strong>1.4 State Machine</strong>  
> </h3>  
>   
> <p>Either a:</p>  
>   
> <ul>  
> <li>UTXO model (like Bitcoin)</li>  
> <li>Account model (like Ethereum)</li>  
> <li>Hybrid or custom model</li>  
> </ul>  
>   
> <p>State transition functions must be deterministic, secure, and efficient.</p>  
>   
> <h3>  
>     
>     
>   <strong>1.5 Cryptography</strong>  
> </h3>  
>   
> <p>A custom chain requires decisions about:</p>  
>   
> <ul>  
> <li>Signature schemes</li>  
> <li>Hashing algorithms</li>  
> <li>Merkle tree construction</li>  
> <li>Key management</li>  
> <li>Wallet compatibility</li>  
> </ul>  
>   
> <p>Bitcoin uses ECDSA secp256k1 and SHA-256 double hashing. Many chains today use Ed25519, BLS signatures, or STARK/STARK-friendly hash functions.</p>  
>   
> <h3>  
>     
>     
>   <strong>1.6 Storage &amp; Database Layers</strong>  
> </h3>  
>   
> <p>You must implement or integrate:</p>  
>   
> <ul>  
> <li>Block storage</li>  
> <li>Indexing</li>  
> <li>State database</li>  
> <li>Pruning</li>  
> <li>Archival modes</li>  
> </ul>  
>   
> <p>Ethereum, for example, uses LevelDB/RocksDB with complex trie structures optimized over years.</p>  
>   
> <h3>  
>     
>     
>   <strong>1.7 Node Instrumentation</strong>  
> </h3>  
>   
> <p>A production-grade blockchain node needs:</p>  
>   
> <ul>  
> <li>RPC server</li>  
> <li>WebSocket endpoints</li>  
> <li>Debug API</li>  
> <li>Logging and metrics</li>  
> <li>Prometheus integration</li>  
> <li>Monitoring and telemetry</li>  
> </ul>  
>   
> <p>Without these, external developers cannot build on your chain.</p>  
>   
> <h3>  
>     
>     
>   <strong>1.8 Wallet Infrastructure</strong>  
> </h3>  
>   
> <p>You need to provide:</p>  
>   
> <ul>  
> <li>Keypair generation</li>  
> <li>Signing utilities</li>  
> <li>Address formats</li>  
> <li>Hardware wallet integrations</li>  
> <li>Client libraries</li>  
> </ul>  
>   
> <p>Building a blockchain from scratch means building all of this.</p>  
>   
> <p>It is fundamentally a distributed systems engineering project, not a Crypto Twitter idea.</p>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>2. Why SDKs exist (and why most chains use them)</strong>  
> </h1>  
>   
> <p>Modern blockchain SDKs exist to save developers from rebuilding components that have already been solved millions of times.</p>  
>   
> <p>A framework like Cosmos SDK or Substrate already includes:</p>  
>   
> <ul>  
> <li>Peer-to-peer networking</li>  
> <li>Production-tested consensus</li>  
> <li>RPC and gRPC servers</li>  
> <li>Modular governance</li>  
> <li>IBC or cross-chain communication</li>  
> <li>Staking and slashing</li>  
> <li>WASM or EVM execution environments</li>  
> <li>State management and storage layers</li>  
> </ul>  
>   
> <p>Using an SDK provides:</p>  
>   
> <ul>  
> <li>Faster development times</li>  
> <li>Safer consensus implementation</li>  
> <li>Easier upgrades</li>  
> <li>Existing toolchains</li>  
> <li>Wallet compatibility</li>  
> <li>Easier validator bootstrapping</li>  
> </ul>  
>   
> <p>Today, more than 90% of new blockchains are built on one of these frameworks. Entire L1 ecosystems such as Osmosis, Sei, Evmos, Injective, and dYdX are built using the Cosmos SDK.</p>  
>   
> <p>The reason is simple: <strong>developers want to build chains, not reinvent distributed networking from scratch.</strong></p>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>3. The dependency misconception</strong>  
> </h1>  
>   
> <p>A common concern is:<br>  
> “If I use Cosmos SDK or Substrate, am I dependent on their codebase?”</p>  
>   
> <p>The truth is more nuanced. SDKs are <em>open-source frameworks</em>, not proprietary platforms.<br>  
> You can:</p>  
>   
> <ul>  
> <li>Fork the code</li>  
> <li>Modify modules</li>  
> <li>Replace consensus</li>  
> <li>Build custom state machines</li>  
> <li>Write new modules in Go or Rust</li>  
> </ul>  
>   
> <p>This is no different from building a web server on top of Linux instead of writing your own operating system.<br>  
> You are not dependent; you are leveraging battle-tested engineering.</p>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>4. When it makes sense to build from scratch</strong>  
> </h1>  
>   
> <p>Despite the complexity, there are legitimate reasons to build your own blockchain from zero.</p>  
>   
> <h3>  
>     
>     
>   <strong>4.1 You are designing new consensus</strong>  
> </h3>  
>   
> <p>For example:</p>  
>   
> <ul>  
> <li>A novel PoW algorithm</li>  
> <li>ASIC-resistant mining</li>  
> <li>New BFT variants</li>  
> <li>DAG-based systems</li>  
> <li>Order-fairness consensus like Pulp Systems or Wimble</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   <strong>4.2 You need a fully custom architecture</strong>  
> </h3>  
>   
> <p>Examples:</p>  
>   
> <ul>  
> <li>Bitcoin’s UTXO model</li>  
> <li>Nano’s block-lattice</li>  
> <li>Kadena’s braided chains</li>  
> <li>Solana’s proof-of-history timestamping</li>  
> <li>Chia’s proof-of-space</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   <strong>4.3 You want full control of networking</strong>  
> </h3>  
>   
> <p>Especially relevant for:</p>  
>   
> <ul>  
> <li>IoT blockchains</li>  
> <li>On-premises or air-gapped systems</li>  
> <li>Ultra-low-latency trading chains</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   <strong>4.4 You are building a research chain or academic project</strong>  
> </h3>  
>   
> <p>Building from scratch is the best way to understand blockchain internals at a low level.</p>  
>   
> <h3>  
>     
>     
>   <strong>4.5 You want to avoid existing technical debt</strong>  
> </h3>  
>   
> <p>Some teams believe building a new stack is better than inheriting old architectural compromises.</p>  
>   
> <p>But these are specific cases, not general needs.</p>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>5. When using an SDK or framework is the right decision</strong>  
> </h1>  
>   
> <p>For 99% of modern blockchain projects, the goal is:</p>  
>   
> <ul>  
> <li>Launching a chain that supports smart contracts</li>  
> <li>Providing cheap blockspace</li>  
> <li>Enabling developers to build apps</li>  
> <li>Creating custom tokenomics or modules</li>  
> <li>Interoperability with other ecosystems</li>  
> <li>Setting up validators and RPC infrastructure</li>  
> </ul>  
>   
> <p>If this describes your use case, writing your own P2P networking library or consensus engine provides no additional value.</p>  
>   
> <h3>  
>     
>     
>   The Priority in 2026 is:  
> </h3>  
>   
> <p>Fewer teams care about reinventing block propagation.<br>  
> They care about:</p>  
>   
> <ul>  
> <li>throughput</li>  
> <li>tooling</li>  
> <li>developer experience</li>  
> <li>stability</li>  
> <li>interoperability</li>  
> <li>predictable upgrades</li>  
> </ul>  
>   
> <p>And these are areas where SDKs excel.</p>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>6. The engineering cost of “from scratch”</strong>  
> </h1>  
>   
> <p>If your goal is a production-grade chain, here are realistic numbers:</p>  
>   
> <h3>  
>     
>     
>   <strong>Team Requirement</strong>  
> </h3>  
>   
> <ul>  
> <li>3–6 core protocol engineers</li>  
> <li>2 distributed systems engineers</li>  
> <li>1–2 cryptographers</li>  
> <li>3 devops + SRE for network ops</li>  
> <li>2 QA engineers</li>  
> <li>1 project manager</li>  
> <li>1 documentation engineer</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   <strong>Timeframe</strong>  
> </h3>  
>   
> <ul>  
> <li>Minimum 18–36 months for mainnet</li>  
> <li>Additional 12 months for tooling, wallets, SDKs</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   <strong>Cost Breakdown</strong>  
> </h3>  
>   
> <ul>  
> <li>Engineering salaries (core protocol): $2–5M</li>  
> <li>Infrastructure (testnet &amp; mainnet): $200k+</li>  
> <li>Security audits: $300k–$1M</li>  
> <li>Maintenance: ongoing costs</li>  
> </ul>  
>   
> <p>This is why frameworks dominate.<br>  
> Not because developers are “lazy”, but because <strong>rebuilding the entire Bitcoin stack is prohibitively expensive unless you are innovating at the protocol level.</strong></p>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>7. The hybrid approach: what most serious chains do today</strong>  
> </h1>  
>   
> <p>Many successful blockchains take an in-between approach:</p>  
>   
> <ul>  
> <li>Use an SDK for networking and consensus</li>  
> <li>Replace or extend the execution environment</li>  
> <li>Write custom modules</li>  
> <li>Write a custom mempool or fee mechanism</li>  
> <li>Modify state machine logic</li>  
> <li>Add their own governance logic</li>  
> </ul>  
>   
> <p>Examples:</p>  
>   
> <ul>  
> <li>dYdX replaced CosmWasm with a custom Rust engine</li>  
> <li>Celestia uses Cosmos SDK but created a new DA layer</li>  
> <li>Aptos built a new VM but reused classical networking</li>  
> <li>Arbitrum Orbit uses OP Stack + custom proving logic</li>  
> <li>Sei modified mempool design for parallel execution</li>  
> </ul>  
>   
> <p>You avoid rewriting everything while still achieving deep differentiation.</p>  
>   
>   
>   
>   
> <h1>  
>     
>     
>   <strong>8. Conclusion</strong>  
> </h1>  
>   
> <p>Building a blockchain from scratch is technically possible and still relevant for certain categories of projects, particularly when designing novel consensus or architectures. However, in 2026, most blockchains do not need to reimplement Bitcoin’s network or Ethereum’s fundamental components. The ecosystem has matured to the point where leveraging frameworks is not a shortcut but an industry standard.</p>  
>   
> <p>If your goal is to launch a secure, scalable chain with modern tooling, smart contract support, and interoperability, using a mature SDK is the effective path.<br>  
> If your goal is fundamental protocol innovation or academic exploration, building from scratch remains a valuable pursuit.</p>  
>   
> <p>The key question is not whether you <em>can</em> build a blockchain from scratch, but whether it is the best use of your engineering time in an ecosystem where mature, open-source frameworks already exist.</p>

---

## [3/10] Can AI Deception Be Detected? Experts Weigh In - KnowTechie
**Source:** "Anthropic" - Google News | **Date:** 2025-12-02T11:34:28.000Z
**URL:** https://news.google.com/rss/articles/CBMiU0FVX3lxTFBicU5KR1VsU1VwckZQRkdkdzFabUljRU5WWU1mOWc2NGdLZlB0dFN6OGJnRVloVUZZM1ZGZGFoSlhpb3NIcmJGdjVBdGVidGhkUDc4?oc=5
**Reasoning:** The article discusses AI deception detection, which is not relevant to our interests in code intelligence or context engineering.

**Content/Abstract:**
> <a href="https://news.google.com/rss/articles/CBMiU0FVX3lxTFBicU5KR1VsU1VwckZQRkdkdzFabUljRU5WWU1mOWc2NGdLZlB0dFN6OGJnRVloVUZZM1ZGZGFoSlhpb3NIcmJGdjVBdGVidGhkUDc4?oc=5">Can AI Deception Be Detected? Experts Weigh In</a>  KnowTechie

---

## [3/10] Transactional and Marketing Emails for AsanTyping.com
**Source:** The Practical Developer | **Date:** 2025-12-02T11:20:03.000Z
**URL:** https://dev.to/danishyarkh/transactional-and-marketing-emails-for-asantypingcom-112i
**Reasoning:** The article is about email services, which is not relevant to our interests in code intelligence or context engineering.
**Authors:** Khalid Danishyar

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F64ya9zqgkribs3n5ctup.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>Over the past few weeks, I’ve been exploring different email service providers for transactional emails for AsanTyping.com. I tested MailerLite, Mailgun, Mailchimp, and even AWS SES.</p>  
>   
> <p>What I found:</p>  
>   
> <p>Most platforms are either too expensive, especially when it comes to marketing emails</p>  
>   
> <p>AWS SES is definitely the cheapest for transactional emails, but once you add marketing features, the pricing climbs quickly</p>  
>   
> <p>For my current needs, I needed something simple, reliable, and budget-friendly</p>  
>   
> <p>So for now, I’ve decided to use Resend’s free plan for all transactional emails — and honestly, it’s been a great fit.</p>  
>   
> <p>I’ve just finished integrating it, and now:<br>  
> ✅ Email verification<br>  
> ✅ Forgot password emails<br>  
> ✅ Welcome emails<br>  
> …are all live and fully functional!</p>  
>   
> <p>It feels good to check this off the list. If you’re building something early-stage and want a lightweight solution for transactional emails, Resend might be worth a look.</p>

---

## [3/10] Zero-Click SEO in 2026: When Winning Means Nobody Visits Your Site
**Source:** The Practical Developer | **Date:** 2025-12-02T11:09:56.000Z
**URL:** https://dev.to/synergistdigitalmedia/zero-click-seo-in-2026-when-winning-means-nobody-visits-your-site-33df
**Reasoning:** The article discusses SEO trends, which is not relevant to our interests in code intelligence or context engineering.
**Authors:** Drew Madore

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fres.cloudinary.com%2Fdstn7hw7h%2Fimage%2Fupload%2Fv1764673784%2Frhfp7pbr5ryfmurvkyxh.png" alt="https%3A%2F%2Fres.cloudinary.com%2Fdstn7"></p><p>Let me paint you a picture: You finally crack the code. Your content ranks #1 for a high-value keyword. You're celebrating with overpriced coffee. Then you check analytics and... nothing. Zero clicks.</p>  
>   
> <p>Welcome to zero-click search, where winning means Google answered the question so well that nobody needs to visit your website. <em>Chef's kiss</em> for user experience. Absolute nightmare for your traffic metrics.</p>  
>   
> <p>But here's the thing—this isn't going away. By late 2025, we're seeing AI Overviews (Google's fancy term for AI-generated answer boxes) on roughly 15-20% of queries. Featured snippets? They've been eating our lunch since 2017. And if you think this trend reverses in 2026, I have a metaverse timeshare to sell you.</p>  
>   
> <p>The question isn't whether to optimize for zero-click results. It's how to do it without tanking your entire content strategy.</p>  
>   
> <h2>  
>     
>     
>   Why Zero-Click Actually Matters (Even Though It Hurts)  
> </h2>  
>   
> <p>I know what you're thinking. "Why would I optimize for something that doesn't send traffic?" Fair question. Here's why you don't have a choice.</p>  
>   
> <p>First, visibility still counts. When your brand shows up in an AI Overview or featured snippet, you're getting prime real estate. Position zero. The penthouse suite of search results. Even if users don't click through immediately, they're seeing your brand as the authority.</p>  
>   
> <p>Second—and this surprised me when I dug into the data—featured snippets actually increase branded search. Users see you as the answer source, then search your brand name directly later. It's indirect traffic, but it's real. One client saw a 34% increase in branded searches three months after consistently winning featured snippets in their niche.</p>  
>   
> <p>Third, you're playing defense. If you don't optimize for these positions, your competitors will. And once they own that snippet real estate, clawing it back is brutal.</p>  
>   
> <h2>  
>     
>     
>   How AI Overviews Actually Work (The Technical Stuff)  
> </h2>  
>   
> <p>Google's AI Overviews pull from multiple sources to generate answers. They're not just grabbing your meta description and calling it a day. The system is:</p>  
>   
> <ul>  
> <li>Analyzing top-ranking pages for the query</li>  
> <li>Extracting relevant information across multiple sources</li>  
> <li>Synthesizing an answer using large language models</li>  
> <li>Citing sources (sometimes) with small attribution links</li>  
> <li>Prioritizing content that's structured, clear, and authoritative</li>  
> </ul>  
>   
> <p>The algorithm favors content that's already ranking well organically. You can't hack your way into an AI Overview from page 47. But if you're on page 1, especially positions 1-5, you're in the game.</p>  
>   
> <p>One critical detail: AI Overviews love recency. Fresh content gets weighted heavily for trending topics or news-related queries. That blog post from 2019? Not making the cut unless you've updated it recently.</p>  
>   
> <h2>  
>     
>     
>   The Featured Snippet Playbook (What Still Works)  
> </h2>  
>   
> <p>Featured snippets are the OG zero-click result. We've had years to figure these out, and the fundamentals haven't changed much:</p>  
>   
> <p><strong>Structure matters more than you think.</strong> Use clear H2s and H3s that match question formats. "What is X?" "How to Y?" "Why does Z happen?" Google loves content that mirrors how people actually search.</p>  
>   
> <p><strong>Answer the question immediately.</strong> Don't bury your answer in paragraph three after telling the entire history of the internet. First 50-60 words should directly address the query. Then you can add context and depth.</p>  
>   
> <p><strong>Lists and tables perform exceptionally well.</strong> If your content can be formatted as a numbered list, bullet points, or a comparison table, do it. These formats are snippet candy. I've seen mediocre content win snippets purely because it was structured as a clean table while better content was buried in paragraphs.</p>  
>   
> <p><strong>The 40-60 word sweet spot is real.</strong> Featured snippet answers typically fall in this range. Too short and you're not providing value. Too long and Google truncates or skips you.</p>  
>   
> <p>Here's what changed in 2025: Google started pulling snippets from video content more aggressively. If you have a YouTube video that answers the query clearly in the first 30 seconds, with good captions, you're now competing for snippet real estate. This opened up a whole new optimization vector.</p>  
>   
> <h2>  
>     
>     
>   Optimizing for AI Overview Citations  
> </h2>  
>   
> <p>This is where it gets interesting. Because AI Overviews synthesize from multiple sources, you're not trying to "win" the entire overview. You're trying to get cited as one of the authoritative sources.</p>  
>   
> <p>The winning strategy I've seen work:</p>  
>   
> <p><strong>Create genuinely comprehensive content.</strong> AI models favor sources that cover a topic thoroughly with supporting evidence. Thin content doesn't cut it. You need depth, examples, data points. The kind of content that takes actual expertise to create.</p>  
>   
> <p><strong>Use clear attribution for data and claims.</strong> When you cite statistics or research, be specific. "According to a 2025 study..." is better than "Research shows..." AI models pick up on this specificity and are more likely to cite you as a reliable source.</p>  
>   
> <p><strong>Answer related questions within your content.</strong> Don't just answer one query. Address the logical follow-up questions. If someone searches "how to optimize for featured snippets," they probably also want to know what featured snippets are, why they matter, and how long it takes to see results. Cover the cluster.</p>  
>   
> <p><strong>Update frequently.</strong> I mentioned this earlier, but it's critical. Set calendar reminders to refresh your top-performing content every 3-6 months. Add new examples, update statistics, incorporate recent developments. AI Overviews have a strong recency bias.</p>  
>   
> <p>One client in the B2B SaaS space started getting cited in AI Overviews after we restructured their content to include more original research and data points. They weren't just rehashing the same advice everyone else was giving—they were adding proprietary insights from their customer data. That originality made them cite-worthy.</p>  
>   
> <h2>  
>     
>     
>   The Schema Markup Advantage  
> </h2>  
>   
> <p>Schema markup is like leaving breadcrumbs for Google's algorithms. It helps search engines understand exactly what your content is about and how it's structured.</p>  
>   
> <p>For zero-click optimization, focus on:</p>  
>   
> <p><strong>FAQ schema</strong> for question-and-answer content. This directly feeds into how AI Overviews structure information. Plus, it can trigger those expandable FAQ sections in search results.</p>  
>   
> <p><strong>HowTo schema</strong> for instructional content. Step-by-step guides with proper schema markup have a much higher chance of appearing in AI-generated answers.</p>  
>   
> <p><strong>Article schema</strong> with proper headline, author, and date published markup. This helps establish content freshness and authority.</p>  
>   
> <p>The implementation isn't rocket science. Tools like Schema.org have templates. Most modern CMS platforms have plugins that handle it. The hard part is actually doing it consistently across your content library.</p>  
>   
> <p>One caveat: Schema markup alone won't save poorly written content. It's an amplifier, not a substitute for quality. Think of it as making sure Google can properly read your already-good content.</p>  
>   
> <h2>  
>     
>     
>   When Zero-Click Becomes a Traffic Problem  
> </h2>  
>   
> <p>Let's address the elephant in the room. What do you do when you're winning featured snippets and AI Overview citations, but your traffic is tanking?</p>  
>   
> <p>First, measure what actually matters. If you're a lead generation business, track conversions, not just traffic. Sometimes fewer, more qualified visitors convert better than high traffic volumes. I've seen cases where traffic dropped 20% but leads increased 15% because the remaining traffic was more targeted.</p>  
>   
> <p>Second, optimize for the click-through on zero-click results. Yes, that sounds contradictory. But even in AI Overviews, there are citation links. Make sure:</p>  
>   
> <ul>  
> <li>Your brand name is clearly visible in citations</li>  
> <li>The preview text (if shown) is compelling</li>  
> <li>Your meta description still works hard even if it's not the primary display</li>  
> </ul>  
>   
> <p>Third, diversify your content strategy. Create content that <em>can't</em> be answered in a snippet. In-depth analyses, original research, interactive tools, comprehensive guides that require scrolling. The kind of content where the snippet is just a teaser.</p>  
>   
> <p>One e-commerce client dealt with this by creating "quick answer" content optimized for snippets, then linking to detailed product guides and comparison tools that required site visits. The snippets built authority and brand awareness. The deeper content drove conversions.</p>  
>   
> <h2>  
>     
>     
>   The 2026 Playbook: Practical Next Steps  
> </h2>  
>   
> <p>Here's what's actually working as we head into 2026:</p>  
>   
> <p><strong>Audit your current snippet performance.</strong> Tools like SEMrush and Ahrefs show you which keywords trigger featured snippets and whether you're winning them. Start there. Low-hanging fruit is content where you rank positions 2-5 for queries with existing snippets.</p>  
>   
> <p><strong>Create a "snippet content" category.</strong> Not every piece of content needs to chase zero-click optimization. But identify 20-30 high-value queries in your niche where winning the snippet or AI Overview citation would build authority. Build content specifically for these.</p>  
>   
> <p><strong>Implement a refresh cadence.</strong> Set up a system to update your top-performing content quarterly. Add new data, fresh examples, recent developments. This keeps you competitive for AI Overviews that favor recency.</p>  
>   
> <p><strong>Test video content for snippet opportunities.</strong> Create short, well-captioned videos that directly answer common questions in your niche. Upload to YouTube with optimized titles and descriptions. This is an underutilized vector for 2026.</p>  
>   
> <p><strong>Track indirect metrics.</strong> Set up custom reports in Google Analytics to track branded search increases, conversion rate changes, and time-to-conversion shifts. Zero-click optimization affects these metrics even when traffic stays flat.</p>  
>   
> <p><strong>Build content clusters, not individual posts.</strong> Create comprehensive topic clusters where your pillar content can get cited in AI Overviews, and your supporting content captures the long-tail traffic that still generates clicks. This is essentially what we covered in our <a href="https://example.com/ai-content-marketing-guide">AI in Content Marketing: 2025 Strategy Guide</a>—the principles apply directly to zero-click optimization.</p>  
>   
> <h2>  
>     
>     
>   The Uncomfortable Truth About 2026  
> </h2>  
>   
> <p>Here's what I've learned after a year of obsessing over this: Zero-click search isn't the enemy. It's just a different game.</p>  
>   
> <p>The old model was simple. Rank high, get clicks, convert visitors. Linear and measurable.</p>  
>   
> <p>The new model is messier. Build authority through zero-click visibility, earn brand recognition, capture traffic through multiple touchpoints, convert over longer timelines. It's harder to attribute. It requires more patience. The metrics don't look as pretty in your monthly reports.</p>  
>   
> <p>But the businesses winning in 2026 aren't the ones fighting this trend. They're the ones who figured out how to use AI Overviews and featured snippets as top-of-funnel brand building, then created content strategies that capture demand at every other stage.</p>  
>   
> <p>You can't optimize your way back to 2015 when every search sent traffic. That world is gone. The question is whether you'll adapt your strategy or keep chasing metrics that matter less every quarter.</p>  
>   
> <p>Start with one high-value query in your niche. Optimize that content for zero-click results using the frameworks above. Measure what happens to branded search and conversions over 90 days. Then scale what works.</p>  
>   
> <p>The traffic might not come directly from that featured snippet. But if you're doing it right, it'll come.</p>

---

## [3/10] Why No Usable Data Found QR Code Scan Errors Occur 2026
**Source:** The Practical Developer | **Date:** 2025-12-02T11:08:39.000Z
**URL:** https://dev.to/eira-wexford/why-no-usable-data-found-qr-code-scan-errors-occur-2026-30bn
**Reasoning:** The article discusses QR code scan errors, which is not relevant to our interests in code intelligence or context engineering.
**Authors:** Eira Wexford

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ft66cw9itgr8szll3m9q8.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>By 2026, successfully scanning a QR code will be the easy part. The real challenge is the frustrating "No Usable Data Found" error that appears even when the scan seems to work.</p>  
>   
> <p>This error signals a shift from simple scanning problems to complex data-layer failures. This guide breaks down the advanced reasons these errors happen now and what you can do about them.</p>  
>   
> <h2>  
>     
>     
>   Understanding the "No Usable Data Found" QR Code Error  
> </h2>  
>   
> <p>Getting this error doesn't mean your phone's camera is broken. It means your device read the code but couldn't understand or access the information it pointed to. The bridge between the physical code and the digital experience is down.</p>  
>   
> <h3>  
>     
>     
>   What "No Usable Data Found" Actually Means  
> </h3>  
>   
> <p>This message indicates a data retrieval failure. Your scanner app successfully decoded the pattern but found the resulting information to be empty, corrupted, or inaccessible. It's like following a map to a location only to find an empty lot where a building should be.</p>  
>   
> <h3>  
>     
>     
>   A Quick Look at How QR Codes Work  
> </h3>  
>   
> <p>A QR code is a visual storage device. It encodes information like a web address, text, or contact details into a black-and-white pixelated square. When you scan it, your device's app translates that pattern back into its original data and attempts to act on it, like opening a website.</p>  
>   
> <h2>  
>     
>     
>   Common Causes for QR Code Scan Failures in 2026  
> </h2>  
>   
> <p>While old-school issues still exist, the reasons for scan failures in 2026 are far more advanced. They often involve the complex data and experiences linked to the code itself.</p>  
>   
> <h3>  
>     
>     
>   Poor QR Code Print Quality or Physical Damage  
> </h3>  
>   
> <p>A classic problem that persists. Scratches, poor contrast, or blurry printing can make a code unreadable. If the code is printed on a glossy surface with heavy glare, scanners can also struggle to decode the pattern correctly.</p>  
>   
> <h3>  
>     
>     
>   Outdated or Incompatible QR Code Scanner Applications  
> </h3>  
>   
> <p>Many modern QR codes link to advanced experiences like augmented reality (AR) or blockchain assets. If your scanner app hasn't been updated, it may not support these new data types, resulting in an error even if the code itself is fine.</p>  
>   
> <h3>  
>     
>     
>   Incorrect QR Code Generation and Data Encoding  
> </h3>  
>   
> <p>This is a creator-side error. If the data was entered incorrectly when the QR code was made, it will point to a nonexistent link or contain garbled text. The code is generated perfectly, but the information inside is flawed from the start.</p>  
>   
> <h3>  
>     
>     
>   Issues with the Scanning Device Camera  
> </h3>  
>   
> <p>A dirty camera lens is a common culprit. Smudges or dirt can obstruct the camera's view, preventing it from clearly seeing the entire QR code pattern. Low-light conditions can also make it difficult for the camera to focus properly.</p>  
>   
> <h3>  
>     
>     
>   Environmental Interference During Scanning  
> </h3>  
>   
> <p>Scanning a code from too far away or at a sharp angle can cause failures. Insufficient lighting, shadows, or even a shaky hand can prevent the app from capturing a stable image of the code to analyze.</p>  
>   
> <h3>  
>     
>     
>   Data Incompatibility or Corrupted Information  
> </h3>  
>   
> <p>This is a major issue in 2026. A QR code might link to a 3D model for an AR experience that your phone doesn't support. It could also point to a digital asset on a blockchain, but the data is inaccessible because the network is congested or the data format is wrong.</p>  
>   
> <h3>  
>     
>     
>   Server-Side Problems with Dynamic QR Codes  
> </h3>  
>   
> <p>Dynamic QR codes point to a URL that then redirects to the final content. This allows the destination to be updated. But if the server managing this redirect is down or misconfigured, your scan will lead nowhere. Ensuring a stable backend is essential, a task often handled through expert <a href="https://indiit.com/mobile-app-development-texas/">mobile app development texas</a> to build reliable systems.</p>  
>   
> <h3>  
>     
>     
>   Application or Platform Specific Restrictions  
> </h3>  
>   
> <p>Data privacy rules are stricter than ever. A QR code might be blocked from showing data because you haven't given consent per GDPR or CCPA rules. Similarly, authenticator apps like Microsoft Authenticator can fail if there are security restrictions or time synchronization issues with your account.</p>  
>   
> <h2>  
>     
>     
>   Practical Solutions to Resolve "No Usable Data Found" Errors  
> </h2>  
>   
> <p>Fixing these new-age QR code errors involves looking beyond the physical scan. Here are practical steps for both users and creators to solve the problem.</p>  
>   
> <h3>  
>     
>     
>   Essential Troubleshooting Steps for Users  
> </h3>  
>   
> <p>Start with the basics. Clean your phone's camera lens with a soft cloth. Check that you have a stable internet connection, as many QR codes require it. Try restarting the scanner app or your phone to clear any temporary glitches.</p>  
>   
> <h3>  
>     
>     
>   Optimizing Your QR Code Scanning Environment  
> </h3>  
>   
> <p>Ensure you are in a well-lit area without harsh shadows or glare. Position your phone directly in front of the code, not at an angle. Move closer or farther away until the camera focuses clearly and the entire code is visible in the frame.</p>  
>   
> <h3>  
>     
>     
>   Updating or Choosing a Better QR Code Scanner App  
> </h3>  
>   
> <p>If you suspect the issue is data incompatibility, check for updates to your scanner app. Consider using your phone's native camera app, as it's often updated with the operating system to support new technologies like AR.</p>  
>   
> <h3>  
>     
>     
>   Verifying and Regenerating the QR Code  
> </h3>  
>   
> <p>If you created the QR code, double-check the source data for typos or errors. Use a different device to scan and test the code yourself. If the data is incorrect, you will need to generate a new QR code with the corrected information.</p>  
>   
> <h3>  
>     
>     
>   Addressing Device Camera Malfunctions  
> </h3>  
>   
> <p>Open your regular camera app and see if it focuses correctly on other objects. If your camera is blurry or unable to focus, you may have a hardware issue that requires professional repair.</p>  
>   
> <h3>  
>     
>     
>   Checking Data Availability for Dynamic QR Codes  
> </h3>  
>   
> <p>If a QR code for a service or event isn't working, check the company's official website or social media for any announced outages. The problem may be with their server, not your device.</p>  
>   
> <h3>  
>     
>     
>   Contacting Support for Specific Application Services  
> </h3>  
>   
> <p>For issues with secure apps like authenticators or banking apps, the problem is likely account-specific. Contact the application's support team directly, as they can check for server-side issues or security flags on your account.</p>  
>   
> <h2>  
>     
>     
>   Preventing Future "No Usable Data Found" QR Code Issues  
> </h2>  
>   
> <p>A few smart habits can help you avoid these errors, whether you are creating QR codes or just scanning them.</p>  
>   
> <h3>  
>     
>     
>   Best Practices for Generating High-Quality QR Codes  
> </h3>  
>   
> <ul>  
> <li>  <strong>Use Dynamic Codes:</strong> Always choose dynamic QR codes over static ones. This lets you fix typos or update the destination link without having to reprint the code.</li>  
> <li>  <strong>Ensure High Resolution:</strong> Export your QR code as a vector file (like SVG or EPS) to ensure it stays sharp at any size.</li>  
> <li>  <strong>Add a Quiet Zone:</strong> Leave a clear, empty margin around the QR code. This "quiet zone" helps scanners distinguish the code from its surroundings.</li>  
> </ul>  
>   
> <h3>  
>     
>     
>   Smart Scanning Habits for Consistent Results  
> </h3>  
>   
> <ul>  
> <li>  <strong>Use the Native Camera:</strong> Your phone's built-in camera app is usually the most reliable and up-to-date scanner.</li>  
> <li>  <strong>Be Aware of Context:</strong> If a QR code promises an AR experience, be prepared for it to require more processing power and a strong internet connection.</li>  
> <li>  <strong>Don't Scan Suspicious Codes:</strong> Be cautious about scanning random QR codes from untrusted sources, as they can lead to phishing sites.</li>  
> </ul>  
>   
> <h2>  
>     
>     
>   Frequently Asked Questions About QR Code Scan Errors  
> </h2>  
>   
> <h3>  
>     
>     
>   Why Is My iPhone Not Scanning QR Codes  
> </h3>  
>   
> <p>First, check your iPhone's settings. Go to Settings &gt; Camera and ensure "Scan QR Codes" is enabled. If it is, the issue could be a dirty lens, poor lighting, or an outdated iOS version that doesn't support the QR code's linked data type.</p>  
>   
> <h3>  
>     
>     
>   What Causes "No Usable Data Found" on Microsoft Authenticator  
> </h3>  
>   
> <p>This error on an authenticator app is almost always a server-side or account-sync issue. It can be caused by the time on your device being out of sync with the server, a temporary service outage, or an invalid activation code from the service you're trying to add.</p>  
>   
> <h3>  
>     
>     
>   Can a QR Code Expire or Become Invalid  
> </h3>  
>   
> <p>A static QR code, which has the data embedded directly, never expires. However, a dynamic QR code points to a web link that can be changed or deleted. If that link is broken or the service is discontinued, the QR code will effectively "expire" and lead to an error.</p>  
>   
> <h3>  
>     
>     
>   How Can I Test a QR Code Before Deployment  
> </h3>  
>   
> <p>Always test your QR code before printing it. Scan it with multiple devices (both iPhone and Android) and different scanner apps. Ensure it leads to the correct destination and that all content loads properly on a mobile device.</p>  
>   
> <h2>  
>     
>     
>   Conclusion  
> </h2>  
>   
> <p>The "No Usable Data Found" error in 2026 marks a shift in how we interact with QR codes. The problem is no longer just about a successful scan but about the reliability of the complex digital experiences they unlock, from personalized content to AR and blockchain data.</p>  
>   
> <p>Understanding these new failure points is key. For users, it means checking app compatibility and internet connections. For creators, it demands a focus on robust backends and testing across multiple platforms.</p>  
>   
> <p>Start by using your phone's native camera for scans and ensuring your apps are always up to date. This prepares your device to handle the advanced, data-rich world that QR codes now connect us to.</p>

---

## [3/10] 🔥 AI News Hub — A Complete AI/Tech News Aggregator SaaS You Fully Own
**Source:** The Practical Developer | **Date:** 2025-12-02T11:07:29.000Z
**URL:** https://dev.to/dhren2019/ai-news-hub-a-complete-aitech-news-aggregator-saas-you-fully-own-2067
**Reasoning:** The article is a generic marketing piece about an AI news aggregator, not directly relevant to our interests.
**Authors:** Dhren

**Content/Abstract:**
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fe63qxfyxh3j8aysf5j58.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fe63qxfyxh3j8aysf5j58.png" alt="" width="800" height="444"></a></p> 
>  
> <p>I built AI News Hub as a complete platform that automatically collects, organizes, and publishes the latest content from the AI world, programming, machine learning, dev tools, and tech tutorials. Every 2 hours, the system scrapes trusted sources, cleans the data, generates SEO-optimized posts, and updates a fully featured dashboard. It also sends push notifications to users whenever new content is available.</p> 
>  
> <p>The whole project is designed to be a plug-and-launch SaaS: it includes authentication, subscriptions, blog system, PRO mode (ads removed), backend API, scraper, SEO, and everything needed to run a polished production website.</p> 
>  
> <p>⚙️ Scraper · Backend · Dashboard · Push Notifications · Authentication · SEO · Blog · Friendly URLs<br> 
> React 18 · FastAPI Python · TailwindCSS · shadcn/ui · MongoDB Atlas · OneSignal · Clerk Auth</p> 
>  
> <p>🧩 FEATURES INCLUDED</p> 
>  
> <p>🎨 Frontend (React + Tailwind + shadcn)<br> 
> What I built on the front:</p> 
>  
> <p>SEO-ready homepage</p> 
>  
> <p>/hub dashboard with all scraped news</p> 
>  
> <p>/subscription page for plans</p> 
>  
> <p>/profile for user details</p> 
>  
> <p>/post/:slug for individual articles</p> 
>  
> <p>/blog with a complete technical blogging system</p> 
>  
> <p>SEO: dynamic titles, meta descriptions, OpenGraph, JSON-LD, sitemap, robots, and clean URLs like:<br> 
> /post/openai-new-model-2025</p> 
>  
> <p>🧠 Backend (FastAPI + Python)<br> 
> The backend exposes clean endpoints:<br> 
> /api/articles<br> 
> /api/post/{slug}<br> 
> /api/dashboard<br> 
> /api/notifications/send</p> 
>  
> <p>I included Pydantic models, error handling, and optional Clerk token validation.</p> 
>  
> <p>🤖 Automated Scraper<br> 
> This part is fully automated:</p> 
>  
> <p>Runs every 2 hours</p> 
>  
> <p>Normalizes and deduplicates content</p> 
>  
> <p>Inserts everything into MongoDB</p> 
>  
> <p>Triggers push notifications when new posts appear</p> 
>  
> <p>📡 Push Notifications<br> 
> Built-in OneSignal integration:</p> 
>  
> <p>Automatic registration</p> 
>  
> <p>Service worker included</p> 
>  
> <p>Works for new article alerts</p> 
>  
> <p>💸 Monetization (Monthly Subscriptions)</p> 
>  
> <p>I added subscription billing using Clerk + Clerk Billing.</p> 
>  
> <p>🔐 PRO Mode:</p> 
>  
> <p>Paying users don’t see ads</p> 
>  
> <p>Free users see ads</p> 
>  
> <p>Automatic monthly billing</p> 
>  
> <p>You can set any price you want.</p> 
>  
> <p>📝 SEO Package</p> 
>  
> <p>Auto-generated titles</p> 
>  
> <p>Optimized meta descriptions</p> 
>  
> <p>Clean SEO-friendly slugs</p> 
>  
> <p>Article schema</p> 
>  
> <p>Dynamic sitemap + robots.txt</p> 
>  
> <p>🎯 Perfect For</p> 
>  
> <p>Developers wanting a ready SaaS</p> 
>  
> <p>Makers shipping a fast MVP</p> 
>  
> <p>Freelancers reselling SaaS to clients</p> 
>  
> <p>Students learning real-world architecture</p> 
>  
> <p>🧾 Summary of What I Built</p> 
>  
> <p>Full frontend</p> 
>  
> <p>Backend API</p> 
>  
> <p>Automated scraper</p> 
>  
> <p>Blog system</p> 
>  
> <p>Push notifications</p> 
>  
> <p>OAuth + Auth</p> 
>  
> <p>Subscriptions + PRO mode</p> 
>  
> <p>SEO + deployment-ready</p> 
>  
> <p><a href="https://ainewshub2025.netlify.app/">FULL PROJECT HERE</a> and if you are interested you can purchase <a href="https://buy.polar.sh/polar_cl_lqtGvMKK6k7MSW521gbPP7U1U1ypSqINbywGp0vAIH8">here</a>!</p> 
>  
> <p>It’s a complete, fully connected SaaS—ready to run or sell.</p>

---

## [3/10] We’re Not Just “Online” Anymore — We’re Connected. Welcome to ConnectApp
**Source:** The Practical Developer | **Date:** 2025-12-02T10:41:03.000Z
**URL:** https://dev.to/tobi_jesee_731fd28a3212b0/were-not-just-online-anymore-were-connected-welcome-to-connectapp-3em
**Reasoning:** The article is a generic marketing piece about a new app, not relevant to our interests.
**Authors:** tobi jesee

**Content/Abstract:**
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgatpz1ajwdpe0gw1akks.jpg"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgatpz1ajwdpe0gw1akks.jpg" alt="" width="612" height="408"></a><br> 
> Let’s be real.</p> 
>  
> <p>We live in a world where you can meet someone in Tokyo, collaborate with someone in Lagos, and build with someone in New York — all before lunch. But even with all this tech, real connection still feels hard.</p> 
>  
> <p>That’s exactly why ConnectApp Inc. exists.</p> 
>  
> <p>We’re not here to be just another app.<br> 
> We’re here to change how Gen Z connects, creates, and grows in the digital world.</p> 
>  
> <p>🌍 Connection, But Make It Real</p> 
>  
> <p>Gen Z doesn’t want noise.<br> 
> We want meaningful connections, authentic communities, and platforms that actually do something for us.</p> 
>  
> <p>ConnectApp is built to:</p> 
>  
> <p>Link people with shared interests and goals</p> 
>  
> <p>Support creators, entrepreneurs, students, and innovators</p> 
>  
> <p>Turn conversations into collaborations</p> 
>  
> <p>Make networking feel natural, not forced</p> 
>  
> <p>No pressure. No fake energy. Just real people building real things.</p> 
>  
> <p>💡 Built for the Future (Not the Past)</p> 
>  
> <p>We’re a generation that moves fast, thinks global, and builds digitally. ConnectApp is designed with:</p> 
>  
> <p>A clean, modern experience</p> 
>  
> <p>Smart networking tools</p> 
>  
> <p>Spaces for projects, ideas, and opportunities</p> 
>  
> <p>Features that grow with your ambition</p> 
>  
> <p>Whether you’re:</p> 
>  
> <p>A startup founder</p> 
>  
> <p>A creative</p> 
>  
> <p>A student</p> 
>  
> <p>A freelancer</p> 
>  
> <p>Or just someone with big ideas</p> 
>  
> <p>There’s space for you here.</p> 
>  
> <p>🔗 More Than an App — It’s a Movement</p> 
>  
> <p>ConnectApp Inc. is about:</p> 
>  
> <p>Collaboration over competition</p> 
>  
> <p>Community over clout</p> 
>  
> <p>Impact over hype</p> 
>  
> <p>We believe the next big things won’t come from isolated individuals — they’ll come from connected minds.</p> 
>  
> <p>And that’s where you come in.</p> 
>  
> <p>✨ Join the New Wave of Digital Connection</p> 
>  
> <p>Gen Z is rewriting the rules of work, networking, and collaboration. ConnectApp is simply giving us the tools to do it better.</p> 
>  
> <p>This is your space to:</p> 
>  
> <p>Connect</p> 
>  
> <p>Create</p> 
>  
> <p>Grow</p> 
>  
> <p>Build the future together</p> 
>  
> <p>The future isn’t just digital. It’s connected.<br> 
> And it starts with us.</p> 
>  
> <p>— ConnectApp.inc</p>

---

## [3/10] Branch development with git
**Source:** The Practical Developer | **Date:** 2025-12-02T10:39:26.000Z
**URL:** https://dev.to/aaronmccollum/branch-development-with-git-joc
**Reasoning:** The article is a basic tutorial about branch development with git, not relevant to our interests.
**Authors:** Aaron McCollum

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fv0v11xs8opjh7ga2hw9p.jpg" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>Branch development is a key aspect of software development. The idea here is you clone (i.e. copy) the code you’re going to work on onto your own computer, then you can make your necessary changes, then lastly save it back to the official code repository. In other words, you take the copied code and “branch” off with it to make changes. Then you “merge” your branch back into the main repository of code.</p>  
>   
> <p>In my old job, Pega handled this on it’s own. It had it’s own branching tools, so I could copy the low code settings into my own branch, make my changes, then check them back in to the official code. But outside of low-code tools like Pega, developers primarily use git to help manage their branch development.</p>  
>   
> <p>With 100Devs, I’ve been pushing the HTML and CSS projects into Github. In the past, I would not create branches. I would just clone the code down, make changes, then merge the code straight into the main branch without creating my own branches or opening a pull request. While that would work for small one-developer projects, that won’t work long-term in my learning.</p>  
>   
> <p>Below is the workflow I’ve been using for the past few weeks. I’m writing this mainly for myself — I forget the terminal commands sometimes and will need a reminder. But hopefully this can also help you, the reader, if you are just starting out and learning git.</p>  
>   
> <p><code>git checkout -b &lt;branchName&gt;</code> : This command does two things — it first creates a new branch with whatever value you provide (without the &lt; and &gt; symbols), and it automatically checks out your code into this new branch.</p>  
>   
> <p><code>git branch</code> : This will list out all the branches you have. The branch you are currently working in might have a * beside it’s name, be colored green, or be bolded in the terminal.</p>  
>   
> <p><code>git status</code> : This shows all the updated files that are not staged for the next commit. Usually a terminal will show these files in the color red.</p>  
>   
> <p><code>git add &lt;fileName&gt; or git add .</code> : The first command adds a specific file name to the staging area for the next commit. The second command adds everything that’s been changed. Be careful with the second command, as you may not want to add everything for the next commit quite yet.</p>  
>   
> <p><code>git commit -m “&lt;commit message&gt;”</code> : This commits the changes in git. This won’t send the code remotely to Github quite yet, but it does save the code into a new “version” of your codebase. I always add a message, and I am learning to follow <a href="https://www.conventionalcommits.org/en/v1.0.0/">conventional commits</a>.</p>  
>   
> <p><code>git push origin &lt;branchname&gt;</code> : This pushes your code remotely into Github. If a branch does not exist yet, it will create a branch in Github with your updated code. Github will then give you the option to open a pull request, or open a draft pull request (which I didn’t know about until a few days ago, and I love that feature).</p>  
>   
> <p>That’s it! I wanted to document the common commands I use in git. I’ll be posting again soon with some more learnings from 100Devs, but I wanted to get this post out first as I know it will come in handy later on for more projects I’m building.</p>  
>   
> <p>What other git commands do you think I should know and put into practice?</p>

---

## [3/10] Funding grants for new research into AI and mental health
**Source:** OpenAI News | **Date:** 2025-12-01T12:00:00.000Z
**URL:** https://openai.com/index/ai-mental-health-research-grants
**Reasoning:** The topic is about AI and mental health, which is not directly related to code intelligence or context engineering.

**Content/Abstract:**
> OpenAI is awarding up to $2 million in grants for research at the intersection of AI and mental health. The program supports projects that study real-world risks, benefits, and applications to improve safety and well-being.

---

## [3/10] Mixpanel security incident: what OpenAI users need to know
**Source:** OpenAI News | **Date:** 2025-11-26T19:00:00.000Z
**URL:** https://openai.com/index/mixpanel-incident
**Reasoning:** The security incident is not related to code intelligence or context engineering.

**Content/Abstract:**
> OpenAI shares details about a Mixpanel security incident involving limited API analytics data. No API content, credentials, or payment details were exposed. Learn what happened and how we’re protecting users.

---

## [2/10] Calculating Web Impact Factor for University Websites of Jammu and Kashmir: A Study
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251101496A/abstract
**Reasoning:** The study on web impact factors for university websites is irrelevant to our interests in AI and software engineering trends.
**Authors:** Ahmad, Muneer, Batcha, M Sadik, Rashid, Wasim, Hafiz, Obaid

**Content/Abstract:**
> This paper examines and explores the web impact factor through a webometric study of the present 12 University Websites of Jammu and Kashmir. Identifies the domain systems of the websites; analyzes the number of web pages and link pages, and calculates the External Link WIF or simple web impact factor (WIF) and external web impact factor of all the University websites. Also reflects that some university websites have higher number of web pages, but correspondingly their link pages are very small in number and websites fall behind in their simple and external link web impact factor. It found that the Cluster University of Jammu ranked 1 (0.9018) in Internal Link WIF of Websites in Jammu and Kashmir. Shri Mata Vaishno Devi University ranked 1 (0.7249) in External Link Web Impact Factor.

---

## [2/10] Reinforcement Learning for Chemical Ordering in Alloy Nanoparticles
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251112260E/abstract
**Reasoning:** The focus on reinforcement learning for chemical ordering in nanoparticles is not relevant to our interests in AI for software engineering.
**Authors:** Elsborg, Jonas, Bhowmik, Arghya

**Content/Abstract:**
> We approach the search for optimal element ordering in bimetallic alloy nanoparticles (NPs) as a reinforcement learning (RL) problem, and have built an RL agent that learns to perform such global optimisation using the geometric graph representation of the NPs. To demonstrate the effectiveness, we train an RL agent to perform composition-conserving atomic swap actions on the icosahedral nanoparticle structure. Trained once on randomised $Ag_{X}Au_{309-X}$ compositions and orderings, the agent discovers previously established ground state structure. We show that this optimization is robust to differently ordered initialisations of the same NP compositions. We also demonstrate that a trained policy can extrapolate effectively to NPs of unseen size. However, the efficacy is limited when multiple alloying elements are involved. Our results demonstrate that RL with pre-trained equivariant graph encodings can navigate combinatorial ordering spaces at the nanoparticle scale, and offer a transferable optimisation strategy with the potential to generalise across composition and reduce repeated individual search cost.

---

## [2/10] Geant4 based library SCoRe4 for Surface Contamination and Roughness Effects simulations in rare event search experiments
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251115844G/abstract
**Reasoning:** The focus on surface contamination simulations in physics experiments is not relevant to our interests in code intelligence or AI in software engineering.
**Authors:** Grüner, Christoph

**Content/Abstract:**
> Surface simulations are important for accurately modeling particle interactions in experiments where background contributions from surface contaminants can significantly affect detector performance. In rare event searches, such as dark matter or neutrinoless double beta decay experiments, standard Geant4 simulations typically assume perfectly smooth surfaces, neglecting the microscopic roughness that exists in real materials. This simplification can lead to inaccurate predictions of energy deposition. To address this limitation, I developed SCoRe4, a Geant4-based library designed to simulate more realistic surface roughness based on experimentally measurable parameters. The code allows users to generate patches of simplified rough surface geometries across a wide range of scales - from square millimeters to square meters - while maintaining computational efficiency. SCoRe4 is open source and can be easily integrated into existing Geant4 setups. This work presents the structure, implementation, and example application of SCoRe4,as well as its potential use in improving the accuracy of background modeling in rare event physics.

---

## [2/10] A Hands-On Molecular Communication Testbed for Undergraduate Education
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T17:25:33.000Z
**URL:** https://arxiv.org/abs/2512.01904v1
**Reasoning:** The molecular communication testbed for education is irrelevant to code intelligence or context engineering.
**Authors:** Arne Gaedeken

**Content/Abstract:**
> This work presents a hands-on molecular communication (MC) testbed developed for the undergraduate \textit{Communication Engineering} lab course at the Institute for Communications Technology (IfN), TU~Braunschweig. The goal of the experiment is to provide students with an intuitive and reproducible introduction to MC concepts using a low-cost and accessible fluidic setup. The system employs a background water flow into which three dye colors are injected and symbols are detected by a multi-wavelength photosensor. A zero-forcing--based estimator is used to separate the spectral components and reliably identify the transmitted colors. The experiment is designed to be completed independently by students within a single laboratory session and requires only basic prior knowledge from introductory communication engineering courses. A detailed script accompanies the experiment, guiding students through channel characterization, color detection, pseudoinverse computation, and simple data transmission using on-off keying. In pilot trials, students successfully reproduced the entire communication chain and achieved stable data rates of up to 0.5~bit/s over a 15~cm channel. The proposed testbed demonstrates that fundamental principles of MC can be taught effectively using a compact and inexpensive experimental setup. The experiment will be integrated into an undergraduate lab course.

---

## [2/10] Tight Bounds for Feedback Vertex Set Parameterized by Clique-width
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T17:21:12.000Z
**URL:** https://arxiv.org/abs/2512.01900v1
**Reasoning:** The focus on feedback vertex set parameterization is not relevant to code intelligence or context engineering.
**Authors:** Narek Bojikian

**Content/Abstract:**
> We introduce a new notion of acyclicity representation in labeled graphs, and present three applications thereof. Our main result is an algorithm that, given a graph $G$ and a $k$-clique expression of $G$, in time $O(6^kn^c)$ counts modulo $2$ the number of feedback vertex sets of $G$ of each size. We achieve this through an involved subroutine for merging partial solutions at union nodes in the expression. In the usual way this results in a one-sided error Monte-Carlo algorithm for solving the decision problem in the same time. We complement these by a matching lower bound under the Strong Exponential-Time Hypothesis (SETH). This closes an open question that appeared multiple times in the literature [ESA 23, ICALP 24, IPEC 25]. 
>   We also present an algorithm that, given a graph $G$ and a tree decomposition of width $k$ of $G$, in time $O(3^kn^c)$ counts modulo $2$ the number of feedback vertex sets of $G$ of each size. This matches the known SETH-tight bound for the decision version, which was obtained using the celebrated cut-and-count technique [FOCS 11, TALG 22]. Unlike other applications of cut-and-count, which use the isolation lemma to reduce a decision problem to counting solutions modulo $2$, this bound was obtained via counting other objects, leaving the complexity of counting solutions modulo $2$ open. 
>   Finally, we present a one-sided error Monte-Carlo algorithm that, given a graph $G$ and a $k$-clique expression of $G$, in time $O(18^kn^c)$ decides the existence of a connected feedback vertex set of size $b$ in $G$. We provide a matching lower bound under SETH.

---

## [2/10] Unifying Sign and Magnitude for Optimizing Deep Vision Networks via ThermoLion
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T17:04:17.000Z
**URL:** https://arxiv.org/abs/2512.01881v1
**Reasoning:** The optimization of deep vision networks is not relevant to code intelligence or context engineering.
**Authors:** Ahmed Nebli

**Content/Abstract:**
> The training of deep vision models is fundamentally a signal recovery problem amidst high-dimensional stochastic noise. Current optimization paradigms impose a static compromise on information channel capacity. For instance, magnitude-based methods, such as AdamW, operate on the assumption that gradient norms are high-fidelity curvature signals. While this allows for precision in smooth regimes, it leads to catastrophic noise amplification when applied to rugged, non-convex landscapes. Conversely, sign-based methods (e.g., Lion) perform a radical 1-bit quantization of the gradient, which aims to provide robust regularization at the cost of discarding fine-grained descent information. We propose that optimal convergence requires neither static prior, but rather a dynamic modulation of the update bitrate. We introduce \textbf{ThermoLion}, a vision-centric framework that utilizes local Signal-to-Noise Ratio (SNR) gating to autonomously transition parameters between a "low-bit" exploration phase and a "high-precision" exploitation phase. Furthermore, we introduce a Momentum Alignment mechanism that detects constructive interference between historical drift and instantaneous gradients to accelerate convergence during stable trajectories. Empirical benchmarks across 12 diverse vision datasets (including CIFAR, SVHN, and GTSRB) demonstrate that ThermoLion serves as a hyperparameter-free generalist, surpassing both AdamW and Lion in convergence speed and terminal accuracy without architecture-specific tuning.

---

## [2/10] Cross-Lingual Interleaving for Speech Language Models
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T16:48:05.000Z
**URL:** https://arxiv.org/abs/2512.01865v1
**Reasoning:** The article is about cross-lingual speech language models, which is outside the scope of our primary interests.
**Authors:** Adel Moumen

**Content/Abstract:**
> Spoken Language Models (SLMs) aim to learn linguistic competence directly from speech using discrete units, widening access to Natural Language Processing (NLP) technologies for languages with limited written resources. However, progress has been largely English-centric due to scarce spoken evaluation benchmarks and training data, making cross-lingual learning difficult. We present a cross-lingual interleaving method that mixes speech tokens across languages without textual supervision. We also release an EN-FR training dataset, TinyStories (~42k hours), together with EN-FR spoken StoryCloze and TopicCloze benchmarks for cross-lingual semantic evaluation, both synthetically generated using GPT-4. On 360M and 1B SLMs under matched training-token budgets, interleaving improves monolingual semantic accuracy, enables robust cross-lingual continuation, and strengthens cross-lingual hidden-state alignment. Taken together, these results indicate that cross-lingual interleaving is a simple, scalable route to building multilingual SLMs that understand and converse across languages. All resources will be made open-source to support reproducibility.

---

## [2/10] Topological Order in Deep State
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T16:46:39.000Z
**URL:** https://arxiv.org/abs/2512.01863v1
**Reasoning:** The focus on topological order in quantum phases is not relevant to our interests in software engineering and AI tools.
**Authors:** Ahmed Abouelkomsan

**Content/Abstract:**
> Topologically ordered states are among the most interesting quantum phases of matter that host emergent quasi-particles having fractional charge and obeying fractional quantum statistics. Theoretical study of such states is however challenging owing to their strong-coupling nature that prevents conventional mean-field treatment. Here, we demonstrate that an attention-based deep neural network provides an expressive variational wavefunction that discovers fractional Chern insulator ground states purely through energy minimization without prior knowledge and achieves remarkable accuracy. We introduce an efficient method to extract ground state topological degeneracy -- a hallmark of topological order -- from a single optimized real-space wavefunction in translation-invariant systems by decomposing it into different many-body momentum sectors. Our results establish neural network variational Monte Carlo as a versatile tool for discovering strongly correlated topological phases.

---

## [2/10] Storage capacity of perceptron with variable selection
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T16:44:57.000Z
**URL:** https://arxiv.org/abs/2512.01861v1
**Reasoning:** The article on perceptron storage capacity is not directly related to our focus areas in code intelligence or context engineering.
**Authors:** Yingying Xu

**Content/Abstract:**
> A central challenge in machine learning is to distinguish genuine structure from chance correlations in high-dimensional data. In this work, we address this issue for the perceptron, a foundational model of neural computation. Specifically, we investigate the relationship between the pattern load $α$ and the variable selection ratio $ρ$ for which a simple perceptron can perfectly classify $P = αN$ random patterns by optimally selecting $M = ρN$ variables out of $N$ variables. While the Cover--Gardner theory establishes that a random subset of $ρN$ dimensions can separate $αN$ random patterns if and only if $α&lt; 2ρ$, we demonstrate that optimal variable selection can surpass this bound by developing a method, based on the replica method from statistical mechanics, for enumerating the combinations of variables that enable perfect pattern classification. This not only provides a quantitative criterion for distinguishing true structure in the data from spurious regularities, but also yields the storage capacity of associative memory models with sparse asymmetric couplings.

---

## [2/10] Excluding a Forest Induced Minor
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T16:43:37.000Z
**URL:** https://arxiv.org/abs/2512.01857v1
**Reasoning:** Graph theory and induced minors are not relevant to our primary interests in code intelligence and context engineering.
**Authors:** Édouard Bonnet

**Content/Abstract:**
> In the first paper of the Graph Minors series [JCTB '83], Robertson and Seymour proved the Forest Minor theorem: the $H$-minor-free graphs have bounded pathwidth if and only if $H$ is a forest. In recent years, considerable effort has been devoted to understanding the unavoidable induced substructures of graphs with large pathwidth or large treewidth. In this paper, we give an induced counterpart of the Forest Minor theorem: for any $t \geqslant 2$, the $K_{t,t}$-subgraph-free $H$-induced-minor-free graphs have bounded pathwidth if and only if $H$ belongs to a class $\mathcal F$ of forests, which we describe as the induced minors of two (very similar) infinite parameterized families. This constitutes a significant step toward classifying the graphs $H$ for which every weakly sparse $H$-induced-minor-free class has bounded treewidth. Our work builds on the theory of constellations developed in the Induced Subgraphs and Tree Decompositions series.

---

## [2/10] Mitigating Gender Bias in Depression Detection via Counterfactual Inference
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T16:14:20.000Z
**URL:** https://arxiv.org/abs/2512.01834v1
**Reasoning:** The focus on gender bias in depression detection is not relevant to our primary interests in code intelligence and context engineering.
**Authors:** Mingxuan Hu

**Content/Abstract:**
> Audio-based depression detection models have demonstrated promising performance but often suffer from gender bias due to imbalanced training data. Epidemiological statistics show a higher prevalence of depression in females, leading models to learn spurious correlations between gender and depression. Consequently, models tend to over-diagnose female patients while underperforming on male patients, raising significant fairness concerns. To address this, we propose a novel Counterfactual Debiasing Framework grounded in causal inference. We construct a causal graph to model the decision-making process and identify gender bias as the direct causal effect of gender on the prediction. During inference, we employ counterfactual inference to estimate and subtract this direct effect, ensuring the model relies primarily on authentic acoustic pathological features. Extensive experiments on the DAIC-WOZ dataset using two advanced acoustic backbones demonstrate that our framework not only significantly reduces gender bias but also improves overall detection performance compared to existing debiasing strategies.

---

## [2/10] JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T15:35:53.000Z
**URL:** https://arxiv.org/abs/2512.01802v1
**Reasoning:** The article on optimizing Bellman-Ford is not directly related to our primary interests in code intelligence.
**Authors:** Xin Wang

**Content/Abstract:**
> We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential.

---

## [2/10] Julia Kasper – Rewetting peatlands is the biggest climate opportunity to cut CO2
**Source:** The Practical Developer | **Date:** 2025-12-02T14:01:38.000Z
**URL:** https://dev.to/ogcr/julia-kasper-rewetting-peatlands-is-the-biggest-climate-opportunity-to-cut-co2-4o5m
**Reasoning:** The article is about climate change and peatlands, which is irrelevant to our interests.
**Authors:** Open Geospatial Carbon Registry (OGCR)

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fq0xk594iubepaepkgkyu.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>In her recent interview, Julia Kasper, co-founder and CEO of <a href="https://www.zukunftmoor.de/en">Zukunftmoor</a>, argues that rewetting drained peatlands represents the single biggest climate opportunity in agriculture today. Although peatlands cover only about 3 % of the global land surface, they store more carbon than all the world’s forests combined. When peatlands are drained, a common practice for agriculture, they don’t just release CO₂ once; they leak carbon continuously, year after year. Restoring peatlands thus stops that “constant leak,” and rewetting them can turn a major source of emissions into a long-term carbon sink.</p>  
>   
> <p>But rewetting alone is not enough: farmers need viable, sustainable livelihoods. That’s where Zukunftmoor’s innovative approach comes in. They propose combining rewetting with cultivation of Sphagnum moss — a natural plant of peatlands — which can serve as a sustainable substitute for extracted peat in horticultural substrates. This turns rewetting from a purely ecological restoration act into a market-driven, economically viable land-use model. By using drones or hand-seeding methods, and developing harvesting and substrate-supply chains, the approach offers farmers a low-input, long-term pathway to maintain income while restoring degraded peatlands.</p>  
>   
> <p>With this combination of climate mitigation and practical agriculture, Kasper’s vision offers peatland regions across Europe a concrete alternative to drainage-based farming — one that aligns environmental restoration with economic viability.</p>  
>   
> <p>Original article published on <a href="https://investinginregenerativeagriculture.com/2025/11/11/julia-kasper/">Investing in Regenerative Agriculture</a>.</p>

---

## [2/10] Gary Tan claims Zoho will be out of business due to vibe coding
**Source:** Hacker News: Front Page | **Date:** 2025-12-02T12:54:56.000Z
**URL:** https://twitter.com/garrytan/status/1995664097007091818
**Reasoning:** The article is about a claim regarding Zoho, which is irrelevant to our interests.
**Authors:** manojlds

**Content/Abstract:**
> <p>Article URL: <a href="https://twitter.com/garrytan/status/1995664097007091818">https://twitter.com/garrytan/status/1995664097007091818</a></p> 
> <p>Comments URL: <a href="https://news.ycombinator.com/item?id=46120728">https://news.ycombinator.com/item?id=46120728</a></p> 
> <p>Points: 30</p> 
> <p># Comments: 22</p>

---

## [2/10] MasHeNe: A Benchmark for Head and Neck CT Mass Segmentation using Window-Enhanced Mamba with Frequency-Domain Integration
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T11:38:05.000Z
**URL:** https://arxiv.org/abs/2512.01563v1
**Reasoning:** The article is about medical imaging, which is not relevant to our focus areas.
**Authors:** Thao Thi Phuong Dao

**Content/Abstract:**
> Head and neck masses are space-occupying lesions that can compress the airway and esophagus and may affect nerves and blood vessels. Available public datasets primarily focus on malignant lesions and often overlook other space-occupying conditions in this region. To address this gap, we introduce MasHeNe, an initial dataset of 3,779 contrast-enhanced CT slices that includes both tumors and cysts with pixel-level annotations. We also establish a benchmark using standard segmentation baselines and report common metrics to enable fair comparison. In addition, we propose the Windowing-Enhanced Mamba with Frequency integration (WEMF) model. WEMF applies tri-window enhancement to enrich the input appearance before feature extraction. It further uses multi-frequency attention to fuse information across skip connections within a U-shaped Mamba backbone. On MasHeNe, WEMF attains the best performance among evaluated methods, with a Dice of 70.45%, IoU of 66.89%, NSD of 72.33%, and HD95 of 5.12 mm. This model indicates stable and strong results on this challenging task. MasHeNe provides a benchmark for head-and-neck mass segmentation beyond malignancy-only datasets. The observed error patterns also suggest that this task remains challenging and requires further research. Our dataset and code are available at https://github.com/drthaodao3101/MasHeNe.git.

---

## [2/10] Q2D2: A Geometry-Aware Audio Codec Leveraging Two-Dimensional Quantization
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T11:06:38.000Z
**URL:** https://arxiv.org/abs/2512.01537v1
**Reasoning:** The article is about audio codecs, which is not relevant to our interests.
**Authors:** Tal Shuster

**Content/Abstract:**
> Recent neural audio codecs have achieved impressive reconstruction quality, typically relying on quantization methods such as Residual Vector Quantization (RVQ), Vector Quantization (VQ) and Finite Scalar Quantization (FSQ). However, these quantization techniques limit the geometric structure of the latent space, make it harder to capture correlations between features leading to inefficiency in representation learning, codebook utilization and token rate. In this paper we introduce Two Dimensional Quantization (Q2D2), a quantization scheme in which feature pairs are projected onto structured 2D grids such as hexagonal, rhombic, or rectangular tiling and quantized to the nearest grid values, yielding an implicit codebook defined by the product of grid levels, with codebook sizes comparable to conventional methods. Despite its simple geometric formulation, Q2D2 improves audio compression efficiency, with low token rates and high codebook utilization while maintaining state of the art reconstruction quality. Specifically, Q2D2 achieves competitive to superior performance in various objective and subjective reconstruction metrics, across extensive experiments in speech domain compared to state of the art models. Comprehensive ablation studies further confirm the effectiveness of our design choices.

---

## [2/10] Deep Unsupervised Anomaly Detection in Brain Imaging: Large-Scale Benchmarking and Bias Analysis
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T11:03:27.000Z
**URL:** https://arxiv.org/abs/2512.01534v1
**Reasoning:** The article is about anomaly detection in brain imaging, which is not relevant to our focus areas.
**Authors:** Alexander Frotscher

**Content/Abstract:**
> Deep unsupervised anomaly detection in brain magnetic resonance imaging offers a promising route to identify pathological deviations without requiring lesion-specific annotations. Yet, fragmented evaluations, heterogeneous datasets, and inconsistent metrics have hindered progress toward clinical translation. Here, we present a large-scale, multi-center benchmark of deep unsupervised anomaly detection for brain imaging. The training cohort comprised 2,976 T1 and 2,972 T2-weighted scans from healthy individuals across six scanners, with ages ranging from 6 to 89 years. Validation used 92 scans to tune hyperparameters and estimate unbiased thresholds. Testing encompassed 2,221 T1w and 1,262 T2w scans spanning healthy datasets and diverse clinical cohorts. Across all algorithms, the Dice-based segmentation performance varied between 0.03 and 0.65, indicating substantial variability. To assess robustness, we systematically evaluated the impact of different scanners, lesion types and sizes, as well as demographics (age, sex). Reconstruction-based methods, particularly diffusion-inspired approaches, achieved the strongest lesion segmentation performance, while feature-based methods showed greater robustness under distributional shifts. However, systematic biases, such as scanner-related effects, were observed for the majority of algorithms, including that small and low-contrast lesions were missed more often, and that false positives varied with age and sex. Increasing healthy training data yields only modest gains, underscoring that current unsupervised anomaly detection frameworks are limited algorithmically rather than by data availability. Our benchmark establishes a transparent foundation for future research and highlights priorities for clinical translation, including image native pretraining, principled deviation measures, fairness-aware modeling, and robust domain adaptation.

---

## [2/10] Diffusion Fuzzy System: Fuzzy Rule Guided Latent Multi-Path Diffusion Modeling
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T11:01:06.000Z
**URL:** https://arxiv.org/abs/2512.01533v1
**Reasoning:** The article is about diffusion models for image generation, which is not relevant to our interests.
**Authors:** Hailong Yang

**Content/Abstract:**
> Diffusion models have emerged as a leading technique for generating images due to their ability to create high-resolution and realistic images. Despite their strong performance, diffusion models still struggle in managing image collections with significant feature differences. They often fail to capture complex features and produce conflicting results. Research has attempted to address this issue by learning different regions of an image through multiple diffusion paths and then combining them. However, this approach leads to inefficient coordination among multiple paths and high computational costs. To tackle these issues, this paper presents a Diffusion Fuzzy System (DFS), a latent-space multi-path diffusion model guided by fuzzy rules. DFS offers several advantages. First, unlike traditional multi-path diffusion methods, DFS uses multiple diffusion paths, each dedicated to learning a specific class of image features. By assigning each path to a different feature type, DFS overcomes the limitations of multi-path models in capturing heterogeneous image features. Second, DFS employs rule-chain-based reasoning to dynamically steer the diffusion process and enable efficient coordination among multiple paths. Finally, DFS introduces a fuzzy membership-based latent-space compression mechanism to reduce the computational costs of multi-path diffusion effectively. We tested our method on three public datasets: LSUN Bedroom, LSUN Church, and MS COCO. The results show that DFS achieves more stable training and faster convergence than existing single-path and multi-path diffusion models. Additionally, DFS surpasses baseline models in both image quality and alignment between text and images, and also shows improved accuracy when comparing generated images to target references.

---

## [2/10] Generating Random Hyperfractal Cities
**Source:** arXiv Query: search_query=(cat:cs.AI OR cat:cs.IR OR cat:cs.ML OR cat:cs.MA OR cat:cs.IT OR cat:cs.GL OR cat:cs.DS OR cat:cs.DL OR cat:cs.DB OR cat:cs.CL OR cat:cs.PL OR cat:cs.SE OR cat:cs.SY)&id_lis | **Date:** 2025-12-01T10:30:47.000Z
**URL:** https://arxiv.org/abs/2512.01505v1
**Reasoning:** The article is about modeling street networks, which is not relevant to our interests.
**Authors:** Geoffrey Deperle

**Content/Abstract:**
> This paper focuses on the challenge of interactively modeling street networks. In this work, we extend the simple fractal model, which is particularly useful for describing small cities or individual districts, by constructing random cities based on a tiling structure over which hyperfractals are distributed. This approach enables the connection of multiple hyperfractal districts, providing a more comprehensive urban representation. Furthermore, we demonstrate how this decomposition can be used to segment a city into distinct districts through fractal analysis. Finally, we present tools for the numerical generation of random cities following this model.

---

## [2/10] AI Cash Ignites a Boom for Multimillion-Dollar San Francisco Homes - Bloomberg.com
**Source:** "Anthropic" - Google News | **Date:** 2025-12-02T13:15:00.000Z
**URL:** https://news.google.com/rss/articles/CBMixAFBVV95cUxOQ1E4TXBpVVA4dUI0YUlpNmdubTE4SzFZeWVpemJjd09ERGVwMU56SE9sMWtJeGEyUlJzTW5qMXNNQnNIOTdRQm9DOXU5T2czZFJ5bFpRQk4wWE5LOXZZd1FBSFU2OW9Ud21fTlpvNFRlTTBha0ZaN29sRk5DU25FMTVSN0F0ZFh1enpGTzBZanpSQzFuZ0NoUjZpOUJaTmt6ZFlfbzNUM2hJVzl3RW1MSXVMVjBJOVRwWEthemxQYXVvVFhx?oc=5
**Reasoning:** The article is about real estate trends influenced by AI wealth, which is not relevant to our interests.

**Content/Abstract:**
> <a href="https://news.google.com/rss/articles/CBMixAFBVV95cUxOQ1E4TXBpVVA4dUI0YUlpNmdubTE4SzFZeWVpemJjd09ERGVwMU56SE9sMWtJeGEyUlJzTW5qMXNNQnNIOTdRQm9DOXU5T2czZFJ5bFpRQk4wWE5LOXZZd1FBSFU2OW9Ud21fTlpvNFRlTTBha0ZaN29sRk5DU25FMTVSN0F0ZFh1enpGTzBZanpSQzFuZ0NoUjZpOUJaTmt6ZFlfbzNUM2hJVzl3RW1MSXVMVjBJOVRwWEthemxQYXVvVFhx?oc=5">AI Cash Ignites a Boom for Multimillion-Dollar San Francisco Homes</a>  Bloomberg.com

---

## [2/10] HTML Invokers: The Coolest API You Aren’t Using Yet
**Source:** The Practical Developer | **Date:** 2025-12-02T13:29:44.000Z
**URL:** https://dev.to/web_dev-usman/html-invokers-the-coolest-api-you-arent-using-yet-35nl
**Reasoning:** The article is about an HTML API, which is not relevant to our primary or secondary interests.
**Authors:** Muhammad Usman

**Content/Abstract:**
> <p>Invokers are the coolest HTML API that you aren’t using. This is right about to drop in all major browsers. It’s available in Safari Technology Preview and Firefox and Chrome already. It is so cool.</p> 
>  
> <p>I want you to support my original content as well please, below is the link: <br> 
> <a href="https://pixicstudio.medium.com/html-invokers-the-coolest-api-you-arent-using-yet-e78c3ddee927">HTML Invokers: The Coolest API You Aren’t Using Yet</a></p> 
>  
> <p><strong>So, what are invokers?</strong></p> 
> <h2> 
>    
>    
>   The Big Idea 
> </h2> 
>  
> <p>Invokers let your buttons actually do stuff without JavaScript. Like, at all. You just tell the button what element to control and what to do with it. That’s the whole thing.</p> 
>  
> <p>It’s wild that this hasn’t existed until now.</p> 
>  
> <p><iframe allowfullscreen="allowfullscreen" height="600" src="https://codepen.io/web-strategist/embed/vEGrYbZ?height=600&amp;default-tab=result&amp;embed-version=2"> 
> </iframe> 
> </p> 
>  
> <h2> 
>    
>    
>   <strong>How We’ve Been Doing It (The Old Way)</strong> 
> </h2> 
>  
> <p>Right now, if you want a button to open a dialog, you’re writing code like this:<br> 
> </p> 
>  
> <div> 
> <pre><code>const button = document.querySelector('#open-button'); 
> const dialog = document.querySelector('#my-dialog'); 
> </code></pre> 
>  
> </div> 
>  
>  
>  
>  
>  
> <div> 
> <pre><code>button.addEventListener('click', () =&gt; { 
>   dialog.showModal(); 
> }); 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>It works, but come on. That’s way too much ceremony for “button opens thing.”</p> 
>  
> <h2> 
>    
>    
>   <strong>How Invokers Work (The New Way)</strong> 
> </h2> 
>  
> <p>You add two attributes to your button, and you’re done:<br> 
> </p> 
>  
> <div> 
> <pre><code>&lt;button commandfor="my-dialog" command="show-modal"&gt; 
>   Open Dialog 
> &lt;/button&gt; 
> </code></pre> 
>  
> </div> 
>  
>  
>  
>  
>  
> <div> 
> <pre><code>&lt;dialog id="my-dialog"&gt; 
>   &lt;h2&gt;Welcome!&lt;/h2&gt; 
>   &lt;p&gt;This dialog opened without a single line of JavaScript.&lt;/p&gt; 
>   &lt;button commandfor="my-dialog" command="close"&gt; 
>     Close This 
>   &lt;/button&gt; 
> &lt;/dialog&gt; 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>That’s it. The button just knows what to do. No JavaScript file. No event listeners, no querying the DOM. It’s beautiful.</p> 
>  
> <h2> 
>    
>    
>   <strong>A Real-World Example</strong> 
> </h2> 
>  
> <p>Here’s something you’d actually build — a settings menu with a popover:<br> 
> </p> 
>  
> <div> 
> <pre><code>&lt;button commandfor="settings-menu" command="toggle-popover"&gt; 
>   ⚙️ Settings 
> &lt;/button&gt; 
> </code></pre> 
>  
> </div> 
>  
>  
>  
>  
>  
> <div> 
> <pre><code>&lt;div id="settings-menu" popover&gt; 
>   &lt;h3&gt;Settings&lt;/h3&gt; 
>   &lt;label&gt; 
>     &lt;input type="checkbox"&gt; Dark Mode 
>   &lt;/label&gt; 
>   &lt;label&gt; 
>     &lt;input type="checkbox"&gt; Notifications 
>   &lt;/label&gt; 
>   &lt;label&gt; 
>     &lt;input type="checkbox"&gt; Auto-save 
>   &lt;/label&gt; 
>   &lt;button commandfor="settings-menu" command="hide-popover"&gt; 
>     Done 
>   &lt;/button&gt; 
> &lt;/div&gt; 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Click the settings button, the menu pops up. Click Done, it closes. Zero JavaScript required. This used to take 20+ lines of code.</p> 
>  
> <h2> 
>    
>    
>   <strong>The Attributes</strong> 
> </h2> 
>  
> <p>There are two attributes that make this magic happen:</p> 
>  
> <p>`<code> - This points to the ID of whatever you want to control. So if you want to control a dialog with </code>id="my-dialog"<code>, you write </code>commandfor="my-dialog"`.</p> 
>  
> <p>`<code> - This is what you want to happen. Like </code>show-modal<code> to open a dialog, or </code>close<code> to close it, or </code>toggle-popover` for popovers.</p> 
>  
> <p>That’s literally all you need to know.</p> 
>  
> <h2> 
>    
>    
>   <strong>What You Can Control</strong> 
> </h2> 
>  
> <p>Right now, you can control:</p> 
>  
> <p><strong>Dialogs: </strong><code>show-modal</code> and <code>close</code> work right out of the box. Finally.</p> 
>  
> <p><strong>Popovers: </strong><code>show-popover</code>, <code>hide-popover</code>, and <code>toggle-popover</code>. Super clean.</p> 
>  
> <p><strong>Details Elements: </strong><code>open</code>, <code>close</code>, and <code>toggle</code> for accordion-style content.</p> 
>  
> <p>And they’re adding more. File inputs, video controls, all that stuff is coming.</p> 
>  
> <h2> 
>    
>    
>   <strong>Custom Commands Are Cool Too</strong> 
> </h2> 
>  
> <p>You can make up your own commands. They just need to start with two dashes:<br> 
> </p> 
>  
> <div> 
> <pre><code>&lt;button commandfor="photo" command="--flip-horizontal"&gt; 
>   Flip Photo 
> &lt;/button&gt; 
> </code></pre> 
>  
> </div> 
>  
>  
>  
>  
>  
> <div> 
> <pre><code>&lt;button commandfor="photo" command="--rotate-90"&gt; 
>   Rotate 90° 
> &lt;/button&gt;&lt;img id="photo" src="vacation.jpg" alt="Beach photo"&gt; 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>Then you write a little JavaScript to listen for these custom commands and do whatever you want:<br> 
> </p> 
>  
> <div> 
> <pre><code>document.getElementById('photo').addEventListener('command', (e) =&gt; { 
>   if (e.command === '--flip-horizontal') { 
>     e.target.style.transform = 'scaleX(-1)'; 
>   } else if (e.command === '--rotate-90') { 
>     e.target.style.transform = 'rotate(90deg)'; 
>   } 
> }); 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>But the whole click-handling setup is already done for you, which is nice.</p> 
>  
> <h2> 
>    
>    
>   <strong>Another Solid Example: Image Gallery</strong> 
> </h2> 
>  
> <p>Here’s a complete image gallery with next/previous buttons:<br> 
> </p> 
>  
> <div> 
> <pre><code>&lt;div class="gallery"&gt; 
>   &lt;button commandfor="gallery-dialog" command="show-modal"&gt; 
>     📷 View Gallery 
>   &lt;/button&gt; 
> &lt;/div&gt; 
> </code></pre> 
>  
> </div> 
>  
>  
>  
>  
>  
> <div> 
> <pre><code>&lt;dialog id="gallery-dialog"&gt; 
>   &lt;div class="gallery-content"&gt; 
>     &lt;img id="current-image" src="photo-1.jpg" alt="Gallery photo"&gt; 
>  
>     &lt;div class="controls"&gt; 
>       &lt;button commandfor="current-image" command="--previous"&gt; 
>         ← Previous 
>       &lt;/button&gt; 
>       &lt;button commandfor="current-image" command="--next"&gt; 
>         Next → 
>       &lt;/button&gt; 
>       &lt;button commandfor="gallery-dialog" command="close"&gt; 
>         ✕ Close 
>       &lt;/button&gt; 
>     &lt;/div&gt; 
>   &lt;/div&gt; 
> &lt;/dialog&gt; 
> </code></pre> 
>  
> </div> 
>  
>  
>  
> <p>The dialog opens and closes with built-in commands. The next/previous buttons use custom commands that you’d wire up with a few lines of JavaScript. But all the button-clicking mechanics? Already handled.</p> 
>  
> <h2> 
>    
>    
>   <strong>Browser Support Right Now</strong> 
> </h2> 
>  
> <p>Here’s where things stand:</p> 
>  
> <ul> 
> <li> 
> <strong>Chrome Canary 134+</strong> Turn on the experimental flag</li> 
> <li> 
> <strong>Firefox Nightly 135+</strong> Enable the flag</li> 
> <li> 
> <strong>Safari Technology Preview </strong>Flag it on</li> 
> </ul> 
>  
> <p>It’s in the experimental phase, but it’s moving fast. This should be available everywhere by mid-2025.</p> 
>  
> <h2> 
>    
>    
>   <strong>Why This Is Actually a Big Deal</strong> 
> </h2> 
>  
> <p>Look, this isn’t changing the world or anything. But it’s fixing something that’s been annoying for years.</p> 
>  
> <p>Buttons controlling other elements is foundational web stuff. We shouldn’t need to import libraries or write boilerplate for it. HTML should just handle it. And now it does.</p> 
>  
> <p>Plus, when stuff is built into the platform, accessibility comes for free. Screen readers work better with it. Keyboard navigation just works. You don’t have to remember to add all the ARIA attributes.</p> 
>  
> <h2> 
>    
>    
>   <strong>Real Talk</strong> 
> </h2> 
>  
> <p>This is one of those features that makes you go “wait, this wasn’t already a thing?” Because it feels so obvious once you see it.</p> 
>  
> <p>The web platform is finally catching up to what we’ve been doing with JavaScript frameworks for years. And that’s great because it means less JavaScript to ship, less code to maintain, and websites that just work better.</p> 
>  
> <p>Invokers are simple, they’re elegant, and they’re about to be everywhere. Get ready to delete a bunch of event listeners.</p> 
>  
> <h2> 
>    
>    
>   <strong>The Bottom Line</strong> 
> </h2> 
>  
> <p>HTML Invokers let buttons control stuff without JavaScript. Two attributes. No libraries. It’s clean, it’s simple, and it’s coming to browsers near you.</p> 
>  
> <p>This is the kind of improvement that makes web development just a little bit more fun. And honestly? We could use more of that.</p> 
>  
> <blockquote> 
> <p><strong><em>Did you learn something good today as a developer?</em></strong><br> 
> <em>Then show some love.</em><br> 
> <em>© </em><a href="https://www.linkedin.com/m/in/muhammad-usman-strategist/"><em>Muhammad Usman</em></a><br> 
> <strong><em>WordPress Developer | Website Strategist | SEO Specialist</em></strong><br> 
> <em>Don’t forget to subscribe to <a href="https://developersjourney.substack.com/">Developer’s Journey</a> to show your support.</em></p> 
> </blockquote> 
>  
> <p><a href="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmdae1flujul96o4a41v1.png"><img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmdae1flujul96o4a41v1.png" alt="" width="800" height="266"></a></p>

---

## [2/10] Day 12: Python Programming
**Source:** The Practical Developer | **Date:** 2025-12-02T13:11:50.000Z
**URL:** https://dev.to/aruna_arun_0cda4eb425bb0f/day-12-python-programming-a3
**Reasoning:** Low-effort tutorial on Python collections, not relevant to our primary interests.
**Authors:** Aruna Arun

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwd6qybklyc95kgs9zm56.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p><strong>Advanced Python Collections (List, Tuple, Set, Dictionary)</strong></p>  
>   
> <p><strong>PART-1: LIST – Advanced Concepts</strong></p>  
>   
> <p><strong>1. List Comprehension</strong><br>  
> Short way to create lists.<br>  
> squares = [x*x for x in range(1, 6)]<br>  
> print(squares)<br>  
> <strong>With condition:</strong><br>  
> even = [x for x in range(10) if x % 2 == 0]</p>  
>   
> <p><strong>2. Nested List Comprehension</strong><br>  
> matrix = [[i*j for j in range(1, 4)] for i in range(1, 4)]</p>  
>   
> <p><strong>3.List Slicing</strong><br>  
> nums = [1,2,3,4,5,6]<br>  
> print(nums[1:5])<br>  
> print(nums[::-1])   # reverse</p>  
>   
> <p><strong>4.Important List Methods</strong><br>  
> append()<br>  
> extend()<br>  
> insert()<br>  
> remove()<br>  
> pop()<br>  
> clear()<br>  
> sort()<br>  
> reverse()<br>  
> count()<br>  
> index()<br>  
> <strong>Example</strong>:<br>  
> items = [5, 2, 9, 1]<br>  
> items.sort()      # ascending<br>  
> items.sort(reverse=True)   # descending</p>  
>   
> <p><strong>5.Cloning a List</strong><br>  
> a = [1, 2, 3]<br>  
> b = a[:]<br>  
> c = a.copy()</p>  
>   
> <p><strong>PART-2: TUPLE – Advanced Concepts</strong><br>  
> <strong>1.Tuple Unpacking</strong><br>  
> a, b, c = (10, 20, 30)</p>  
>   
> <p><strong>2.Tuple with * (Extended Unpacking)</strong><br>  
> a, *b = (1, 2, 3, 4, 5)</p>  
>   
> <p><strong>3.Why Tuple Is Faster?</strong></p>  
>   
> <ul>  
> <li>Immutable → stored in continuous memory</li>  
> <li>Faster iteration than list</li>  
> <li>Used for fixed data (lat/long, config)</li>  
> </ul>  
>   
> <p><strong>4.Convert List to Tuple</strong><br>  
> t = tuple([1, 2, 3])</p>  
>   
> <p><strong>PART-3: SET – Advanced Concepts</strong></p>  
>   
> <p><strong>1.Set Operations</strong><br>  
> A = {1, 2, 3}<br>  
> B = {3, 4, 5}</p>  
>   
> <p>print(A | B)   # union<br>  
> print(A &amp; B)   # intersection<br>  
> print(A - B)   # difference</p>  
>   
> <p><strong>✔2.Remove Duplicates from List Using Set</strong><br>  
> lst = [1,2,2,3,3,4]<br>  
> unique = list(set(lst))</p>  
>   
> <p><strong>3.Add &amp; Remove Elements</strong><br>  
> s = {1,2,3}<br>  
> s.add(4)<br>  
> s.discard(2)<br>  
> s.remove(3)   # error if 3 not exists</p>  
>   
> <p><strong>4.Set Comprehension</strong><br>  
> s = {x*x for x in range(5)}</p>  
>   
> <p><strong>PART-4: DICTIONARY – Advanced Concepts</strong></p>  
>   
> <p><strong>1.Dictionary Comprehension</strong><br>  
> squares = {x: x*x for x in range(5)}<br>  
> <strong>2.Looping Through Dictionary</strong><br>  
> d = {"a":1, "b":2, "c":3}<br>  
> for key in d:<br>  
>     print(key, d[key])<br>  
> for k, v in d.items():<br>  
>     print(k, v)</p>  
>   
> <p><strong>3.Merging Two Dictionaries</strong><br>  
> d1 = {"a": 1, "b": 2}<br>  
> d2 = {"c": 3}<br>  
> merged = {**d1, **d2}</p>  
>   
> <p><strong>4.Dictionary Methods</strong><br>  
> get()<br>  
> items()<br>  
> keys()<br>  
> values()<br>  
> pop()<br>  
> popitem()<br>  
> update()<br>  
> clear()</p>  
>   
> <p><strong>5.Using Dictionary as Counter</strong><br>  
> data = "aabbccc"<br>  
> count = {}<br>  
> for ch in data:<br>  
>     count[ch] = count.get(ch, 0) + 1<br>  
> print(count)</p>  
>   
> <p><strong>PART-5: Collections Module (Important for Interviews)</strong><br>  
> from collections import Counter, defaultdict, deque, OrderedDict</p>  
>   
> <p><strong>1.Counter</strong><br>  
> Counts frequency of elements.<br>  
> from collections import Counter<br>  
> print(Counter("aabbbcccc"))</p>  
>   
> <p><strong>2.defaultdict</strong><br>  
> Automatically creates default values if key missing.<br>  
> from collections import defaultdict<br>  
> d = defaultdict(int)<br>  
> d["a"] += 1<br>  
> d["b"] += 2<br>  
> print(d)</p>  
>   
> <p><strong>3.deque</strong><br>  
> Fast append/pop from both ends.<br>  
> from collections import deque<br>  
> q = deque([1,2,3])<br>  
> q.appendleft(0)<br>  
> q.append(4)<br>  
> print(q)</p>  
>   
> <p><strong>4.OrderedDict</strong><br>  
> Remembers insertion order<br>  
> (Python 3.7+ dict already preserves order)</p>  
>   
> <p><strong>PART-6: Interview Programs Using Collections</strong></p>  
>   
> <p><strong>1.Find the most repeated element</strong><br>  
> from collections import Counter<br>  
> print(Counter([1,2,2,3,3,3]).most_common(1))</p>  
>   
> <p><strong>2.Reverse a list without using reverse()</strong><br>  
> lst = [1,2,3]<br>  
> print(lst[::-1])</p>  
>   
> <p><strong>3.Merge two lists into dictionary</strong><br>  
> keys = ["name", "age"]<br>  
> values = ["Arun", 22]<br>  
> d = dict(zip(keys, values))</p>  
>   
> <p><strong>4.Remove duplicates while preserving order</strong><br>  
> result = []<br>  
> for x in [1,2,2,3,1]:<br>  
>     if x not in result:<br>  
>         result.append(x)</p>  
>   
> <p><strong>5.Convert dictionary to list of tuples</strong><br>  
> d = {"a":1, "b":2}<br>  
> print(list(d.items()))</p>

---

## [2/10] Comment on New research: How to make time for the work that matters by Hybrid Work Habits: Your Success Guide - Serenity7Wellness.com
**Source:** Comments for Atlassian Blog Work Life | **Date:** 2025-12-02T13:02:30.000Z
**URL:** https://www.atlassian.com/blog/distributed-work/calendar-redesign-experiment#comment-24917
**Reasoning:** Commentary on work habits, not relevant to code intelligence or AI in software engineering.
**Authors:** Hybrid Work Habits: Your Success Guide - Serenity7Wellness.com

**Content/Abstract:**
> <p><img src="https://www.atlassian.com/blog/wp-content/uploads/2023/12/blog_hero_img_calendar-experiment-how-to-find-time_b@2x-scaled.jpg" alt="blog_hero_img_calendar-experiment-how-to"></p><p>[…] Encourage open communication about mental health. […]</p>

---

## [2/10] A series of vignettes from my childhood and early career
**Source:** Hacker News: Front Page | **Date:** 2025-12-02T12:28:34.000Z
**URL:** https://www.jasonscheirer.com/weblog/vignettes/
**Reasoning:** Personal anecdotes, not relevant to our interests.
**Authors:** absqueued

**Content/Abstract:**
> <p>Article URL: <a href="https://www.jasonscheirer.com/weblog/vignettes/">https://www.jasonscheirer.com/weblog/vignettes/</a></p> 
> <p>Comments URL: <a href="https://news.ycombinator.com/item?id=46120549">https://news.ycombinator.com/item?id=46120549</a></p> 
> <p>Points: 39</p> 
> <p># Comments: 18</p>

---

## [2/10] Addressing the adding situation
**Source:** Hacker News: Front Page | **Date:** 2025-12-02T11:30:29.000Z
**URL:** https://xania.org/202512/02-adding-integers
**Reasoning:** The article seems to be a general discussion on adding integers, which is not relevant to our interests.
**Authors:** messe

**Content/Abstract:**
> <p>Article URL: <a href="https://xania.org/202512/02-adding-integers">https://xania.org/202512/02-adding-integers</a></p> 
> <p>Comments URL: <a href="https://news.ycombinator.com/item?id=46120181">https://news.ycombinator.com/item?id=46120181</a></p> 
> <p>Points: 108</p> 
> <p># Comments: 25</p>

---

## [2/10] &#128712; Mastering CSS Tooltips: A Quick & Practical Guide
**Source:** The Practical Developer | **Date:** 2025-12-02T11:17:37.000Z
**URL:** https://dev.to/arsalanmeee/mastering-css-tooltips-a-quick-practical-guide-378e
**Reasoning:** The article is a tutorial on CSS tooltips, which is not relevant to our interests.
**Authors:** Arsalan Mlaik

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjpp17f8i7tt8l0smz4xn.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p><a href="https://makemychance.com/css-tooltip/">Published on Makemychance.com</a><br>  
> Tooltips are small UI elements that display helpful text when users hover or tap on an element. They improve UX without adding clutter — and with just a few lines of CSS, you can create clean, modern tooltips for any website.</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   🔍 What Is a Tooltip?  
> </h2>  
>   
> <p>A tooltip is a small popup text that appears on <strong>hover</strong>, <strong>focus</strong>, or <strong>tap</strong>.<br>  
> Great for icons, buttons, forms, and feature explanations.</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   🧩 The Easiest Tooltip (HTML Only)  
> </h2>  
>   
>   
>   
> <div>  
> <pre><code><span>&lt;button</span> <span>title=</span><span>"Click to submit"</span><span>&gt;</span>Submit<span>&lt;/button&gt;</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>✔ Super quick<br>  
> ✘ Can't customize the style</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   🎨 Custom CSS Tooltip (Modern UI)  
> </h2>  
>   
> <h3>  
>     
>     
>   HTML  
> </h3>  
>   
>   
>   
> <div>  
> <pre><code><span>&lt;div</span> <span>class=</span><span>"tooltip"</span><span>&gt;</span>  
>   Hover me  
>   <span>&lt;span</span> <span>class=</span><span>"tooltip-text"</span><span>&gt;</span>This is a tooltip<span>&lt;/span&gt;</span>  
> <span>&lt;/div&gt;</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <h3>  
>     
>     
>   CSS  
> </h3>  
>   
>   
>   
> <div>  
> <pre><code><span>.tooltip</span> <span>{</span>  
>   <span>position</span><span>:</span> <span>relative</span><span>;</span>  
>   <span>cursor</span><span>:</span> <span>pointer</span><span>;</span>  
> <span>}</span>  
> <span>.tooltip</span> <span>.tooltip-text</span> <span>{</span>  
>   <span>visibility</span><span>:</span> <span>hidden</span><span>;</span>  
>   <span>background</span><span>:</span> <span>#333</span><span>;</span>  
>   <span>color</span><span>:</span> <span>#fff</span><span>;</span>  
>   <span>padding</span><span>:</span> <span>6px</span> <span>10px</span><span>;</span>  
>   <span>border-radius</span><span>:</span> <span>4px</span><span>;</span>  
>   <span>position</span><span>:</span> <span>absolute</span><span>;</span>  
>   <span>bottom</span><span>:</span> <span>130%</span><span>;</span>  
>   <span>left</span><span>:</span> <span>50%</span><span>;</span>  
>   <span>transform</span><span>:</span> <span>translateX</span><span>(</span><span>-50%</span><span>);</span>  
>   <span>opacity</span><span>:</span> <span>0</span><span>;</span>  
>   <span>transition</span><span>:</span> <span>0.3s</span> <span>ease</span><span>;</span>  
>   <span>white-space</span><span>:</span> <span>nowrap</span><span>;</span>  
> <span>}</span>  
> <span>.tooltip</span><span>:hover</span> <span>.tooltip-text</span> <span>{</span>  
>   <span>visibility</span><span>:</span> <span>visible</span><span>;</span>  
>   <span>opacity</span><span>:</span> <span>1</span><span>;</span>  
> <span>}</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
>   
>   
>   
> <h2>  
>     
>     
>   📌 Tooltip Positions  
> </h2>  
>   
> <ul>  
> <li>  
> <strong>Top</strong> (default)</li>  
> <li><strong>Bottom</strong></li>  
> <li><strong>Left</strong></li>  
> <li><strong>Right</strong></li>  
> </ul>  
>   
> <p>Just adjust <code>top</code>, <code>left</code>, <code>bottom</code>, or <code>transform</code> values.</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   🛈 Tooltip with Arrow  
> </h2>  
>   
>   
>   
> <div>  
> <pre><code><span>.tooltip</span> <span>.tooltip-text</span><span>::after</span> <span>{</span>  
>   <span>content</span><span>:</span> <span>""</span><span>;</span>  
>   <span>position</span><span>:</span> <span>absolute</span><span>;</span>  
>   <span>top</span><span>:</span> <span>100%</span><span>;</span>  
>   <span>left</span><span>:</span> <span>50%</span><span>;</span>  
>   <span>border</span><span>:</span> <span>5px</span> <span>solid</span> <span>transparent</span><span>;</span>  
>   <span>border-top-color</span><span>:</span> <span>#333</span><span>;</span>  
>   <span>transform</span><span>:</span> <span>translateX</span><span>(</span><span>-50%</span><span>);</span>  
> <span>}</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
>   
>   
>   
> <h2>  
>     
>     
>   📱 Tooltip on Mobile?  
> </h2>  
>   
> <p>Since there's no hover on mobile:<br>  
> ✔ Show tooltip on <strong>click</strong><br>  
> ✔ Use an <strong>info icon</strong> <code>(i)</code><br>  
> ✔ Add a <strong>small JS toggle</strong> if needed</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   🛑 Common Mistakes  
> </h2>  
>   
> <p>❌ Too much text<br>  
> ❌ Tooltip covering the element<br>  
> ❌ Bad contrast<br>  
> ❌ No mobile alternative</p>  
>   
> <p>Keep it simple and readable.</p>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   🔗 Helpful References  
> </h2>  
>   
> <ul>  
> <li>MDN Tooltip Basics  
> <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/title">https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/title</a>  
> </li>  
> <li>CSS-Tricks Tooltip Guide  
> <a href="https://css-tricks.com/css-tooltips/">https://css-tricks.com/css-tooltips/</a>  
> </li>  
> </ul>  
>   
>   
>   
>   
> <h2>  
>     
>     
>   🚀 Final Note  
> </h2>  
>   
> <p>Tooltips are tiny but powerful UX boosters. With simple CSS, you can create clean, smooth, and accessible tooltips that fit perfectly into any modern UI.</p>

---

## [2/10] How To Lead | Ben Horowitz on My First Million
**Source:** a16z Podcast | **Date:** 2025-12-02T11:00:00.000Z
**URL:** https://a16z.simplecast.com/episodes/how-to-lead-ben-horowitz-on-my-first-million-hZ9jj4__
**Reasoning:** This is a leadership podcast episode, not relevant to our primary or secondary interests.
**Authors:** content+a16zpodcast@a16z.com (Shaan Puri, Sam Parr, Ben Horowitz)

**Content/Abstract:**
> <p>A16Z co-founder Ben Horowitz joins Shaan Puri and Sam Parr on My First Million to talk about how to be a great leader.</p><p> </p><p>Resources:</p><p>Follow Ben on X: https://x.com/bhorowitz</p><p>Follow Shaan on X: https://x.com/ShaanVP</p><p>Follow Sam on X: https://x.com/thesamparr</p><p> </p><p>Stay Updated:</p><p>If you enjoyed this episode, be sure to like, subscribe, and share with your friends!</p><p>Find a16z on X: <a href="https://x.com/a16z">https://x.com/a16z</a></p><p>Find a16z on LinkedIn: https://www.linkedin.com/company/a16z</p><p>Listen to the a16z Podcast on Spotify: https://open.spotify.com/show/5bC65RDvs3oxnLyqqvkUYX</p><p>Listen to the a16z Podcast on Apple Podcasts: https://podcasts.apple.com/us/podcast/a16z-podcast/id842818711</p><p>Follow our host: https://x.com/eriktorenberg</p><p>Please note that the content here is for informational purposes only; should NOT be taken as legal, business, tax, or investment advice or be used to evaluate any investment or security; and is not directed at any investors or potential investors in any a16z fund. a16z and its affiliates may maintain investments in the companies discussed. For more details please see http://a16z.com/disclosures.</p> 
> <p></p><p><strong>Stay Updated:</strong></p><p>Find a16z on <a href="https://x.com/a16z">X</a></p><p>Find a16z on<a href="https://www.linkedin.com/company/a16z"> LinkedIn</a></p><p>Listen to the a16z Podcast on <a href="https://open.spotify.com/show/5bC65RDvs3oxnLyqqvkUYX?si=3E8B3qT9TyiwAHJ7JnaKbg">Spotify</a></p><p>Listen to the a16z Podcast on <a href="https://podcasts.apple.com/us/podcast/a16z-podcast/id842818711">Apple Podcasts</a></p><p>Follow our host: https://twitter.com/eriktorenberg</p><p> </p><p>Please note that the content here is for informational purposes only; should NOT be taken as legal, business, tax, or investment advice or be used to evaluate any investment or security; and is not directed at any investors or potential investors in any a16z fund. a16z and its affiliates may maintain investments in the companies discussed. For more details please see a16z.com/disclosures.</p><br> <p>Hosted by Simplecast, an AdsWizz company. See <a href="https://pcm.adswizz.com">pcm.adswizz.com</a> for information about our collection and use of personal data for advertising.</p>

---

## [2/10] Copy and Paste on Proxmox VM
**Source:** The Practical Developer | **Date:** 2025-12-02T10:52:48.000Z
**URL:** https://dev.to/woovi/copy-and-paste-on-proxmox-vm-15il
**Reasoning:** The article is a low-effort tutorial about copy-paste functionality in Proxmox, not relevant to our interests.
**Authors:** Sibelius Seraphini

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frncah6jqm7ku0jwkba62.jpg" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>Copy and paste does not work well on Proxmox KVM on the UI.<br>  
> You can try to connect using a serial port to enable copy and paste over the UI.</p>  
> <h2>  
>     
>     
>   Using a Script  
> </h2>  
>   
> <p>We created a script to simulate keyboard typing and paste data over KVM when using the UI.</p>  
>   
> <p>Open your Chrome Dev Tools, and paste this.<br>  
> </p>  
>   
> <div>  
> <pre><code><span>function</span> <span>simulateKeyEvent</span><span>(</span><span>el</span><span>,</span> <span>eventType</span><span>,</span> <span>key</span><span>,</span> <span>options</span> <span>=</span> <span>{})</span> <span>{</span>  
>   <span>const</span> <span>evt</span> <span>=</span> <span>new</span> <span>KeyboardEvent</span><span>(</span><span>eventType</span><span>,</span> <span>{</span> <span>key</span><span>,</span> <span>...</span><span>options</span> <span>});</span>  
>   <span>el</span><span>.</span><span>dispatchEvent</span><span>(</span><span>evt</span><span>);</span>  
> <span>}</span>  
>   
> <span>const</span> <span>sendKey</span> <span>=</span> <span>(</span><span>char</span><span>)</span> <span>=&gt;</span> <span>{</span>  
>   <span>let</span> <span>capsLockOn</span> <span>=</span> <span>false</span><span>;</span>  
>   <span>const</span> <span>SHIFT_NEEDED</span> <span>=</span> <span>/</span><span>[</span><span>A-Z!@#$%^&amp;*()_+{}:"&lt;&gt;?~|</span><span>]</span><span>/</span><span>;</span>  
>   
>   <span>const</span> <span>canvas</span> <span>=</span> <span>document</span><span>.</span><span>querySelector</span><span>(</span><span>"</span><span>canvas</span><span>"</span><span>);</span>  
>   
>   <span>canvas</span><span>.</span><span>focus</span><span>();</span>  
>   
>   <span>if </span><span>(</span><span>char</span> <span>===</span> <span>'</span><span>\n</span><span>'</span><span>)</span> <span>{</span>  
>     <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>"</span><span>keydown</span><span>"</span><span>,</span> <span>"</span><span>Enter</span><span>"</span><span>);</span>  
>     <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>"</span><span>keyup</span><span>"</span><span>,</span> <span>"</span><span>Enter</span><span>"</span><span>);</span>  
>   <span>}</span> <span>else</span> <span>{</span>  
>     <span>const</span> <span>needsShift</span> <span>=</span> <span>SHIFT_NEEDED</span><span>.</span><span>test</span><span>(</span><span>char</span><span>);</span>  
>     <span>const</span> <span>isUpperCase</span> <span>=</span> <span>char</span> <span>&gt;=</span> <span>'</span><span>A</span><span>'</span> <span>&amp;&amp;</span> <span>char</span> <span>&lt;=</span> <span>'</span><span>Z</span><span>'</span><span>;</span>  
>   
>     <span>if </span><span>(</span><span>needsShift</span><span>)</span> <span>{</span>  
>       <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>"</span><span>keydown</span><span>"</span><span>,</span> <span>"</span><span>Shift</span><span>"</span><span>,</span> <span>{</span> <span>keyCode</span><span>:</span> <span>16</span> <span>});</span>  
>     <span>}</span>  
>   
>     <span>if </span><span>(</span><span>isUpperCase</span> <span>&amp;&amp;</span> <span>capsLockOn</span><span>)</span> <span>{</span>  
>       <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>"</span><span>keydown</span><span>"</span><span>,</span> <span>char</span><span>.</span><span>toLowerCase</span><span>());</span>  
>       <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>"</span><span>keyup</span><span>"</span><span>,</span> <span>char</span><span>.</span><span>toLowerCase</span><span>());</span>  
>     <span>}</span> <span>else</span> <span>{</span>  
>       <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>"</span><span>keydown</span><span>"</span><span>,</span> <span>char</span><span>);</span>  
>       <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>"</span><span>keyup</span><span>"</span><span>,</span> <span>char</span><span>);</span>  
>     <span>}</span>  
>   
>     <span>if </span><span>(</span><span>needsShift</span><span>)</span> <span>{</span>  
>       <span>simulateKeyEvent</span><span>(</span><span>canvas</span><span>,</span> <span>"</span><span>keyup</span><span>"</span><span>,</span> <span>"</span><span>Shift</span><span>"</span><span>,</span> <span>{</span> <span>keyCode</span><span>:</span> <span>16</span> <span>});</span>  
>     <span>}</span>  
>   
>     <span>if </span><span>(</span><span>char</span> <span>===</span> <span>"</span><span>CapsLock</span><span>"</span><span>)</span> <span>{</span>  
>       <span>capsLockOn</span> <span>=</span> <span>!</span><span>capsLockOn</span><span>;</span>  
>       <span>console</span><span>.</span><span>log</span><span>(</span><span>"</span><span>Caps Lock state changed:</span><span>"</span><span>,</span> <span>capsLockOn</span><span>);</span>  
>     <span>}</span>  
>   <span>}</span>  
> <span>};</span>  
>   
> <span>function</span> <span>cp</span><span>(</span><span>text</span><span>)</span> <span>{</span>  
>   <span>let</span> <span>index</span> <span>=</span> <span>0</span><span>;</span>  
>   
>   <span>function</span> <span>typeChar</span><span>()</span> <span>{</span>  
>     <span>if </span><span>(</span><span>index</span> <span>&lt;</span> <span>text</span><span>.</span><span>length</span><span>)</span> <span>{</span>  
>       <span>sendKey</span><span>(</span><span>text</span><span>[</span><span>index</span><span>]);</span>  
>       <span>index</span><span>++</span><span>;</span>  
>       <span>setTimeout</span><span>(</span><span>typeChar</span><span>,</span> <span>100</span><span>);</span> <span>// Adjust delay as needed</span>  
>     <span>}</span>  
>   <span>}</span>  
>   
>   <span>typeChar</span><span>();</span>  
> <span>}</span>  
> </code></pre>  
>   
> </div>  
>   
>   
>   
> <p>To paste something, you first need to focus on the UI, and then type <code>cp('mydata')</code></p>  
>   
> <p>You can use this when you don't have access to the VM over SSH.</p>  
>   
> <h2>  
>     
>     
>   In Summary  
> </h2>  
>   
> <p>If you are creative enough, you can solve these limitations.</p>  
>   
>   
>   
>   
> <p>Woovi<br>  
> <a href="https://www.woovi.com">Woovi</a> is a fintech platform revolutionizing how businesses and developers handle payments in Brazil. Built with a developer-first mindset, Woovi simplifies integration with instant payment methods like Pix, enabling companies to receive payments seamlessly and automate financial workflows.</p>  
>   
> <p>If you want to work with us, we are <a href="https://woovi.com/jobs/">hiring</a>!</p>

---

## [2/10] Designing with Web Components: Custom Elements & Shadow DOM in HTML
**Source:** The Practical Developer | **Date:** 2025-12-02T10:43:32.000Z
**URL:** https://dev.to/djamware_tutorial_eba1a61/designing-with-web-components-custom-elements-shadow-dom-in-html-1j45
**Reasoning:** The article is a tutorial on web components, not relevant to our interests.
**Authors:** Djamware Tutorial

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fae7cxvwsbnuyrdgalpdn.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><p>I’ve published a new tutorial on Djamware.com that dives deep into building native, framework-agnostic Web Components using modern browser standards.</p>  
>   
> <p>In this guide, you’ll learn:</p>  
>   
> <ul>  
> <li>What Web Components are and why they matter</li>  
> <li>How Custom Elements and Shadow DOM work</li>  
> <li>How to build components using templates and slots</li>  
> <li>How to pass data with attributes and properties</li>  
> <li>Best practices for performance, accessibility, and component APIs</li>  
> <li>How to build a real  component with encapsulated styles</li>  
> </ul>  
>   
> <p>👉 Read the full tutorial:<br>  
> <a href="https://www.djamware.com/post/692e739ecfbd910fde2b32a1/designing-with-web-components-custom-elements-shadow-dom-in-html">https://www.djamware.com/post/692e739ecfbd910fde2b32a1/designing-with-web-components-custom-elements-shadow-dom-in-html</a></p>  
>   
> <p>If you're building reusability into your UI or creating cross-framework components, this tutorial is for you.</p>  
>   
> <p>Let me know what components you’d like me to build next!</p>

---

## [2/10] Unleash Dynamic Content: Mastering the Elementor Flip Box Widget for Engaging WordPress Sites
**Source:** The Practical Developer | **Date:** 2025-12-02T10:39:40.000Z
**URL:** https://dev.to/artarasaneh2025/unleash-dynamic-content-mastering-the-elementor-flip-box-widget-for-engaging-wordpress-sites-3akh
**Reasoning:** The article is a tutorial about a WordPress widget, not relevant to our interests.
**Authors:** Artarasaneh

**Content/Abstract:**
> <p><img src="https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fci0z41ue036ivddgbifa.png" alt="https%3A%2F%2Fdev-to-uploads.s3.amazonaw"></p><h2>Unleash Dynamic Content: Mastering the Elementor Flip Box Widget for Engaging WordPress Sites</h2>  
>   
> <p>In today's competitive digital landscape, capturing and retaining user attention is paramount for any website owner. Merely presenting static information often falls short of creating a memorable user experience. This is where dynamic, interactive elements come into play, transforming passive browsing into an engaging journey. For WordPress users leveraging the power of Elementor, a popular page builder, there's a straightforward yet incredibly effective tool to achieve this: the Flip Box widget. Designed to add a layer of interactivity and visual flair, the Elementor Flip Box allows you to display information in a novel way that encourages exploration and makes your content truly pop.</p>  
>   
> <h3>What is the Elementor Flip Box Widget?</h3>  
>   
> <p>The Elementor Flip Box widget is an innovative design element that presents content on two distinct sides – a 'front' and a 'back' – within a single container. Initially, users see the front side, which typically contains a concise piece of information, an icon, or a brief heading. Upon a specific interaction, usually a hover action or a click, the box animates and 'flips' to reveal the hidden content on its back side. This secondary content often provides more detailed information, a call to action, or supplementary media.</p>  
>   
> <p>As a native feature within Elementor, it's easily accessible for any WordPress user, eliminating the need for complex custom coding to achieve sophisticated interactive effects. Leveraging the Flip Box allows you to condense information into visually appealing, bite-sized chunks, enhancing both aesthetics and functional efficiency. It exemplifies how Elementor empowers users to create professional, dynamic layouts with intuitive drag-and-drop functionality, significantly improving the user experience on any WordPress-powered platform.</p>  
>   
> <h3>Elevate Your Design: Key Benefits of Using Elementor Flip Boxes</h3>  
>   
> <p>Integrating the Flip Box widget into your Elementor-built WordPress site offers a multitude of advantages, significantly boosting your site's appeal and effectiveness:</p>  
>   
> <ul>  
>   <li>  
> <strong>Enhanced User Engagement:</strong> Its interactive nature naturally draws users in, prompting deeper interaction and content discovery, leading to longer session durations and memorable experiences.</li>  
>   <li>  
> <strong>Optimal Space Efficiency:</strong> Flip Boxes effectively present a summary on the front and detailed information on the back, saving valuable screen real estate while maintaining a clean, professional layout.</li>  
>   <li>  
> <strong>Increased Visual Appeal:</strong> Offering various flip directions (horizontal, vertical, 3D cube) and customizable animations, Flip Boxes add a modern, dynamic touch, making your site visually distinct and engaging.</li>  
>   <li>  
> <strong>Effective Call-to-Actions (CTAs):</strong> The back side is ideal for embedding compelling CTAs or contact information, guiding engaged users seamlessly towards their next step.</li>  
>   <li>  
> <strong>Versatile Application:</strong> Incredibly versatile, the Elementor Flip Box widget seamlessly adapts to showcase team bios, product features, services, or FAQs, fitting various content types and design needs across your WordPress website.</li>  
> </ul>  
>   
> <h3>Mastering the Flip Box: Setup &amp; Customization Tips for WordPress Users</h3>  
>   
> <p>Implementing and customizing an Elementor Flip Box on your WordPress site is a straightforward process, thanks to Elementor's intuitive interface. Here’s a basic guide to get you started and some tips for mastery:</p>  
>   
> <ol>  
>   <li>  
> <strong>Adding the Widget:</strong> Open your page or post in the Elementor editor. Search for "Flip Box" in the widgets panel and drag it onto your canvas.</li>  
>   <li>  
> <strong>Content Configuration:</strong> In the 'Content' tab, you'll find options for both the 'Front' and 'Back' sides. For the front, you can add an icon, title, description, and even a button. Repeat this process for the back side, often including a more detailed description or a primary call-to-action. You can also choose your desired 'Flip Effect' (e.g., Slide, Push, Zoom, 3D), 'Flip Direction', and whether it flips on 'Hover' or 'Click'.</li>  
>   <li>  
> <strong>Styling for Impact:</strong> The 'Style' tab is where you bring your Flip Box to life. Customize the background (color or image), padding, border, and typography for both the front and back. Adjust the icon and button styles to match your brand's aesthetics. Experiment with different animations and timings to find the perfect balance between subtlety and impact.</li>  
>   <li>  
> <strong>Responsiveness:</strong> Always check how your Flip Box appears on different devices (desktop, tablet, mobile) using Elementor's responsive mode. Adjust settings as needed to ensure optimal display and functionality across all screen sizes.</li>  
> </ol>  
>   
> <p>For a deeper dive into all the practical steps, advanced settings, and creative ideas for leveraging this powerful Elementor tool to its fullest potential on your WordPress site, consider exploring comprehensive guides available online. These resources often provide detailed walkthroughs and examples that can significantly enhance your design capabilities. For instance, to dive deeper into the practical steps and creative ideas for leveraging this powerful tool, explore comprehensive guides like this one on the <a href="https://artarasaneh.com/flip-box-widget-elementor">Elementor Flip Box widget</a>.</p>  
>   
> <h3>Conclusion: Transform Your WordPress Site with Elementor Flip Boxes</h3>  
>   
> <p>The Elementor Flip Box widget is more than just a visually appealing element; it's a strategic tool for enhancing user engagement, optimizing content display, and adding a professional polish to any WordPress website. By effectively utilizing its dual-sided design and customizable animations, you can captivate your audience, convey information efficiently, and guide them seamlessly through your site. Whether showcasing services, introducing team members, or highlighting product features, the Flip Box offers a dynamic and interactive solution that elevates the overall user experience. Embrace this powerful Elementor feature to transform your static pages into interactive masterpieces, making your WordPress site truly stand out.</p>

---

## [2/10] OpenAI and NORAD team up to bring new magic to “NORAD Tracks Santa”
**Source:** OpenAI News | **Date:** 2025-12-01T06:00:00.000Z
**URL:** https://openai.com/index/norad-holiday-collaboration
**Reasoning:** This is a consumer-focused collaboration involving ChatGPT, not relevant to code intelligence or context engineering.

**Content/Abstract:**
> OpenAI and NORAD are bringing new magic to “NORAD Tracks Santa” with three ChatGPT holiday tools that let families create festive elves, toy coloring pages, and custom Christmas stories.

---

## [2/10] [Subscribers only] Dev Writers Retreat 2025: WRITING FOR HUMANS — 10 Fellowship spots left!
**Source:** Latent Space | **Date:** 2025-11-28T03:21:27.000Z
**URL:** https://www.latent.space/p/dwr2025
**Reasoning:** This is a writing fellowship announcement, not relevant to code intelligence or context engineering.
**Authors:** Shawn swyx Wang

**Content/Abstract:**
> A unique most-expenses-paid Writing Fellowship to take stock of 2025, work on your non-fiction writing skills, and meet fellow subscribers in sunny San Diego!

---

## [0/10] A Fast Earth-scattering Formalism for Light Dark Matter with Dark Photon Mediators
**Source:** Academic Paper | **Date:** 2025-11-01T00:00:00.000Z
**URL:** https://ui.adsabs.harvard.edu/abs/2025arXiv251110589L/abstract
**Reasoning:** The paper is about dark matter detection, which is irrelevant to our interests in code intelligence or context engineering.
**Authors:** Lantero-Barreda, Agustín, Centeno, Carlos, Kavanagh, Bradley J., Castelló-Mor, Nuria

**Content/Abstract:**
> While Dark Matter (DM) is typically assumed to interact only very weakly with the particles of the Standard Model, many direct detection experiments are currently exploring regions of parameter space where DM can have a large scattering cross section. In this scenario, DM may scatter in the atmosphere and Earth before reaching the detector, leading to a distortion of the DM flux and a daily modulation of the signal rate as the detector is shielded by more or less of the Earth at different times of day. This modulation is a distinctive signature of strongly-interacting DM and provides a powerful method of discriminating against time-independent backgrounds. However, the calculation of these Earth-scattering effects by Monte Carlo methods is computationally intensive, inhibiting a systematic exploration of the DM parameter space. Here, we present a semi-analytic formalism for calculating Earth-scattering effects, for models of MeV-mass DM which interacts via a dark photon mediator, and release the associated code Verne2. This formalism assumes that DM travels along straight-line trajectories until it scatters and is reflected back along its incoming path, along us to taking into account the affects of both attenuation and reflection in the Earth. We compare this formalism with the results of full Monte Carlo simulations for cross sections within reach of current and future DM-electron scattering searches. We find that Verne2 is accurate to better than 10-30%, making it suitable for performing signal modeling in the search for daily modulation, while reducing the computational cost by a factor of $\sim10^4$ compared to full Monte Carlo simulations.

---

## [0/10] First time since 1988, the U.S. is not officially commemorating World AIDS Day
**Source:** Hacker News: Front Page | **Date:** 2025-12-02T14:08:01.000Z
**URL:** https://www.npr.org/sections/goats-and-soda/2025/12/01/g-s1-99925/world-aids-day-trump
**Reasoning:** The article is about World AIDS Day, which is irrelevant to our interests.
**Authors:** stopbulying

**Content/Abstract:**
> <p><img src="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2632x1481+0+0/resize/1400/quality/100/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fb4%2Fa1%2Fc20a006d46a288f8be90d77f2233%2Fworld-aids-day-diptych.jpg" alt="?url=http%3A%2F%2Fnpr-brightspot.s3.amaz"></p><p>Article URL: <a href="https://www.npr.org/sections/goats-and-soda/2025/12/01/g-s1-99925/world-aids-day-trump">https://www.npr.org/sections/goats-and-soda/2025/12/01/g-s1-99925/world-aids-day-trump</a></p>  
> <p>Comments URL: <a href="https://news.ycombinator.com/item?id=46121300">https://news.ycombinator.com/item?id=46121300</a></p>  
> <p>Points: 10</p>  
> <p># Comments: 0</p>

---

